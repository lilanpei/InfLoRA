logs/cifar100/10_10_sip/InfLoRA/adam/10/0.95_1.0-0.0005/42
2025-12-10 01:55:33,476 [trainer.py] => config: configs/cifar100_inflora_seed42.json
2025-12-10 01:55:33,477 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 01:55:33,477 [trainer.py] => prefix: reproduce
2025-12-10 01:55:33,477 [trainer.py] => dataset: cifar100
2025-12-10 01:55:33,477 [trainer.py] => data_path: data/
2025-12-10 01:55:33,477 [trainer.py] => memory_size: 0
2025-12-10 01:55:33,477 [trainer.py] => memory_per_class: 0
2025-12-10 01:55:33,477 [trainer.py] => fixed_memory: True
2025-12-10 01:55:33,477 [trainer.py] => shuffle: False
2025-12-10 01:55:33,477 [trainer.py] => init_cls: 10
2025-12-10 01:55:33,477 [trainer.py] => increment: 10
2025-12-10 01:55:33,477 [trainer.py] => model_name: InfLoRA
2025-12-10 01:55:33,477 [trainer.py] => net_type: sip
2025-12-10 01:55:33,477 [trainer.py] => embd_dim: 768
2025-12-10 01:55:33,477 [trainer.py] => num_heads: 12
2025-12-10 01:55:33,477 [trainer.py] => total_sessions: 10
2025-12-10 01:55:33,477 [trainer.py] => seed: 42
2025-12-10 01:55:33,477 [trainer.py] => EPSILON: 1e-08
2025-12-10 01:55:33,477 [trainer.py] => init_epoch: 20
2025-12-10 01:55:33,477 [trainer.py] => optim: adam
2025-12-10 01:55:33,477 [trainer.py] => init_lr: 0.0005
2025-12-10 01:55:33,477 [trainer.py] => init_lr_decay: 0.1
2025-12-10 01:55:33,478 [trainer.py] => init_weight_decay: 0.0
2025-12-10 01:55:33,478 [trainer.py] => epochs: 20
2025-12-10 01:55:33,478 [trainer.py] => lrate: 0.0005
2025-12-10 01:55:33,478 [trainer.py] => lrate_decay: 0.1
2025-12-10 01:55:33,478 [trainer.py] => batch_size: 128
2025-12-10 01:55:33,478 [trainer.py] => weight_decay: 0.0
2025-12-10 01:55:33,478 [trainer.py] => rank: 10
2025-12-10 01:55:33,478 [trainer.py] => lamb: 0.95
2025-12-10 01:55:33,478 [trainer.py] => lame: 1.0
2025-12-10 01:55:33,478 [trainer.py] => num_workers: 8
2025-12-10 01:55:35,368 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 482, unexpected 0
2025-12-10 01:55:37,262 [trainer.py] => All params: 111194651
2025-12-10 01:55:37,264 [trainer.py] => Trainable params: 111194651
2025-12-10 01:55:37,264 [inflora.py] => Learning on 0-10
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.3.attn.lora_B_v.0.weight'}
2025-12-10 02:05:10,714 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.089, Train_accy 96.92
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 13/768 type remove
Layer 6 : 14/768 type remove
Layer 7 : 13/768 type remove
Layer 8 : 19/768 type remove
Layer 9 : 20/768 type remove
Layer 10 : 17/768 type remove
Layer 11 : 6/768 type remove
Layer 12 : 10/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:05:41,561 [trainer.py] => Time:604.2975964546204
1000 1000
1000 1000
2025-12-10 02:05:44,839 [trainer.py] => Time:3.2770752906799316
2025-12-10 02:05:44,839 [inflora.py] => Exemplar size: 0
2025-12-10 02:05:44,839 [trainer.py] => CNN: {'total': np.float64(99.4), '00-09': np.float64(99.4), 'old': 0, 'new': np.float64(99.4)}
2025-12-10 02:05:44,839 [trainer.py] => CNN top1 curve: [np.float64(99.4)]
2025-12-10 02:05:44,839 [trainer.py] => CNN top1 with task curve: [np.float64(99.4)]
2025-12-10 02:05:44,839 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 02:05:45,386 [trainer.py] => All params: 111194651
2025-12-10 02:05:45,387 [trainer.py] => Trainable params: 192010
2025-12-10 02:05:45,388 [inflora.py] => Learning on 10-20
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'classifier_pool.1.bias'}
2025-12-10 02:15:17,339 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.136, Train_accy 95.54
Threshold:  0.955
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 18/768 type remove
Layer 6 : 22/768 type remove
Layer 7 : 21/768 type remove
Layer 8 : 29/768 type remove
Layer 9 : 39/768 type remove
Layer 10 : 39/768 type remove
Layer 11 : 13/768 type remove
Layer 12 : 22/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:15:50,035 [trainer.py] => Time:604.6471767425537
2000 2000
2000 2000
2025-12-10 02:15:55,880 [trainer.py] => Time:5.845041513442993
2025-12-10 02:15:55,880 [inflora.py] => Exemplar size: 0
2025-12-10 02:15:55,880 [trainer.py] => CNN: {'total': np.float64(96.3), '00-09': np.float64(98.9), '10-19': np.float64(93.7), 'old': np.float64(98.9), 'new': np.float64(93.7)}
2025-12-10 02:15:55,880 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3)]
2025-12-10 02:15:55,881 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3)]
2025-12-10 02:15:55,881 [trainer.py] => CNN top1 task curve: [1.0, 0.967]
2025-12-10 02:15:56,384 [trainer.py] => All params: 111194651
2025-12-10 02:15:56,386 [trainer.py] => Trainable params: 192010
2025-12-10 02:15:56,386 [inflora.py] => Learning on 20-30
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight'}
2025-12-10 02:25:29,851 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.123, Train_accy 95.98
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 15/768 type remove
Layer 5 : 21/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 29/768 type remove
Layer 8 : 40/768 type remove
Layer 9 : 55/768 type remove
Layer 10 : 52/768 type remove
Layer 11 : 19/768 type remove
Layer 12 : 32/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:26:02,721 [trainer.py] => Time:606.3347685337067
3000 3000
3000 3000
2025-12-10 02:26:11,128 [trainer.py] => Time:8.406779766082764
2025-12-10 02:26:11,128 [inflora.py] => Exemplar size: 0
2025-12-10 02:26:11,128 [trainer.py] => CNN: {'total': np.float64(94.57), '00-09': np.float64(97.3), '10-19': np.float64(93.8), '20-29': np.float64(92.6), 'old': np.float64(95.55), 'new': np.float64(92.6)}
2025-12-10 02:26:11,128 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57)]
2025-12-10 02:26:11,128 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73)]
2025-12-10 02:26:11,128 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334]
2025-12-10 02:26:12,922 [trainer.py] => All params: 111194651
2025-12-10 02:26:12,924 [trainer.py] => Trainable params: 192010
2025-12-10 02:26:12,924 [inflora.py] => Learning on 30-40
Parameters to be updated: {'classifier_pool.3.bias', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight'}
2025-12-10 02:35:47,195 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.118, Train_accy 96.32
Threshold:  0.965
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 25/768 type remove
Layer 6 : 33/768 type remove
Layer 7 : 38/768 type remove
Layer 8 : 54/768 type remove
Layer 9 : 81/768 type remove
Layer 10 : 83/768 type remove
Layer 11 : 32/768 type remove
Layer 12 : 42/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:36:20,272 [trainer.py] => Time:607.347177028656
4000 4000
4000 4000
2025-12-10 02:36:31,211 [trainer.py] => Time:10.938941717147827
2025-12-10 02:36:31,211 [inflora.py] => Exemplar size: 0
2025-12-10 02:36:31,211 [trainer.py] => CNN: {'total': np.float64(92.88), '00-09': np.float64(97.3), '10-19': np.float64(89.5), '20-29': np.float64(93.6), '30-39': np.float64(91.1), 'old': np.float64(93.47), 'new': np.float64(91.1)}
2025-12-10 02:36:31,211 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88)]
2025-12-10 02:36:31,211 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75)]
2025-12-10 02:36:31,211 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335]
2025-12-10 02:36:31,932 [trainer.py] => All params: 111194651
2025-12-10 02:36:31,934 [trainer.py] => Trainable params: 192010
2025-12-10 02:36:31,934 [inflora.py] => Learning on 40-50
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight'}
2025-12-10 02:46:06,628 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.119, Train_accy 95.98
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 30/768 type remove
Layer 6 : 38/768 type remove
Layer 7 : 43/768 type remove
Layer 8 : 62/768 type remove
Layer 9 : 96/768 type remove
Layer 10 : 102/768 type remove
Layer 11 : 43/768 type remove
Layer 12 : 55/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:46:39,542 [trainer.py] => Time:607.6085336208344
5000 5000
5000 5000
2025-12-10 02:46:53,076 [trainer.py] => Time:13.533109188079834
2025-12-10 02:46:53,076 [inflora.py] => Exemplar size: 0
2025-12-10 02:46:53,076 [trainer.py] => CNN: {'total': np.float64(91.58), '00-09': np.float64(96.1), '10-19': np.float64(87.0), '20-29': np.float64(92.1), '30-39': np.float64(90.1), '40-49': np.float64(92.6), 'old': np.float64(91.32), 'new': np.float64(92.6)}
2025-12-10 02:46:53,076 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58)]
2025-12-10 02:46:53,076 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8)]
2025-12-10 02:46:53,076 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206]
2025-12-10 02:46:53,650 [trainer.py] => All params: 111194651
2025-12-10 02:46:53,652 [trainer.py] => Trainable params: 192010
2025-12-10 02:46:53,652 [inflora.py] => Learning on 50-60
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.2.attn.lora_B_v.5.weight'}
2025-12-10 02:56:28,333 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.131, Train_accy 95.18
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 36/768 type remove
Layer 6 : 47/768 type remove
Layer 7 : 54/768 type remove
Layer 8 : 84/768 type remove
Layer 9 : 124/768 type remove
Layer 10 : 124/768 type remove
Layer 11 : 55/768 type remove
Layer 12 : 68/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:57:01,584 [trainer.py] => Time:607.9317843914032
6000 6000
6000 6000
2025-12-10 02:57:17,667 [trainer.py] => Time:16.082905054092407
2025-12-10 02:57:17,667 [inflora.py] => Exemplar size: 0
2025-12-10 02:57:17,667 [trainer.py] => CNN: {'total': np.float64(89.1), '00-09': np.float64(94.9), '10-19': np.float64(85.7), '20-29': np.float64(90.1), '30-39': np.float64(90.1), '40-49': np.float64(91.8), '50-59': np.float64(82.0), 'old': np.float64(90.52), 'new': np.float64(82.0)}
2025-12-10 02:57:17,668 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58), np.float64(89.1)]
2025-12-10 02:57:17,668 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8), np.float64(98.58)]
2025-12-10 02:57:17,668 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206, 0.8955]
2025-12-10 02:57:18,614 [trainer.py] => All params: 111194651
2025-12-10 02:57:18,616 [trainer.py] => Trainable params: 192010
2025-12-10 02:57:18,616 [inflora.py] => Learning on 60-70
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight'}
2025-12-10 03:06:53,032 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.115, Train_accy 95.74
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 41/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 68/768 type remove
Layer 8 : 104/768 type remove
Layer 9 : 150/768 type remove
Layer 10 : 148/768 type remove
Layer 11 : 69/768 type remove
Layer 12 : 86/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:07:26,181 [trainer.py] => Time:607.56489610672
7000 7000
7000 7000
2025-12-10 03:07:44,840 [trainer.py] => Time:18.65874481201172
2025-12-10 03:07:44,840 [inflora.py] => Exemplar size: 0
2025-12-10 03:07:44,840 [trainer.py] => CNN: {'total': np.float64(88.1), '00-09': np.float64(94.5), '10-19': np.float64(84.9), '20-29': np.float64(88.2), '30-39': np.float64(89.8), '40-49': np.float64(90.8), '50-59': np.float64(80.5), '60-69': np.float64(88.0), 'old': np.float64(88.12), 'new': np.float64(88.0)}
2025-12-10 03:07:44,840 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58), np.float64(89.1), np.float64(88.1)]
2025-12-10 03:07:44,840 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8), np.float64(98.58), np.float64(98.59)]
2025-12-10 03:07:44,840 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206, 0.8955, 0.8851428571428571]
2025-12-10 03:07:45,390 [trainer.py] => All params: 111194651
2025-12-10 03:07:45,392 [trainer.py] => Trainable params: 192010
2025-12-10 03:07:45,392 [inflora.py] => Learning on 70-80
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight'}
2025-12-10 03:17:20,437 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.116, Train_accy 95.86
Threshold:  0.985
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 25/768 type remove
Layer 4 : 34/768 type remove
Layer 5 : 51/768 type remove
Layer 6 : 73/768 type remove
Layer 7 : 87/768 type remove
Layer 8 : 141/768 type remove
Layer 9 : 194/768 type remove
Layer 10 : 193/768 type remove
Layer 11 : 90/768 type remove
Layer 12 : 104/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:17:52,836 [trainer.py] => Time:607.4440121650696
8000 8000
8000 8000
2025-12-10 03:18:14,094 [trainer.py] => Time:21.257420301437378
2025-12-10 03:18:14,094 [inflora.py] => Exemplar size: 0
2025-12-10 03:18:14,095 [trainer.py] => CNN: {'total': np.float64(87.86), '00-09': np.float64(94.1), '10-19': np.float64(84.9), '20-29': np.float64(87.1), '30-39': np.float64(89.1), '40-49': np.float64(91.1), '50-59': np.float64(80.8), '60-69': np.float64(86.8), '70-79': np.float64(89.0), 'old': np.float64(87.7), 'new': np.float64(89.0)}
2025-12-10 03:18:14,095 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58), np.float64(89.1), np.float64(88.1), np.float64(87.86)]
2025-12-10 03:18:14,095 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8), np.float64(98.58), np.float64(98.59), np.float64(98.52)]
2025-12-10 03:18:14,095 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206, 0.8955, 0.8851428571428571, 0.8825]
2025-12-10 03:18:14,622 [trainer.py] => All params: 111194651
2025-12-10 03:18:14,624 [trainer.py] => Trainable params: 192010
2025-12-10 03:18:14,624 [inflora.py] => Learning on 80-90
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight'}
2025-12-10 03:27:48,833 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.123, Train_accy 96.48
Threshold:  0.99
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 33/768 type remove
Layer 4 : 44/768 type remove
Layer 5 : 67/768 type remove
Layer 6 : 100/768 type remove
Layer 7 : 121/768 type remove
Layer 8 : 187/768 type remove
Layer 9 : 264/768 type remove
Layer 10 : 291/768 type remove
Layer 11 : 178/768 type remove
Layer 12 : 181/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:28:21,054 [trainer.py] => Time:606.4301211833954
9000 9000
9000 9000
2025-12-10 03:28:44,845 [trainer.py] => Time:23.791013717651367
2025-12-10 03:28:44,845 [inflora.py] => Exemplar size: 0
2025-12-10 03:28:44,846 [trainer.py] => CNN: {'total': np.float64(88.34), '00-09': np.float64(94.4), '10-19': np.float64(85.1), '20-29': np.float64(87.0), '30-39': np.float64(87.3), '40-49': np.float64(92.0), '50-59': np.float64(82.3), '60-69': np.float64(86.1), '70-79': np.float64(88.1), '80-89': np.float64(92.8), 'old': np.float64(87.79), 'new': np.float64(92.8)}
2025-12-10 03:28:44,846 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58), np.float64(89.1), np.float64(88.1), np.float64(87.86), np.float64(88.34)]
2025-12-10 03:28:44,846 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8), np.float64(98.58), np.float64(98.59), np.float64(98.52), np.float64(98.62)]
2025-12-10 03:28:44,846 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206, 0.8955, 0.8851428571428571, 0.8825, 0.887]
2025-12-10 03:28:46,065 [trainer.py] => All params: 111194651
2025-12-10 03:28:46,067 [trainer.py] => Trainable params: 192010
2025-12-10 03:28:46,067 [inflora.py] => Learning on 90-100
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight'}
2025-12-10 03:38:20,083 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.091, Train_accy 97.10
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 64/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 147/768 type remove
Layer 7 : 188/768 type remove
Layer 8 : 275/768 type remove
Layer 9 : 371/768 type remove
Layer 10 : 354/768 type retain
Layer 11 : 302/768 type remove
Layer 12 : 318/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:38:52,642 [trainer.py] => Time:606.5748128890991
10000 10000
10000 10000
2025-12-10 03:39:19,093 [trainer.py] => Time:26.450162649154663
2025-12-10 03:39:19,093 [inflora.py] => Exemplar size: 0
2025-12-10 03:39:19,093 [trainer.py] => CNN: {'total': np.float64(86.74), '00-09': np.float64(94.2), '10-19': np.float64(83.6), '20-29': np.float64(85.9), '30-39': np.float64(85.5), '40-49': np.float64(91.3), '50-59': np.float64(79.9), '60-69': np.float64(85.3), '70-79': np.float64(85.4), '80-89': np.float64(92.0), '90-99': np.float64(84.3), 'old': np.float64(87.01), 'new': np.float64(84.3)}
2025-12-10 03:39:19,093 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.3), np.float64(94.57), np.float64(92.88), np.float64(91.58), np.float64(89.1), np.float64(88.1), np.float64(87.86), np.float64(88.34), np.float64(86.74)]
2025-12-10 03:39:19,093 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.3), np.float64(98.73), np.float64(98.75), np.float64(98.8), np.float64(98.58), np.float64(98.59), np.float64(98.52), np.float64(98.62), np.float64(98.66)]
2025-12-10 03:39:19,093 [trainer.py] => CNN top1 task curve: [1.0, 0.967, 0.9533333333333334, 0.9335, 0.9206, 0.8955, 0.8851428571428571, 0.8825, 0.887, 0.8705]
