logs/ImageNet_R/20_20_sip/InfLoRA/adam/10/0.98_1.0-0.0005/42
2025-12-11 17:05:43,489 [trainer.py] => config: configs/mimg10_inflora_seed42.json
2025-12-11 17:05:43,490 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-11 17:05:43,490 [trainer.py] => prefix: reproduce
2025-12-11 17:05:43,490 [trainer.py] => dataset: ImageNet_R
2025-12-11 17:05:43,490 [trainer.py] => data_path: data/imagenet-r
2025-12-11 17:05:43,490 [trainer.py] => memory_size: 0
2025-12-11 17:05:43,490 [trainer.py] => memory_per_class: 0
2025-12-11 17:05:43,490 [trainer.py] => fixed_memory: True
2025-12-11 17:05:43,490 [trainer.py] => shuffle: False
2025-12-11 17:05:43,490 [trainer.py] => init_cls: 20
2025-12-11 17:05:43,490 [trainer.py] => increment: 20
2025-12-11 17:05:43,490 [trainer.py] => model_name: InfLoRA
2025-12-11 17:05:43,490 [trainer.py] => net_type: sip
2025-12-11 17:05:43,490 [trainer.py] => embd_dim: 768
2025-12-11 17:05:43,490 [trainer.py] => num_heads: 12
2025-12-11 17:05:43,490 [trainer.py] => total_sessions: 10
2025-12-11 17:05:43,491 [trainer.py] => seed: 42
2025-12-11 17:05:43,491 [trainer.py] => EPSILON: 1e-08
2025-12-11 17:05:43,491 [trainer.py] => init_epoch: 50
2025-12-11 17:05:43,491 [trainer.py] => optim: adam
2025-12-11 17:05:43,491 [trainer.py] => init_lr: 0.0005
2025-12-11 17:05:43,491 [trainer.py] => init_lr_decay: 0.1
2025-12-11 17:05:43,491 [trainer.py] => init_weight_decay: 0.0
2025-12-11 17:05:43,491 [trainer.py] => epochs: 50
2025-12-11 17:05:43,491 [trainer.py] => lrate: 0.0005
2025-12-11 17:05:43,491 [trainer.py] => lrate_decay: 0.1
2025-12-11 17:05:43,491 [trainer.py] => batch_size: 128
2025-12-11 17:05:43,491 [trainer.py] => weight_decay: 0.0
2025-12-11 17:05:43,491 [trainer.py] => rank: 10
2025-12-11 17:05:43,491 [trainer.py] => lamb: 0.98
2025-12-11 17:05:43,491 [trainer.py] => lame: 1.0
2025-12-11 17:05:43,491 [trainer.py] => num_workers: 8
2025-12-11 17:05:43,491 [trainer.py] => use_wncm: True
2025-12-11 17:05:43,491 [trainer.py] => wncm_lambda: 0.07
2025-12-11 17:05:43,491 [trainer.py] => save_checkpoints: False
2025-12-11 17:05:43,763 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 482, unexpected 0
2025-12-11 17:05:45,512 [whitened_ncm_head.py] => WhitenedNCM: Using CPU
2025-12-11 17:05:45,514 [trainer.py] => All params: 111348451
2025-12-11 17:05:45,516 [trainer.py] => Trainable params: 111348451
2025-12-11 17:05:45,516 [inflora.py] => Learning on 0-20
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'classifier_pool.0.bias', 'classifier_pool.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight'}
2025-12-11 17:18:21,938 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.125, Train_accy 96.40
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 27/768 type remove
Layer 6 : 26/768 type remove
Layer 7 : 29/768 type remove
Layer 8 : 33/768 type remove
Layer 9 : 60/768 type remove
Layer 10 : 71/768 type remove
Layer 11 : 29/768 type remove
Layer 12 : 72/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:18:39,756 [trainer.py] => Time:774.2401716709137
650 650
650 650
2025-12-11 17:18:42,468 [trainer.py] => Time:2.71100115776062
2025-12-11 17:18:42,468 [inflora.py] => Exemplar size: 0
2025-12-11 17:18:42,469 [trainer.py] => CNN: {'total': np.float64(88.77), '00-19': np.float64(88.77), 'old': 0, 'new': np.float64(88.77)}
2025-12-11 17:18:42,469 [trainer.py] => CNN top1 curve: [np.float64(88.77)]
2025-12-11 17:18:42,469 [trainer.py] => CNN top1 with task curve: [np.float64(88.77)]
2025-12-11 17:18:42,469 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-11 17:18:52,622 [trainer.py] => W-NCM: {'00-19': 88.3076923076923}
2025-12-11 17:18:52,623 [trainer.py] => Ave Acc (W-NCM): 88.31%
2025-12-11 17:18:52,623 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 88.31% (best 88.31%)
2025-12-11 17:18:52,623 [trainer.py] => Average forgetting (W-NCM): 0.00% | Max forgetting (W-NCM): 0.00%
2025-12-11 17:18:52,625 [trainer.py] => All params: 111348451
2025-12-11 17:18:52,627 [trainer.py] => Trainable params: 199700
2025-12-11 17:18:52,627 [inflora.py] => Learning on 20-40
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight'}
2025-12-11 17:32:45,504 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.167, Train_accy 95.62
Threshold:  0.982
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 36/768 type remove
Layer 6 : 34/768 type remove
Layer 7 : 40/768 type remove
Layer 8 : 45/768 type remove
Layer 9 : 80/768 type remove
Layer 10 : 96/768 type remove
Layer 11 : 45/768 type remove
Layer 12 : 133/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:33:06,901 [trainer.py] => Time:854.2737307548523
1400 1400
1400 1400
2025-12-11 17:33:11,452 [trainer.py] => Time:4.55136775970459
2025-12-11 17:33:11,453 [inflora.py] => Exemplar size: 0
2025-12-11 17:33:11,453 [trainer.py] => CNN: {'total': np.float64(86.71), '00-19': np.float64(85.85), '20-39': np.float64(87.47), 'old': np.float64(85.85), 'new': np.float64(87.47)}
2025-12-11 17:33:11,453 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71)]
2025-12-11 17:33:11,453 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21)]
2025-12-11 17:33:11,453 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857]
2025-12-11 17:33:24,232 [trainer.py] => W-NCM: {'00-19': 82.15384615384616, '20-39': 88.53333333333333}
2025-12-11 17:33:24,233 [trainer.py] => Ave Acc (W-NCM): 85.34%
2025-12-11 17:33:24,233 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 82.15% (best 88.31%); T2: W-NCM 88.53% (best 88.53%)
2025-12-11 17:33:24,233 [trainer.py] => Average forgetting (W-NCM): 6.15% | Max forgetting (W-NCM): 6.15%
2025-12-11 17:33:24,235 [trainer.py] => All params: 111348451
2025-12-11 17:33:24,236 [trainer.py] => Trainable params: 199700
2025-12-11 17:33:24,236 [inflora.py] => Learning on 40-60
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight'}
2025-12-11 17:42:15,375 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.204, Train_accy 93.82
Threshold:  0.984
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 31/768 type remove
Layer 5 : 43/768 type remove
Layer 6 : 41/768 type remove
Layer 7 : 51/768 type remove
Layer 8 : 58/768 type remove
Layer 9 : 100/768 type remove
Layer 10 : 113/768 type remove
Layer 11 : 65/768 type remove
Layer 12 : 147/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:42:30,220 [trainer.py] => Time:545.9832582473755
1827 1827
1827 1827
2025-12-11 17:42:35,848 [trainer.py] => Time:5.628460168838501
2025-12-11 17:42:35,849 [inflora.py] => Exemplar size: 0
2025-12-11 17:42:35,849 [trainer.py] => CNN: {'total': np.float64(82.81), '00-19': np.float64(84.0), '20-39': np.float64(84.13), '40-59': np.float64(78.69), 'old': np.float64(84.07), 'new': np.float64(78.69)}
2025-12-11 17:42:35,849 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81)]
2025-12-11 17:42:35,849 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05)]
2025-12-11 17:42:35,849 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416]
2025-12-11 17:42:47,002 [trainer.py] => W-NCM: {'00-19': 81.53846153846153, '20-39': 84.53333333333333, '40-59': 83.13817330210773}
2025-12-11 17:42:47,003 [trainer.py] => Ave Acc (W-NCM): 83.07%
2025-12-11 17:42:47,003 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 81.54% (best 88.31%); T2: W-NCM 84.53% (best 88.53%); T3: W-NCM 83.14% (best 83.14%)
2025-12-11 17:42:47,003 [trainer.py] => Average forgetting (W-NCM): 5.38% | Max forgetting (W-NCM): 6.77%
2025-12-11 17:42:47,005 [trainer.py] => All params: 111348451
2025-12-11 17:42:47,007 [trainer.py] => Trainable params: 199700
2025-12-11 17:42:47,007 [inflora.py] => Learning on 60-80
Parameters to be updated: {'classifier_pool.3.bias', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight'}
2025-12-11 17:54:24,997 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.175, Train_accy 94.99
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 31/768 type remove
Layer 4 : 40/768 type remove
Layer 5 : 54/768 type remove
Layer 6 : 52/768 type remove
Layer 7 : 64/768 type remove
Layer 8 : 74/768 type remove
Layer 9 : 130/768 type remove
Layer 10 : 144/768 type remove
Layer 11 : 87/768 type remove
Layer 12 : 172/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:54:42,889 [trainer.py] => Time:715.8823325634003
2403 2403
2403 2403
2025-12-11 17:54:49,997 [trainer.py] => Time:7.1074371337890625
2025-12-11 17:54:49,997 [inflora.py] => Exemplar size: 0
2025-12-11 17:54:49,997 [trainer.py] => CNN: {'total': np.float64(80.4), '00-19': np.float64(83.23), '20-39': np.float64(80.8), '40-59': np.float64(76.11), '60-79': np.float64(79.86), 'old': np.float64(80.57), 'new': np.float64(79.86)}
2025-12-11 17:54:49,997 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4)]
2025-12-11 17:54:49,997 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39)]
2025-12-11 17:54:49,998 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537]
2025-12-11 17:55:04,096 [trainer.py] => W-NCM: {'00-19': 80.15384615384616, '20-39': 81.86666666666666, '40-59': 78.68852459016394, '60-79': 84.375}
2025-12-11 17:55:04,096 [trainer.py] => Ave Acc (W-NCM): 81.27%
2025-12-11 17:55:04,096 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 80.15% (best 88.31%); T2: W-NCM 81.87% (best 88.53%); T3: W-NCM 78.69% (best 83.14%); T4: W-NCM 84.38% (best 84.38%)
2025-12-11 17:55:04,096 [trainer.py] => Average forgetting (W-NCM): 6.42% | Max forgetting (W-NCM): 8.15%
2025-12-11 17:55:04,098 [trainer.py] => All params: 111348451
2025-12-11 17:55:04,100 [trainer.py] => Trainable params: 199700
2025-12-11 17:55:04,100 [inflora.py] => Learning on 80-100
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight'}
2025-12-11 18:08:44,491 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.196, Train_accy 93.94
Threshold:  0.988
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 38/768 type remove
Layer 4 : 49/768 type remove
Layer 5 : 68/768 type remove
Layer 6 : 68/768 type remove
Layer 7 : 86/768 type remove
Layer 8 : 101/768 type remove
Layer 9 : 170/768 type remove
Layer 10 : 202/768 type remove
Layer 11 : 126/768 type remove
Layer 12 : 266/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:09:05,146 [trainer.py] => Time:841.0458006858826
3119 3119
3119 3119
2025-12-11 18:09:14,103 [trainer.py] => Time:8.957060813903809
2025-12-11 18:09:14,104 [inflora.py] => Exemplar size: 0
2025-12-11 18:09:14,104 [trainer.py] => CNN: {'total': np.float64(79.93), '00-19': np.float64(83.85), '20-39': np.float64(81.2), '40-59': np.float64(72.6), '60-79': np.float64(78.65), '80-99': np.float64(80.45), 'old': np.float64(79.78), 'new': np.float64(80.45)}
2025-12-11 18:09:14,104 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93)]
2025-12-11 18:09:14,104 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01)]
2025-12-11 18:09:14,104 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098]
2025-12-11 18:09:31,156 [trainer.py] => W-NCM: {'00-19': 80.15384615384616, '20-39': 80.26666666666667, '40-59': 76.81498829039812, '60-79': 77.77777777777779, '80-99': 84.63687150837988}
2025-12-11 18:09:31,156 [trainer.py] => Ave Acc (W-NCM): 79.93%
2025-12-11 18:09:31,156 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 80.15% (best 88.31%); T2: W-NCM 80.27% (best 88.53%); T3: W-NCM 76.81% (best 83.14%); T4: W-NCM 77.78% (best 84.38%); T5: W-NCM 84.64% (best 84.64%)
2025-12-11 18:09:31,156 [trainer.py] => Average forgetting (W-NCM): 7.34% | Max forgetting (W-NCM): 8.27%
2025-12-11 18:09:31,158 [trainer.py] => All params: 111348451
2025-12-11 18:09:31,160 [trainer.py] => Trainable params: 199700
2025-12-11 18:09:31,160 [inflora.py] => Learning on 100-120
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight'}
2025-12-11 18:20:29,084 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.207, Train_accy 94.08
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 79/768 type remove
Layer 6 : 83/768 type remove
Layer 7 : 103/768 type remove
Layer 8 : 125/768 type remove
Layer 9 : 212/768 type remove
Layer 10 : 264/768 type remove
Layer 11 : 180/768 type remove
Layer 12 : 361/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:20:46,365 [trainer.py] => Time:675.205320596695
3677 3677
3677 3677
2025-12-11 18:20:56,769 [trainer.py] => Time:10.40325665473938
2025-12-11 18:20:56,769 [inflora.py] => Exemplar size: 0
2025-12-11 18:20:56,769 [trainer.py] => CNN: {'total': np.float64(78.6), '00-19': np.float64(80.46), '20-39': np.float64(81.2), '40-59': np.float64(68.62), '60-79': np.float64(76.56), '80-99': np.float64(79.33), '100-119': np.float64(81.72), 'old': np.float64(78.04), 'new': np.float64(81.72)}
2025-12-11 18:20:56,769 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93), np.float64(78.6)]
2025-12-11 18:20:56,769 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01), np.float64(88.2)]
2025-12-11 18:20:56,770 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098, 0.8354636932281752]
2025-12-11 18:21:13,774 [trainer.py] => W-NCM: {'00-19': 78.15384615384615, '20-39': 79.46666666666667, '40-59': 79.62529274004683, '60-79': 75.0, '80-99': 79.88826815642457, '100-119': 85.84229390681004}
2025-12-11 18:21:13,775 [trainer.py] => Ave Acc (W-NCM): 79.66%
2025-12-11 18:21:13,775 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 78.15% (best 88.31%); T2: W-NCM 79.47% (best 88.53%); T3: W-NCM 79.63% (best 83.14%); T4: W-NCM 75.00% (best 84.38%); T5: W-NCM 79.89% (best 84.64%); T6: W-NCM 85.84% (best 85.84%)
2025-12-11 18:21:13,775 [trainer.py] => Average forgetting (W-NCM): 7.37% | Max forgetting (W-NCM): 10.15%
2025-12-11 18:21:13,777 [trainer.py] => All params: 111348451
2025-12-11 18:21:13,778 [trainer.py] => Trainable params: 199700
2025-12-11 18:21:13,779 [inflora.py] => Learning on 120-140
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight'}
2025-12-11 18:32:32,685 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.243, Train_accy 92.65
Threshold:  0.992
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 67/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 104/768 type remove
Layer 7 : 132/768 type remove
Layer 8 : 159/768 type remove
Layer 9 : 263/768 type remove
Layer 10 : 337/768 type remove
Layer 11 : 254/768 type remove
Layer 12 : 296/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:32:50,914 [trainer.py] => Time:697.1353847980499
4234 4234
4234 4234
2025-12-11 18:33:02,756 [trainer.py] => Time:11.842125415802002
2025-12-11 18:33:02,757 [inflora.py] => Exemplar size: 0
2025-12-11 18:33:02,757 [trainer.py] => CNN: {'total': np.float64(76.57), '00-19': np.float64(79.38), '20-39': np.float64(78.67), '40-59': np.float64(70.73), '60-79': np.float64(74.48), '80-99': np.float64(76.26), '100-119': np.float64(78.32), '120-139': np.float64(75.76), 'old': np.float64(76.69), 'new': np.float64(75.76)}
2025-12-11 18:33:02,757 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93), np.float64(78.6), np.float64(76.57)]
2025-12-11 18:33:02,757 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01), np.float64(88.2), np.float64(87.55)]
2025-12-11 18:33:02,757 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098, 0.8354636932281752, 0.8134152102031176]
2025-12-11 18:33:21,277 [trainer.py] => W-NCM: {'00-19': 77.07692307692308, '20-39': 78.66666666666666, '40-59': 78.92271662763466, '60-79': 72.39583333333334, '80-99': 77.09497206703911, '100-119': 82.61648745519713, '120-139': 82.2262118491921}
2025-12-11 18:33:21,278 [trainer.py] => Ave Acc (W-NCM): 78.43%
2025-12-11 18:33:21,278 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 77.08% (best 88.31%); T2: W-NCM 78.67% (best 88.53%); T3: W-NCM 78.92% (best 83.14%); T4: W-NCM 72.40% (best 84.38%); T5: W-NCM 77.09% (best 84.64%); T6: W-NCM 82.62% (best 85.84%); T7: W-NCM 82.23% (best 82.23%)
2025-12-11 18:33:21,278 [trainer.py] => Average forgetting (W-NCM): 8.01% | Max forgetting (W-NCM): 11.98%
2025-12-11 18:33:21,280 [trainer.py] => All params: 111348451
2025-12-11 18:33:21,282 [trainer.py] => Trainable params: 199700
2025-12-11 18:33:21,282 [inflora.py] => Learning on 140-160
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight'}
2025-12-11 18:42:34,886 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.210, Train_accy 93.94
Threshold:  0.994
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 58/768 type remove
Layer 4 : 82/768 type remove
Layer 5 : 111/768 type remove
Layer 6 : 132/768 type remove
Layer 7 : 165/768 type remove
Layer 8 : 198/768 type remove
Layer 9 : 307/768 type remove
Layer 10 : 380/768 type remove
Layer 11 : 304/768 type remove
Layer 12 : 253/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:42:50,523 [trainer.py] => Time:569.2414751052856
4679 4679
4679 4679
2025-12-11 18:43:03,475 [trainer.py] => Time:12.951683044433594
2025-12-11 18:43:03,476 [inflora.py] => Exemplar size: 0
2025-12-11 18:43:03,476 [trainer.py] => CNN: {'total': np.float64(75.4), '00-19': np.float64(78.31), '20-39': np.float64(79.47), '40-59': np.float64(69.79), '60-79': np.float64(73.26), '80-99': np.float64(74.58), '100-119': np.float64(76.7), '120-139': np.float64(73.97), '140-159': np.float64(73.93), 'old': np.float64(75.56), 'new': np.float64(73.93)}
2025-12-11 18:43:03,476 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93), np.float64(78.6), np.float64(76.57), np.float64(75.4)]
2025-12-11 18:43:03,476 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01), np.float64(88.2), np.float64(87.55), np.float64(87.01)]
2025-12-11 18:43:03,476 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098, 0.8354636932281752, 0.8134152102031176, 0.7993160931823039]
2025-12-11 18:43:21,996 [trainer.py] => W-NCM: {'00-19': 74.76923076923076, '20-39': 76.93333333333334, '40-59': 77.04918032786885, '60-79': 71.52777777777779, '80-99': 75.83798882681563, '100-119': 80.64516129032258, '120-139': 76.84021543985638, '140-159': 82.69662921348313}
2025-12-11 18:43:21,997 [trainer.py] => Ave Acc (W-NCM): 77.04%
2025-12-11 18:43:21,997 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 74.77% (best 88.31%); T2: W-NCM 76.93% (best 88.53%); T3: W-NCM 77.05% (best 83.14%); T4: W-NCM 71.53% (best 84.38%); T5: W-NCM 75.84% (best 84.64%); T6: W-NCM 80.65% (best 85.84%); T7: W-NCM 76.84% (best 82.23%); T8: W-NCM 82.70% (best 82.70%)
2025-12-11 18:43:21,997 [trainer.py] => Average forgetting (W-NCM): 9.07% | Max forgetting (W-NCM): 13.54%
2025-12-11 18:43:21,999 [trainer.py] => All params: 111348451
2025-12-11 18:43:22,001 [trainer.py] => Trainable params: 199700
2025-12-11 18:43:22,001 [inflora.py] => Learning on 160-180
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight'}
2025-12-11 18:55:35,219 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.180, Train_accy 94.97
Threshold:  0.996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 85/768 type remove
Layer 4 : 121/768 type remove
Layer 5 : 162/768 type remove
Layer 6 : 182/768 type remove
Layer 7 : 222/768 type remove
Layer 8 : 262/768 type remove
Layer 9 : 379/768 type retain
Layer 10 : 297/768 type retain
Layer 11 : 364/768 type retain
Layer 12 : 191/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:55:54,305 [trainer.py] => Time:752.3038573265076
5306 5306
5306 5306
2025-12-11 18:56:08,866 [trainer.py] => Time:14.561192750930786
2025-12-11 18:56:08,866 [inflora.py] => Exemplar size: 0
2025-12-11 18:56:08,867 [trainer.py] => CNN: {'total': np.float64(75.84), '00-19': np.float64(77.54), '20-39': np.float64(79.33), '40-59': np.float64(70.02), '60-79': np.float64(72.92), '80-99': np.float64(73.88), '100-119': np.float64(76.7), '120-139': np.float64(71.1), '140-159': np.float64(73.03), '160-179': np.float64(84.21), 'old': np.float64(74.72), 'new': np.float64(84.21)}
2025-12-11 18:56:08,867 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93), np.float64(78.6), np.float64(76.57), np.float64(75.4), np.float64(75.84)]
2025-12-11 18:56:08,867 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01), np.float64(88.2), np.float64(87.55), np.float64(87.01), np.float64(87.6)]
2025-12-11 18:56:08,867 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098, 0.8354636932281752, 0.8134152102031176, 0.7993160931823039, 0.7983415001884658]
2025-12-11 18:56:30,529 [trainer.py] => W-NCM: {'00-19': 75.53846153846155, '20-39': 76.8, '40-59': 78.22014051522248, '60-79': 72.56944444444444, '80-99': 74.72067039106145, '100-119': 79.92831541218638, '120-139': 74.32675044883304, '140-159': 78.20224719101122, '160-179': 88.51674641148325}
2025-12-11 18:56:30,530 [trainer.py] => Ave Acc (W-NCM): 77.65%
2025-12-11 18:56:30,530 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 75.54% (best 88.31%); T2: W-NCM 76.80% (best 88.53%); T3: W-NCM 78.22% (best 83.14%); T4: W-NCM 72.57% (best 84.38%); T5: W-NCM 74.72% (best 84.64%); T6: W-NCM 79.93% (best 85.84%); T7: W-NCM 74.33% (best 82.23%); T8: W-NCM 78.20% (best 82.70%); T9: W-NCM 88.52% (best 88.52%)
2025-12-11 18:56:30,530 [trainer.py] => Average forgetting (W-NCM): 8.68% | Max forgetting (W-NCM): 12.77%
2025-12-11 18:56:30,532 [trainer.py] => All params: 111348451
2025-12-11 18:56:30,534 [trainer.py] => Trainable params: 199700
2025-12-11 18:56:30,534 [inflora.py] => Learning on 180-200
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight'}
2025-12-11 19:10:22,895 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.171, Train_accy 95.22
Threshold:  0.998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 17/768 type remove
Layer 2 : 49/768 type remove
Layer 3 : 117/768 type remove
Layer 4 : 171/768 type remove
Layer 5 : 223/768 type remove
Layer 6 : 257/768 type remove
Layer 7 : 311/768 type remove
Layer 8 : 365/768 type remove
Layer 9 : 279/768 type retain
Layer 10 : 206/768 type retain
Layer 11 : 241/768 type retain
Layer 12 : 117/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:10:44,221 [trainer.py] => Time:853.6865494251251
6000 6000
6000 6000
2025-12-11 19:11:00,562 [trainer.py] => Time:16.340962171554565
2025-12-11 19:11:00,562 [inflora.py] => Exemplar size: 0
2025-12-11 19:11:00,562 [trainer.py] => CNN: {'total': np.float64(75.5), '00-19': np.float64(76.46), '20-39': np.float64(79.6), '40-59': np.float64(68.38), '60-79': np.float64(71.35), '80-99': np.float64(74.3), '100-119': np.float64(76.34), '120-139': np.float64(68.94), '140-159': np.float64(71.24), '160-179': np.float64(82.46), '180-199': np.float64(80.26), 'old': np.float64(74.88), 'new': np.float64(80.26)}
2025-12-11 19:11:00,562 [trainer.py] => CNN top1 curve: [np.float64(88.77), np.float64(86.71), np.float64(82.81), np.float64(80.4), np.float64(79.93), np.float64(78.6), np.float64(76.57), np.float64(75.4), np.float64(75.84), np.float64(75.5)]
2025-12-11 19:11:00,562 [trainer.py] => CNN top1 with task curve: [np.float64(88.77), np.float64(90.21), np.float64(89.05), np.float64(88.39), np.float64(88.01), np.float64(88.2), np.float64(87.55), np.float64(87.01), np.float64(87.6), np.float64(87.38)]
2025-12-11 19:11:00,563 [trainer.py] => CNN top1 task curve: [1.0, 0.9357142857142857, 0.8932676518883416, 0.8672492717436537, 0.8573260660468098, 0.8354636932281752, 0.8134152102031176, 0.7993160931823039, 0.7983415001884658, 0.7931666666666667]
2025-12-11 19:11:24,895 [trainer.py] => W-NCM: {'00-19': 75.07692307692308, '20-39': 76.8, '40-59': 77.98594847775175, '60-79': 71.875, '80-99': 74.72067039106145, '100-119': 78.31541218637993, '120-139': 72.17235188509873, '140-159': 77.75280898876404, '160-179': 85.96491228070175, '180-199': 84.5821325648415}
2025-12-11 19:11:24,895 [trainer.py] => Ave Acc (W-NCM): 77.52%
2025-12-11 19:11:24,895 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 75.08% (best 88.31%); T2: W-NCM 76.80% (best 88.53%); T3: W-NCM 77.99% (best 83.14%); T4: W-NCM 71.88% (best 84.38%); T5: W-NCM 74.72% (best 84.64%); T6: W-NCM 78.32% (best 85.84%); T7: W-NCM 72.17% (best 82.23%); T8: W-NCM 77.75% (best 82.70%); T9: W-NCM 85.96% (best 88.52%); T10: W-NCM 84.58% (best 84.58%)
2025-12-11 19:11:24,895 [trainer.py] => Average forgetting (W-NCM): 8.62% | Max forgetting (W-NCM): 13.23%
2025-12-11 19:11:24,896 [trainer.py] => 
===== Summary =====
2025-12-11 19:11:24,896 [trainer.py] => Final average accuracy: 75.50%
2025-12-11 19:11:24,896 [trainer.py] => Average accuracy over tasks: 80.05%
2025-12-11 19:11:24,896 [trainer.py] => Final average forgetting: 6.87%
2025-12-11 19:11:24,896 [trainer.py] => Final max forgetting: 12.31%
2025-12-11 19:11:24,896 [trainer.py] => W-NCM final average accuracy: 77.52%
2025-12-11 19:11:24,896 [trainer.py] => W-NCM average accuracy over tasks: 80.82%
2025-12-11 19:11:24,896 [trainer.py] => W-NCM final average forgetting: 8.62%
2025-12-11 19:11:24,896 [trainer.py] => W-NCM final max forgetting: 13.23%
