logs/ImageNet_R/10_10_sip/InfLoRA/adam/10/0.98_1.0-0.0005/42
2025-12-10 01:55:33,094 [trainer.py] => config: configs/mimg20_inflora_seed42.json
2025-12-10 01:55:33,095 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 01:55:33,095 [trainer.py] => prefix: reproduce
2025-12-10 01:55:33,095 [trainer.py] => dataset: ImageNet_R
2025-12-10 01:55:33,095 [trainer.py] => data_path: data/imagenet-r
2025-12-10 01:55:33,095 [trainer.py] => memory_size: 0
2025-12-10 01:55:33,095 [trainer.py] => memory_per_class: 0
2025-12-10 01:55:33,095 [trainer.py] => fixed_memory: True
2025-12-10 01:55:33,095 [trainer.py] => shuffle: False
2025-12-10 01:55:33,095 [trainer.py] => init_cls: 10
2025-12-10 01:55:33,095 [trainer.py] => increment: 10
2025-12-10 01:55:33,095 [trainer.py] => model_name: InfLoRA
2025-12-10 01:55:33,095 [trainer.py] => net_type: sip
2025-12-10 01:55:33,095 [trainer.py] => embd_dim: 768
2025-12-10 01:55:33,095 [trainer.py] => num_heads: 12
2025-12-10 01:55:33,095 [trainer.py] => total_sessions: 20
2025-12-10 01:55:33,095 [trainer.py] => seed: 42
2025-12-10 01:55:33,095 [trainer.py] => EPSILON: 1e-08
2025-12-10 01:55:33,096 [trainer.py] => init_epoch: 50
2025-12-10 01:55:33,096 [trainer.py] => optim: adam
2025-12-10 01:55:33,096 [trainer.py] => init_lr: 0.0005
2025-12-10 01:55:33,096 [trainer.py] => init_lr_decay: 0.1
2025-12-10 01:55:33,096 [trainer.py] => init_weight_decay: 0.0
2025-12-10 01:55:33,096 [trainer.py] => epochs: 50
2025-12-10 01:55:33,096 [trainer.py] => lrate: 0.0005
2025-12-10 01:55:33,096 [trainer.py] => lrate_decay: 0.1
2025-12-10 01:55:33,096 [trainer.py] => batch_size: 128
2025-12-10 01:55:33,096 [trainer.py] => weight_decay: 0.0
2025-12-10 01:55:33,096 [trainer.py] => rank: 10
2025-12-10 01:55:33,096 [trainer.py] => lamb: 0.98
2025-12-10 01:55:33,096 [trainer.py] => lame: 1.0
2025-12-10 01:55:33,096 [trainer.py] => num_workers: 8
2025-12-10 01:55:33,372 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 962, unexpected 0
2025-12-10 01:55:35,250 [trainer.py] => All params: 115034851
2025-12-10 01:55:35,254 [trainer.py] => Trainable params: 115034851
2025-12-10 01:55:35,254 [inflora.py] => Learning on 0-10
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight'}
2025-12-10 02:02:54,440 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.137, Train_accy 96.36
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 17/768 type remove
Layer 5 : 27/768 type remove
Layer 6 : 25/768 type remove
Layer 7 : 27/768 type remove
Layer 8 : 27/768 type remove
Layer 9 : 48/768 type remove
Layer 10 : 48/768 type remove
Layer 11 : 19/768 type remove
Layer 12 : 49/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:03:05,855 [trainer.py] => Time:450.6013536453247
373 373
373 373
2025-12-10 02:03:07,872 [trainer.py] => Time:2.01643443107605
2025-12-10 02:03:07,872 [inflora.py] => Exemplar size: 0
2025-12-10 02:03:07,872 [trainer.py] => CNN: {'total': np.float64(90.62), '00-09': np.float64(90.62), 'old': 0, 'new': np.float64(90.62)}
2025-12-10 02:03:07,872 [trainer.py] => CNN top1 curve: [np.float64(90.62)]
2025-12-10 02:03:07,872 [trainer.py] => CNN top1 with task curve: [np.float64(90.62)]
2025-12-10 02:03:07,872 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 02:03:08,492 [trainer.py] => All params: 115034851
2025-12-10 02:03:08,495 [trainer.py] => Trainable params: 192010
2025-12-10 02:03:08,496 [inflora.py] => Learning on 10-20
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'classifier_pool.10.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'classifier_pool.10.bias', 'classifier_pool.1.bias', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight'}
2025-12-10 02:09:14,385 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.124, Train_accy 96.26
Threshold:  0.981
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 37/768 type remove
Layer 6 : 35/768 type remove
Layer 7 : 38/768 type remove
Layer 8 : 41/768 type remove
Layer 9 : 68/768 type remove
Layer 10 : 70/768 type remove
Layer 11 : 30/768 type remove
Layer 12 : 62/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:09:25,946 [trainer.py] => Time:377.45061135292053
650 650
650 650
2025-12-10 02:09:28,585 [trainer.py] => Time:2.6389095783233643
2025-12-10 02:09:28,586 [inflora.py] => Exemplar size: 0
2025-12-10 02:09:28,586 [trainer.py] => CNN: {'total': np.float64(86.77), '00-09': np.float64(85.52), '10-19': np.float64(88.45), 'old': np.float64(85.52), 'new': np.float64(88.45)}
2025-12-10 02:09:28,586 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77)]
2025-12-10 02:09:28,586 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77)]
2025-12-10 02:09:28,586 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077]
2025-12-10 02:09:29,275 [trainer.py] => All params: 115034851
2025-12-10 02:09:29,278 [trainer.py] => Trainable params: 2112110
2025-12-10 02:09:29,278 [inflora.py] => Learning on 20-30
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight'}
2025-12-10 02:18:08,478 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.123, Train_accy 96.51
Threshold:  0.982
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 26/768 type remove
Layer 5 : 43/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 42/768 type remove
Layer 8 : 46/768 type remove
Layer 9 : 77/768 type remove
Layer 10 : 84/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 89/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:18:22,698 [trainer.py] => Time:533.4201576709747
1114 1114
1114 1114
2025-12-10 02:18:26,578 [trainer.py] => Time:3.879347324371338
2025-12-10 02:18:26,578 [inflora.py] => Exemplar size: 0
2025-12-10 02:18:26,578 [trainer.py] => CNN: {'total': np.float64(85.82), '00-09': np.float64(82.84), '10-19': np.float64(86.28), '20-29': np.float64(87.93), 'old': np.float64(84.31), 'new': np.float64(87.93)}
2025-12-10 02:18:26,578 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82)]
2025-12-10 02:18:26,578 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29)]
2025-12-10 02:18:26,578 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289]
2025-12-10 02:18:27,122 [trainer.py] => All params: 115034851
2025-12-10 02:18:27,125 [trainer.py] => Trainable params: 192010
2025-12-10 02:18:27,125 [inflora.py] => Learning on 30-40
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight'}
2025-12-10 02:24:33,382 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.136, Train_accy 96.04
Threshold:  0.983
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 47/768 type remove
Layer 6 : 44/768 type remove
Layer 7 : 47/768 type remove
Layer 8 : 50/768 type remove
Layer 9 : 87/768 type remove
Layer 10 : 96/768 type remove
Layer 11 : 48/768 type remove
Layer 12 : 112/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:24:44,923 [trainer.py] => Time:377.79834818840027
1400 1400
1400 1400
2025-12-10 02:24:49,445 [trainer.py] => Time:4.521416425704956
2025-12-10 02:24:49,445 [inflora.py] => Exemplar size: 0
2025-12-10 02:24:49,445 [trainer.py] => CNN: {'total': np.float64(84.07), '00-09': np.float64(80.43), '10-19': np.float64(85.92), '20-29': np.float64(86.64), '30-39': np.float64(82.87), 'old': np.float64(84.38), 'new': np.float64(82.87)}
2025-12-10 02:24:49,445 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07)]
2025-12-10 02:24:49,445 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0)]
2025-12-10 02:24:49,445 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143]
2025-12-10 02:24:49,988 [trainer.py] => All params: 115034851
2025-12-10 02:24:49,991 [trainer.py] => Trainable params: 192010
2025-12-10 02:24:49,991 [inflora.py] => Learning on 40-50
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.5.attn.lora_B_k.4.weight'}
2025-12-10 02:29:38,158 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.150, Train_accy 95.84
Threshold:  0.984
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 31/768 type remove
Layer 5 : 52/768 type remove
Layer 6 : 48/768 type remove
Layer 7 : 55/768 type remove
Layer 8 : 58/768 type remove
Layer 9 : 100/768 type remove
Layer 10 : 108/768 type remove
Layer 11 : 62/768 type remove
Layer 12 : 122/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:29:48,420 [trainer.py] => Time:298.42950987815857
1634 1634
1634 1634
2025-12-10 02:29:53,579 [trainer.py] => Time:5.158307075500488
2025-12-10 02:29:53,579 [inflora.py] => Exemplar size: 0
2025-12-10 02:29:53,579 [trainer.py] => CNN: {'total': np.float64(81.52), '00-09': np.float64(78.28), '10-19': np.float64(82.31), '20-29': np.float64(83.84), '30-39': np.float64(81.12), '40-49': np.float64(81.62), 'old': np.float64(81.5), 'new': np.float64(81.62)}
2025-12-10 02:29:53,579 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52)]
2025-12-10 02:29:53,579 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94)]
2025-12-10 02:29:53,579 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389]
2025-12-10 02:29:54,111 [trainer.py] => All params: 115034851
2025-12-10 02:29:54,114 [trainer.py] => Trainable params: 192010
2025-12-10 02:29:54,114 [inflora.py] => Learning on 50-60
Parameters to be updated: {'classifier_pool.5.bias', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight'}
2025-12-10 02:34:46,490 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.162, Train_accy 94.10
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 36/768 type remove
Layer 5 : 59/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 66/768 type remove
Layer 8 : 69/768 type remove
Layer 9 : 115/768 type remove
Layer 10 : 120/768 type remove
Layer 11 : 75/768 type remove
Layer 12 : 133/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:34:56,843 [trainer.py] => Time:302.7282691001892
1827 1827
1827 1827
2025-12-10 02:35:02,469 [trainer.py] => Time:5.626261234283447
2025-12-10 02:35:02,469 [inflora.py] => Exemplar size: 0
2025-12-10 02:35:02,469 [trainer.py] => CNN: {'total': np.float64(79.69), '00-09': np.float64(78.28), '10-19': np.float64(81.59), '20-29': np.float64(82.97), '30-39': np.float64(81.12), '40-49': np.float64(79.49), '50-59': np.float64(69.95), 'old': np.float64(80.84), 'new': np.float64(69.95)}
2025-12-10 02:35:02,469 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69)]
2025-12-10 02:35:02,469 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64)]
2025-12-10 02:35:02,470 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992]
2025-12-10 02:35:03,038 [trainer.py] => All params: 115034851
2025-12-10 02:35:03,041 [trainer.py] => Trainable params: 192010
2025-12-10 02:35:03,041 [inflora.py] => Learning on 60-70
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight'}
2025-12-10 02:40:08,972 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.142, Train_accy 95.48
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 39/768 type remove
Layer 5 : 63/768 type remove
Layer 6 : 60/768 type remove
Layer 7 : 72/768 type remove
Layer 8 : 75/768 type remove
Layer 9 : 128/768 type remove
Layer 10 : 134/768 type remove
Layer 11 : 85/768 type remove
Layer 12 : 139/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:40:19,543 [trainer.py] => Time:316.5014228820801
2047 2047
2047 2047
2025-12-10 02:40:25,772 [trainer.py] => Time:6.229213237762451
2025-12-10 02:40:25,772 [inflora.py] => Exemplar size: 0
2025-12-10 02:40:25,772 [trainer.py] => CNN: {'total': np.float64(77.04), '00-09': np.float64(77.21), '10-19': np.float64(79.42), '20-29': np.float64(80.17), '30-39': np.float64(76.57), '40-49': np.float64(79.91), '50-59': np.float64(65.8), '60-69': np.float64(74.55), 'old': np.float64(77.34), 'new': np.float64(74.55)}
2025-12-10 02:40:25,772 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04)]
2025-12-10 02:40:25,773 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18)]
2025-12-10 02:40:25,773 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847]
2025-12-10 02:40:26,312 [trainer.py] => All params: 115034851
2025-12-10 02:40:26,315 [trainer.py] => Trainable params: 192010
2025-12-10 02:40:26,316 [inflora.py] => Learning on 70-80
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight'}
2025-12-10 02:47:49,452 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.132, Train_accy 95.02
Threshold:  0.987
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 35/768 type remove
Layer 4 : 49/768 type remove
Layer 5 : 76/768 type remove
Layer 6 : 72/768 type remove
Layer 7 : 84/768 type remove
Layer 8 : 91/768 type remove
Layer 9 : 159/768 type remove
Layer 10 : 175/768 type remove
Layer 11 : 107/768 type remove
Layer 12 : 167/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:48:02,584 [trainer.py] => Time:456.268616437912
2403 2403
2403 2403
2025-12-10 02:48:09,691 [trainer.py] => Time:7.1070616245269775
2025-12-10 02:48:09,692 [inflora.py] => Exemplar size: 0
2025-12-10 02:48:09,692 [trainer.py] => CNN: {'total': np.float64(76.07), '00-09': np.float64(76.94), '10-19': np.float64(79.78), '20-29': np.float64(80.17), '30-39': np.float64(78.67), '40-49': np.float64(79.06), '50-59': np.float64(61.66), '60-69': np.float64(73.18), '70-79': np.float64(72.47), 'old': np.float64(76.7), 'new': np.float64(72.47)}
2025-12-10 02:48:09,692 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07)]
2025-12-10 02:48:09,692 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3)]
2025-12-10 02:48:09,692 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286]
2025-12-10 02:48:10,230 [trainer.py] => All params: 115034851
2025-12-10 02:48:10,233 [trainer.py] => Trainable params: 192010
2025-12-10 02:48:10,233 [inflora.py] => Learning on 80-90
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight'}
2025-12-10 02:56:12,465 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.153, Train_accy 95.04
Threshold:  0.988
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 40/768 type remove
Layer 4 : 55/768 type remove
Layer 5 : 85/768 type remove
Layer 6 : 79/768 type remove
Layer 7 : 96/768 type remove
Layer 8 : 107/768 type remove
Layer 9 : 184/768 type remove
Layer 10 : 206/768 type remove
Layer 11 : 126/768 type remove
Layer 12 : 190/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 02:56:26,639 [trainer.py] => Time:496.40585827827454
2786 2786
2786 2786
2025-12-10 02:56:34,694 [trainer.py] => Time:8.054794073104858
2025-12-10 02:56:34,694 [inflora.py] => Exemplar size: 0
2025-12-10 02:56:34,694 [trainer.py] => CNN: {'total': np.float64(76.49), '00-09': np.float64(73.46), '10-19': np.float64(81.23), '20-29': np.float64(81.68), '30-39': np.float64(79.37), '40-49': np.float64(79.06), '50-59': np.float64(65.28), '60-69': np.float64(70.91), '70-79': np.float64(68.26), '80-89': np.float64(82.51), 'old': np.float64(75.53), 'new': np.float64(82.51)}
2025-12-10 02:56:34,695 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49)]
2025-12-10 02:56:34,695 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42)]
2025-12-10 02:56:34,695 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735]
2025-12-10 02:56:35,262 [trainer.py] => All params: 115034851
2025-12-10 02:56:35,265 [trainer.py] => Trainable params: 192010
2025-12-10 02:56:35,265 [inflora.py] => Learning on 90-100
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight'}
2025-12-10 03:03:06,002 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.206, Train_accy 93.59
Threshold:  0.989
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 59/768 type remove
Layer 5 : 91/768 type remove
Layer 6 : 85/768 type remove
Layer 7 : 105/768 type remove
Layer 8 : 114/768 type remove
Layer 9 : 195/768 type remove
Layer 10 : 226/768 type remove
Layer 11 : 144/768 type remove
Layer 12 : 244/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:03:18,453 [trainer.py] => Time:403.18814516067505
3119 3119
3119 3119
2025-12-10 03:03:27,522 [trainer.py] => Time:9.06707239151001
2025-12-10 03:03:27,523 [inflora.py] => Exemplar size: 0
2025-12-10 03:03:27,523 [trainer.py] => CNN: {'total': np.float64(76.08), '00-09': np.float64(72.92), '10-19': np.float64(80.51), '20-29': np.float64(82.33), '30-39': np.float64(81.82), '40-49': np.float64(77.78), '50-59': np.float64(65.8), '60-69': np.float64(76.36), '70-79': np.float64(69.38), '80-89': np.float64(80.16), '90-99': np.float64(69.37), 'old': np.float64(76.88), 'new': np.float64(69.37)}
2025-12-10 03:03:27,523 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08)]
2025-12-10 03:03:27,523 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38)]
2025-12-10 03:03:27,523 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845]
2025-12-10 03:03:28,065 [trainer.py] => All params: 115034851
2025-12-10 03:03:28,068 [trainer.py] => Trainable params: 192010
2025-12-10 03:03:28,068 [inflora.py] => Learning on 100-110
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight'}
2025-12-10 03:09:54,073 [inflora.py] => Task 10, Epoch 50/50 => Loss 0.218, Train_accy 93.13
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 46/768 type remove
Layer 4 : 66/768 type remove
Layer 5 : 100/768 type remove
Layer 6 : 96/768 type remove
Layer 7 : 117/768 type remove
Layer 8 : 129/768 type remove
Layer 9 : 213/768 type remove
Layer 10 : 247/768 type remove
Layer 11 : 163/768 type remove
Layer 12 : 269/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:10:06,136 [trainer.py] => Time:398.0679440498352
3437 3437
3437 3437
2025-12-10 03:10:15,966 [trainer.py] => Time:9.829915761947632
2025-12-10 03:10:15,967 [inflora.py] => Exemplar size: 0
2025-12-10 03:10:15,967 [trainer.py] => CNN: {'total': np.float64(74.69), '00-09': np.float64(69.97), '10-19': np.float64(79.42), '20-29': np.float64(82.97), '30-39': np.float64(77.27), '40-49': np.float64(75.21), '50-59': np.float64(67.88), '60-69': np.float64(74.55), '70-79': np.float64(74.16), '80-89': np.float64(77.02), '90-99': np.float64(67.57), '100-109': np.float64(70.75), 'old': np.float64(75.09), 'new': np.float64(70.75)}
2025-12-10 03:10:15,967 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69)]
2025-12-10 03:10:15,967 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61)]
2025-12-10 03:10:15,967 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269]
2025-12-10 03:10:16,519 [trainer.py] => All params: 115034851
2025-12-10 03:10:16,522 [trainer.py] => Trainable params: 192010
2025-12-10 03:10:16,522 [inflora.py] => Learning on 110-120
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight'}
2025-12-10 03:15:40,202 [inflora.py] => Task 11, Epoch 50/50 => Loss 0.109, Train_accy 96.57
Threshold:  0.991
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 27/768 type remove
Layer 3 : 51/768 type remove
Layer 4 : 71/768 type remove
Layer 5 : 108/768 type remove
Layer 6 : 107/768 type remove
Layer 7 : 132/768 type remove
Layer 8 : 152/768 type remove
Layer 9 : 252/768 type remove
Layer 10 : 298/768 type remove
Layer 11 : 194/768 type remove
Layer 12 : 354/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:15:51,030 [trainer.py] => Time:334.5080680847168
3677 3677
3677 3677
2025-12-10 03:16:01,446 [trainer.py] => Time:10.415975570678711
2025-12-10 03:16:01,447 [inflora.py] => Exemplar size: 0
2025-12-10 03:16:01,448 [trainer.py] => CNN: {'total': np.float64(73.73), '00-09': np.float64(68.63), '10-19': np.float64(77.26), '20-29': np.float64(80.82), '30-39': np.float64(74.48), '40-49': np.float64(76.07), '50-59': np.float64(67.36), '60-69': np.float64(73.18), '70-79': np.float64(70.22), '80-89': np.float64(75.46), '90-99': np.float64(65.77), '100-109': np.float64(66.67), '110-119': np.float64(89.17), 'old': np.float64(72.65), 'new': np.float64(89.17)}
2025-12-10 03:16:01,448 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73)]
2025-12-10 03:16:01,448 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15)]
2025-12-10 03:16:01,448 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483]
2025-12-10 03:16:02,952 [trainer.py] => All params: 115034851
2025-12-10 03:16:02,955 [trainer.py] => Trainable params: 192010
2025-12-10 03:16:02,955 [inflora.py] => Learning on 120-130
Parameters to be updated: {'classifier_pool.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight'}
2025-12-10 03:22:21,374 [inflora.py] => Task 12, Epoch 50/50 => Loss 0.219, Train_accy 93.32
Threshold:  0.992
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 54/768 type remove
Layer 4 : 77/768 type remove
Layer 5 : 118/768 type remove
Layer 6 : 121/768 type remove
Layer 7 : 151/768 type remove
Layer 8 : 171/768 type remove
Layer 9 : 270/768 type remove
Layer 10 : 322/768 type remove
Layer 11 : 216/768 type remove
Layer 12 : 373/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:22:33,702 [trainer.py] => Time:390.74613213539124
3975 3975
3975 3975
2025-12-10 03:22:44,885 [trainer.py] => Time:11.18355679512024
2025-12-10 03:22:44,886 [inflora.py] => Exemplar size: 0
2025-12-10 03:22:44,886 [trainer.py] => CNN: {'total': np.float64(71.82), '00-09': np.float64(67.02), '10-19': np.float64(73.65), '20-29': np.float64(79.53), '30-39': np.float64(73.08), '40-49': np.float64(76.5), '50-59': np.float64(63.73), '60-69': np.float64(71.82), '70-79': np.float64(68.26), '80-89': np.float64(75.2), '90-99': np.float64(65.17), '100-109': np.float64(64.15), '110-119': np.float64(89.17), '120-129': np.float64(66.11), 'old': np.float64(72.29), 'new': np.float64(66.11)}
2025-12-10 03:22:44,886 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82)]
2025-12-10 03:22:44,886 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33)]
2025-12-10 03:22:44,886 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704]
2025-12-10 03:22:45,432 [trainer.py] => All params: 115034851
2025-12-10 03:22:45,435 [trainer.py] => Trainable params: 192010
2025-12-10 03:22:45,435 [inflora.py] => Learning on 130-140
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight'}
2025-12-10 03:28:40,344 [inflora.py] => Task 13, Epoch 50/50 => Loss 0.136, Train_accy 95.74
Threshold:  0.993
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 31/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 92/768 type remove
Layer 5 : 135/768 type remove
Layer 6 : 142/768 type remove
Layer 7 : 174/768 type remove
Layer 8 : 202/768 type remove
Layer 9 : 315/768 type remove
Layer 10 : 384/768 type remove
Layer 11 : 278/768 type remove
Layer 12 : 310/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:28:52,029 [trainer.py] => Time:366.59457898139954
4234 4234
4234 4234
2025-12-10 03:29:03,916 [trainer.py] => Time:11.885908603668213
2025-12-10 03:29:03,917 [inflora.py] => Exemplar size: 0
2025-12-10 03:29:03,917 [trainer.py] => CNN: {'total': np.float64(70.29), '00-09': np.float64(68.1), '10-19': np.float64(71.84), '20-29': np.float64(77.37), '30-39': np.float64(72.73), '40-49': np.float64(76.5), '50-59': np.float64(60.1), '60-69': np.float64(70.45), '70-79': np.float64(64.61), '80-89': np.float64(72.58), '90-99': np.float64(62.76), '100-109': np.float64(62.26), '110-119': np.float64(85.0), '120-129': np.float64(65.77), '130-139': np.float64(73.75), 'old': np.float64(70.06), 'new': np.float64(73.75)}
2025-12-10 03:29:03,917 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29)]
2025-12-10 03:29:03,917 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93)]
2025-12-10 03:29:03,917 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935]
2025-12-10 03:29:04,475 [trainer.py] => All params: 115034851
2025-12-10 03:29:04,478 [trainer.py] => Trainable params: 192010
2025-12-10 03:29:04,478 [inflora.py] => Learning on 140-150
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight'}
2025-12-10 03:33:36,792 [inflora.py] => Task 14, Epoch 50/50 => Loss 0.187, Train_accy 94.15
Threshold:  0.994
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 67/768 type remove
Layer 4 : 97/768 type remove
Layer 5 : 143/768 type remove
Layer 6 : 153/768 type remove
Layer 7 : 190/768 type remove
Layer 8 : 222/768 type remove
Layer 9 : 335/768 type remove
Layer 10 : 364/768 type retain
Layer 11 : 306/768 type remove
Layer 12 : 282/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:33:46,617 [trainer.py] => Time:282.139363527298
4435 4435
4435 4435
2025-12-10 03:33:59,034 [trainer.py] => Time:12.416584491729736
2025-12-10 03:33:59,035 [inflora.py] => Exemplar size: 0
2025-12-10 03:33:59,035 [trainer.py] => CNN: {'total': np.float64(70.42), '00-09': np.float64(68.63), '10-19': np.float64(74.01), '20-29': np.float64(77.8), '30-39': np.float64(72.03), '40-49': np.float64(76.92), '50-59': np.float64(58.55), '60-69': np.float64(71.82), '70-79': np.float64(64.33), '80-89': np.float64(72.32), '90-99': np.float64(62.76), '100-109': np.float64(61.32), '110-119': np.float64(84.58), '120-129': np.float64(64.77), '130-139': np.float64(74.13), '140-149': np.float64(72.64), 'old': np.float64(70.31), 'new': np.float64(72.64)}
2025-12-10 03:33:59,035 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42)]
2025-12-10 03:33:59,035 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82)]
2025-12-10 03:33:59,035 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439]
2025-12-10 03:33:59,588 [trainer.py] => All params: 115034851
2025-12-10 03:33:59,591 [trainer.py] => Trainable params: 192010
2025-12-10 03:33:59,592 [inflora.py] => Learning on 150-160
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'classifier_pool.15.weight'}
2025-12-10 03:39:35,110 [inflora.py] => Task 15, Epoch 50/50 => Loss 0.195, Train_accy 92.52
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 75/768 type remove
Layer 4 : 109/768 type remove
Layer 5 : 155/768 type remove
Layer 6 : 168/768 type remove
Layer 7 : 208/768 type remove
Layer 8 : 243/768 type remove
Layer 9 : 354/768 type remove
Layer 10 : 350/768 type retain
Layer 11 : 322/768 type remove
Layer 12 : 269/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:39:46,510 [trainer.py] => Time:346.9181218147278
4679 4679
4679 4679
2025-12-10 03:39:59,602 [trainer.py] => Time:13.092595100402832
2025-12-10 03:39:59,603 [inflora.py] => Exemplar size: 0
2025-12-10 03:39:59,603 [trainer.py] => CNN: {'total': np.float64(69.82), '00-09': np.float64(67.56), '10-19': np.float64(75.09), '20-29': np.float64(77.59), '30-39': np.float64(71.68), '40-49': np.float64(74.79), '50-59': np.float64(60.1), '60-69': np.float64(68.18), '70-79': np.float64(61.8), '80-89': np.float64(74.41), '90-99': np.float64(61.86), '100-109': np.float64(60.06), '110-119': np.float64(85.42), '120-129': np.float64(63.76), '130-139': np.float64(70.66), '140-149': np.float64(71.64), '150-159': np.float64(72.54), 'old': np.float64(69.67), 'new': np.float64(72.54)}
2025-12-10 03:39:59,603 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42), np.float64(69.82)]
2025-12-10 03:39:59,603 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82), np.float64(87.97)]
2025-12-10 03:39:59,603 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439, 0.7253686685189144]
2025-12-10 03:40:00,197 [trainer.py] => All params: 115034851
2025-12-10 03:40:00,200 [trainer.py] => Trainable params: 192010
2025-12-10 03:40:00,200 [inflora.py] => Learning on 160-170
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight'}
2025-12-10 03:46:57,153 [inflora.py] => Task 16, Epoch 50/50 => Loss 0.094, Train_accy 96.99
Threshold:  0.996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 96/768 type remove
Layer 4 : 138/768 type remove
Layer 5 : 192/768 type remove
Layer 6 : 202/768 type remove
Layer 7 : 242/768 type remove
Layer 8 : 282/768 type remove
Layer 9 : 361/768 type retain
Layer 10 : 296/768 type retain
Layer 11 : 376/768 type remove
Layer 12 : 247/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:47:09,978 [trainer.py] => Time:429.777428150177
5005 5005
5005 5005
2025-12-10 03:47:23,887 [trainer.py] => Time:13.909023761749268
2025-12-10 03:47:23,887 [inflora.py] => Exemplar size: 0
2025-12-10 03:47:23,887 [trainer.py] => CNN: {'total': np.float64(69.35), '00-09': np.float64(67.29), '10-19': np.float64(72.2), '20-29': np.float64(77.16), '30-39': np.float64(71.33), '40-49': np.float64(73.93), '50-59': np.float64(55.44), '60-69': np.float64(67.73), '70-79': np.float64(60.96), '80-89': np.float64(73.37), '90-99': np.float64(61.56), '100-109': np.float64(59.43), '110-119': np.float64(83.75), '120-129': np.float64(60.07), '130-139': np.float64(66.8), '140-149': np.float64(70.65), '150-159': np.float64(72.95), '160-169': np.float64(80.98), 'old': np.float64(68.54), 'new': np.float64(80.98)}
2025-12-10 03:47:23,887 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42), np.float64(69.82), np.float64(69.35)]
2025-12-10 03:47:23,887 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82), np.float64(87.97), np.float64(87.91)]
2025-12-10 03:47:23,888 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439, 0.7253686685189144, 0.7182817182817183]
2025-12-10 03:47:24,604 [trainer.py] => All params: 115034851
2025-12-10 03:47:24,607 [trainer.py] => Trainable params: 192010
2025-12-10 03:47:24,607 [inflora.py] => Learning on 170-180
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'classifier_pool.17.weight', 'classifier_pool.17.bias'}
2025-12-10 03:53:35,211 [inflora.py] => Task 17, Epoch 50/50 => Loss 0.146, Train_accy 94.80
Threshold:  0.997
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 104/768 type remove
Layer 4 : 156/768 type remove
Layer 5 : 213/768 type remove
Layer 6 : 224/768 type remove
Layer 7 : 270/768 type remove
Layer 8 : 315/768 type remove
Layer 9 : 338/768 type retain
Layer 10 : 273/768 type retain
Layer 11 : 355/768 type retain
Layer 12 : 205/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 03:53:47,240 [trainer.py] => Time:382.632435798645
5306 5306
5306 5306
2025-12-10 03:54:01,923 [trainer.py] => Time:14.682636737823486
2025-12-10 03:54:01,923 [inflora.py] => Exemplar size: 0
2025-12-10 03:54:01,923 [trainer.py] => CNN: {'total': np.float64(70.69), '00-09': np.float64(68.9), '10-19': np.float64(71.84), '20-29': np.float64(78.45), '30-39': np.float64(72.03), '40-49': np.float64(73.93), '50-59': np.float64(60.1), '60-69': np.float64(68.64), '70-79': np.float64(60.67), '80-89': np.float64(74.15), '90-99': np.float64(62.46), '100-109': np.float64(59.43), '110-119': np.float64(85.42), '120-129': np.float64(61.41), '130-139': np.float64(67.57), '140-149': np.float64(70.15), '150-159': np.float64(72.13), '160-169': np.float64(81.29), '170-179': np.float64(80.73), 'old': np.float64(70.09), 'new': np.float64(80.73)}
2025-12-10 03:54:01,923 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42), np.float64(69.82), np.float64(69.35), np.float64(70.69)]
2025-12-10 03:54:01,923 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82), np.float64(87.97), np.float64(87.91), np.float64(88.88)]
2025-12-10 03:54:01,923 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439, 0.7253686685189144, 0.7182817182817183, 0.7299283829626838]
2025-12-10 03:54:02,476 [trainer.py] => All params: 115034851
2025-12-10 03:54:02,479 [trainer.py] => Trainable params: 192010
2025-12-10 03:54:02,480 [inflora.py] => Learning on 180-190
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight'}
2025-12-10 04:02:05,185 [inflora.py] => Task 18, Epoch 50/50 => Loss 0.189, Train_accy 94.32
Threshold:  0.998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 17/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 129/768 type remove
Layer 4 : 194/768 type remove
Layer 5 : 260/768 type remove
Layer 6 : 275/768 type remove
Layer 7 : 331/768 type remove
Layer 8 : 381/768 type retain
Layer 9 : 273/768 type retain
Layer 10 : 214/768 type retain
Layer 11 : 276/768 type retain
Layer 12 : 151/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 04:02:19,598 [trainer.py] => Time:497.11857199668884
5683 5683
5683 5683
2025-12-10 04:02:35,276 [trainer.py] => Time:15.677470922470093
2025-12-10 04:02:35,276 [inflora.py] => Exemplar size: 0
2025-12-10 04:02:35,276 [trainer.py] => CNN: {'total': np.float64(70.67), '00-09': np.float64(68.36), '10-19': np.float64(72.2), '20-29': np.float64(79.74), '30-39': np.float64(72.38), '40-49': np.float64(73.08), '50-59': np.float64(59.07), '60-69': np.float64(69.09), '70-79': np.float64(59.55), '80-89': np.float64(74.15), '90-99': np.float64(60.96), '100-109': np.float64(59.75), '110-119': np.float64(85.0), '120-129': np.float64(59.73), '130-139': np.float64(66.8), '140-149': np.float64(69.15), '150-159': np.float64(71.31), '160-169': np.float64(80.06), '170-179': np.float64(80.4), '180-189': np.float64(76.13), 'old': np.float64(70.28), 'new': np.float64(76.13)}
2025-12-10 04:02:35,276 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42), np.float64(69.82), np.float64(69.35), np.float64(70.69), np.float64(70.67)]
2025-12-10 04:02:35,277 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82), np.float64(87.97), np.float64(87.91), np.float64(88.88), np.float64(88.83)]
2025-12-10 04:02:35,277 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439, 0.7253686685189144, 0.7182817182817183, 0.7299283829626838, 0.7302481083934541]
2025-12-10 04:02:35,891 [trainer.py] => All params: 115034851
2025-12-10 04:02:35,894 [trainer.py] => Trainable params: 192010
2025-12-10 04:02:35,894 [inflora.py] => Learning on 190-200
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight'}
2025-12-10 04:09:19,394 [inflora.py] => Task 19, Epoch 50/50 => Loss 0.142, Train_accy 96.01
Threshold:  0.999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 33/768 type remove
Layer 2 : 76/768 type remove
Layer 3 : 167/768 type remove
Layer 4 : 251/768 type remove
Layer 5 : 328/768 type remove
Layer 6 : 352/768 type remove
Layer 7 : 355/768 type retain
Layer 8 : 301/768 type retain
Layer 9 : 197/768 type retain
Layer 10 : 141/768 type retain
Layer 11 : 183/768 type retain
Layer 12 : 92/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 04:09:32,278 [trainer.py] => Time:416.38345217704773
6000 6000
6000 6000
2025-12-10 04:09:48,817 [trainer.py] => Time:16.538640022277832
2025-12-10 04:09:48,817 [inflora.py] => Exemplar size: 0
2025-12-10 04:09:48,817 [trainer.py] => CNN: {'total': np.float64(70.15), '00-09': np.float64(65.95), '10-19': np.float64(70.4), '20-29': np.float64(79.96), '30-39': np.float64(70.28), '40-49': np.float64(73.08), '50-59': np.float64(59.07), '60-69': np.float64(69.55), '70-79': np.float64(60.11), '80-89': np.float64(74.67), '90-99': np.float64(61.26), '100-109': np.float64(58.18), '110-119': np.float64(84.58), '120-129': np.float64(57.72), '130-139': np.float64(64.48), '140-149': np.float64(67.66), '150-159': np.float64(70.9), '160-169': np.float64(78.22), '170-179': np.float64(79.73), '180-189': np.float64(75.07), '190-199': np.float64(75.71), 'old': np.float64(69.84), 'new': np.float64(75.71)}
2025-12-10 04:09:48,817 [trainer.py] => CNN top1 curve: [np.float64(90.62), np.float64(86.77), np.float64(85.82), np.float64(84.07), np.float64(81.52), np.float64(79.69), np.float64(77.04), np.float64(76.07), np.float64(76.49), np.float64(76.08), np.float64(74.69), np.float64(73.73), np.float64(71.82), np.float64(70.29), np.float64(70.42), np.float64(69.82), np.float64(69.35), np.float64(70.69), np.float64(70.67), np.float64(70.15)]
2025-12-10 04:09:48,817 [trainer.py] => CNN top1 with task curve: [np.float64(90.62), np.float64(90.77), np.float64(91.29), np.float64(92.0), np.float64(90.94), np.float64(90.64), np.float64(90.18), np.float64(90.3), np.float64(90.42), np.float64(90.38), np.float64(89.61), np.float64(89.15), np.float64(88.33), np.float64(87.93), np.float64(87.82), np.float64(87.97), np.float64(87.91), np.float64(88.88), np.float64(88.83), np.float64(89.03)]
2025-12-10 04:09:48,817 [trainer.py] => CNN top1 task curve: [1.0, 0.9276923076923077, 0.900359066427289, 0.8742857142857143, 0.8537331701346389, 0.8336070060207992, 0.8055691255495847, 0.7948397836038286, 0.8000717875089735, 0.7919204873356845, 0.7774221704975269, 0.7682893663312483, 0.7444025157232704, 0.7272083136513935, 0.7321307779030439, 0.7253686685189144, 0.7182817182817183, 0.7299283829626838, 0.7302481083934541, 0.7238333333333333]
