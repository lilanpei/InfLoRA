logs/ImageNet_R/4_4_sip/InfLoRA/adam/10/0.98_1.0-0.0005/42
2025-12-10 13:29:04,103 [trainer.py] => config: configs/mimg50_inflora_seed42.json
2025-12-10 13:29:04,104 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 13:29:04,104 [trainer.py] => prefix: reproduce
2025-12-10 13:29:04,104 [trainer.py] => dataset: ImageNet_R
2025-12-10 13:29:04,104 [trainer.py] => data_path: data/imagenet-r
2025-12-10 13:29:04,104 [trainer.py] => memory_size: 0
2025-12-10 13:29:04,104 [trainer.py] => memory_per_class: 0
2025-12-10 13:29:04,104 [trainer.py] => fixed_memory: True
2025-12-10 13:29:04,104 [trainer.py] => shuffle: False
2025-12-10 13:29:04,104 [trainer.py] => init_cls: 4
2025-12-10 13:29:04,104 [trainer.py] => increment: 4
2025-12-10 13:29:04,104 [trainer.py] => model_name: InfLoRA
2025-12-10 13:29:04,104 [trainer.py] => net_type: sip
2025-12-10 13:29:04,104 [trainer.py] => embd_dim: 768
2025-12-10 13:29:04,104 [trainer.py] => num_heads: 12
2025-12-10 13:29:04,104 [trainer.py] => total_sessions: 50
2025-12-10 13:29:04,104 [trainer.py] => seed: 42
2025-12-10 13:29:04,104 [trainer.py] => EPSILON: 1e-08
2025-12-10 13:29:04,104 [trainer.py] => init_epoch: 50
2025-12-10 13:29:04,104 [trainer.py] => optim: adam
2025-12-10 13:29:04,105 [trainer.py] => init_lr: 0.0005
2025-12-10 13:29:04,105 [trainer.py] => init_lr_decay: 0.1
2025-12-10 13:29:04,105 [trainer.py] => init_weight_decay: 0.0
2025-12-10 13:29:04,105 [trainer.py] => epochs: 50
2025-12-10 13:29:04,105 [trainer.py] => lrate: 0.0005
2025-12-10 13:29:04,105 [trainer.py] => lrate_decay: 0.1
2025-12-10 13:29:04,105 [trainer.py] => batch_size: 128
2025-12-10 13:29:04,105 [trainer.py] => weight_decay: 0.0
2025-12-10 13:29:04,105 [trainer.py] => rank: 10
2025-12-10 13:29:04,105 [trainer.py] => lamb: 0.98
2025-12-10 13:29:04,105 [trainer.py] => lame: 1.0
2025-12-10 13:29:04,105 [trainer.py] => num_workers: 8
2025-12-10 13:29:04,404 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 2402, unexpected 0
2025-12-10 13:29:06,891 [trainer.py] => All params: 126094051
2025-12-10 13:29:06,897 [trainer.py] => Trainable params: 126094051
2025-12-10 13:29:06,897 [inflora.py] => Learning on 0-4
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight'}
2025-12-10 13:32:38,343 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.092, Train_accy 96.00
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 30/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 27/768 type remove
Layer 8 : 34/768 type remove
Layer 9 : 62/768 type remove
Layer 10 : 66/768 type remove
Layer 11 : 30/768 type remove
Layer 12 : 52/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:32:45,019 [trainer.py] => Time:218.12158489227295
155 155
155 155
2025-12-10 13:32:46,530 [trainer.py] => Time:1.511420726776123
2025-12-10 13:32:46,531 [inflora.py] => Exemplar size: 0
2025-12-10 13:32:46,531 [trainer.py] => CNN: {'total': np.float64(90.32), '00-03': np.float64(90.32), 'old': 0, 'new': np.float64(90.32)}
2025-12-10 13:32:46,531 [trainer.py] => CNN top1 curve: [np.float64(90.32)]
2025-12-10 13:32:46,531 [trainer.py] => CNN top1 with task curve: [np.float64(90.32)]
2025-12-10 13:32:46,531 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 13:32:47,223 [trainer.py] => All params: 126094051
2025-12-10 13:32:47,230 [trainer.py] => Trainable params: 187396
2025-12-10 13:32:47,230 [inflora.py] => Learning on 4-8
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight'}
2025-12-10 13:35:54,252 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.074, Train_accy 96.67
Threshold:  0.9803999999999999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 20/768 type remove
Layer 5 : 33/768 type remove
Layer 6 : 30/768 type remove
Layer 7 : 32/768 type remove
Layer 8 : 38/768 type remove
Layer 9 : 69/768 type remove
Layer 10 : 79/768 type remove
Layer 11 : 37/768 type remove
Layer 12 : 63/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:36:02,489 [trainer.py] => Time:195.2592556476593
300 300
300 300
2025-12-10 13:36:04,194 [trainer.py] => Time:1.7047481536865234
2025-12-10 13:36:04,194 [inflora.py] => Exemplar size: 0
2025-12-10 13:36:04,194 [trainer.py] => CNN: {'total': np.float64(82.67), '00-03': np.float64(83.23), '04-07': np.float64(82.07), 'old': np.float64(83.23), 'new': np.float64(82.07)}
2025-12-10 13:36:04,194 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67)]
2025-12-10 13:36:04,194 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67)]
2025-12-10 13:36:04,194 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333]
2025-12-10 13:36:04,876 [trainer.py] => All params: 126094051
2025-12-10 13:36:04,882 [trainer.py] => Trainable params: 2061356
2025-12-10 13:36:04,883 [inflora.py] => Learning on 8-12
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'classifier_pool.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'classifier_pool.2.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight'}
2025-12-10 13:39:26,074 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.081, Train_accy 96.89
Threshold:  0.9808
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 39/768 type remove
Layer 6 : 33/768 type remove
Layer 7 : 35/768 type remove
Layer 8 : 42/768 type remove
Layer 9 : 76/768 type remove
Layer 10 : 88/768 type remove
Layer 11 : 43/768 type remove
Layer 12 : 69/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:39:34,375 [trainer.py] => Time:209.49274015426636
414 414
414 414
2025-12-10 13:39:36,460 [trainer.py] => Time:2.0848538875579834
2025-12-10 13:39:36,461 [inflora.py] => Exemplar size: 0
2025-12-10 13:39:36,461 [trainer.py] => CNN: {'total': np.float64(79.47), '00-03': np.float64(78.71), '04-07': np.float64(80.0), '08-11': np.float64(79.82), 'old': np.float64(79.33), 'new': np.float64(79.82)}
2025-12-10 13:39:36,461 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47)]
2025-12-10 13:39:36,461 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89)]
2025-12-10 13:39:36,461 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043]
2025-12-10 13:39:37,172 [trainer.py] => All params: 126094051
2025-12-10 13:39:37,178 [trainer.py] => Trainable params: 2061356
2025-12-10 13:39:37,178 [inflora.py] => Learning on 12-16
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'classifier_pool.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight'}
2025-12-10 13:42:29,460 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.103, Train_accy 96.64
Threshold:  0.9812
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 26/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 46/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 41/768 type remove
Layer 8 : 48/768 type remove
Layer 9 : 83/768 type remove
Layer 10 : 100/768 type remove
Layer 11 : 49/768 type remove
Layer 12 : 74/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:42:37,212 [trainer.py] => Time:180.03410816192627
524 524
524 524
2025-12-10 13:42:39,501 [trainer.py] => Time:2.2880911827087402
2025-12-10 13:42:39,501 [inflora.py] => Exemplar size: 0
2025-12-10 13:42:39,501 [trainer.py] => CNN: {'total': np.float64(78.82), '00-03': np.float64(75.48), '04-07': np.float64(80.69), '08-11': np.float64(78.95), '12-15': np.float64(80.91), 'old': np.float64(78.26), 'new': np.float64(80.91)}
2025-12-10 13:42:39,501 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82)]
2025-12-10 13:42:39,501 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46)]
2025-12-10 13:42:39,501 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794]
2025-12-10 13:42:40,193 [trainer.py] => All params: 126094051
2025-12-10 13:42:40,200 [trainer.py] => Trainable params: 2061356
2025-12-10 13:42:40,200 [inflora.py] => Learning on 16-20
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'classifier_pool.45.bias', 'classifier_pool.46.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'classifier_pool.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'classifier_pool.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'classifier_pool.43.bias', 'classifier_pool.48.bias', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight'}
2025-12-10 13:45:48,820 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.077, Train_accy 96.82
Threshold:  0.9816
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 32/768 type remove
Layer 5 : 49/768 type remove
Layer 6 : 44/768 type remove
Layer 7 : 46/768 type remove
Layer 8 : 57/768 type remove
Layer 9 : 96/768 type remove
Layer 10 : 117/768 type remove
Layer 11 : 59/768 type remove
Layer 12 : 83/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:45:56,825 [trainer.py] => Time:196.62523555755615
650 650
650 650
2025-12-10 13:45:59,491 [trainer.py] => Time:2.6660406589508057
2025-12-10 13:45:59,491 [inflora.py] => Exemplar size: 0
2025-12-10 13:45:59,491 [trainer.py] => CNN: {'total': np.float64(77.85), '00-03': np.float64(75.48), '04-07': np.float64(80.69), '08-11': np.float64(79.82), '12-15': np.float64(77.27), '16-19': np.float64(76.19), 'old': np.float64(78.24), 'new': np.float64(76.19)}
2025-12-10 13:45:59,492 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85)]
2025-12-10 13:45:59,492 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38)]
2025-12-10 13:45:59,492 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616]
2025-12-10 13:46:00,185 [trainer.py] => All params: 126094051
2025-12-10 13:46:00,192 [trainer.py] => Trainable params: 2061356
2025-12-10 13:46:00,192 [inflora.py] => Learning on 20-24
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight'}
2025-12-10 13:50:25,829 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.108, Train_accy 96.71
Threshold:  0.982
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 35/768 type remove
Layer 5 : 51/768 type remove
Layer 6 : 46/768 type remove
Layer 7 : 48/768 type remove
Layer 8 : 59/768 type remove
Layer 9 : 99/768 type remove
Layer 10 : 120/768 type remove
Layer 11 : 62/768 type remove
Layer 12 : 88/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:50:35,201 [trainer.py] => Time:275.0095076560974
869 869
869 869
2025-12-10 13:50:38,454 [trainer.py] => Time:3.252549886703491
2025-12-10 13:50:38,454 [inflora.py] => Exemplar size: 0
2025-12-10 13:50:38,454 [trainer.py] => CNN: {'total': np.float64(77.68), '00-03': np.float64(76.13), '04-07': np.float64(80.0), '08-11': np.float64(78.95), '12-15': np.float64(80.91), '16-19': np.float64(76.19), '20-23': np.float64(75.8), 'old': np.float64(78.31), 'new': np.float64(75.8)}
2025-12-10 13:50:38,454 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68)]
2025-12-10 13:50:38,454 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06)]
2025-12-10 13:50:38,454 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099]
2025-12-10 13:50:39,194 [trainer.py] => All params: 126094051
2025-12-10 13:50:39,200 [trainer.py] => Trainable params: 187396
2025-12-10 13:50:39,200 [inflora.py] => Learning on 24-28
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'classifier_pool.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight'}
2025-12-10 13:54:29,833 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.067, Train_accy 97.45
Threshold:  0.9823999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 30/768 type remove
Layer 4 : 39/768 type remove
Layer 5 : 55/768 type remove
Layer 6 : 50/768 type remove
Layer 7 : 54/768 type remove
Layer 8 : 64/768 type remove
Layer 9 : 110/768 type remove
Layer 10 : 135/768 type remove
Layer 11 : 72/768 type remove
Layer 12 : 109/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:54:38,750 [trainer.py] => Time:239.5492649078369
1033 1033
1033 1033
2025-12-10 13:54:42,419 [trainer.py] => Time:3.6688363552093506
2025-12-10 13:54:42,419 [inflora.py] => Exemplar size: 0
2025-12-10 13:54:42,419 [trainer.py] => CNN: {'total': np.float64(78.03), '00-03': np.float64(76.13), '04-07': np.float64(80.0), '08-11': np.float64(76.32), '12-15': np.float64(81.82), '16-19': np.float64(78.57), '20-23': np.float64(73.52), '24-27': np.float64(82.32), 'old': np.float64(77.22), 'new': np.float64(82.32)}
2025-12-10 13:54:42,419 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03)]
2025-12-10 13:54:42,419 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84)]
2025-12-10 13:54:42,419 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215]
2025-12-10 13:54:46,438 [trainer.py] => All params: 126094051
2025-12-10 13:54:46,444 [trainer.py] => Trainable params: 187396
2025-12-10 13:54:46,444 [inflora.py] => Learning on 28-32
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'classifier_pool.7.weight'}
2025-12-10 13:58:44,174 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.090, Train_accy 96.16
Threshold:  0.9828
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 32/768 type remove
Layer 4 : 41/768 type remove
Layer 5 : 59/768 type remove
Layer 6 : 54/768 type remove
Layer 7 : 59/768 type remove
Layer 8 : 69/768 type remove
Layer 9 : 114/768 type remove
Layer 10 : 139/768 type remove
Layer 11 : 76/768 type remove
Layer 12 : 125/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:58:53,205 [trainer.py] => Time:246.7608413696289
1224 1224
1224 1224
2025-12-10 13:58:57,349 [trainer.py] => Time:4.143932819366455
2025-12-10 13:58:57,349 [inflora.py] => Exemplar size: 0
2025-12-10 13:58:57,349 [trainer.py] => CNN: {'total': np.float64(74.18), '00-03': np.float64(76.77), '04-07': np.float64(79.31), '08-11': np.float64(76.32), '12-15': np.float64(83.64), '16-19': np.float64(76.19), '20-23': np.float64(74.43), '24-27': np.float64(78.05), '28-31': np.float64(56.54), 'old': np.float64(77.44), 'new': np.float64(56.54)}
2025-12-10 13:58:57,349 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18)]
2025-12-10 13:58:57,349 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81)]
2025-12-10 13:58:57,350 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843]
2025-12-10 13:59:02,593 [trainer.py] => All params: 126094051
2025-12-10 13:59:02,599 [trainer.py] => Trainable params: 187396
2025-12-10 13:59:02,599 [inflora.py] => Learning on 32-36
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight'}
2025-12-10 14:01:19,782 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.090, Train_accy 96.76
Threshold:  0.9832
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 33/768 type remove
Layer 4 : 42/768 type remove
Layer 5 : 60/768 type remove
Layer 6 : 55/768 type remove
Layer 7 : 61/768 type remove
Layer 8 : 71/768 type remove
Layer 9 : 116/768 type remove
Layer 10 : 141/768 type remove
Layer 11 : 78/768 type remove
Layer 12 : 129/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:01:27,109 [trainer.py] => Time:144.5099995136261
1299 1299
1299 1299
2025-12-10 14:01:31,470 [trainer.py] => Time:4.360681772232056
2025-12-10 14:01:31,470 [inflora.py] => Exemplar size: 0
2025-12-10 14:01:31,470 [trainer.py] => CNN: {'total': np.float64(73.75), '00-03': np.float64(73.55), '04-07': np.float64(79.31), '08-11': np.float64(78.07), '12-15': np.float64(82.73), '16-19': np.float64(73.81), '20-23': np.float64(74.89), '24-27': np.float64(79.27), '28-31': np.float64(55.5), '32-35': np.float64(74.67), 'old': np.float64(73.69), 'new': np.float64(74.67)}
2025-12-10 14:01:31,470 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75)]
2025-12-10 14:01:31,470 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92)]
2025-12-10 14:01:31,470 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554]
2025-12-10 14:01:35,795 [trainer.py] => All params: 126094051
2025-12-10 14:01:35,801 [trainer.py] => Trainable params: 187396
2025-12-10 14:01:35,801 [inflora.py] => Learning on 36-40
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight'}
2025-12-10 14:04:20,636 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.083, Train_accy 96.61
Threshold:  0.9836
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 34/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 62/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 63/768 type remove
Layer 8 : 73/768 type remove
Layer 9 : 119/768 type remove
Layer 10 : 145/768 type remove
Layer 11 : 83/768 type remove
Layer 12 : 135/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:04:28,415 [trainer.py] => Time:172.61381483078003
1400 1400
1400 1400
2025-12-10 14:04:32,994 [trainer.py] => Time:4.578277826309204
2025-12-10 14:04:32,994 [inflora.py] => Exemplar size: 0
2025-12-10 14:04:32,994 [trainer.py] => CNN: {'total': np.float64(73.93), '00-03': np.float64(70.97), '04-07': np.float64(79.31), '08-11': np.float64(77.19), '12-15': np.float64(83.64), '16-19': np.float64(72.22), '20-23': np.float64(74.43), '24-27': np.float64(79.27), '28-31': np.float64(57.07), '32-35': np.float64(73.33), '36-39': np.float64(81.19), 'old': np.float64(73.36), 'new': np.float64(81.19)}
2025-12-10 14:04:32,994 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93)]
2025-12-10 14:04:32,994 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14)]
2025-12-10 14:04:32,994 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429]
2025-12-10 14:04:39,419 [trainer.py] => All params: 126094051
2025-12-10 14:04:39,425 [trainer.py] => Trainable params: 187396
2025-12-10 14:04:39,425 [inflora.py] => Learning on 40-44
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight'}
2025-12-10 14:06:53,275 [inflora.py] => Task 10, Epoch 50/50 => Loss 0.180, Train_accy 92.86
Threshold:  0.984
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 35/768 type remove
Layer 4 : 46/768 type remove
Layer 5 : 66/768 type remove
Layer 6 : 61/768 type remove
Layer 7 : 67/768 type remove
Layer 8 : 77/768 type remove
Layer 9 : 124/768 type remove
Layer 10 : 150/768 type remove
Layer 11 : 90/768 type remove
Layer 12 : 140/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:07:00,321 [trainer.py] => Time:140.8959481716156
1486 1486
1486 1486
2025-12-10 14:07:05,137 [trainer.py] => Time:4.815977334976196
2025-12-10 14:07:05,137 [inflora.py] => Exemplar size: 0
2025-12-10 14:07:05,138 [trainer.py] => CNN: {'total': np.float64(72.41), '00-03': np.float64(72.26), '04-07': np.float64(78.62), '08-11': np.float64(76.32), '12-15': np.float64(84.55), '16-19': np.float64(73.81), '20-23': np.float64(72.6), '24-27': np.float64(79.88), '28-31': np.float64(52.88), '32-35': np.float64(74.67), '36-39': np.float64(79.21), '40-43': np.float64(58.14), 'old': np.float64(73.29), 'new': np.float64(58.14)}
2025-12-10 14:07:05,138 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41)]
2025-12-10 14:07:05,138 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19)]
2025-12-10 14:07:05,138 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992]
2025-12-10 14:07:09,274 [trainer.py] => All params: 126094051
2025-12-10 14:07:09,280 [trainer.py] => Trainable params: 187396
2025-12-10 14:07:09,280 [inflora.py] => Learning on 44-48
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight'}
2025-12-10 14:09:16,180 [inflora.py] => Task 11, Epoch 50/50 => Loss 0.152, Train_accy 93.99
Threshold:  0.9843999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 36/768 type remove
Layer 4 : 48/768 type remove
Layer 5 : 69/768 type remove
Layer 6 : 66/768 type remove
Layer 7 : 71/768 type remove
Layer 8 : 81/768 type remove
Layer 9 : 130/768 type remove
Layer 10 : 155/768 type remove
Layer 11 : 95/768 type remove
Layer 12 : 145/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:09:22,869 [trainer.py] => Time:133.58837890625
1540 1540
1540 1540
2025-12-10 14:09:27,872 [trainer.py] => Time:5.003034830093384
2025-12-10 14:09:27,872 [inflora.py] => Exemplar size: 0
2025-12-10 14:09:27,872 [trainer.py] => CNN: {'total': np.float64(70.52), '00-03': np.float64(70.32), '04-07': np.float64(78.62), '08-11': np.float64(74.56), '12-15': np.float64(84.55), '16-19': np.float64(72.22), '20-23': np.float64(71.69), '24-27': np.float64(78.66), '28-31': np.float64(52.36), '32-35': np.float64(73.33), '36-39': np.float64(76.24), '40-43': np.float64(59.3), '44-47': np.float64(46.3), 'old': np.float64(71.4), 'new': np.float64(46.3)}
2025-12-10 14:09:27,872 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52)]
2025-12-10 14:09:27,872 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82)]
2025-12-10 14:09:27,872 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974]
2025-12-10 14:09:30,847 [trainer.py] => All params: 126094051
2025-12-10 14:09:30,853 [trainer.py] => Trainable params: 187396
2025-12-10 14:09:30,853 [inflora.py] => Learning on 48-52
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight'}
2025-12-10 14:12:23,446 [inflora.py] => Task 12, Epoch 50/50 => Loss 0.060, Train_accy 98.23
Threshold:  0.9848
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 37/768 type remove
Layer 4 : 51/768 type remove
Layer 5 : 73/768 type remove
Layer 6 : 69/768 type remove
Layer 7 : 75/768 type remove
Layer 8 : 84/768 type remove
Layer 9 : 134/768 type remove
Layer 10 : 159/768 type remove
Layer 11 : 100/768 type remove
Layer 12 : 150/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:12:30,845 [trainer.py] => Time:179.99116849899292
1671 1671
1671 1671
2025-12-10 14:12:36,184 [trainer.py] => Time:5.33948540687561
2025-12-10 14:12:36,185 [inflora.py] => Exemplar size: 0
2025-12-10 14:12:36,185 [trainer.py] => CNN: {'total': np.float64(70.38), '00-03': np.float64(72.26), '04-07': np.float64(75.86), '08-11': np.float64(70.18), '12-15': np.float64(83.64), '16-19': np.float64(69.84), '20-23': np.float64(71.23), '24-27': np.float64(81.1), '28-31': np.float64(49.74), '32-35': np.float64(77.33), '36-39': np.float64(72.28), '40-43': np.float64(59.3), '44-47': np.float64(51.85), '48-51': np.float64(76.34), 'old': np.float64(69.87), 'new': np.float64(76.34)}
2025-12-10 14:12:36,185 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38)]
2025-12-10 14:12:36,185 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22)]
2025-12-10 14:12:36,185 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024]
2025-12-10 14:12:39,356 [trainer.py] => All params: 126094051
2025-12-10 14:12:39,362 [trainer.py] => Trainable params: 187396
2025-12-10 14:12:39,362 [inflora.py] => Learning on 52-56
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight'}
2025-12-10 14:14:54,506 [inflora.py] => Task 13, Epoch 50/50 => Loss 0.126, Train_accy 96.75
Threshold:  0.9852
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 38/768 type remove
Layer 4 : 54/768 type remove
Layer 5 : 77/768 type remove
Layer 6 : 73/768 type remove
Layer 7 : 80/768 type remove
Layer 8 : 89/768 type remove
Layer 9 : 139/768 type remove
Layer 10 : 162/768 type remove
Layer 11 : 105/768 type remove
Layer 12 : 154/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:15:01,773 [trainer.py] => Time:142.41138005256653
1732 1732
1732 1732
2025-12-10 14:15:07,234 [trainer.py] => Time:5.460822343826294
2025-12-10 14:15:07,235 [inflora.py] => Exemplar size: 0
2025-12-10 14:15:07,235 [trainer.py] => CNN: {'total': np.float64(69.75), '00-03': np.float64(69.03), '04-07': np.float64(76.55), '08-11': np.float64(67.54), '12-15': np.float64(83.64), '16-19': np.float64(72.22), '20-23': np.float64(71.23), '24-27': np.float64(81.1), '28-31': np.float64(50.26), '32-35': np.float64(78.67), '36-39': np.float64(69.31), '40-43': np.float64(60.47), '44-47': np.float64(42.59), '48-51': np.float64(77.86), '52-55': np.float64(63.93), 'old': np.float64(69.96), 'new': np.float64(63.93)}
2025-12-10 14:15:07,235 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75)]
2025-12-10 14:15:07,235 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21)]
2025-12-10 14:15:07,235 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656]
2025-12-10 14:15:13,359 [trainer.py] => All params: 126094051
2025-12-10 14:15:13,365 [trainer.py] => Trainable params: 187396
2025-12-10 14:15:13,366 [inflora.py] => Learning on 56-60
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight'}
2025-12-10 14:17:55,869 [inflora.py] => Task 14, Epoch 50/50 => Loss 0.116, Train_accy 95.73
Threshold:  0.9856
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 39/768 type remove
Layer 4 : 55/768 type remove
Layer 5 : 79/768 type remove
Layer 6 : 75/768 type remove
Layer 7 : 83/768 type remove
Layer 8 : 92/768 type remove
Layer 9 : 143/768 type remove
Layer 10 : 166/768 type remove
Layer 11 : 109/768 type remove
Layer 12 : 157/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:18:03,633 [trainer.py] => Time:170.26692152023315
1827 1827
1827 1827
2025-12-10 14:18:09,358 [trainer.py] => Time:5.724216938018799
2025-12-10 14:18:09,358 [inflora.py] => Exemplar size: 0
2025-12-10 14:18:09,358 [trainer.py] => CNN: {'total': np.float64(68.86), '00-03': np.float64(66.45), '04-07': np.float64(75.86), '08-11': np.float64(69.3), '12-15': np.float64(85.45), '16-19': np.float64(73.81), '20-23': np.float64(70.78), '24-27': np.float64(82.93), '28-31': np.float64(50.26), '32-35': np.float64(70.67), '36-39': np.float64(70.3), '40-43': np.float64(62.79), '44-47': np.float64(48.15), '48-51': np.float64(77.86), '52-55': np.float64(59.02), '56-59': np.float64(52.63), 'old': np.float64(69.75), 'new': np.float64(52.63)}
2025-12-10 14:18:09,358 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86)]
2025-12-10 14:18:09,358 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17)]
2025-12-10 14:18:09,358 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142]
2025-12-10 14:18:15,386 [trainer.py] => All params: 126094051
2025-12-10 14:18:15,392 [trainer.py] => Trainable params: 187396
2025-12-10 14:18:15,392 [inflora.py] => Learning on 60-64
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight'}
2025-12-10 14:21:44,612 [inflora.py] => Task 15, Epoch 50/50 => Loss 0.051, Train_accy 98.63
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 40/768 type remove
Layer 4 : 57/768 type remove
Layer 5 : 82/768 type remove
Layer 6 : 78/768 type remove
Layer 7 : 87/768 type remove
Layer 8 : 96/768 type remove
Layer 9 : 148/768 type remove
Layer 10 : 171/768 type remove
Layer 11 : 114/768 type remove
Layer 12 : 160/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:21:53,417 [trainer.py] => Time:218.0253164768219
1974 1974
1974 1974
2025-12-10 14:21:59,526 [trainer.py] => Time:6.108584403991699
2025-12-10 14:21:59,526 [inflora.py] => Exemplar size: 0
2025-12-10 14:21:59,526 [trainer.py] => CNN: {'total': np.float64(68.84), '00-03': np.float64(68.39), '04-07': np.float64(74.48), '08-11': np.float64(71.93), '12-15': np.float64(83.64), '16-19': np.float64(72.22), '20-23': np.float64(71.69), '24-27': np.float64(82.32), '28-31': np.float64(48.69), '32-35': np.float64(70.67), '36-39': np.float64(67.33), '40-43': np.float64(63.95), '44-47': np.float64(42.59), '48-51': np.float64(77.1), '52-55': np.float64(42.62), '56-59': np.float64(46.32), '60-63': np.float64(85.03), 'old': np.float64(67.54), 'new': np.float64(85.03)}
2025-12-10 14:21:59,526 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84)]
2025-12-10 14:21:59,526 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45)]
2025-12-10 14:21:59,526 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796]
2025-12-10 14:22:03,076 [trainer.py] => All params: 126094051
2025-12-10 14:22:03,082 [trainer.py] => Trainable params: 187396
2025-12-10 14:22:03,082 [inflora.py] => Learning on 64-68
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight'}
2025-12-10 14:23:57,500 [inflora.py] => Task 16, Epoch 50/50 => Loss 0.128, Train_accy 95.48
Threshold:  0.9863999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 41/768 type remove
Layer 4 : 59/768 type remove
Layer 5 : 85/768 type remove
Layer 6 : 81/768 type remove
Layer 7 : 92/768 type remove
Layer 8 : 101/768 type remove
Layer 9 : 154/768 type remove
Layer 10 : 176/768 type remove
Layer 11 : 118/768 type remove
Layer 12 : 164/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:24:04,178 [trainer.py] => Time:121.09591460227966
2023 2023
2023 2023
2025-12-10 14:24:10,395 [trainer.py] => Time:6.216400861740112
2025-12-10 14:24:10,395 [inflora.py] => Exemplar size: 0
2025-12-10 14:24:10,395 [trainer.py] => CNN: {'total': np.float64(67.33), '00-03': np.float64(65.16), '04-07': np.float64(72.41), '08-11': np.float64(71.93), '12-15': np.float64(83.64), '16-19': np.float64(71.43), '20-23': np.float64(69.86), '24-27': np.float64(83.54), '28-31': np.float64(47.64), '32-35': np.float64(70.67), '36-39': np.float64(69.31), '40-43': np.float64(62.79), '44-47': np.float64(46.3), '48-51': np.float64(69.47), '52-55': np.float64(42.62), '56-59': np.float64(43.16), '60-63': np.float64(84.35), '64-67': np.float64(55.1), 'old': np.float64(67.63), 'new': np.float64(55.1)}
2025-12-10 14:24:10,395 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33)]
2025-12-10 14:24:10,395 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84)]
2025-12-10 14:24:10,395 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413]
2025-12-10 14:24:13,836 [trainer.py] => All params: 126094051
2025-12-10 14:24:13,842 [trainer.py] => Trainable params: 187396
2025-12-10 14:24:13,842 [inflora.py] => Learning on 68-72
Parameters to be updated: {'classifier_pool.17.bias', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight'}
2025-12-10 14:26:44,127 [inflora.py] => Task 17, Epoch 50/50 => Loss 0.119, Train_accy 95.41
Threshold:  0.9868
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 90/768 type remove
Layer 6 : 85/768 type remove
Layer 7 : 96/768 type remove
Layer 8 : 107/768 type remove
Layer 9 : 164/768 type remove
Layer 10 : 188/768 type remove
Layer 11 : 126/768 type remove
Layer 12 : 170/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:26:51,362 [trainer.py] => Time:157.52007269859314
2120 2120
2120 2120
2025-12-10 14:26:57,802 [trainer.py] => Time:6.4391584396362305
2025-12-10 14:26:57,802 [inflora.py] => Exemplar size: 0
2025-12-10 14:26:57,802 [trainer.py] => CNN: {'total': np.float64(66.13), '00-03': np.float64(61.94), '04-07': np.float64(73.1), '08-11': np.float64(70.18), '12-15': np.float64(84.55), '16-19': np.float64(71.43), '20-23': np.float64(70.32), '24-27': np.float64(81.71), '28-31': np.float64(47.64), '32-35': np.float64(65.33), '36-39': np.float64(66.34), '40-43': np.float64(65.12), '44-47': np.float64(38.89), '48-51': np.float64(72.52), '52-55': np.float64(40.98), '56-59': np.float64(48.42), '60-63': np.float64(81.63), '64-67': np.float64(55.1), '68-71': np.float64(53.61), 'old': np.float64(66.73), 'new': np.float64(53.61)}
2025-12-10 14:26:57,802 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13)]
2025-12-10 14:26:57,802 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17)]
2025-12-10 14:26:57,802 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679]
2025-12-10 14:26:59,192 [trainer.py] => All params: 126094051
2025-12-10 14:26:59,198 [trainer.py] => Trainable params: 187396
2025-12-10 14:26:59,199 [inflora.py] => Learning on 72-76
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight'}
2025-12-10 14:30:21,542 [inflora.py] => Task 18, Epoch 50/50 => Loss 0.060, Train_accy 97.83
Threshold:  0.9872
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 23/768 type remove
Layer 3 : 47/768 type remove
Layer 4 : 73/768 type remove
Layer 5 : 97/768 type remove
Layer 6 : 91/768 type remove
Layer 7 : 106/768 type remove
Layer 8 : 120/768 type remove
Layer 9 : 190/768 type remove
Layer 10 : 213/768 type remove
Layer 11 : 147/768 type remove
Layer 12 : 192/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:30:29,951 [trainer.py] => Time:210.75281167030334
2238 2238
2238 2238
2025-12-10 14:30:36,743 [trainer.py] => Time:6.791302442550659
2025-12-10 14:30:36,743 [inflora.py] => Exemplar size: 0
2025-12-10 14:30:36,744 [trainer.py] => CNN: {'total': np.float64(63.18), '00-03': np.float64(57.42), '04-07': np.float64(70.34), '08-11': np.float64(62.28), '12-15': np.float64(79.09), '16-19': np.float64(70.63), '20-23': np.float64(69.41), '24-27': np.float64(78.66), '28-31': np.float64(45.03), '32-35': np.float64(68.0), '36-39': np.float64(65.35), '40-43': np.float64(59.3), '44-47': np.float64(38.89), '48-51': np.float64(58.02), '52-55': np.float64(39.34), '56-59': np.float64(46.32), '60-63': np.float64(80.27), '64-67': np.float64(51.02), '68-71': np.float64(52.58), '72-75': np.float64(69.49), 'old': np.float64(62.83), 'new': np.float64(69.49)}
2025-12-10 14:30:36,744 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18)]
2025-12-10 14:30:36,744 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91)]
2025-12-10 14:30:36,744 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196]
2025-12-10 14:30:41,836 [trainer.py] => All params: 126094051
2025-12-10 14:30:41,842 [trainer.py] => Trainable params: 187396
2025-12-10 14:30:41,843 [inflora.py] => Learning on 76-80
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight'}
2025-12-10 14:34:27,022 [inflora.py] => Task 19, Epoch 50/50 => Loss 0.076, Train_accy 97.46
Threshold:  0.9876
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 75/768 type remove
Layer 5 : 99/768 type remove
Layer 6 : 93/768 type remove
Layer 7 : 108/768 type remove
Layer 8 : 122/768 type remove
Layer 9 : 194/768 type remove
Layer 10 : 219/768 type remove
Layer 11 : 150/768 type remove
Layer 12 : 196/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:34:35,448 [trainer.py] => Time:233.60500359535217
2403 2403
2403 2403
2025-12-10 14:34:42,703 [trainer.py] => Time:7.254361629486084
2025-12-10 14:34:42,703 [inflora.py] => Exemplar size: 0
2025-12-10 14:34:42,703 [trainer.py] => CNN: {'total': np.float64(62.59), '00-03': np.float64(55.48), '04-07': np.float64(71.03), '08-11': np.float64(57.89), '12-15': np.float64(80.0), '16-19': np.float64(73.02), '20-23': np.float64(68.49), '24-27': np.float64(78.05), '28-31': np.float64(48.69), '32-35': np.float64(65.33), '36-39': np.float64(67.33), '40-43': np.float64(52.33), '44-47': np.float64(40.74), '48-51': np.float64(45.8), '52-55': np.float64(42.62), '56-59': np.float64(46.32), '60-63': np.float64(74.15), '64-67': np.float64(53.06), '68-71': np.float64(53.61), '72-75': np.float64(63.56), '76-79': np.float64(73.94), 'old': np.float64(61.75), 'new': np.float64(73.94)}
2025-12-10 14:34:42,703 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59)]
2025-12-10 14:34:42,703 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22)]
2025-12-10 14:34:42,703 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873]
2025-12-10 14:34:45,819 [trainer.py] => All params: 126094051
2025-12-10 14:34:45,825 [trainer.py] => Trainable params: 187396
2025-12-10 14:34:45,825 [inflora.py] => Learning on 80-84
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight'}
2025-12-10 14:38:51,860 [inflora.py] => Task 20, Epoch 50/50 => Loss 0.093, Train_accy 96.19
Threshold:  0.988
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 51/768 type remove
Layer 4 : 81/768 type remove
Layer 5 : 105/768 type remove
Layer 6 : 98/768 type remove
Layer 7 : 114/768 type remove
Layer 8 : 130/768 type remove
Layer 9 : 205/768 type remove
Layer 10 : 233/768 type remove
Layer 11 : 160/768 type remove
Layer 12 : 218/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:39:01,111 [trainer.py] => Time:255.28553223609924
2593 2593
2593 2593
2025-12-10 14:39:08,888 [trainer.py] => Time:7.77740216255188
2025-12-10 14:39:08,889 [inflora.py] => Exemplar size: 0
2025-12-10 14:39:08,889 [trainer.py] => CNN: {'total': np.float64(64.87), '00-03': np.float64(60.65), '04-07': np.float64(70.34), '08-11': np.float64(60.53), '12-15': np.float64(81.82), '16-19': np.float64(76.19), '20-23': np.float64(73.52), '24-27': np.float64(79.88), '28-31': np.float64(53.4), '32-35': np.float64(69.33), '36-39': np.float64(70.3), '40-43': np.float64(50.0), '44-47': np.float64(38.89), '48-51': np.float64(53.44), '52-55': np.float64(47.54), '56-59': np.float64(49.47), '60-63': np.float64(74.15), '64-67': np.float64(46.94), '68-71': np.float64(54.64), '72-75': np.float64(61.86), '76-79': np.float64(70.91), '80-83': np.float64(67.89), 'old': np.float64(64.63), 'new': np.float64(67.89)}
2025-12-10 14:39:08,890 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87)]
2025-12-10 14:39:08,890 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17)]
2025-12-10 14:39:08,890 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994]
2025-12-10 14:39:12,134 [trainer.py] => All params: 126094051
2025-12-10 14:39:12,140 [trainer.py] => Trainable params: 187396
2025-12-10 14:39:12,140 [inflora.py] => Learning on 84-88
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight'}
2025-12-10 14:42:48,762 [inflora.py] => Task 21, Epoch 50/50 => Loss 0.106, Train_accy 96.36
Threshold:  0.9884
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 53/768 type remove
Layer 4 : 83/768 type remove
Layer 5 : 110/768 type remove
Layer 6 : 105/768 type remove
Layer 7 : 121/768 type remove
Layer 8 : 138/768 type remove
Layer 9 : 218/768 type remove
Layer 10 : 249/768 type remove
Layer 11 : 167/768 type remove
Layer 12 : 222/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:42:57,514 [trainer.py] => Time:225.37397170066833
2726 2726
2726 2726
2025-12-10 14:43:05,638 [trainer.py] => Time:8.123523712158203
2025-12-10 14:43:05,638 [inflora.py] => Exemplar size: 0
2025-12-10 14:43:05,639 [trainer.py] => CNN: {'total': np.float64(65.41), '00-03': np.float64(65.16), '04-07': np.float64(69.66), '08-11': np.float64(62.28), '12-15': np.float64(82.73), '16-19': np.float64(79.37), '20-23': np.float64(75.34), '24-27': np.float64(80.49), '28-31': np.float64(51.31), '32-35': np.float64(72.0), '36-39': np.float64(71.29), '40-43': np.float64(51.16), '44-47': np.float64(42.59), '48-51': np.float64(51.15), '52-55': np.float64(47.54), '56-59': np.float64(47.37), '60-63': np.float64(71.43), '64-67': np.float64(48.98), '68-71': np.float64(54.64), '72-75': np.float64(61.02), '76-79': np.float64(70.3), '80-83': np.float64(71.58), '84-87': np.float64(63.16), 'old': np.float64(65.52), 'new': np.float64(63.16)}
2025-12-10 14:43:05,639 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41)]
2025-12-10 14:43:05,639 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41)]
2025-12-10 14:43:05,639 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086]
2025-12-10 14:43:09,606 [trainer.py] => All params: 126094051
2025-12-10 14:43:09,613 [trainer.py] => Trainable params: 187396
2025-12-10 14:43:09,613 [inflora.py] => Learning on 88-92
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight'}
2025-12-10 14:45:50,730 [inflora.py] => Task 22, Epoch 50/50 => Loss 0.104, Train_accy 97.72
Threshold:  0.9888
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 55/768 type remove
Layer 4 : 88/768 type remove
Layer 5 : 117/768 type remove
Layer 6 : 114/768 type remove
Layer 7 : 134/768 type remove
Layer 8 : 152/768 type remove
Layer 9 : 234/768 type remove
Layer 10 : 259/768 type remove
Layer 11 : 175/768 type remove
Layer 12 : 232/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:45:58,528 [trainer.py] => Time:168.9156665802002
2826 2826
2826 2826
2025-12-10 14:46:06,882 [trainer.py] => Time:8.35288381576538
2025-12-10 14:46:06,882 [inflora.py] => Exemplar size: 0
2025-12-10 14:46:06,882 [trainer.py] => CNN: {'total': np.float64(65.32), '00-03': np.float64(64.52), '04-07': np.float64(66.21), '08-11': np.float64(63.16), '12-15': np.float64(82.73), '16-19': np.float64(79.37), '20-23': np.float64(74.89), '24-27': np.float64(80.49), '28-31': np.float64(50.79), '32-35': np.float64(70.67), '36-39': np.float64(74.26), '40-43': np.float64(55.81), '44-47': np.float64(38.89), '48-51': np.float64(57.25), '52-55': np.float64(50.82), '56-59': np.float64(47.37), '60-63': np.float64(70.75), '64-67': np.float64(46.94), '68-71': np.float64(51.55), '72-75': np.float64(57.63), '76-79': np.float64(65.45), '80-83': np.float64(66.84), '84-87': np.float64(63.16), '88-91': np.float64(82.0), 'old': np.float64(64.71), 'new': np.float64(82.0)}
2025-12-10 14:46:06,882 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32)]
2025-12-10 14:46:06,882 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46)]
2025-12-10 14:46:06,882 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602]
2025-12-10 14:46:10,199 [trainer.py] => All params: 126094051
2025-12-10 14:46:10,205 [trainer.py] => Trainable params: 187396
2025-12-10 14:46:10,205 [inflora.py] => Learning on 92-96
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight'}
2025-12-10 14:49:05,660 [inflora.py] => Task 23, Epoch 50/50 => Loss 0.122, Train_accy 95.51
Threshold:  0.9892
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 27/768 type remove
Layer 3 : 56/768 type remove
Layer 4 : 89/768 type remove
Layer 5 : 119/768 type remove
Layer 6 : 120/768 type remove
Layer 7 : 140/768 type remove
Layer 8 : 159/768 type remove
Layer 9 : 246/768 type remove
Layer 10 : 269/768 type remove
Layer 11 : 184/768 type remove
Layer 12 : 249/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:49:13,573 [trainer.py] => Time:183.36785435676575
2946 2946
2946 2946
2025-12-10 14:49:22,226 [trainer.py] => Time:8.652782678604126
2025-12-10 14:49:22,227 [inflora.py] => Exemplar size: 0
2025-12-10 14:49:22,227 [trainer.py] => CNN: {'total': np.float64(64.09), '00-03': np.float64(63.87), '04-07': np.float64(65.52), '08-11': np.float64(64.04), '12-15': np.float64(80.91), '16-19': np.float64(76.98), '20-23': np.float64(72.6), '24-27': np.float64(81.1), '28-31': np.float64(53.4), '32-35': np.float64(72.0), '36-39': np.float64(75.25), '40-43': np.float64(55.81), '44-47': np.float64(37.04), '48-51': np.float64(55.73), '52-55': np.float64(47.54), '56-59': np.float64(47.37), '60-63': np.float64(70.07), '64-67': np.float64(48.98), '68-71': np.float64(50.52), '72-75': np.float64(61.02), '76-79': np.float64(60.0), '80-83': np.float64(65.79), '84-87': np.float64(58.65), '88-91': np.float64(79.0), '92-95': np.float64(55.83), 'old': np.float64(64.44), 'new': np.float64(55.83)}
2025-12-10 14:49:22,227 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09)]
2025-12-10 14:49:22,227 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16)]
2025-12-10 14:49:22,227 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969]
2025-12-10 14:49:27,552 [trainer.py] => All params: 126094051
2025-12-10 14:49:27,558 [trainer.py] => Trainable params: 187396
2025-12-10 14:49:27,558 [inflora.py] => Learning on 96-100
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight'}
2025-12-10 14:53:19,387 [inflora.py] => Task 24, Epoch 50/50 => Loss 0.177, Train_accy 94.46
Threshold:  0.9896
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 58/768 type remove
Layer 4 : 92/768 type remove
Layer 5 : 123/768 type remove
Layer 6 : 124/768 type remove
Layer 7 : 144/768 type remove
Layer 8 : 163/768 type remove
Layer 9 : 253/768 type remove
Layer 10 : 281/768 type remove
Layer 11 : 197/768 type remove
Layer 12 : 268/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:53:28,485 [trainer.py] => Time:240.9267497062683
3119 3119
3119 3119
2025-12-10 14:53:37,651 [trainer.py] => Time:9.165284156799316
2025-12-10 14:53:37,651 [inflora.py] => Exemplar size: 0
2025-12-10 14:53:37,651 [trainer.py] => CNN: {'total': np.float64(63.16), '00-03': np.float64(64.52), '04-07': np.float64(65.52), '08-11': np.float64(64.91), '12-15': np.float64(83.64), '16-19': np.float64(78.57), '20-23': np.float64(72.15), '24-27': np.float64(81.71), '28-31': np.float64(50.26), '32-35': np.float64(74.67), '36-39': np.float64(73.27), '40-43': np.float64(56.98), '44-47': np.float64(35.19), '48-51': np.float64(42.75), '52-55': np.float64(49.18), '56-59': np.float64(48.42), '60-63': np.float64(69.39), '64-67': np.float64(51.02), '68-71': np.float64(49.48), '72-75': np.float64(63.56), '76-79': np.float64(64.24), '80-83': np.float64(67.89), '84-87': np.float64(53.38), '88-91': np.float64(76.0), '92-95': np.float64(53.33), '96-99': np.float64(55.49), 'old': np.float64(63.61), 'new': np.float64(55.49)}
2025-12-10 14:53:37,651 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16)]
2025-12-10 14:53:37,651 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18)]
2025-12-10 14:53:37,651 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797]
2025-12-10 14:53:43,309 [trainer.py] => All params: 126094051
2025-12-10 14:53:43,316 [trainer.py] => Trainable params: 187396
2025-12-10 14:53:43,316 [inflora.py] => Learning on 100-104
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'classifier_pool.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight'}
2025-12-10 14:56:40,400 [inflora.py] => Task 25, Epoch 50/50 => Loss 0.076, Train_accy 97.61
Threshold:  0.99
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 29/768 type remove
Layer 3 : 59/768 type remove
Layer 4 : 94/768 type remove
Layer 5 : 126/768 type remove
Layer 6 : 129/768 type remove
Layer 7 : 149/768 type remove
Layer 8 : 168/768 type remove
Layer 9 : 257/768 type remove
Layer 10 : 286/768 type remove
Layer 11 : 203/768 type remove
Layer 12 : 277/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:56:48,638 [trainer.py] => Time:185.32258105278015
3237 3237
3237 3237
2025-12-10 14:56:58,115 [trainer.py] => Time:9.476488590240479
2025-12-10 14:56:58,116 [inflora.py] => Exemplar size: 0
2025-12-10 14:56:58,116 [trainer.py] => CNN: {'total': np.float64(63.05), '00-03': np.float64(63.87), '04-07': np.float64(65.52), '08-11': np.float64(66.67), '12-15': np.float64(80.0), '16-19': np.float64(73.81), '20-23': np.float64(71.23), '24-27': np.float64(79.27), '28-31': np.float64(48.17), '32-35': np.float64(70.67), '36-39': np.float64(72.28), '40-43': np.float64(59.3), '44-47': np.float64(35.19), '48-51': np.float64(50.38), '52-55': np.float64(47.54), '56-59': np.float64(50.53), '60-63': np.float64(74.83), '64-67': np.float64(51.02), '68-71': np.float64(50.52), '72-75': np.float64(59.32), '76-79': np.float64(64.24), '80-83': np.float64(66.32), '84-87': np.float64(54.14), '88-91': np.float64(75.0), '92-95': np.float64(50.83), '96-99': np.float64(55.49), '100-103': np.float64(70.34), 'old': np.float64(62.78), 'new': np.float64(70.34)}
2025-12-10 14:56:58,116 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05)]
2025-12-10 14:56:58,116 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18)]
2025-12-10 14:56:58,116 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698]
2025-12-10 14:57:05,386 [trainer.py] => All params: 126094051
2025-12-10 14:57:05,393 [trainer.py] => Trainable params: 187396
2025-12-10 14:57:05,393 [inflora.py] => Learning on 104-108
Parameters to be updated: {'classifier_pool.26.bias', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight'}
2025-12-10 15:00:18,832 [inflora.py] => Task 26, Epoch 50/50 => Loss 0.273, Train_accy 88.52
Threshold:  0.9904
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 32/768 type remove
Layer 3 : 63/768 type remove
Layer 4 : 100/768 type remove
Layer 5 : 135/768 type remove
Layer 6 : 138/768 type remove
Layer 7 : 158/768 type remove
Layer 8 : 178/768 type remove
Layer 9 : 269/768 type remove
Layer 10 : 298/768 type remove
Layer 11 : 214/768 type remove
Layer 12 : 281/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:00:27,271 [trainer.py] => Time:201.87839722633362
3370 3370
3370 3370
2025-12-10 15:00:37,051 [trainer.py] => Time:9.779197216033936
2025-12-10 15:00:37,051 [inflora.py] => Exemplar size: 0
2025-12-10 15:00:37,051 [trainer.py] => CNN: {'total': np.float64(61.13), '00-03': np.float64(62.58), '04-07': np.float64(66.21), '08-11': np.float64(65.79), '12-15': np.float64(81.82), '16-19': np.float64(73.81), '20-23': np.float64(72.15), '24-27': np.float64(80.49), '28-31': np.float64(46.07), '32-35': np.float64(78.67), '36-39': np.float64(73.27), '40-43': np.float64(58.14), '44-47': np.float64(35.19), '48-51': np.float64(43.51), '52-55': np.float64(47.54), '56-59': np.float64(49.47), '60-63': np.float64(76.87), '64-67': np.float64(55.1), '68-71': np.float64(49.48), '72-75': np.float64(58.47), '76-79': np.float64(64.85), '80-83': np.float64(63.16), '84-87': np.float64(54.14), '88-91': np.float64(74.0), '92-95': np.float64(50.0), '96-99': np.float64(54.34), '100-103': np.float64(68.64), '104-107': np.float64(23.31), 'old': np.float64(62.68), 'new': np.float64(23.31)}
2025-12-10 15:00:37,051 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13)]
2025-12-10 15:00:37,052 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02)]
2025-12-10 15:00:37,052 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603]
2025-12-10 15:00:43,966 [trainer.py] => All params: 126094051
2025-12-10 15:00:43,972 [trainer.py] => Trainable params: 187396
2025-12-10 15:00:43,973 [inflora.py] => Learning on 108-112
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight'}
2025-12-10 15:04:03,588 [inflora.py] => Task 27, Epoch 50/50 => Loss 0.092, Train_accy 96.65
Threshold:  0.9908
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 102/768 type remove
Layer 5 : 140/768 type remove
Layer 6 : 146/768 type remove
Layer 7 : 168/768 type remove
Layer 8 : 192/768 type remove
Layer 9 : 290/768 type remove
Layer 10 : 320/768 type remove
Layer 11 : 230/768 type remove
Layer 12 : 306/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:04:11,809 [trainer.py] => Time:207.83652591705322
3497 3497
3497 3497
2025-12-10 15:04:21,922 [trainer.py] => Time:10.112943410873413
2025-12-10 15:04:21,923 [inflora.py] => Exemplar size: 0
2025-12-10 15:04:21,923 [trainer.py] => CNN: {'total': np.float64(60.54), '00-03': np.float64(61.94), '04-07': np.float64(64.14), '08-11': np.float64(64.91), '12-15': np.float64(81.82), '16-19': np.float64(73.02), '20-23': np.float64(71.23), '24-27': np.float64(78.05), '28-31': np.float64(42.41), '32-35': np.float64(70.67), '36-39': np.float64(75.25), '40-43': np.float64(52.33), '44-47': np.float64(35.19), '48-51': np.float64(49.62), '52-55': np.float64(54.1), '56-59': np.float64(50.53), '60-63': np.float64(74.83), '64-67': np.float64(61.22), '68-71': np.float64(47.42), '72-75': np.float64(54.24), '76-79': np.float64(63.64), '80-83': np.float64(57.89), '84-87': np.float64(51.13), '88-91': np.float64(74.0), '92-95': np.float64(50.83), '96-99': np.float64(56.07), '100-103': np.float64(68.64), '104-107': np.float64(28.57), '108-111': np.float64(66.14), 'old': np.float64(60.33), 'new': np.float64(66.14)}
2025-12-10 15:04:21,923 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54)]
2025-12-10 15:04:21,923 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76)]
2025-12-10 15:04:21,923 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867]
2025-12-10 15:04:27,224 [trainer.py] => All params: 126094051
2025-12-10 15:04:27,231 [trainer.py] => Trainable params: 187396
2025-12-10 15:04:27,231 [inflora.py] => Learning on 112-116
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight'}
2025-12-10 15:06:48,494 [inflora.py] => Task 28, Epoch 50/50 => Loss 0.051, Train_accy 98.48
Threshold:  0.9912
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 65/768 type remove
Layer 4 : 104/768 type remove
Layer 5 : 143/768 type remove
Layer 6 : 154/768 type remove
Layer 7 : 181/768 type remove
Layer 8 : 206/768 type remove
Layer 9 : 314/768 type remove
Layer 10 : 347/768 type remove
Layer 11 : 258/768 type remove
Layer 12 : 336/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:06:55,959 [trainer.py] => Time:148.72766995429993
3589 3589
3589 3589
2025-12-10 15:07:06,381 [trainer.py] => Time:10.42218279838562
2025-12-10 15:07:06,382 [inflora.py] => Exemplar size: 0
2025-12-10 15:07:06,382 [trainer.py] => CNN: {'total': np.float64(59.77), '00-03': np.float64(61.94), '04-07': np.float64(62.76), '08-11': np.float64(64.91), '12-15': np.float64(83.64), '16-19': np.float64(73.81), '20-23': np.float64(71.69), '24-27': np.float64(76.83), '28-31': np.float64(41.88), '32-35': np.float64(60.0), '36-39': np.float64(74.26), '40-43': np.float64(48.84), '44-47': np.float64(29.63), '48-51': np.float64(51.91), '52-55': np.float64(42.62), '56-59': np.float64(51.58), '60-63': np.float64(74.15), '64-67': np.float64(57.14), '68-71': np.float64(45.36), '72-75': np.float64(51.69), '76-79': np.float64(60.61), '80-83': np.float64(53.68), '84-87': np.float64(51.88), '88-91': np.float64(76.0), '92-95': np.float64(47.5), '96-99': np.float64(54.91), '100-103': np.float64(66.95), '104-107': np.float64(26.32), '108-111': np.float64(63.78), '112-115': np.float64(85.87), 'old': np.float64(59.08), 'new': np.float64(85.87)}
2025-12-10 15:07:06,382 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77)]
2025-12-10 15:07:06,382 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36)]
2025-12-10 15:07:06,382 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511]
2025-12-10 15:07:12,513 [trainer.py] => All params: 126094051
2025-12-10 15:07:12,519 [trainer.py] => Trainable params: 187396
2025-12-10 15:07:12,519 [inflora.py] => Learning on 116-120
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight'}
2025-12-10 15:09:44,785 [inflora.py] => Task 29, Epoch 50/50 => Loss 0.086, Train_accy 97.04
Threshold:  0.9916
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 67/768 type remove
Layer 4 : 107/768 type remove
Layer 5 : 148/768 type remove
Layer 6 : 161/768 type remove
Layer 7 : 189/768 type remove
Layer 8 : 215/768 type remove
Layer 9 : 332/768 type remove
Layer 10 : 379/768 type remove
Layer 11 : 289/768 type remove
Layer 12 : 361/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:09:52,249 [trainer.py] => Time:159.7301914691925
3677 3677
3677 3677
2025-12-10 15:10:02,856 [trainer.py] => Time:10.606635093688965
2025-12-10 15:10:02,857 [inflora.py] => Exemplar size: 0
2025-12-10 15:10:02,857 [trainer.py] => CNN: {'total': np.float64(59.91), '00-03': np.float64(61.29), '04-07': np.float64(62.07), '08-11': np.float64(65.79), '12-15': np.float64(82.73), '16-19': np.float64(74.6), '20-23': np.float64(70.32), '24-27': np.float64(78.66), '28-31': np.float64(39.27), '32-35': np.float64(68.0), '36-39': np.float64(74.26), '40-43': np.float64(51.16), '44-47': np.float64(31.48), '48-51': np.float64(47.33), '52-55': np.float64(45.9), '56-59': np.float64(51.58), '60-63': np.float64(73.47), '64-67': np.float64(55.1), '68-71': np.float64(45.36), '72-75': np.float64(52.54), '76-79': np.float64(59.39), '80-83': np.float64(49.47), '84-87': np.float64(51.88), '88-91': np.float64(75.0), '92-95': np.float64(48.33), '96-99': np.float64(56.65), '100-103': np.float64(65.25), '104-107': np.float64(26.32), '108-111': np.float64(60.63), '112-115': np.float64(86.96), '116-119': np.float64(81.82), 'old': np.float64(59.38), 'new': np.float64(81.82)}
2025-12-10 15:10:02,857 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91)]
2025-12-10 15:10:02,857 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76)]
2025-12-10 15:10:02,857 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096]
2025-12-10 15:10:07,750 [trainer.py] => All params: 126094051
2025-12-10 15:10:07,756 [trainer.py] => Trainable params: 187396
2025-12-10 15:10:07,756 [inflora.py] => Learning on 120-124
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight'}
2025-12-10 15:13:04,824 [inflora.py] => Task 30, Epoch 50/50 => Loss 0.104, Train_accy 96.82
Threshold:  0.992
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 35/768 type remove
Layer 3 : 68/768 type remove
Layer 4 : 111/768 type remove
Layer 5 : 153/768 type remove
Layer 6 : 170/768 type remove
Layer 7 : 201/768 type remove
Layer 8 : 230/768 type remove
Layer 9 : 352/768 type remove
Layer 10 : 367/768 type retain
Layer 11 : 305/768 type remove
Layer 12 : 341/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:13:12,890 [trainer.py] => Time:185.13427352905273
3794 3794
3794 3794
2025-12-10 15:13:23,890 [trainer.py] => Time:10.999740362167358
2025-12-10 15:13:23,891 [inflora.py] => Exemplar size: 0
2025-12-10 15:13:23,891 [trainer.py] => CNN: {'total': np.float64(59.62), '00-03': np.float64(61.29), '04-07': np.float64(66.21), '08-11': np.float64(65.79), '12-15': np.float64(81.82), '16-19': np.float64(74.6), '20-23': np.float64(68.95), '24-27': np.float64(74.39), '28-31': np.float64(38.74), '32-35': np.float64(66.67), '36-39': np.float64(73.27), '40-43': np.float64(51.16), '44-47': np.float64(33.33), '48-51': np.float64(53.44), '52-55': np.float64(47.54), '56-59': np.float64(52.63), '60-63': np.float64(73.47), '64-67': np.float64(59.18), '68-71': np.float64(41.24), '72-75': np.float64(45.76), '76-79': np.float64(56.97), '80-83': np.float64(45.26), '84-87': np.float64(51.13), '88-91': np.float64(77.0), '92-95': np.float64(48.33), '96-99': np.float64(50.87), '100-103': np.float64(66.95), '104-107': np.float64(33.08), '108-111': np.float64(62.2), '112-115': np.float64(86.96), '116-119': np.float64(78.41), '120-123': np.float64(65.81), 'old': np.float64(59.42), 'new': np.float64(65.81)}
2025-12-10 15:13:23,891 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62)]
2025-12-10 15:13:23,892 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01)]
2025-12-10 15:13:23,892 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839]
2025-12-10 15:13:29,397 [trainer.py] => All params: 126094051
2025-12-10 15:13:29,403 [trainer.py] => Trainable params: 187396
2025-12-10 15:13:29,404 [inflora.py] => Learning on 124-128
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'classifier_pool.31.bias', 'classifier_pool.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight'}
2025-12-10 15:16:32,069 [inflora.py] => Task 31, Epoch 50/50 => Loss 0.174, Train_accy 93.78
Threshold:  0.9924
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 70/768 type remove
Layer 4 : 113/768 type remove
Layer 5 : 155/768 type remove
Layer 6 : 172/768 type remove
Layer 7 : 205/768 type remove
Layer 8 : 233/768 type remove
Layer 9 : 355/768 type remove
Layer 10 : 361/768 type retain
Layer 11 : 321/768 type remove
Layer 12 : 288/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:16:40,466 [trainer.py] => Time:191.062753200531
3896 3896
3896 3896
2025-12-10 15:16:51,724 [trainer.py] => Time:11.257930994033813
2025-12-10 15:16:51,725 [inflora.py] => Exemplar size: 0
2025-12-10 15:16:51,725 [trainer.py] => CNN: {'total': np.float64(58.29), '00-03': np.float64(63.23), '04-07': np.float64(68.28), '08-11': np.float64(66.67), '12-15': np.float64(80.91), '16-19': np.float64(76.98), '20-23': np.float64(68.49), '24-27': np.float64(73.78), '28-31': np.float64(39.27), '32-35': np.float64(64.0), '36-39': np.float64(71.29), '40-43': np.float64(53.49), '44-47': np.float64(29.63), '48-51': np.float64(45.04), '52-55': np.float64(47.54), '56-59': np.float64(51.58), '60-63': np.float64(70.75), '64-67': np.float64(57.14), '68-71': np.float64(43.3), '72-75': np.float64(48.31), '76-79': np.float64(55.76), '80-83': np.float64(44.21), '84-87': np.float64(48.12), '88-91': np.float64(74.0), '92-95': np.float64(49.17), '96-99': np.float64(50.29), '100-103': np.float64(65.25), '104-107': np.float64(25.56), '108-111': np.float64(61.42), '112-115': np.float64(85.87), '116-119': np.float64(77.27), '120-123': np.float64(65.81), '124-127': np.float64(42.16), 'old': np.float64(58.72), 'new': np.float64(42.16)}
2025-12-10 15:16:51,725 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29)]
2025-12-10 15:16:51,725 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81)]
2025-12-10 15:16:51,725 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785]
2025-12-10 15:16:54,066 [trainer.py] => All params: 126094051
2025-12-10 15:16:54,072 [trainer.py] => Trainable params: 187396
2025-12-10 15:16:54,072 [inflora.py] => Learning on 128-132
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight'}
2025-12-10 15:19:38,999 [inflora.py] => Task 32, Epoch 50/50 => Loss 0.172, Train_accy 94.90
Threshold:  0.9928
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 38/768 type remove
Layer 3 : 73/768 type remove
Layer 4 : 119/768 type remove
Layer 5 : 161/768 type remove
Layer 6 : 179/768 type remove
Layer 7 : 213/768 type remove
Layer 8 : 242/768 type remove
Layer 9 : 362/768 type remove
Layer 10 : 344/768 type retain
Layer 11 : 343/768 type remove
Layer 12 : 265/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:19:47,007 [trainer.py] => Time:172.93468260765076
4020 4020
4020 4020
2025-12-10 15:19:58,530 [trainer.py] => Time:11.523183822631836
2025-12-10 15:19:58,531 [inflora.py] => Exemplar size: 0
2025-12-10 15:19:58,531 [trainer.py] => CNN: {'total': np.float64(58.41), '00-03': np.float64(65.16), '04-07': np.float64(66.21), '08-11': np.float64(68.42), '12-15': np.float64(80.0), '16-19': np.float64(74.6), '20-23': np.float64(70.32), '24-27': np.float64(73.17), '28-31': np.float64(41.88), '32-35': np.float64(65.33), '36-39': np.float64(73.27), '40-43': np.float64(51.16), '44-47': np.float64(35.19), '48-51': np.float64(43.51), '52-55': np.float64(44.26), '56-59': np.float64(52.63), '60-63': np.float64(70.75), '64-67': np.float64(55.1), '68-71': np.float64(43.3), '72-75': np.float64(46.61), '76-79': np.float64(55.15), '80-83': np.float64(49.47), '84-87': np.float64(45.86), '88-91': np.float64(74.0), '92-95': np.float64(50.0), '96-99': np.float64(49.13), '100-103': np.float64(63.56), '104-107': np.float64(28.57), '108-111': np.float64(61.42), '112-115': np.float64(85.87), '116-119': np.float64(77.27), '120-123': np.float64(62.39), '124-127': np.float64(41.18), '128-131': np.float64(57.26), 'old': np.float64(58.44), 'new': np.float64(57.26)}
2025-12-10 15:19:58,531 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41)]
2025-12-10 15:19:58,531 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87)]
2025-12-10 15:19:58,531 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865]
2025-12-10 15:20:00,916 [trainer.py] => All params: 126094051
2025-12-10 15:20:00,923 [trainer.py] => Trainable params: 187396
2025-12-10 15:20:00,923 [inflora.py] => Learning on 132-136
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight'}
2025-12-10 15:23:19,700 [inflora.py] => Task 33, Epoch 50/50 => Loss 0.092, Train_accy 97.38
Threshold:  0.9932
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 41/768 type remove
Layer 3 : 76/768 type remove
Layer 4 : 123/768 type remove
Layer 5 : 169/768 type remove
Layer 6 : 187/768 type remove
Layer 7 : 222/768 type remove
Layer 8 : 249/768 type remove
Layer 9 : 371/768 type remove
Layer 10 : 331/768 type retain
Layer 11 : 360/768 type remove
Layer 12 : 254/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:23:28,469 [trainer.py] => Time:207.54615783691406
4132 4132
4132 4132
2025-12-10 15:23:40,335 [trainer.py] => Time:11.86636209487915
2025-12-10 15:23:40,336 [inflora.py] => Exemplar size: 0
2025-12-10 15:23:40,336 [trainer.py] => CNN: {'total': np.float64(57.09), '00-03': np.float64(62.58), '04-07': np.float64(60.69), '08-11': np.float64(64.91), '12-15': np.float64(77.27), '16-19': np.float64(76.19), '20-23': np.float64(68.49), '24-27': np.float64(71.34), '28-31': np.float64(41.36), '32-35': np.float64(61.33), '36-39': np.float64(69.31), '40-43': np.float64(50.0), '44-47': np.float64(35.19), '48-51': np.float64(43.51), '52-55': np.float64(44.26), '56-59': np.float64(47.37), '60-63': np.float64(67.35), '64-67': np.float64(51.02), '68-71': np.float64(38.14), '72-75': np.float64(42.37), '76-79': np.float64(50.3), '80-83': np.float64(44.74), '84-87': np.float64(45.86), '88-91': np.float64(74.0), '92-95': np.float64(52.5), '96-99': np.float64(48.55), '100-103': np.float64(61.02), '104-107': np.float64(28.57), '108-111': np.float64(61.42), '112-115': np.float64(88.04), '116-119': np.float64(78.41), '120-123': np.float64(63.25), '124-127': np.float64(39.22), '128-131': np.float64(58.87), '132-135': np.float64(71.43), 'old': np.float64(56.69), 'new': np.float64(71.43)}
2025-12-10 15:23:40,336 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09)]
2025-12-10 15:23:40,336 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14)]
2025-12-10 15:23:40,336 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476]
2025-12-10 15:23:42,686 [trainer.py] => All params: 126094051
2025-12-10 15:23:42,692 [trainer.py] => Trainable params: 187396
2025-12-10 15:23:42,693 [inflora.py] => Learning on 136-140
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'classifier_pool.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight'}
2025-12-10 15:26:28,317 [inflora.py] => Task 34, Epoch 50/50 => Loss 0.075, Train_accy 97.60
Threshold:  0.9936
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 82/768 type remove
Layer 4 : 133/768 type remove
Layer 5 : 181/768 type remove
Layer 6 : 200/768 type remove
Layer 7 : 236/768 type remove
Layer 8 : 267/768 type remove
Layer 9 : 373/768 type retain
Layer 10 : 297/768 type retain
Layer 11 : 368/768 type retain
Layer 12 : 245/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:26:36,650 [trainer.py] => Time:173.95767641067505
4234 4234
4234 4234
2025-12-10 15:26:48,775 [trainer.py] => Time:12.124671936035156
2025-12-10 15:26:48,776 [inflora.py] => Exemplar size: 0
2025-12-10 15:26:48,776 [trainer.py] => CNN: {'total': np.float64(57.98), '00-03': np.float64(63.87), '04-07': np.float64(64.14), '08-11': np.float64(67.54), '12-15': np.float64(80.91), '16-19': np.float64(76.19), '20-23': np.float64(68.49), '24-27': np.float64(71.34), '28-31': np.float64(41.88), '32-35': np.float64(62.67), '36-39': np.float64(73.27), '40-43': np.float64(52.33), '44-47': np.float64(35.19), '48-51': np.float64(45.04), '52-55': np.float64(45.9), '56-59': np.float64(46.32), '60-63': np.float64(68.71), '64-67': np.float64(48.98), '68-71': np.float64(43.3), '72-75': np.float64(45.76), '76-79': np.float64(51.52), '80-83': np.float64(49.47), '84-87': np.float64(40.6), '88-91': np.float64(74.0), '92-95': np.float64(48.33), '96-99': np.float64(46.82), '100-103': np.float64(62.71), '104-107': np.float64(24.06), '108-111': np.float64(57.48), '112-115': np.float64(85.87), '116-119': np.float64(79.55), '120-123': np.float64(64.96), '124-127': np.float64(37.25), '128-131': np.float64(55.65), '132-135': np.float64(73.21), '136-139': np.float64(76.47), 'old': np.float64(57.53), 'new': np.float64(76.47)}
2025-12-10 15:26:48,776 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98)]
2025-12-10 15:26:48,776 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59)]
2025-12-10 15:26:48,776 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437]
2025-12-10 15:26:56,709 [trainer.py] => All params: 126094051
2025-12-10 15:26:56,715 [trainer.py] => Trainable params: 187396
2025-12-10 15:26:56,715 [inflora.py] => Learning on 140-144
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight'}
2025-12-10 15:29:24,130 [inflora.py] => Task 35, Epoch 50/50 => Loss 0.122, Train_accy 96.31
Threshold:  0.994
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 83/768 type remove
Layer 4 : 134/768 type remove
Layer 5 : 182/768 type remove
Layer 6 : 203/768 type remove
Layer 7 : 243/768 type remove
Layer 8 : 278/768 type remove
Layer 9 : 365/768 type retain
Layer 10 : 290/768 type retain
Layer 11 : 362/768 type retain
Layer 12 : 235/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:29:31,867 [trainer.py] => Time:155.15122509002686
4329 4329
4329 4329
2025-12-10 15:29:44,238 [trainer.py] => Time:12.37101697921753
2025-12-10 15:29:44,238 [inflora.py] => Exemplar size: 0
2025-12-10 15:29:44,238 [trainer.py] => CNN: {'total': np.float64(57.01), '00-03': np.float64(60.0), '04-07': np.float64(66.21), '08-11': np.float64(64.04), '12-15': np.float64(80.91), '16-19': np.float64(74.6), '20-23': np.float64(67.12), '24-27': np.float64(71.34), '28-31': np.float64(38.74), '32-35': np.float64(58.67), '36-39': np.float64(69.31), '40-43': np.float64(50.0), '44-47': np.float64(35.19), '48-51': np.float64(40.46), '52-55': np.float64(49.18), '56-59': np.float64(46.32), '60-63': np.float64(65.99), '64-67': np.float64(51.02), '68-71': np.float64(42.27), '72-75': np.float64(46.61), '76-79': np.float64(49.09), '80-83': np.float64(49.47), '84-87': np.float64(42.86), '88-91': np.float64(73.0), '92-95': np.float64(51.67), '96-99': np.float64(49.13), '100-103': np.float64(61.02), '104-107': np.float64(24.81), '108-111': np.float64(59.84), '112-115': np.float64(84.78), '116-119': np.float64(80.68), '120-123': np.float64(63.25), '124-127': np.float64(33.33), '128-131': np.float64(50.0), '132-135': np.float64(70.54), '136-139': np.float64(75.49), '140-143': np.float64(58.95), 'old': np.float64(56.97), 'new': np.float64(58.95)}
2025-12-10 15:29:44,238 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01)]
2025-12-10 15:29:44,239 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52)]
2025-12-10 15:29:44,239 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771]
2025-12-10 15:29:54,526 [trainer.py] => All params: 126094051
2025-12-10 15:29:54,533 [trainer.py] => Trainable params: 187396
2025-12-10 15:29:54,533 [inflora.py] => Learning on 144-148
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight'}
2025-12-10 15:32:14,643 [inflora.py] => Task 36, Epoch 50/50 => Loss 0.058, Train_accy 97.85
Threshold:  0.9944
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 84/768 type remove
Layer 4 : 135/768 type remove
Layer 5 : 183/768 type remove
Layer 6 : 205/768 type remove
Layer 7 : 247/768 type remove
Layer 8 : 281/768 type remove
Layer 9 : 362/768 type retain
Layer 10 : 287/768 type retain
Layer 11 : 358/768 type retain
Layer 12 : 227/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:32:22,319 [trainer.py] => Time:147.7864055633545
4402 4402
4402 4402
2025-12-10 15:32:34,856 [trainer.py] => Time:12.536841630935669
2025-12-10 15:32:34,857 [inflora.py] => Exemplar size: 0
2025-12-10 15:32:34,857 [trainer.py] => CNN: {'total': np.float64(56.66), '00-03': np.float64(58.06), '04-07': np.float64(66.9), '08-11': np.float64(63.16), '12-15': np.float64(78.18), '16-19': np.float64(75.4), '20-23': np.float64(68.04), '24-27': np.float64(70.73), '28-31': np.float64(38.22), '32-35': np.float64(61.33), '36-39': np.float64(63.37), '40-43': np.float64(52.33), '44-47': np.float64(40.74), '48-51': np.float64(39.69), '52-55': np.float64(47.54), '56-59': np.float64(49.47), '60-63': np.float64(65.31), '64-67': np.float64(51.02), '68-71': np.float64(41.24), '72-75': np.float64(48.31), '76-79': np.float64(47.27), '80-83': np.float64(46.84), '84-87': np.float64(40.6), '88-91': np.float64(72.0), '92-95': np.float64(45.83), '96-99': np.float64(51.45), '100-103': np.float64(61.02), '104-107': np.float64(21.05), '108-111': np.float64(60.63), '112-115': np.float64(83.7), '116-119': np.float64(80.68), '120-123': np.float64(62.39), '124-127': np.float64(34.31), '128-131': np.float64(50.0), '132-135': np.float64(73.21), '136-139': np.float64(76.47), '140-143': np.float64(54.74), '144-147': np.float64(67.12), 'old': np.float64(56.48), 'new': np.float64(67.12)}
2025-12-10 15:32:34,857 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66)]
2025-12-10 15:32:34,857 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37)]
2025-12-10 15:32:34,857 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427]
2025-12-10 15:32:40,920 [trainer.py] => All params: 126094051
2025-12-10 15:32:40,926 [trainer.py] => Trainable params: 187396
2025-12-10 15:32:40,926 [inflora.py] => Learning on 148-152
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight'}
2025-12-10 15:34:44,059 [inflora.py] => Task 37, Epoch 50/50 => Loss 0.124, Train_accy 96.58
Threshold:  0.9948
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 85/768 type remove
Layer 4 : 136/768 type remove
Layer 5 : 185/768 type remove
Layer 6 : 207/768 type remove
Layer 7 : 250/768 type remove
Layer 8 : 284/768 type remove
Layer 9 : 359/768 type retain
Layer 10 : 286/768 type retain
Layer 11 : 357/768 type retain
Layer 12 : 222/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:34:51,291 [trainer.py] => Time:130.36524724960327
4473 4473
4473 4473
2025-12-10 15:35:04,076 [trainer.py] => Time:12.785171508789062
2025-12-10 15:35:04,077 [inflora.py] => Exemplar size: 0
2025-12-10 15:35:04,077 [trainer.py] => CNN: {'total': np.float64(56.76), '00-03': np.float64(58.71), '04-07': np.float64(66.9), '08-11': np.float64(64.91), '12-15': np.float64(78.18), '16-19': np.float64(76.98), '20-23': np.float64(68.95), '24-27': np.float64(69.51), '28-31': np.float64(36.13), '32-35': np.float64(64.0), '36-39': np.float64(68.32), '40-43': np.float64(50.0), '44-47': np.float64(40.74), '48-51': np.float64(40.46), '52-55': np.float64(50.82), '56-59': np.float64(45.26), '60-63': np.float64(62.59), '64-67': np.float64(55.1), '68-71': np.float64(42.27), '72-75': np.float64(51.69), '76-79': np.float64(46.67), '80-83': np.float64(47.89), '84-87': np.float64(38.35), '88-91': np.float64(69.0), '92-95': np.float64(50.0), '96-99': np.float64(52.02), '100-103': np.float64(61.86), '104-107': np.float64(21.8), '108-111': np.float64(65.35), '112-115': np.float64(82.61), '116-119': np.float64(78.41), '120-123': np.float64(67.52), '124-127': np.float64(36.27), '128-131': np.float64(49.19), '132-135': np.float64(72.32), '136-139': np.float64(74.51), '140-143': np.float64(56.84), '144-147': np.float64(65.75), '148-151': np.float64(36.62), 'old': np.float64(57.09), 'new': np.float64(36.62)}
2025-12-10 15:35:04,077 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76)]
2025-12-10 15:35:04,077 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39)]
2025-12-10 15:35:04,077 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321]
2025-12-10 15:35:06,883 [trainer.py] => All params: 126094051
2025-12-10 15:35:06,889 [trainer.py] => Trainable params: 187396
2025-12-10 15:35:06,889 [inflora.py] => Learning on 152-156
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight'}
2025-12-10 15:37:41,715 [inflora.py] => Task 38, Epoch 50/50 => Loss 0.071, Train_accy 97.87
Threshold:  0.9952
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 89/768 type remove
Layer 4 : 143/768 type remove
Layer 5 : 191/768 type remove
Layer 6 : 215/768 type remove
Layer 7 : 262/768 type remove
Layer 8 : 293/768 type remove
Layer 9 : 352/768 type retain
Layer 10 : 281/768 type retain
Layer 11 : 352/768 type retain
Layer 12 : 216/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:37:49,821 [trainer.py] => Time:162.93211555480957
4560 4560
4560 4560
2025-12-10 15:38:02,831 [trainer.py] => Time:13.009671211242676
2025-12-10 15:38:02,832 [inflora.py] => Exemplar size: 0
2025-12-10 15:38:02,832 [trainer.py] => CNN: {'total': np.float64(55.83), '00-03': np.float64(60.65), '04-07': np.float64(64.14), '08-11': np.float64(63.16), '12-15': np.float64(75.45), '16-19': np.float64(76.98), '20-23': np.float64(68.04), '24-27': np.float64(68.29), '28-31': np.float64(33.51), '32-35': np.float64(62.67), '36-39': np.float64(61.39), '40-43': np.float64(45.35), '44-47': np.float64(40.74), '48-51': np.float64(36.64), '52-55': np.float64(50.82), '56-59': np.float64(46.32), '60-63': np.float64(60.54), '64-67': np.float64(51.02), '68-71': np.float64(38.14), '72-75': np.float64(49.15), '76-79': np.float64(45.45), '80-83': np.float64(42.63), '84-87': np.float64(40.6), '88-91': np.float64(71.0), '92-95': np.float64(50.83), '96-99': np.float64(53.18), '100-103': np.float64(61.86), '104-107': np.float64(21.8), '108-111': np.float64(62.99), '112-115': np.float64(82.61), '116-119': np.float64(78.41), '120-123': np.float64(70.09), '124-127': np.float64(40.2), '128-131': np.float64(46.77), '132-135': np.float64(72.32), '136-139': np.float64(73.53), '140-143': np.float64(57.89), '144-147': np.float64(58.9), '148-151': np.float64(32.39), '152-155': np.float64(70.11), 'old': np.float64(55.56), 'new': np.float64(70.11)}
2025-12-10 15:38:02,832 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83)]
2025-12-10 15:38:02,832 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43)]
2025-12-10 15:38:02,832 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825]
2025-12-10 15:38:09,019 [trainer.py] => All params: 126094051
2025-12-10 15:38:09,026 [trainer.py] => Trainable params: 187396
2025-12-10 15:38:09,026 [inflora.py] => Learning on 156-160
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight'}
2025-12-10 15:41:21,898 [inflora.py] => Task 39, Epoch 50/50 => Loss 0.150, Train_accy 94.95
Threshold:  0.9956
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 91/768 type remove
Layer 4 : 145/768 type remove
Layer 5 : 196/768 type remove
Layer 6 : 220/768 type remove
Layer 7 : 266/768 type remove
Layer 8 : 302/768 type remove
Layer 9 : 348/768 type retain
Layer 10 : 279/768 type retain
Layer 11 : 350/768 type retain
Layer 12 : 212/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:41:30,695 [trainer.py] => Time:201.66954731941223
4679 4679
4679 4679
2025-12-10 15:41:44,058 [trainer.py] => Time:13.362917184829712
2025-12-10 15:41:44,059 [inflora.py] => Exemplar size: 0
2025-12-10 15:41:44,059 [trainer.py] => CNN: {'total': np.float64(55.23), '00-03': np.float64(52.9), '04-07': np.float64(64.83), '08-11': np.float64(62.28), '12-15': np.float64(69.09), '16-19': np.float64(76.19), '20-23': np.float64(68.04), '24-27': np.float64(65.24), '28-31': np.float64(34.03), '32-35': np.float64(53.33), '36-39': np.float64(62.38), '40-43': np.float64(41.86), '44-47': np.float64(46.3), '48-51': np.float64(45.8), '52-55': np.float64(42.62), '56-59': np.float64(36.84), '60-63': np.float64(61.9), '64-67': np.float64(48.98), '68-71': np.float64(36.08), '72-75': np.float64(51.69), '76-79': np.float64(44.24), '80-83': np.float64(46.84), '84-87': np.float64(41.35), '88-91': np.float64(69.0), '92-95': np.float64(52.5), '96-99': np.float64(49.71), '100-103': np.float64(60.17), '104-107': np.float64(18.05), '108-111': np.float64(69.29), '112-115': np.float64(81.52), '116-119': np.float64(79.55), '120-123': np.float64(67.52), '124-127': np.float64(38.24), '128-131': np.float64(45.16), '132-135': np.float64(72.32), '136-139': np.float64(73.53), '140-143': np.float64(61.05), '144-147': np.float64(56.16), '148-151': np.float64(23.94), '152-155': np.float64(68.97), '156-159': np.float64(66.39), 'old': np.float64(54.93), 'new': np.float64(66.39)}
2025-12-10 15:41:44,059 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23)]
2025-12-10 15:41:44,059 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72)]
2025-12-10 15:41:44,059 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127]
2025-12-10 15:41:53,993 [trainer.py] => All params: 126094051
2025-12-10 15:41:54,000 [trainer.py] => Trainable params: 187396
2025-12-10 15:41:54,000 [inflora.py] => Learning on 160-164
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.8.attn.lora_B_k.40.weight'}
2025-12-10 15:44:49,621 [inflora.py] => Task 40, Epoch 50/50 => Loss 0.046, Train_accy 98.44
Threshold:  0.996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 49/768 type remove
Layer 3 : 98/768 type remove
Layer 4 : 156/768 type remove
Layer 5 : 210/768 type remove
Layer 6 : 236/768 type remove
Layer 7 : 281/768 type remove
Layer 8 : 322/768 type remove
Layer 9 : 330/768 type retain
Layer 10 : 268/768 type retain
Layer 11 : 342/768 type retain
Layer 12 : 207/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:44:57,860 [trainer.py] => Time:183.86053657531738
4786 4786
4786 4786
2025-12-10 15:45:11,496 [trainer.py] => Time:13.635296821594238
2025-12-10 15:45:11,496 [inflora.py] => Exemplar size: 0
2025-12-10 15:45:11,496 [trainer.py] => CNN: {'total': np.float64(55.45), '00-03': np.float64(51.61), '04-07': np.float64(63.45), '08-11': np.float64(59.65), '12-15': np.float64(72.73), '16-19': np.float64(75.4), '20-23': np.float64(67.58), '24-27': np.float64(66.46), '28-31': np.float64(39.79), '32-35': np.float64(53.33), '36-39': np.float64(62.38), '40-43': np.float64(43.02), '44-47': np.float64(46.3), '48-51': np.float64(45.8), '52-55': np.float64(42.62), '56-59': np.float64(35.79), '60-63': np.float64(60.54), '64-67': np.float64(46.94), '68-71': np.float64(37.11), '72-75': np.float64(49.15), '76-79': np.float64(44.24), '80-83': np.float64(43.68), '84-87': np.float64(42.11), '88-91': np.float64(74.0), '92-95': np.float64(49.17), '96-99': np.float64(44.51), '100-103': np.float64(58.47), '104-107': np.float64(18.05), '108-111': np.float64(66.93), '112-115': np.float64(83.7), '116-119': np.float64(80.68), '120-123': np.float64(62.39), '124-127': np.float64(36.27), '128-131': np.float64(44.35), '132-135': np.float64(71.43), '136-139': np.float64(72.55), '140-143': np.float64(61.05), '144-147': np.float64(57.53), '148-151': np.float64(29.58), '152-155': np.float64(67.82), '156-159': np.float64(66.39), '160-163': np.float64(83.18), 'old': np.float64(54.82), 'new': np.float64(83.18)}
2025-12-10 15:45:11,496 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45)]
2025-12-10 15:45:11,497 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64)]
2025-12-10 15:45:11,497 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428]
2025-12-10 15:45:13,791 [trainer.py] => All params: 126094051
2025-12-10 15:45:13,797 [trainer.py] => Trainable params: 187396
2025-12-10 15:45:13,797 [inflora.py] => Learning on 164-168
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight'}
2025-12-10 15:48:34,562 [inflora.py] => Task 41, Epoch 50/50 => Loss 0.084, Train_accy 97.61
Threshold:  0.9964
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 16/768 type remove
Layer 2 : 55/768 type remove
Layer 3 : 107/768 type remove
Layer 4 : 170/768 type remove
Layer 5 : 229/768 type remove
Layer 6 : 255/768 type remove
Layer 7 : 298/768 type remove
Layer 8 : 340/768 type remove
Layer 9 : 303/768 type retain
Layer 10 : 240/768 type retain
Layer 11 : 319/768 type retain
Layer 12 : 202/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:48:43,389 [trainer.py] => Time:209.59192276000977
4933 4933
4933 4933
2025-12-10 15:48:57,372 [trainer.py] => Time:13.98244309425354
2025-12-10 15:48:57,372 [inflora.py] => Exemplar size: 0
2025-12-10 15:48:57,372 [trainer.py] => CNN: {'total': np.float64(55.95), '00-03': np.float64(51.61), '04-07': np.float64(63.45), '08-11': np.float64(62.28), '12-15': np.float64(70.91), '16-19': np.float64(73.81), '20-23': np.float64(68.95), '24-27': np.float64(67.07), '28-31': np.float64(38.74), '32-35': np.float64(57.33), '36-39': np.float64(64.36), '40-43': np.float64(41.86), '44-47': np.float64(44.44), '48-51': np.float64(47.33), '52-55': np.float64(44.26), '56-59': np.float64(34.74), '60-63': np.float64(63.27), '64-67': np.float64(46.94), '68-71': np.float64(36.08), '72-75': np.float64(50.0), '76-79': np.float64(46.06), '80-83': np.float64(40.53), '84-87': np.float64(42.86), '88-91': np.float64(72.0), '92-95': np.float64(50.83), '96-99': np.float64(45.66), '100-103': np.float64(56.78), '104-107': np.float64(21.05), '108-111': np.float64(65.35), '112-115': np.float64(78.26), '116-119': np.float64(80.68), '120-123': np.float64(61.54), '124-127': np.float64(38.24), '128-131': np.float64(41.13), '132-135': np.float64(70.54), '136-139': np.float64(72.55), '140-143': np.float64(58.95), '144-147': np.float64(58.9), '148-151': np.float64(21.13), '152-155': np.float64(66.67), '156-159': np.float64(65.55), '160-163': np.float64(84.11), '164-167': np.float64(76.87), 'old': np.float64(55.31), 'new': np.float64(76.87)}
2025-12-10 15:48:57,372 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95)]
2025-12-10 15:48:57,372 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65)]
2025-12-10 15:48:57,372 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892]
2025-12-10 15:49:08,049 [trainer.py] => All params: 126094051
2025-12-10 15:49:08,055 [trainer.py] => Trainable params: 187396
2025-12-10 15:49:08,055 [inflora.py] => Learning on 168-172
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight'}
2025-12-10 15:52:38,360 [inflora.py] => Task 42, Epoch 50/50 => Loss 0.111, Train_accy 97.04
Threshold:  0.9968
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 16/768 type remove
Layer 2 : 55/768 type remove
Layer 3 : 111/768 type remove
Layer 4 : 177/768 type remove
Layer 5 : 242/768 type remove
Layer 6 : 270/768 type remove
Layer 7 : 316/768 type remove
Layer 8 : 358/768 type remove
Layer 9 : 287/768 type retain
Layer 10 : 225/768 type retain
Layer 11 : 303/768 type retain
Layer 12 : 195/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:52:47,652 [trainer.py] => Time:219.5973675251007
5072 5072
5072 5072
2025-12-10 15:53:02,048 [trainer.py] => Time:14.394997358322144
2025-12-10 15:53:02,048 [inflora.py] => Exemplar size: 0
2025-12-10 15:53:02,048 [trainer.py] => CNN: {'total': np.float64(57.28), '00-03': np.float64(56.13), '04-07': np.float64(60.0), '08-11': np.float64(64.91), '12-15': np.float64(72.73), '16-19': np.float64(73.81), '20-23': np.float64(69.86), '24-27': np.float64(70.73), '28-31': np.float64(38.74), '32-35': np.float64(56.0), '36-39': np.float64(72.28), '40-43': np.float64(48.84), '44-47': np.float64(46.3), '48-51': np.float64(48.09), '52-55': np.float64(39.34), '56-59': np.float64(37.89), '60-63': np.float64(65.99), '64-67': np.float64(46.94), '68-71': np.float64(38.14), '72-75': np.float64(50.85), '76-79': np.float64(46.67), '80-83': np.float64(42.63), '84-87': np.float64(45.11), '88-91': np.float64(74.0), '92-95': np.float64(48.33), '96-99': np.float64(47.4), '100-103': np.float64(56.78), '104-107': np.float64(23.31), '108-111': np.float64(64.57), '112-115': np.float64(81.52), '116-119': np.float64(81.82), '120-123': np.float64(61.54), '124-127': np.float64(41.18), '128-131': np.float64(38.71), '132-135': np.float64(72.32), '136-139': np.float64(73.53), '140-143': np.float64(58.95), '144-147': np.float64(67.12), '148-151': np.float64(18.31), '152-155': np.float64(65.52), '156-159': np.float64(61.34), '160-163': np.float64(78.5), '164-167': np.float64(76.19), '168-171': np.float64(70.5), 'old': np.float64(56.9), 'new': np.float64(70.5)}
2025-12-10 15:53:02,048 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28)]
2025-12-10 15:53:02,049 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38)]
2025-12-10 15:53:02,049 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873]
2025-12-10 15:53:05,369 [trainer.py] => All params: 126094051
2025-12-10 15:53:05,375 [trainer.py] => Trainable params: 187396
2025-12-10 15:53:05,376 [inflora.py] => Learning on 172-176
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'classifier_pool.43.bias', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight'}
2025-12-10 15:56:03,596 [inflora.py] => Task 43, Epoch 50/50 => Loss 0.103, Train_accy 95.88
Threshold:  0.9972
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 16/768 type remove
Layer 2 : 55/768 type remove
Layer 3 : 113/768 type remove
Layer 4 : 182/768 type remove
Layer 5 : 246/768 type remove
Layer 6 : 275/768 type remove
Layer 7 : 325/768 type remove
Layer 8 : 368/768 type remove
Layer 9 : 280/768 type retain
Layer 10 : 218/768 type retain
Layer 11 : 293/768 type retain
Layer 12 : 188/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:56:11,965 [trainer.py] => Time:186.58904266357422
5201 5201
5201 5201
2025-12-10 15:56:26,713 [trainer.py] => Time:14.748388528823853
2025-12-10 15:56:26,714 [inflora.py] => Exemplar size: 0
2025-12-10 15:56:26,714 [trainer.py] => CNN: {'total': np.float64(57.05), '00-03': np.float64(54.84), '04-07': np.float64(60.0), '08-11': np.float64(63.16), '12-15': np.float64(71.82), '16-19': np.float64(74.6), '20-23': np.float64(67.12), '24-27': np.float64(69.51), '28-31': np.float64(39.27), '32-35': np.float64(53.33), '36-39': np.float64(68.32), '40-43': np.float64(44.19), '44-47': np.float64(44.44), '48-51': np.float64(46.56), '52-55': np.float64(37.7), '56-59': np.float64(34.74), '60-63': np.float64(65.99), '64-67': np.float64(48.98), '68-71': np.float64(36.08), '72-75': np.float64(50.85), '76-79': np.float64(46.67), '80-83': np.float64(42.63), '84-87': np.float64(46.62), '88-91': np.float64(74.0), '92-95': np.float64(49.17), '96-99': np.float64(46.82), '100-103': np.float64(55.93), '104-107': np.float64(21.05), '108-111': np.float64(62.99), '112-115': np.float64(81.52), '116-119': np.float64(79.55), '120-123': np.float64(62.39), '124-127': np.float64(40.2), '128-131': np.float64(40.32), '132-135': np.float64(69.64), '136-139': np.float64(75.49), '140-143': np.float64(60.0), '144-147': np.float64(65.75), '148-151': np.float64(21.13), '152-155': np.float64(66.67), '156-159': np.float64(64.71), '160-163': np.float64(77.57), '164-167': np.float64(76.87), '168-171': np.float64(69.78), '172-175': np.float64(69.77), 'old': np.float64(56.72), 'new': np.float64(69.77)}
2025-12-10 15:56:26,714 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05)]
2025-12-10 15:56:26,714 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14)]
2025-12-10 15:56:26,714 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495]
2025-12-10 15:56:29,922 [trainer.py] => All params: 126094051
2025-12-10 15:56:29,928 [trainer.py] => Trainable params: 187396
2025-12-10 15:56:29,928 [inflora.py] => Learning on 176-180
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight'}
2025-12-10 15:59:27,821 [inflora.py] => Task 44, Epoch 50/50 => Loss 0.133, Train_accy 96.05
Threshold:  0.9976
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 16/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 125/768 type remove
Layer 4 : 199/768 type remove
Layer 5 : 266/768 type remove
Layer 6 : 296/768 type remove
Layer 7 : 351/768 type remove
Layer 8 : 371/768 type retain
Layer 9 : 256/768 type retain
Layer 10 : 198/768 type retain
Layer 11 : 262/768 type retain
Layer 12 : 159/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:59:36,316 [trainer.py] => Time:186.38764691352844
5306 5306
5306 5306
2025-12-10 15:59:51,352 [trainer.py] => Time:15.036235332489014
2025-12-10 15:59:51,353 [inflora.py] => Exemplar size: 0
2025-12-10 15:59:51,353 [trainer.py] => CNN: {'total': np.float64(57.78), '00-03': np.float64(56.13), '04-07': np.float64(60.69), '08-11': np.float64(63.16), '12-15': np.float64(74.55), '16-19': np.float64(75.4), '20-23': np.float64(66.67), '24-27': np.float64(71.34), '28-31': np.float64(41.88), '32-35': np.float64(53.33), '36-39': np.float64(70.3), '40-43': np.float64(45.35), '44-47': np.float64(44.44), '48-51': np.float64(51.15), '52-55': np.float64(44.26), '56-59': np.float64(36.84), '60-63': np.float64(68.03), '64-67': np.float64(51.02), '68-71': np.float64(36.08), '72-75': np.float64(46.61), '76-79': np.float64(46.67), '80-83': np.float64(43.16), '84-87': np.float64(45.11), '88-91': np.float64(75.0), '92-95': np.float64(50.83), '96-99': np.float64(49.13), '100-103': np.float64(55.93), '104-107': np.float64(21.8), '108-111': np.float64(66.14), '112-115': np.float64(78.26), '116-119': np.float64(78.41), '120-123': np.float64(60.68), '124-127': np.float64(42.16), '128-131': np.float64(42.74), '132-135': np.float64(66.96), '136-139': np.float64(72.55), '140-143': np.float64(57.89), '144-147': np.float64(65.75), '148-151': np.float64(21.13), '152-155': np.float64(65.52), '156-159': np.float64(65.55), '160-163': np.float64(80.37), '164-167': np.float64(77.55), '168-171': np.float64(69.78), '172-175': np.float64(70.54), '176-179': np.float64(60.95), 'old': np.float64(57.72), 'new': np.float64(60.95)}
2025-12-10 15:59:51,353 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78)]
2025-12-10 15:59:51,353 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39)]
2025-12-10 15:59:51,353 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232]
2025-12-10 15:59:55,661 [trainer.py] => All params: 126094051
2025-12-10 15:59:55,667 [trainer.py] => Trainable params: 187396
2025-12-10 15:59:55,667 [inflora.py] => Learning on 180-184
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'classifier_pool.45.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight'}
2025-12-10 16:03:31,063 [inflora.py] => Task 45, Epoch 50/50 => Loss 0.102, Train_accy 95.97
Threshold:  0.998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 21/768 type remove
Layer 2 : 65/768 type remove
Layer 3 : 140/768 type remove
Layer 4 : 217/768 type remove
Layer 5 : 290/768 type remove
Layer 6 : 324/768 type remove
Layer 7 : 380/768 type remove
Layer 8 : 343/768 type retain
Layer 9 : 227/768 type retain
Layer 10 : 170/768 type retain
Layer 11 : 222/768 type retain
Layer 12 : 135/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:03:40,256 [trainer.py] => Time:224.5891251564026
5441 5441
5441 5441
2025-12-10 16:03:55,700 [trainer.py] => Time:15.443373918533325
2025-12-10 16:03:55,700 [inflora.py] => Exemplar size: 0
2025-12-10 16:03:55,701 [trainer.py] => CNN: {'total': np.float64(57.95), '00-03': np.float64(60.65), '04-07': np.float64(59.31), '08-11': np.float64(64.04), '12-15': np.float64(76.36), '16-19': np.float64(75.4), '20-23': np.float64(68.49), '24-27': np.float64(70.73), '28-31': np.float64(42.93), '32-35': np.float64(54.67), '36-39': np.float64(67.33), '40-43': np.float64(44.19), '44-47': np.float64(44.44), '48-51': np.float64(51.91), '52-55': np.float64(44.26), '56-59': np.float64(37.89), '60-63': np.float64(68.71), '64-67': np.float64(51.02), '68-71': np.float64(37.11), '72-75': np.float64(44.92), '76-79': np.float64(49.09), '80-83': np.float64(44.21), '84-87': np.float64(46.62), '88-91': np.float64(76.0), '92-95': np.float64(51.67), '96-99': np.float64(49.13), '100-103': np.float64(56.78), '104-107': np.float64(23.31), '108-111': np.float64(66.93), '112-115': np.float64(78.26), '116-119': np.float64(75.0), '120-123': np.float64(61.54), '124-127': np.float64(40.2), '128-131': np.float64(37.9), '132-135': np.float64(64.29), '136-139': np.float64(71.57), '140-143': np.float64(56.84), '144-147': np.float64(65.75), '148-151': np.float64(18.31), '152-155': np.float64(65.52), '156-159': np.float64(67.23), '160-163': np.float64(77.57), '164-167': np.float64(78.91), '168-171': np.float64(71.22), '172-175': np.float64(67.44), '176-179': np.float64(62.86), '180-183': np.float64(57.04), 'old': np.float64(57.97), 'new': np.float64(57.04)}
2025-12-10 16:03:55,701 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78), np.float64(57.95)]
2025-12-10 16:03:55,702 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39), np.float64(91.31)]
2025-12-10 16:03:55,702 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232, 0.5861054953133615]
2025-12-10 16:04:00,776 [trainer.py] => All params: 126094051
2025-12-10 16:04:00,782 [trainer.py] => Trainable params: 187396
2025-12-10 16:04:00,782 [inflora.py] => Learning on 184-188
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight'}
2025-12-10 16:07:40,874 [inflora.py] => Task 46, Epoch 50/50 => Loss 0.104, Train_accy 95.89
Threshold:  0.9984
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 21/768 type remove
Layer 2 : 69/768 type remove
Layer 3 : 150/768 type remove
Layer 4 : 235/768 type remove
Layer 5 : 314/768 type remove
Layer 6 : 350/768 type remove
Layer 7 : 357/768 type retain
Layer 8 : 309/768 type retain
Layer 9 : 197/768 type retain
Layer 10 : 148/768 type retain
Layer 11 : 194/768 type retain
Layer 12 : 117/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:07:50,249 [trainer.py] => Time:229.46718072891235
5592 5592
5592 5592
2025-12-10 16:08:06,047 [trainer.py] => Time:15.797693014144897
2025-12-10 16:08:06,048 [inflora.py] => Exemplar size: 0
2025-12-10 16:08:06,048 [trainer.py] => CNN: {'total': np.float64(59.07), '00-03': np.float64(63.23), '04-07': np.float64(61.38), '08-11': np.float64(65.79), '12-15': np.float64(76.36), '16-19': np.float64(74.6), '20-23': np.float64(68.95), '24-27': np.float64(71.34), '28-31': np.float64(43.98), '32-35': np.float64(58.67), '36-39': np.float64(67.33), '40-43': np.float64(48.84), '44-47': np.float64(44.44), '48-51': np.float64(55.73), '52-55': np.float64(45.9), '56-59': np.float64(41.05), '60-63': np.float64(69.39), '64-67': np.float64(55.1), '68-71': np.float64(39.18), '72-75': np.float64(45.76), '76-79': np.float64(48.48), '80-83': np.float64(45.26), '84-87': np.float64(45.11), '88-91': np.float64(75.0), '92-95': np.float64(49.17), '96-99': np.float64(49.71), '100-103': np.float64(55.08), '104-107': np.float64(26.32), '108-111': np.float64(67.72), '112-115': np.float64(78.26), '116-119': np.float64(78.41), '120-123': np.float64(62.39), '124-127': np.float64(39.22), '128-131': np.float64(38.71), '132-135': np.float64(65.18), '136-139': np.float64(75.49), '140-143': np.float64(58.95), '144-147': np.float64(67.12), '148-151': np.float64(26.76), '152-155': np.float64(66.67), '156-159': np.float64(63.87), '160-163': np.float64(77.57), '164-167': np.float64(79.59), '168-171': np.float64(69.06), '172-175': np.float64(67.44), '176-179': np.float64(62.86), '180-183': np.float64(59.26), '184-187': np.float64(66.89), 'old': np.float64(58.85), 'new': np.float64(66.89)}
2025-12-10 16:08:06,048 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78), np.float64(57.95), np.float64(59.07)]
2025-12-10 16:08:06,048 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39), np.float64(91.31), np.float64(91.72)]
2025-12-10 16:08:06,048 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232, 0.5861054953133615, 0.5974606580829757]
2025-12-10 16:08:13,400 [trainer.py] => All params: 126094051
2025-12-10 16:08:13,406 [trainer.py] => Trainable params: 187396
2025-12-10 16:08:13,406 [inflora.py] => Learning on 188-192
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'classifier_pool.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight'}
2025-12-10 16:12:25,556 [inflora.py] => Task 47, Epoch 50/50 => Loss 0.089, Train_accy 97.09
Threshold:  0.9988
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 26/768 type remove
Layer 2 : 76/768 type remove
Layer 3 : 165/768 type remove
Layer 4 : 259/768 type remove
Layer 5 : 344/768 type remove
Layer 6 : 382/768 type remove
Layer 7 : 318/768 type retain
Layer 8 : 267/768 type retain
Layer 9 : 166/768 type retain
Layer 10 : 123/768 type retain
Layer 11 : 164/768 type retain
Layer 12 : 104/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:12:35,075 [trainer.py] => Time:261.66834330558777
5771 5771
5771 5771
2025-12-10 16:12:51,460 [trainer.py] => Time:16.38542342185974
2025-12-10 16:12:51,461 [inflora.py] => Exemplar size: 0
2025-12-10 16:12:51,461 [trainer.py] => CNN: {'total': np.float64(59.49), '00-03': np.float64(64.52), '04-07': np.float64(62.76), '08-11': np.float64(65.79), '12-15': np.float64(78.18), '16-19': np.float64(74.6), '20-23': np.float64(69.41), '24-27': np.float64(70.12), '28-31': np.float64(43.98), '32-35': np.float64(58.67), '36-39': np.float64(70.3), '40-43': np.float64(48.84), '44-47': np.float64(46.3), '48-51': np.float64(56.49), '52-55': np.float64(45.9), '56-59': np.float64(44.21), '60-63': np.float64(68.03), '64-67': np.float64(51.02), '68-71': np.float64(40.21), '72-75': np.float64(45.76), '76-79': np.float64(49.09), '80-83': np.float64(47.37), '84-87': np.float64(45.86), '88-91': np.float64(75.0), '92-95': np.float64(49.17), '96-99': np.float64(50.87), '100-103': np.float64(57.63), '104-107': np.float64(25.56), '108-111': np.float64(66.14), '112-115': np.float64(77.17), '116-119': np.float64(71.59), '120-123': np.float64(61.54), '124-127': np.float64(38.24), '128-131': np.float64(41.13), '132-135': np.float64(66.96), '136-139': np.float64(73.53), '140-143': np.float64(57.89), '144-147': np.float64(67.12), '148-151': np.float64(28.17), '152-155': np.float64(68.97), '156-159': np.float64(60.5), '160-163': np.float64(75.7), '164-167': np.float64(80.27), '168-171': np.float64(68.35), '172-175': np.float64(68.99), '176-179': np.float64(62.86), '180-183': np.float64(59.26), '184-187': np.float64(66.23), '188-191': np.float64(67.6), 'old': np.float64(59.23), 'new': np.float64(67.6)}
2025-12-10 16:12:51,461 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78), np.float64(57.95), np.float64(59.07), np.float64(59.49)]
2025-12-10 16:12:51,461 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39), np.float64(91.31), np.float64(91.72), np.float64(91.99)]
2025-12-10 16:12:51,461 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232, 0.5861054953133615, 0.5974606580829757, 0.6007624328539248]
2025-12-10 16:12:58,837 [trainer.py] => All params: 126094051
2025-12-10 16:12:58,843 [trainer.py] => Trainable params: 187396
2025-12-10 16:12:58,843 [inflora.py] => Learning on 192-196
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight'}
2025-12-10 16:15:24,289 [inflora.py] => Task 48, Epoch 50/50 => Loss 0.144, Train_accy 95.32
Threshold:  0.9992
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 48/768 type remove
Layer 2 : 89/768 type remove
Layer 3 : 191/768 type remove
Layer 4 : 295/768 type remove
Layer 5 : 383/768 type remove
Layer 6 : 344/768 type retain
Layer 7 : 280/768 type retain
Layer 8 : 232/768 type retain
Layer 9 : 133/768 type retain
Layer 10 : 91/768 type retain
Layer 11 : 115/768 type retain
Layer 12 : 66/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:15:32,157 [trainer.py] => Time:153.31329870224
5854 5854
5854 5854
2025-12-10 16:15:48,669 [trainer.py] => Time:16.512434720993042
2025-12-10 16:15:48,670 [inflora.py] => Exemplar size: 0
2025-12-10 16:15:48,670 [trainer.py] => CNN: {'total': np.float64(59.53), '00-03': np.float64(65.81), '04-07': np.float64(62.76), '08-11': np.float64(67.54), '12-15': np.float64(79.09), '16-19': np.float64(76.98), '20-23': np.float64(68.95), '24-27': np.float64(72.56), '28-31': np.float64(42.41), '32-35': np.float64(57.33), '36-39': np.float64(70.3), '40-43': np.float64(48.84), '44-47': np.float64(46.3), '48-51': np.float64(56.49), '52-55': np.float64(49.18), '56-59': np.float64(45.26), '60-63': np.float64(68.71), '64-67': np.float64(55.1), '68-71': np.float64(39.18), '72-75': np.float64(46.61), '76-79': np.float64(49.7), '80-83': np.float64(47.37), '84-87': np.float64(45.11), '88-91': np.float64(75.0), '92-95': np.float64(49.17), '96-99': np.float64(51.45), '100-103': np.float64(56.78), '104-107': np.float64(24.81), '108-111': np.float64(64.57), '112-115': np.float64(79.35), '116-119': np.float64(72.73), '120-123': np.float64(60.68), '124-127': np.float64(39.22), '128-131': np.float64(40.32), '132-135': np.float64(64.29), '136-139': np.float64(73.53), '140-143': np.float64(58.95), '144-147': np.float64(68.49), '148-151': np.float64(28.17), '152-155': np.float64(70.11), '156-159': np.float64(62.18), '160-163': np.float64(75.7), '164-167': np.float64(76.87), '168-171': np.float64(68.35), '172-175': np.float64(67.44), '176-179': np.float64(63.81), '180-183': np.float64(62.22), '184-187': np.float64(66.23), '188-191': np.float64(68.16), '192-195': np.float64(46.99), 'old': np.float64(59.71), 'new': np.float64(46.99)}
2025-12-10 16:15:48,670 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78), np.float64(57.95), np.float64(59.07), np.float64(59.49), np.float64(59.53)]
2025-12-10 16:15:48,670 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39), np.float64(91.31), np.float64(91.72), np.float64(91.99), np.float64(91.9)]
2025-12-10 16:15:48,670 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232, 0.5861054953133615, 0.5974606580829757, 0.6007624328539248, 0.6016399043389136]
2025-12-10 16:15:52,716 [trainer.py] => All params: 126094051
2025-12-10 16:15:52,722 [trainer.py] => Trainable params: 187396
2025-12-10 16:15:52,722 [inflora.py] => Learning on 196-200
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight'}
2025-12-10 16:19:23,568 [inflora.py] => Task 49, Epoch 50/50 => Loss 0.121, Train_accy 97.05
Threshold:  0.9996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 54/768 type remove
Layer 2 : 110/768 type remove
Layer 3 : 234/768 type remove
Layer 4 : 359/768 type remove
Layer 5 : 307/768 type retain
Layer 6 : 265/768 type retain
Layer 7 : 209/768 type retain
Layer 8 : 168/768 type retain
Layer 9 : 82/768 type retain
Layer 10 : 48/768 type retain
Layer 11 : 65/768 type retain
Layer 12 : 30/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:19:32,374 [trainer.py] => Time:219.65210127830505
6000 6000
6000 6000
2025-12-10 16:19:49,310 [trainer.py] => Time:16.935646772384644
2025-12-10 16:19:49,311 [inflora.py] => Exemplar size: 0
2025-12-10 16:19:49,311 [trainer.py] => CNN: {'total': np.float64(59.12), '00-03': np.float64(65.81), '04-07': np.float64(62.76), '08-11': np.float64(66.67), '12-15': np.float64(79.09), '16-19': np.float64(73.02), '20-23': np.float64(69.41), '24-27': np.float64(71.34), '28-31': np.float64(41.88), '32-35': np.float64(58.67), '36-39': np.float64(70.3), '40-43': np.float64(50.0), '44-47': np.float64(44.44), '48-51': np.float64(57.25), '52-55': np.float64(49.18), '56-59': np.float64(46.32), '60-63': np.float64(68.03), '64-67': np.float64(55.1), '68-71': np.float64(38.14), '72-75': np.float64(46.61), '76-79': np.float64(47.27), '80-83': np.float64(46.84), '84-87': np.float64(45.11), '88-91': np.float64(75.0), '92-95': np.float64(47.5), '96-99': np.float64(51.45), '100-103': np.float64(58.47), '104-107': np.float64(25.56), '108-111': np.float64(65.35), '112-115': np.float64(78.26), '116-119': np.float64(75.0), '120-123': np.float64(61.54), '124-127': np.float64(39.22), '128-131': np.float64(40.32), '132-135': np.float64(64.29), '136-139': np.float64(72.55), '140-143': np.float64(55.79), '144-147': np.float64(67.12), '148-151': np.float64(28.17), '152-155': np.float64(70.11), '156-159': np.float64(59.66), '160-163': np.float64(76.64), '164-167': np.float64(78.23), '168-171': np.float64(66.91), '172-175': np.float64(68.22), '176-179': np.float64(64.76), '180-183': np.float64(60.0), '184-187': np.float64(66.23), '188-191': np.float64(65.92), '192-195': np.float64(45.78), '196-199': np.float64(56.85), 'old': np.float64(59.17), 'new': np.float64(56.85)}
2025-12-10 16:19:49,311 [trainer.py] => CNN top1 curve: [np.float64(90.32), np.float64(82.67), np.float64(79.47), np.float64(78.82), np.float64(77.85), np.float64(77.68), np.float64(78.03), np.float64(74.18), np.float64(73.75), np.float64(73.93), np.float64(72.41), np.float64(70.52), np.float64(70.38), np.float64(69.75), np.float64(68.86), np.float64(68.84), np.float64(67.33), np.float64(66.13), np.float64(63.18), np.float64(62.59), np.float64(64.87), np.float64(65.41), np.float64(65.32), np.float64(64.09), np.float64(63.16), np.float64(63.05), np.float64(61.13), np.float64(60.54), np.float64(59.77), np.float64(59.91), np.float64(59.62), np.float64(58.29), np.float64(58.41), np.float64(57.09), np.float64(57.98), np.float64(57.01), np.float64(56.66), np.float64(56.76), np.float64(55.83), np.float64(55.23), np.float64(55.45), np.float64(55.95), np.float64(57.28), np.float64(57.05), np.float64(57.78), np.float64(57.95), np.float64(59.07), np.float64(59.49), np.float64(59.53), np.float64(59.12)]
2025-12-10 16:19:49,311 [trainer.py] => CNN top1 with task curve: [np.float64(90.32), np.float64(90.67), np.float64(88.89), np.float64(90.46), np.float64(91.38), np.float64(92.06), np.float64(92.84), np.float64(92.81), np.float64(92.92), np.float64(93.14), np.float64(92.19), np.float64(91.82), np.float64(92.22), np.float64(92.21), np.float64(92.17), np.float64(92.45), np.float64(91.84), np.float64(92.17), np.float64(91.91), np.float64(92.22), np.float64(92.17), np.float64(92.41), np.float64(92.46), np.float64(92.16), np.float64(92.18), np.float64(92.18), np.float64(92.02), np.float64(91.76), np.float64(91.36), np.float64(91.76), np.float64(92.01), np.float64(91.81), np.float64(91.87), np.float64(91.14), np.float64(91.59), np.float64(91.52), np.float64(91.37), np.float64(91.39), np.float64(91.43), np.float64(90.72), np.float64(90.64), np.float64(90.65), np.float64(91.38), np.float64(91.14), np.float64(91.39), np.float64(91.31), np.float64(91.72), np.float64(91.99), np.float64(91.9), np.float64(91.98)]
2025-12-10 16:19:49,311 [trainer.py] => CNN top1 task curve: [1.0, 0.8933333333333333, 0.8695652173913043, 0.8435114503816794, 0.8215384615384616, 0.807825086306099, 0.8005808325266215, 0.7622549019607843, 0.7575057736720554, 0.7528571428571429, 0.7442799461641992, 0.724025974025974, 0.720526630760024, 0.7130484988452656, 0.7038861521620142, 0.7051671732522796, 0.6915472071181413, 0.6792452830188679, 0.6474530831099196, 0.6367041198501873, 0.6579251831854994, 0.6628760088041086, 0.6627742392073602, 0.6510522742701969, 0.6415517794164797, 0.6385542168674698, 0.6210682492581603, 0.6130969402344867, 0.60490387294511, 0.607016589611096, 0.6022667369530839, 0.5895790554414785, 0.5902985074626865, 0.5769603097773476, 0.5873878129428437, 0.5770385770385771, 0.5740572467060427, 0.5756762799016321, 0.5653508771929825, 0.5595212652276127, 0.5616381111575428, 0.5665923373200892, 0.5800473186119873, 0.577004422226495, 0.5849981153411232, 0.5861054953133615, 0.5974606580829757, 0.6007624328539248, 0.6016399043389136, 0.5963333333333334]
