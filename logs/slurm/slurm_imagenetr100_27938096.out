logs/ImageNet_R/2_2_sip/InfLoRA/adam/10/0.98_1.0-0.0005/42
2025-12-10 13:29:03,867 [trainer.py] => config: configs/mimg100_inflora_seed42.json
2025-12-10 13:29:03,868 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 13:29:03,868 [trainer.py] => prefix: reproduce
2025-12-10 13:29:03,868 [trainer.py] => dataset: ImageNet_R
2025-12-10 13:29:03,868 [trainer.py] => data_path: data/imagenet-r
2025-12-10 13:29:03,868 [trainer.py] => memory_size: 0
2025-12-10 13:29:03,868 [trainer.py] => memory_per_class: 0
2025-12-10 13:29:03,868 [trainer.py] => fixed_memory: True
2025-12-10 13:29:03,868 [trainer.py] => shuffle: False
2025-12-10 13:29:03,868 [trainer.py] => init_cls: 2
2025-12-10 13:29:03,868 [trainer.py] => increment: 2
2025-12-10 13:29:03,868 [trainer.py] => model_name: InfLoRA
2025-12-10 13:29:03,868 [trainer.py] => net_type: sip
2025-12-10 13:29:03,868 [trainer.py] => embd_dim: 768
2025-12-10 13:29:03,868 [trainer.py] => num_heads: 12
2025-12-10 13:29:03,868 [trainer.py] => total_sessions: 100
2025-12-10 13:29:03,868 [trainer.py] => seed: 42
2025-12-10 13:29:03,868 [trainer.py] => EPSILON: 1e-08
2025-12-10 13:29:03,868 [trainer.py] => init_epoch: 50
2025-12-10 13:29:03,868 [trainer.py] => optim: adam
2025-12-10 13:29:03,869 [trainer.py] => init_lr: 0.0005
2025-12-10 13:29:03,869 [trainer.py] => init_lr_decay: 0.1
2025-12-10 13:29:03,869 [trainer.py] => init_weight_decay: 0.0
2025-12-10 13:29:03,869 [trainer.py] => epochs: 50
2025-12-10 13:29:03,869 [trainer.py] => lrate: 0.0005
2025-12-10 13:29:03,869 [trainer.py] => lrate_decay: 0.1
2025-12-10 13:29:03,869 [trainer.py] => batch_size: 128
2025-12-10 13:29:03,869 [trainer.py] => weight_decay: 0.0
2025-12-10 13:29:03,869 [trainer.py] => rank: 10
2025-12-10 13:29:03,869 [trainer.py] => lamb: 0.98
2025-12-10 13:29:03,869 [trainer.py] => lame: 1.0
2025-12-10 13:29:03,869 [trainer.py] => num_workers: 8
2025-12-10 13:29:04,173 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 4802, unexpected 0
2025-12-10 13:29:08,187 [trainer.py] => All params: 144526051
2025-12-10 13:29:08,200 [trainer.py] => Trainable params: 144526051
2025-12-10 13:29:08,200 [inflora.py] => Learning on 0-2
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'classifier_pool.0.bias'}
2025-12-10 13:31:38,166 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.017, Train_accy 99.42
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 37/768 type remove
Layer 6 : 35/768 type remove
Layer 7 : 37/768 type remove
Layer 8 : 46/768 type remove
Layer 9 : 67/768 type remove
Layer 10 : 49/768 type remove
Layer 11 : 18/768 type remove
Layer 12 : 35/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:31:44,097 [trainer.py] => Time:155.89650630950928
93 93
93 93
2025-12-10 13:31:45,206 [trainer.py] => Time:1.108449935913086
2025-12-10 13:31:45,206 [inflora.py] => Exemplar size: 0
2025-12-10 13:31:45,206 [trainer.py] => CNN: {'total': np.float64(98.92), '00-01': np.float64(98.92), 'old': 0, 'new': np.float64(98.92)}
2025-12-10 13:31:45,206 [trainer.py] => CNN top1 curve: [np.float64(98.92)]
2025-12-10 13:31:45,206 [trainer.py] => CNN top1 with task curve: [np.float64(98.92)]
2025-12-10 13:31:45,206 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 13:31:46,182 [trainer.py] => All params: 144526051
2025-12-10 13:31:46,194 [trainer.py] => Trainable params: 185858
2025-12-10 13:31:46,194 [inflora.py] => Learning on 2-4
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'classifier_pool.11.weight', 'classifier_pool.11.bias', 'classifier_pool.16.bias', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'classifier_pool.15.bias', 'classifier_pool.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'classifier_pool.14.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight'}
2025-12-10 13:33:35,568 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.088, Train_accy 95.67
Threshold:  0.9802
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 41/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 42/768 type remove
Layer 8 : 54/768 type remove
Layer 9 : 77/768 type remove
Layer 10 : 58/768 type remove
Layer 11 : 22/768 type remove
Layer 12 : 54/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:33:42,372 [trainer.py] => Time:116.17809438705444
155 155
155 155
2025-12-10 13:33:43,735 [trainer.py] => Time:1.3629570007324219
2025-12-10 13:33:43,736 [inflora.py] => Exemplar size: 0
2025-12-10 13:33:43,736 [trainer.py] => CNN: {'total': np.float64(78.71), '00-01': np.float64(79.57), '02-03': np.float64(77.42), 'old': np.float64(79.57), 'new': np.float64(77.42)}
2025-12-10 13:33:43,736 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71)]
2025-12-10 13:33:43,736 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77)]
2025-12-10 13:33:43,736 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742]
2025-12-10 13:33:44,712 [trainer.py] => All params: 144526051
2025-12-10 13:33:44,724 [trainer.py] => Trainable params: 2044438
2025-12-10 13:33:44,725 [inflora.py] => Learning on 4-6
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'classifier_pool.2.weight', 'classifier_pool.21.bias', 'classifier_pool.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'classifier_pool.29.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'classifier_pool.28.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight'}
2025-12-10 13:36:03,508 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.040, Train_accy 98.80
Threshold:  0.9803999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 27/768 type remove
Layer 5 : 45/768 type remove
Layer 6 : 42/768 type remove
Layer 7 : 45/768 type remove
Layer 8 : 57/768 type remove
Layer 9 : 79/768 type remove
Layer 10 : 61/768 type remove
Layer 11 : 24/768 type remove
Layer 12 : 59/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:36:10,678 [trainer.py] => Time:145.9529755115509
244 244
244 244
2025-12-10 13:36:12,275 [trainer.py] => Time:1.5975158214569092
2025-12-10 13:36:12,276 [inflora.py] => Exemplar size: 0
2025-12-10 13:36:12,276 [trainer.py] => CNN: {'total': np.float64(70.9), '00-01': np.float64(80.65), '02-03': np.float64(72.58), '04-05': np.float64(59.55), 'old': np.float64(77.42), 'new': np.float64(59.55)}
2025-12-10 13:36:12,276 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9)]
2025-12-10 13:36:12,276 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08)]
2025-12-10 13:36:12,276 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229]
2025-12-10 13:36:13,393 [trainer.py] => All params: 144526051
2025-12-10 13:36:13,405 [trainer.py] => Trainable params: 2044438
2025-12-10 13:36:13,405 [inflora.py] => Learning on 6-8
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'classifier_pool.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight'}
2025-12-10 13:37:51,834 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.017, Train_accy 98.88
Threshold:  0.9806
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 29/768 type remove
Layer 5 : 47/768 type remove
Layer 6 : 44/768 type remove
Layer 7 : 48/768 type remove
Layer 8 : 60/768 type remove
Layer 9 : 83/768 type remove
Layer 10 : 68/768 type remove
Layer 11 : 28/768 type remove
Layer 12 : 62/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:37:58,087 [trainer.py] => Time:104.68157029151917
300 300
300 300
2025-12-10 13:37:59,865 [trainer.py] => Time:1.7785961627960205
2025-12-10 13:37:59,866 [inflora.py] => Exemplar size: 0
2025-12-10 13:37:59,866 [trainer.py] => CNN: {'total': np.float64(71.33), '00-01': np.float64(80.65), '02-03': np.float64(70.97), '04-05': np.float64(59.55), '06-07': np.float64(75.0), 'old': np.float64(70.49), 'new': np.float64(75.0)}
2025-12-10 13:37:59,866 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33)]
2025-12-10 13:37:59,866 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67)]
2025-12-10 13:37:59,866 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334]
2025-12-10 13:38:00,836 [trainer.py] => All params: 144526051
2025-12-10 13:38:00,848 [trainer.py] => Trainable params: 2044438
2025-12-10 13:38:00,848 [inflora.py] => Learning on 8-10
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'classifier_pool.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'classifier_pool.43.bias', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight'}
2025-12-10 13:40:28,165 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.049, Train_accy 98.26
Threshold:  0.9808
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 33/768 type remove
Layer 5 : 52/768 type remove
Layer 6 : 48/768 type remove
Layer 7 : 52/768 type remove
Layer 8 : 64/768 type remove
Layer 9 : 88/768 type remove
Layer 10 : 76/768 type remove
Layer 11 : 34/768 type remove
Layer 12 : 64/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:40:35,680 [trainer.py] => Time:154.83135056495667
373 373
373 373
2025-12-10 13:40:37,667 [trainer.py] => Time:1.9873106479644775
2025-12-10 13:40:37,667 [inflora.py] => Exemplar size: 0
2025-12-10 13:40:37,667 [trainer.py] => CNN: {'total': np.float64(65.68), '00-01': np.float64(75.27), '02-03': np.float64(66.13), '04-05': np.float64(60.67), '06-07': np.float64(78.57), '08-09': np.float64(49.32), 'old': np.float64(69.67), 'new': np.float64(49.32)}
2025-12-10 13:40:37,668 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68)]
2025-12-10 13:40:37,668 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98)]
2025-12-10 13:40:37,668 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054]
2025-12-10 13:40:38,671 [trainer.py] => All params: 144526051
2025-12-10 13:40:38,684 [trainer.py] => Trainable params: 2044438
2025-12-10 13:40:38,684 [inflora.py] => Learning on 10-12
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.51.weight', 'image_encoder.blocks.8.attn.lora_B_k.53.weight', 'image_encoder.blocks.6.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.55.weight', 'image_encoder.blocks.1.attn.lora_B_k.51.weight', 'image_encoder.blocks.0.attn.lora_B_k.53.weight', 'image_encoder.blocks.3.attn.lora_B_v.52.weight', 'image_encoder.blocks.11.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_k.53.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_v.52.weight', 'image_encoder.blocks.1.attn.lora_B_v.50.weight', 'image_encoder.blocks.0.attn.lora_B_k.50.weight', 'image_encoder.blocks.2.attn.lora_B_k.55.weight', 'image_encoder.blocks.3.attn.lora_B_v.53.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'classifier_pool.50.weight', 'classifier_pool.58.bias', 'image_encoder.blocks.3.attn.lora_B_v.56.weight', 'image_encoder.blocks.4.attn.lora_B_k.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.57.weight', 'image_encoder.blocks.6.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.59.weight', 'image_encoder.blocks.1.attn.lora_B_k.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.57.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.55.weight', 'image_encoder.blocks.7.attn.lora_B_k.59.weight', 'image_encoder.blocks.7.attn.lora_B_v.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_v.59.weight', 'image_encoder.blocks.8.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.51.weight', 'image_encoder.blocks.8.attn.lora_B_v.57.weight', 'image_encoder.blocks.6.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_k.55.weight', 'image_encoder.blocks.10.attn.lora_B_v.52.weight', 'image_encoder.blocks.8.attn.lora_B_v.50.weight', 'image_encoder.blocks.6.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_k.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.54.weight', 'image_encoder.blocks.3.attn.lora_B_k.51.weight', 'classifier_pool.55.bias', 'image_encoder.blocks.11.attn.lora_B_v.51.weight', 'image_encoder.blocks.5.attn.lora_B_k.58.weight', 'image_encoder.blocks.0.attn.lora_B_v.50.weight', 'image_encoder.blocks.1.attn.lora_B_v.53.weight', 'image_encoder.blocks.9.attn.lora_B_v.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.52.weight', 'image_encoder.blocks.1.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_v.51.weight', 'image_encoder.blocks.6.attn.lora_B_v.55.weight', 'classifier_pool.57.weight', 'image_encoder.blocks.2.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_v.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.51.weight', 'image_encoder.blocks.3.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.52.weight', 'image_encoder.blocks.1.attn.lora_B_k.54.weight', 'image_encoder.blocks.2.attn.lora_B_v.50.weight', 'image_encoder.blocks.1.attn.lora_B_k.57.weight', 'image_encoder.blocks.4.attn.lora_B_v.54.weight', 'image_encoder.blocks.10.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_v.50.weight', 'image_encoder.blocks.8.attn.lora_B_v.52.weight', 'image_encoder.blocks.11.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_v.58.weight', 'image_encoder.blocks.9.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_k.55.weight', 'classifier_pool.53.bias', 'image_encoder.blocks.6.attn.lora_B_k.58.weight', 'image_encoder.blocks.0.attn.lora_B_v.59.weight', 'image_encoder.blocks.2.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_v.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.50.weight', 'image_encoder.blocks.3.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_k.52.weight', 'image_encoder.blocks.11.attn.lora_B_v.52.weight', 'image_encoder.blocks.7.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_k.58.weight', 'image_encoder.blocks.5.attn.lora_B_v.58.weight', 'image_encoder.blocks.3.attn.lora_B_v.59.weight', 'image_encoder.blocks.6.attn.lora_B_v.53.weight', 'image_encoder.blocks.11.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_v.50.weight', 'classifier_pool.56.weight', 'image_encoder.blocks.4.attn.lora_B_k.58.weight', 'image_encoder.blocks.9.attn.lora_B_k.57.weight', 'image_encoder.blocks.0.attn.lora_B_v.51.weight', 'image_encoder.blocks.3.attn.lora_B_v.58.weight', 'image_encoder.blocks.11.attn.lora_B_k.59.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'classifier_pool.58.weight', 'classifier_pool.57.bias', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_v.50.weight', 'classifier_pool.59.weight', 'image_encoder.blocks.2.attn.lora_B_v.51.weight', 'image_encoder.blocks.3.attn.lora_B_v.55.weight', 'image_encoder.blocks.8.attn.lora_B_k.50.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.55.weight', 'image_encoder.blocks.3.attn.lora_B_k.53.weight', 'image_encoder.blocks.3.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_k.56.weight', 'image_encoder.blocks.9.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.54.weight', 'image_encoder.blocks.5.attn.lora_B_k.51.weight', 'classifier_pool.50.bias', 'image_encoder.blocks.5.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_k.50.weight', 'classifier_pool.53.weight', 'image_encoder.blocks.0.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.59.weight', 'image_encoder.blocks.7.attn.lora_B_v.54.weight', 'image_encoder.blocks.3.attn.lora_B_k.55.weight', 'image_encoder.blocks.2.attn.lora_B_v.52.weight', 'image_encoder.blocks.3.attn.lora_B_k.50.weight', 'image_encoder.blocks.3.attn.lora_B_k.52.weight', 'image_encoder.blocks.8.attn.lora_B_v.59.weight', 'image_encoder.blocks.10.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_v.57.weight', 'image_encoder.blocks.11.attn.lora_B_k.57.weight', 'image_encoder.blocks.9.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.58.weight', 'image_encoder.blocks.7.attn.lora_B_k.55.weight', 'image_encoder.blocks.6.attn.lora_B_v.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.55.weight', 'classifier_pool.51.weight', 'image_encoder.blocks.8.attn.lora_B_v.51.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.57.weight', 'image_encoder.blocks.8.attn.lora_B_k.54.weight', 'image_encoder.blocks.2.attn.lora_B_k.59.weight', 'image_encoder.blocks.9.attn.lora_B_k.59.weight', 'classifier_pool.56.bias', 'classifier_pool.52.bias', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.59.weight', 'image_encoder.blocks.9.attn.lora_B_k.58.weight', 'image_encoder.blocks.11.attn.lora_B_k.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.52.weight', 'image_encoder.blocks.2.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.58.weight', 'image_encoder.blocks.1.attn.lora_B_v.54.weight', 'image_encoder.blocks.2.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_v.59.weight', 'image_encoder.blocks.6.attn.lora_B_v.52.weight', 'image_encoder.blocks.10.attn.lora_B_k.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.50.weight', 'image_encoder.blocks.1.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.54.weight', 'image_encoder.blocks.1.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_k.51.weight', 'image_encoder.blocks.9.attn.lora_B_v.57.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.55.weight', 'image_encoder.blocks.6.attn.lora_B_k.52.weight', 'image_encoder.blocks.9.attn.lora_B_v.52.weight', 'image_encoder.blocks.4.attn.lora_B_v.53.weight', 'image_encoder.blocks.4.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.59.weight', 'image_encoder.blocks.0.attn.lora_B_k.59.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.53.weight', 'image_encoder.blocks.8.attn.lora_B_k.58.weight', 'image_encoder.blocks.4.attn.lora_B_v.50.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.58.weight', 'image_encoder.blocks.0.attn.lora_B_v.55.weight', 'image_encoder.blocks.1.attn.lora_B_v.56.weight', 'image_encoder.blocks.3.attn.lora_B_k.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.54.weight', 'image_encoder.blocks.10.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_k.51.weight', 'classifier_pool.52.weight', 'classifier_pool.59.bias', 'image_encoder.blocks.5.attn.lora_B_v.56.weight', 'image_encoder.blocks.8.attn.lora_B_v.55.weight', 'image_encoder.blocks.6.attn.lora_B_k.57.weight', 'image_encoder.blocks.2.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_v.56.weight', 'image_encoder.blocks.1.attn.lora_B_k.59.weight', 'classifier_pool.51.bias', 'classifier_pool.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.53.weight', 'image_encoder.blocks.11.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.57.weight', 'image_encoder.blocks.8.attn.lora_B_k.55.weight', 'image_encoder.blocks.10.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.59.weight', 'image_encoder.blocks.9.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_k.50.weight', 'image_encoder.blocks.10.attn.lora_B_k.58.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.58.weight', 'image_encoder.blocks.1.attn.lora_B_v.59.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.7.attn.lora_B_k.57.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.53.weight', 'image_encoder.blocks.11.attn.lora_B_v.59.weight', 'image_encoder.blocks.5.attn.lora_B_k.59.weight', 'image_encoder.blocks.10.attn.lora_B_k.51.weight', 'image_encoder.blocks.3.attn.lora_B_v.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.57.weight', 'image_encoder.blocks.5.attn.lora_B_k.53.weight', 'image_encoder.blocks.0.attn.lora_B_k.51.weight', 'image_encoder.blocks.7.attn.lora_B_v.53.weight', 'image_encoder.blocks.8.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.55.weight', 'image_encoder.blocks.10.attn.lora_B_k.54.weight', 'image_encoder.blocks.6.attn.lora_B_k.54.weight', 'classifier_pool.54.weight', 'image_encoder.blocks.0.attn.lora_B_v.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.57.weight', 'image_encoder.blocks.10.attn.lora_B_v.54.weight', 'image_encoder.blocks.4.attn.lora_B_k.51.weight', 'image_encoder.blocks.5.attn.lora_B_v.52.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.58.weight', 'image_encoder.blocks.10.attn.lora_B_v.56.weight', 'image_encoder.blocks.9.attn.lora_B_v.54.weight', 'image_encoder.blocks.10.attn.lora_B_k.52.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.56.weight', 'image_encoder.blocks.8.attn.lora_B_k.57.weight', 'image_encoder.blocks.9.attn.lora_B_k.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.56.weight', 'image_encoder.blocks.6.attn.lora_B_k.55.weight', 'image_encoder.blocks.5.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_v.56.weight', 'image_encoder.blocks.5.attn.lora_B_k.50.weight', 'image_encoder.blocks.10.attn.lora_B_v.53.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.59.weight', 'image_encoder.blocks.4.attn.lora_B_v.58.weight', 'image_encoder.blocks.2.attn.lora_B_v.56.weight', 'image_encoder.blocks.10.attn.lora_B_k.59.weight', 'image_encoder.blocks.2.attn.lora_B_k.56.weight', 'image_encoder.blocks.9.attn.lora_B_k.50.weight', 'image_encoder.blocks.6.attn.lora_B_v.51.weight', 'image_encoder.blocks.0.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_k.53.weight', 'classifier_pool.54.bias', 'image_encoder.blocks.4.attn.lora_B_k.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_k.51.weight', 'classifier_pool.55.weight', 'image_encoder.blocks.8.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.58.weight', 'image_encoder.blocks.10.attn.lora_B_v.57.weight', 'image_encoder.blocks.9.attn.lora_B_v.58.weight'}
2025-12-10 13:42:22,668 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.041, Train_accy 98.51
Threshold:  0.981
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 35/768 type remove
Layer 5 : 54/768 type remove
Layer 6 : 50/768 type remove
Layer 7 : 54/768 type remove
Layer 8 : 66/768 type remove
Layer 9 : 90/768 type remove
Layer 10 : 80/768 type remove
Layer 11 : 37/768 type remove
Layer 12 : 70/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:42:29,477 [trainer.py] => Time:110.79285931587219
414 414
414 414
2025-12-10 13:42:31,450 [trainer.py] => Time:1.972658395767212
2025-12-10 13:42:31,450 [inflora.py] => Exemplar size: 0
2025-12-10 13:42:31,450 [trainer.py] => CNN: {'total': np.float64(62.8), '00-01': np.float64(77.42), '02-03': np.float64(67.74), '04-05': np.float64(61.8), '06-07': np.float64(69.64), '08-09': np.float64(49.32), '10-11': np.float64(39.02), 'old': np.float64(65.42), 'new': np.float64(39.02)}
2025-12-10 13:42:31,450 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8)]
2025-12-10 13:42:31,450 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17)]
2025-12-10 13:42:31,450 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976]
2025-12-10 13:42:32,409 [trainer.py] => All params: 144526051
2025-12-10 13:42:32,421 [trainer.py] => Trainable params: 2044438
2025-12-10 13:42:32,421 [inflora.py] => Learning on 12-14
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.67.weight', 'image_encoder.blocks.7.attn.lora_B_k.64.weight', 'image_encoder.blocks.8.attn.lora_B_k.60.weight', 'image_encoder.blocks.8.attn.lora_B_v.69.weight', 'image_encoder.blocks.3.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.66.weight', 'classifier_pool.62.weight', 'image_encoder.blocks.1.attn.lora_B_k.63.weight', 'image_encoder.blocks.2.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_k.60.weight', 'image_encoder.blocks.7.attn.lora_B_v.65.weight', 'image_encoder.blocks.10.attn.lora_B_k.69.weight', 'image_encoder.blocks.5.attn.lora_B_v.68.weight', 'image_encoder.blocks.5.attn.lora_B_k.63.weight', 'image_encoder.blocks.3.attn.lora_B_k.66.weight', 'image_encoder.blocks.10.attn.lora_B_k.63.weight', 'image_encoder.blocks.1.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.63.weight', 'image_encoder.blocks.9.attn.lora_B_v.60.weight', 'image_encoder.blocks.1.attn.lora_B_v.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.65.weight', 'image_encoder.blocks.10.attn.lora_B_k.62.weight', 'image_encoder.blocks.10.attn.lora_B_v.69.weight', 'image_encoder.blocks.11.attn.lora_B_k.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_v.62.weight', 'image_encoder.blocks.5.attn.lora_B_k.61.weight', 'image_encoder.blocks.2.attn.lora_B_k.61.weight', 'image_encoder.blocks.2.attn.lora_B_v.60.weight', 'image_encoder.blocks.3.attn.lora_B_k.61.weight', 'image_encoder.blocks.4.attn.lora_B_k.69.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.60.weight', 'image_encoder.blocks.7.attn.lora_B_k.66.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.66.weight', 'image_encoder.blocks.1.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_k.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.63.weight', 'classifier_pool.66.weight', 'image_encoder.blocks.4.attn.lora_B_k.64.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.61.weight', 'classifier_pool.66.bias', 'image_encoder.blocks.2.attn.lora_B_k.63.weight', 'image_encoder.blocks.8.attn.lora_B_k.61.weight', 'image_encoder.blocks.10.attn.lora_B_k.61.weight', 'image_encoder.blocks.11.attn.lora_B_k.66.weight', 'image_encoder.blocks.2.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.63.weight', 'image_encoder.blocks.4.attn.lora_B_v.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.64.weight', 'image_encoder.blocks.11.attn.lora_B_k.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.67.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.62.weight', 'image_encoder.blocks.7.attn.lora_B_k.61.weight', 'image_encoder.blocks.8.attn.lora_B_k.63.weight', 'image_encoder.blocks.1.attn.lora_B_k.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.60.weight', 'image_encoder.blocks.9.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_v.69.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_k.69.weight', 'image_encoder.blocks.0.attn.lora_B_v.66.weight', 'image_encoder.blocks.9.attn.lora_B_v.69.weight', 'image_encoder.blocks.8.attn.lora_B_k.69.weight', 'image_encoder.blocks.0.attn.lora_B_v.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.62.weight', 'image_encoder.blocks.11.attn.lora_B_k.69.weight', 'image_encoder.blocks.2.attn.lora_B_k.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.60.weight', 'image_encoder.blocks.2.attn.lora_B_k.68.weight', 'image_encoder.blocks.9.attn.lora_B_v.67.weight', 'image_encoder.blocks.7.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_k.63.weight', 'image_encoder.blocks.10.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.64.weight', 'image_encoder.blocks.3.attn.lora_B_k.65.weight', 'image_encoder.blocks.0.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.66.weight', 'image_encoder.blocks.3.attn.lora_B_v.65.weight', 'image_encoder.blocks.4.attn.lora_B_v.62.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.63.weight', 'image_encoder.blocks.4.attn.lora_B_v.60.weight', 'classifier_pool.67.weight', 'image_encoder.blocks.10.attn.lora_B_v.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_k.67.weight', 'image_encoder.blocks.10.attn.lora_B_k.64.weight', 'image_encoder.blocks.3.attn.lora_B_v.69.weight', 'image_encoder.blocks.4.attn.lora_B_k.60.weight', 'image_encoder.blocks.1.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.65.weight', 'image_encoder.blocks.4.attn.lora_B_k.65.weight', 'classifier_pool.61.weight', 'image_encoder.blocks.4.attn.lora_B_v.68.weight', 'image_encoder.blocks.11.attn.lora_B_v.68.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.67.weight', 'image_encoder.blocks.9.attn.lora_B_k.68.weight', 'classifier_pool.62.bias', 'image_encoder.blocks.10.attn.lora_B_k.68.weight', 'image_encoder.blocks.0.attn.lora_B_k.62.weight', 'image_encoder.blocks.3.attn.lora_B_v.62.weight', 'image_encoder.blocks.7.attn.lora_B_v.60.weight', 'image_encoder.blocks.7.attn.lora_B_k.68.weight', 'image_encoder.blocks.10.attn.lora_B_k.67.weight', 'image_encoder.blocks.0.attn.lora_B_k.61.weight', 'image_encoder.blocks.10.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.66.weight', 'image_encoder.blocks.9.attn.lora_B_v.68.weight', 'image_encoder.blocks.8.attn.lora_B_v.63.weight', 'image_encoder.blocks.11.attn.lora_B_k.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_v.63.weight', 'classifier_pool.68.weight', 'image_encoder.blocks.8.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.66.weight', 'image_encoder.blocks.1.attn.lora_B_v.67.weight', 'image_encoder.blocks.1.attn.lora_B_k.67.weight', 'image_encoder.blocks.6.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_k.61.weight', 'classifier_pool.65.bias', 'classifier_pool.69.weight', 'image_encoder.blocks.2.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.62.weight', 'image_encoder.blocks.7.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_v.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.64.weight', 'classifier_pool.64.bias', 'image_encoder.blocks.0.attn.lora_B_v.68.weight', 'image_encoder.blocks.1.attn.lora_B_k.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_k.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.67.weight', 'image_encoder.blocks.9.attn.lora_B_v.65.weight', 'image_encoder.blocks.10.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.65.weight', 'image_encoder.blocks.4.attn.lora_B_k.68.weight', 'image_encoder.blocks.0.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.65.weight', 'image_encoder.blocks.2.attn.lora_B_v.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_v.60.weight', 'image_encoder.blocks.3.attn.lora_B_k.67.weight', 'image_encoder.blocks.3.attn.lora_B_v.60.weight', 'image_encoder.blocks.6.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_v.61.weight', 'image_encoder.blocks.11.attn.lora_B_v.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.66.weight', 'image_encoder.blocks.0.attn.lora_B_v.61.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.3.attn.lora_B_v.61.weight', 'image_encoder.blocks.7.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.63.weight', 'image_encoder.blocks.2.attn.lora_B_k.64.weight', 'image_encoder.blocks.10.attn.lora_B_v.62.weight', 'image_encoder.blocks.9.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_k.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_v.66.weight', 'image_encoder.blocks.2.attn.lora_B_v.66.weight', 'image_encoder.blocks.2.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_k.66.weight', 'image_encoder.blocks.2.attn.lora_B_k.65.weight', 'image_encoder.blocks.2.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_v.67.weight', 'image_encoder.blocks.6.attn.lora_B_k.63.weight', 'image_encoder.blocks.0.attn.lora_B_v.65.weight', 'image_encoder.blocks.6.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.63.weight', 'image_encoder.blocks.5.attn.lora_B_v.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.63.weight', 'image_encoder.blocks.9.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.60.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.66.weight', 'image_encoder.blocks.5.attn.lora_B_v.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.68.weight', 'classifier_pool.63.bias', 'image_encoder.blocks.0.attn.lora_B_v.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.68.weight', 'image_encoder.blocks.2.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.60.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.62.weight', 'image_encoder.blocks.9.attn.lora_B_v.66.weight', 'classifier_pool.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.67.weight', 'image_encoder.blocks.5.attn.lora_B_v.62.weight', 'image_encoder.blocks.7.attn.lora_B_k.65.weight', 'image_encoder.blocks.0.attn.lora_B_k.65.weight', 'classifier_pool.60.bias', 'image_encoder.blocks.3.attn.lora_B_k.64.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.61.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'classifier_pool.63.weight', 'image_encoder.blocks.8.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.61.weight', 'image_encoder.blocks.9.attn.lora_B_v.62.weight', 'classifier_pool.60.weight', 'image_encoder.blocks.0.attn.lora_B_v.63.weight', 'image_encoder.blocks.9.attn.lora_B_k.66.weight', 'classifier_pool.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_v.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.69.weight', 'image_encoder.blocks.2.attn.lora_B_v.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.67.weight', 'classifier_pool.61.bias', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.62.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_v.62.weight', 'image_encoder.blocks.0.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.69.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.67.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.64.weight', 'image_encoder.blocks.6.attn.lora_B_k.61.weight', 'image_encoder.blocks.6.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.64.weight', 'classifier_pool.68.bias', 'image_encoder.blocks.9.attn.lora_B_k.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.64.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_v.67.weight', 'image_encoder.blocks.1.attn.lora_B_k.65.weight', 'image_encoder.blocks.6.attn.lora_B_v.69.weight', 'image_encoder.blocks.7.attn.lora_B_v.66.weight', 'image_encoder.blocks.1.attn.lora_B_k.69.weight', 'image_encoder.blocks.7.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_k.65.weight', 'image_encoder.blocks.10.attn.lora_B_v.66.weight', 'image_encoder.blocks.4.attn.lora_B_k.61.weight', 'image_encoder.blocks.9.attn.lora_B_k.64.weight', 'classifier_pool.67.bias', 'image_encoder.blocks.5.attn.lora_B_k.65.weight', 'image_encoder.blocks.4.attn.lora_B_k.66.weight', 'classifier_pool.69.bias', 'image_encoder.blocks.10.attn.lora_B_v.64.weight', 'image_encoder.blocks.0.attn.lora_B_v.60.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_v.63.weight', 'image_encoder.blocks.8.attn.lora_B_k.64.weight', 'image_encoder.blocks.9.attn.lora_B_k.61.weight', 'image_encoder.blocks.7.attn.lora_B_v.62.weight'}
2025-12-10 13:44:20,254 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.022, Train_accy 99.55
Threshold:  0.9812
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 33/768 type remove
Layer 4 : 40/768 type remove
Layer 5 : 60/768 type remove
Layer 6 : 53/768 type remove
Layer 7 : 58/768 type remove
Layer 8 : 69/768 type remove
Layer 9 : 92/768 type remove
Layer 10 : 84/768 type remove
Layer 11 : 40/768 type remove
Layer 12 : 73/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:44:26,989 [trainer.py] => Time:114.56787514686584
468 468
468 468
2025-12-10 13:44:29,218 [trainer.py] => Time:2.2284443378448486
2025-12-10 13:44:29,218 [inflora.py] => Exemplar size: 0
2025-12-10 13:44:29,218 [trainer.py] => CNN: {'total': np.float64(61.75), '00-01': np.float64(76.34), '02-03': np.float64(64.52), '04-05': np.float64(60.67), '06-07': np.float64(73.21), '08-09': np.float64(49.32), '10-11': np.float64(36.59), '12-13': np.float64(59.26), 'old': np.float64(62.08), 'new': np.float64(59.26)}
2025-12-10 13:44:29,218 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75)]
2025-12-10 13:44:29,218 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51)]
2025-12-10 13:44:29,218 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675]
2025-12-10 13:44:30,338 [trainer.py] => All params: 144526051
2025-12-10 13:44:30,350 [trainer.py] => Trainable params: 2044438
2025-12-10 13:44:30,350 [inflora.py] => Learning on 14-16
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.79.weight', 'image_encoder.blocks.2.attn.lora_B_v.75.weight', 'image_encoder.blocks.9.attn.lora_B_k.78.weight', 'image_encoder.blocks.4.attn.lora_B_v.75.weight', 'image_encoder.blocks.6.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_k.74.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.72.weight', 'image_encoder.blocks.11.attn.lora_B_v.72.weight', 'image_encoder.blocks.11.attn.lora_B_k.73.weight', 'image_encoder.blocks.10.attn.lora_B_k.76.weight', 'image_encoder.blocks.6.attn.lora_B_k.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_v.70.weight', 'image_encoder.blocks.0.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_k.75.weight', 'classifier_pool.72.bias', 'image_encoder.blocks.7.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.70.weight', 'image_encoder.blocks.8.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.77.weight', 'image_encoder.blocks.8.attn.lora_B_v.79.weight', 'classifier_pool.78.weight', 'classifier_pool.70.weight', 'image_encoder.blocks.0.attn.lora_B_v.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.77.weight', 'image_encoder.blocks.7.attn.lora_B_k.74.weight', 'image_encoder.blocks.9.attn.lora_B_k.71.weight', 'image_encoder.blocks.7.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_v.73.weight', 'classifier_pool.79.weight', 'image_encoder.blocks.10.attn.lora_B_k.72.weight', 'image_encoder.blocks.1.attn.lora_B_v.71.weight', 'image_encoder.blocks.2.attn.lora_B_k.73.weight', 'image_encoder.blocks.2.attn.lora_B_v.70.weight', 'image_encoder.blocks.4.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_k.77.weight', 'image_encoder.blocks.6.attn.lora_B_v.79.weight', 'image_encoder.blocks.9.attn.lora_B_k.77.weight', 'image_encoder.blocks.11.attn.lora_B_v.74.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.74.weight', 'image_encoder.blocks.10.attn.lora_B_v.77.weight', 'image_encoder.blocks.3.attn.lora_B_v.79.weight', 'image_encoder.blocks.9.attn.lora_B_k.70.weight', 'image_encoder.blocks.4.attn.lora_B_v.72.weight', 'image_encoder.blocks.11.attn.lora_B_k.71.weight', 'image_encoder.blocks.3.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.72.weight', 'classifier_pool.71.weight', 'image_encoder.blocks.6.attn.lora_B_k.77.weight', 'classifier_pool.73.bias', 'image_encoder.blocks.10.attn.lora_B_v.73.weight', 'image_encoder.blocks.3.attn.lora_B_k.75.weight', 'image_encoder.blocks.4.attn.lora_B_v.79.weight', 'image_encoder.blocks.1.attn.lora_B_v.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.78.weight', 'image_encoder.blocks.6.attn.lora_B_k.70.weight', 'image_encoder.blocks.6.attn.lora_B_k.76.weight', 'image_encoder.blocks.5.attn.lora_B_v.71.weight', 'image_encoder.blocks.9.attn.lora_B_v.77.weight', 'image_encoder.blocks.10.attn.lora_B_k.71.weight', 'classifier_pool.70.bias', 'image_encoder.blocks.6.attn.lora_B_k.73.weight', 'image_encoder.blocks.7.attn.lora_B_k.70.weight', 'image_encoder.blocks.2.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.70.weight', 'image_encoder.blocks.1.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_k.77.weight', 'image_encoder.blocks.5.attn.lora_B_v.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.75.weight', 'image_encoder.blocks.6.attn.lora_B_v.71.weight', 'image_encoder.blocks.9.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.74.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.71.weight', 'image_encoder.blocks.9.attn.lora_B_k.75.weight', 'image_encoder.blocks.7.attn.lora_B_v.79.weight', 'image_encoder.blocks.9.attn.lora_B_v.78.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.78.weight', 'image_encoder.blocks.4.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.78.weight', 'image_encoder.blocks.3.attn.lora_B_v.76.weight', 'image_encoder.blocks.8.attn.lora_B_k.76.weight', 'image_encoder.blocks.9.attn.lora_B_k.72.weight', 'image_encoder.blocks.9.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.78.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.75.weight', 'image_encoder.blocks.6.attn.lora_B_v.73.weight', 'image_encoder.blocks.1.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.78.weight', 'image_encoder.blocks.5.attn.lora_B_k.70.weight', 'image_encoder.blocks.6.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_k.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.72.weight', 'image_encoder.blocks.1.attn.lora_B_k.74.weight', 'image_encoder.blocks.11.attn.lora_B_v.75.weight', 'classifier_pool.74.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.73.weight', 'image_encoder.blocks.0.attn.lora_B_k.71.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.74.weight', 'image_encoder.blocks.7.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.77.weight', 'image_encoder.blocks.1.attn.lora_B_v.76.weight', 'image_encoder.blocks.3.attn.lora_B_v.72.weight', 'image_encoder.blocks.7.attn.lora_B_k.75.weight', 'image_encoder.blocks.8.attn.lora_B_k.74.weight', 'image_encoder.blocks.9.attn.lora_B_v.70.weight', 'image_encoder.blocks.2.attn.lora_B_k.78.weight', 'image_encoder.blocks.6.attn.lora_B_v.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.70.weight', 'image_encoder.blocks.6.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_v.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.71.weight', 'image_encoder.blocks.9.attn.lora_B_v.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.79.weight', 'image_encoder.blocks.8.attn.lora_B_v.70.weight', 'image_encoder.blocks.4.attn.lora_B_k.74.weight', 'image_encoder.blocks.4.attn.lora_B_k.75.weight', 'image_encoder.blocks.1.attn.lora_B_v.78.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.70.weight', 'image_encoder.blocks.9.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_v.78.weight', 'image_encoder.blocks.0.attn.lora_B_v.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.75.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.11.attn.lora_B_k.76.weight', 'classifier_pool.77.bias', 'image_encoder.blocks.7.attn.lora_B_k.71.weight', 'image_encoder.blocks.8.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.79.weight', 'image_encoder.blocks.4.attn.lora_B_k.72.weight', 'classifier_pool.78.bias', 'image_encoder.blocks.7.attn.lora_B_k.76.weight', 'image_encoder.blocks.8.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.75.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.75.weight', 'image_encoder.blocks.2.attn.lora_B_k.74.weight', 'image_encoder.blocks.5.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.72.weight', 'image_encoder.blocks.5.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.75.weight', 'classifier_pool.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.71.weight', 'image_encoder.blocks.7.attn.lora_B_v.74.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.79.weight', 'classifier_pool.71.bias', 'image_encoder.blocks.11.attn.lora_B_k.70.weight', 'image_encoder.blocks.4.attn.lora_B_v.71.weight', 'image_encoder.blocks.5.attn.lora_B_k.76.weight', 'image_encoder.blocks.5.attn.lora_B_v.78.weight', 'image_encoder.blocks.5.attn.lora_B_v.77.weight', 'image_encoder.blocks.5.attn.lora_B_v.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_k.71.weight', 'image_encoder.blocks.10.attn.lora_B_k.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.77.weight', 'image_encoder.blocks.10.attn.lora_B_v.74.weight', 'image_encoder.blocks.10.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.79.weight', 'classifier_pool.72.weight', 'image_encoder.blocks.6.attn.lora_B_v.70.weight', 'classifier_pool.74.bias', 'classifier_pool.75.weight', 'classifier_pool.75.bias', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.70.weight', 'image_encoder.blocks.8.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.79.weight', 'image_encoder.blocks.2.attn.lora_B_k.77.weight', 'image_encoder.blocks.3.attn.lora_B_k.74.weight', 'image_encoder.blocks.5.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.72.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.76.weight', 'image_encoder.blocks.9.attn.lora_B_v.74.weight', 'image_encoder.blocks.0.attn.lora_B_v.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.77.weight', 'image_encoder.blocks.9.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_v.73.weight', 'image_encoder.blocks.1.attn.lora_B_k.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.76.weight', 'image_encoder.blocks.9.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.76.weight', 'image_encoder.blocks.3.attn.lora_B_k.72.weight', 'classifier_pool.73.weight', 'image_encoder.blocks.0.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_v.74.weight', 'image_encoder.blocks.9.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_v.72.weight', 'image_encoder.blocks.7.attn.lora_B_v.71.weight', 'image_encoder.blocks.4.attn.lora_B_v.78.weight', 'image_encoder.blocks.5.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.73.weight', 'classifier_pool.76.bias', 'image_encoder.blocks.8.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_k.79.weight', 'image_encoder.blocks.2.attn.lora_B_v.72.weight', 'image_encoder.blocks.5.attn.lora_B_k.78.weight', 'classifier_pool.79.bias', 'image_encoder.blocks.1.attn.lora_B_v.73.weight', 'image_encoder.blocks.6.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_v.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.77.weight', 'image_encoder.blocks.0.attn.lora_B_v.70.weight', 'image_encoder.blocks.2.attn.lora_B_v.74.weight', 'image_encoder.blocks.8.attn.lora_B_v.74.weight', 'image_encoder.blocks.0.attn.lora_B_v.73.weight', 'image_encoder.blocks.10.attn.lora_B_k.78.weight', 'image_encoder.blocks.7.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.76.weight', 'image_encoder.blocks.11.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.72.weight', 'image_encoder.blocks.10.attn.lora_B_v.72.weight', 'image_encoder.blocks.10.attn.lora_B_k.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.73.weight', 'image_encoder.blocks.3.attn.lora_B_k.77.weight', 'image_encoder.blocks.3.attn.lora_B_k.79.weight', 'image_encoder.blocks.2.attn.lora_B_v.73.weight', 'image_encoder.blocks.8.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.78.weight', 'image_encoder.blocks.3.attn.lora_B_v.75.weight', 'image_encoder.blocks.3.attn.lora_B_k.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.79.weight', 'image_encoder.blocks.1.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_k.76.weight', 'classifier_pool.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.78.weight', 'image_encoder.blocks.0.attn.lora_B_k.79.weight', 'image_encoder.blocks.0.attn.lora_B_v.71.weight'}
2025-12-10 13:46:22,757 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.033, Train_accy 98.67
Threshold:  0.9813999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 36/768 type remove
Layer 4 : 44/768 type remove
Layer 5 : 64/768 type remove
Layer 6 : 56/768 type remove
Layer 7 : 61/768 type remove
Layer 8 : 72/768 type remove
Layer 9 : 97/768 type remove
Layer 10 : 97/768 type remove
Layer 11 : 49/768 type remove
Layer 12 : 79/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:46:29,593 [trainer.py] => Time:119.24339246749878
524 524
524 524
2025-12-10 13:46:31,931 [trainer.py] => Time:2.3375437259674072
2025-12-10 13:46:31,931 [inflora.py] => Exemplar size: 0
2025-12-10 13:46:31,931 [trainer.py] => CNN: {'total': np.float64(61.07), '00-01': np.float64(75.27), '02-03': np.float64(66.13), '04-05': np.float64(62.92), '06-07': np.float64(71.43), '08-09': np.float64(46.58), '10-11': np.float64(34.15), '12-13': np.float64(57.41), '14-15': np.float64(60.71), 'old': np.float64(61.11), 'new': np.float64(60.71)}
2025-12-10 13:46:31,931 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07)]
2025-12-10 13:46:31,931 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23)]
2025-12-10 13:46:31,931 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634]
2025-12-10 13:46:32,890 [trainer.py] => All params: 144526051
2025-12-10 13:46:32,902 [trainer.py] => Trainable params: 2044438
2025-12-10 13:46:32,902 [inflora.py] => Learning on 16-18
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.83.weight', 'image_encoder.blocks.1.attn.lora_B_v.84.weight', 'image_encoder.blocks.7.attn.lora_B_k.82.weight', 'image_encoder.blocks.2.attn.lora_B_v.82.weight', 'image_encoder.blocks.9.attn.lora_B_v.81.weight', 'image_encoder.blocks.1.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.83.weight', 'image_encoder.blocks.4.attn.lora_B_v.87.weight', 'classifier_pool.84.weight', 'image_encoder.blocks.0.attn.lora_B_k.85.weight', 'image_encoder.blocks.4.attn.lora_B_k.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.84.weight', 'image_encoder.blocks.0.attn.lora_B_v.89.weight', 'image_encoder.blocks.0.attn.lora_B_k.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_k.81.weight', 'image_encoder.blocks.2.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_k.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.87.weight', 'image_encoder.blocks.4.attn.lora_B_v.86.weight', 'image_encoder.blocks.1.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.87.weight', 'image_encoder.blocks.7.attn.lora_B_k.83.weight', 'image_encoder.blocks.3.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_v.81.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.4.attn.lora_B_v.82.weight', 'image_encoder.blocks.8.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_v.88.weight', 'classifier_pool.82.bias', 'image_encoder.blocks.5.attn.lora_B_k.80.weight', 'image_encoder.blocks.5.attn.lora_B_k.85.weight', 'image_encoder.blocks.6.attn.lora_B_v.82.weight', 'image_encoder.blocks.8.attn.lora_B_k.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.80.weight', 'image_encoder.blocks.9.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_v.80.weight', 'image_encoder.blocks.11.attn.lora_B_v.84.weight', 'classifier_pool.85.weight', 'image_encoder.blocks.11.attn.lora_B_v.86.weight', 'image_encoder.blocks.5.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.81.weight', 'image_encoder.blocks.8.attn.lora_B_v.85.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.87.weight', 'image_encoder.blocks.5.attn.lora_B_v.88.weight', 'image_encoder.blocks.8.attn.lora_B_k.85.weight', 'image_encoder.blocks.1.attn.lora_B_k.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.86.weight', 'image_encoder.blocks.5.attn.lora_B_v.84.weight', 'image_encoder.blocks.6.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.87.weight', 'image_encoder.blocks.8.attn.lora_B_k.80.weight', 'image_encoder.blocks.11.attn.lora_B_k.81.weight', 'image_encoder.blocks.5.attn.lora_B_k.82.weight', 'image_encoder.blocks.5.attn.lora_B_v.89.weight', 'image_encoder.blocks.7.attn.lora_B_v.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'classifier_pool.86.weight', 'classifier_pool.86.bias', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.80.weight', 'image_encoder.blocks.6.attn.lora_B_v.88.weight', 'classifier_pool.80.bias', 'image_encoder.blocks.1.attn.lora_B_k.82.weight', 'image_encoder.blocks.3.attn.lora_B_k.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.81.weight', 'image_encoder.blocks.8.attn.lora_B_v.84.weight', 'image_encoder.blocks.1.attn.lora_B_k.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.81.weight', 'image_encoder.blocks.3.attn.lora_B_k.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.80.weight', 'image_encoder.blocks.4.attn.lora_B_k.87.weight', 'classifier_pool.87.bias', 'image_encoder.blocks.3.attn.lora_B_v.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.83.weight', 'image_encoder.blocks.7.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.83.weight', 'image_encoder.blocks.4.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_v.86.weight', 'image_encoder.blocks.5.attn.lora_B_v.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.88.weight', 'image_encoder.blocks.9.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.89.weight', 'image_encoder.blocks.2.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_v.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_k.86.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.87.weight', 'image_encoder.blocks.8.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_v.83.weight', 'image_encoder.blocks.6.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_v.88.weight', 'classifier_pool.83.bias', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_k.88.weight', 'image_encoder.blocks.8.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.87.weight', 'image_encoder.blocks.2.attn.lora_B_v.89.weight', 'image_encoder.blocks.8.attn.lora_B_k.83.weight', 'image_encoder.blocks.2.attn.lora_B_k.88.weight', 'image_encoder.blocks.0.attn.lora_B_k.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.88.weight', 'image_encoder.blocks.0.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.86.weight', 'image_encoder.blocks.11.attn.lora_B_v.89.weight', 'classifier_pool.88.bias', 'image_encoder.blocks.2.attn.lora_B_k.85.weight', 'classifier_pool.83.weight', 'image_encoder.blocks.7.attn.lora_B_k.85.weight', 'image_encoder.blocks.1.attn.lora_B_v.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.87.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_v.80.weight', 'image_encoder.blocks.4.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_k.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.87.weight', 'image_encoder.blocks.11.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.83.weight', 'classifier_pool.85.bias', 'image_encoder.blocks.3.attn.lora_B_v.85.weight', 'classifier_pool.82.weight', 'image_encoder.blocks.8.attn.lora_B_v.89.weight', 'classifier_pool.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.85.weight', 'image_encoder.blocks.11.attn.lora_B_v.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.83.weight', 'image_encoder.blocks.6.attn.lora_B_v.86.weight', 'image_encoder.blocks.0.attn.lora_B_k.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.81.weight', 'image_encoder.blocks.2.attn.lora_B_v.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.88.weight', 'image_encoder.blocks.6.attn.lora_B_v.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.87.weight', 'image_encoder.blocks.6.attn.lora_B_k.85.weight', 'image_encoder.blocks.3.attn.lora_B_v.89.weight', 'image_encoder.blocks.7.attn.lora_B_v.89.weight', 'classifier_pool.80.weight', 'image_encoder.blocks.4.attn.lora_B_v.80.weight', 'classifier_pool.81.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.85.weight', 'image_encoder.blocks.0.attn.lora_B_v.84.weight', 'classifier_pool.88.weight', 'image_encoder.blocks.2.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.82.weight', 'image_encoder.blocks.1.attn.lora_B_k.83.weight', 'image_encoder.blocks.3.attn.lora_B_k.88.weight', 'image_encoder.blocks.8.attn.lora_B_v.88.weight', 'image_encoder.blocks.0.attn.lora_B_k.80.weight', 'image_encoder.blocks.0.attn.lora_B_v.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.84.weight', 'image_encoder.blocks.5.attn.lora_B_k.89.weight', 'image_encoder.blocks.7.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_k.85.weight', 'image_encoder.blocks.8.attn.lora_B_k.84.weight', 'image_encoder.blocks.11.attn.lora_B_k.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.84.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_v.87.weight', 'classifier_pool.89.bias', 'image_encoder.blocks.2.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.89.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.87.weight', 'image_encoder.blocks.7.attn.lora_B_v.85.weight', 'image_encoder.blocks.9.attn.lora_B_v.86.weight', 'image_encoder.blocks.10.attn.lora_B_k.84.weight', 'classifier_pool.84.bias', 'image_encoder.blocks.0.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.89.weight', 'image_encoder.blocks.2.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_k.88.weight', 'image_encoder.blocks.3.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_k.87.weight', 'image_encoder.blocks.6.attn.lora_B_v.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_k.87.weight', 'image_encoder.blocks.3.attn.lora_B_k.85.weight', 'image_encoder.blocks.1.attn.lora_B_k.85.weight', 'image_encoder.blocks.7.attn.lora_B_v.81.weight', 'image_encoder.blocks.11.attn.lora_B_v.85.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_k.83.weight', 'image_encoder.blocks.2.attn.lora_B_v.87.weight', 'image_encoder.blocks.4.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.82.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_v.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.88.weight', 'image_encoder.blocks.11.attn.lora_B_k.85.weight', 'image_encoder.blocks.2.attn.lora_B_v.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.88.weight', 'image_encoder.blocks.0.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_k.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.86.weight', 'image_encoder.blocks.9.attn.lora_B_v.85.weight', 'image_encoder.blocks.6.attn.lora_B_v.89.weight', 'image_encoder.blocks.9.attn.lora_B_k.81.weight', 'image_encoder.blocks.6.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.86.weight', 'image_encoder.blocks.9.attn.lora_B_v.88.weight', 'image_encoder.blocks.8.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.81.weight', 'image_encoder.blocks.1.attn.lora_B_k.86.weight', 'image_encoder.blocks.7.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_v.81.weight', 'image_encoder.blocks.3.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_v.88.weight', 'image_encoder.blocks.2.attn.lora_B_k.83.weight', 'image_encoder.blocks.2.attn.lora_B_v.86.weight', 'image_encoder.blocks.4.attn.lora_B_v.85.weight', 'classifier_pool.87.weight', 'image_encoder.blocks.6.attn.lora_B_k.87.weight', 'image_encoder.blocks.2.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_v.83.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'classifier_pool.81.bias', 'image_encoder.blocks.6.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_k.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.84.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.86.weight', 'image_encoder.blocks.5.attn.lora_B_k.83.weight', 'image_encoder.blocks.4.attn.lora_B_k.89.weight'}
2025-12-10 13:48:44,454 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.031, Train_accy 98.96
Threshold:  0.9816
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 37/768 type remove
Layer 4 : 46/768 type remove
Layer 5 : 66/768 type remove
Layer 6 : 58/768 type remove
Layer 7 : 64/768 type remove
Layer 8 : 78/768 type remove
Layer 9 : 104/768 type remove
Layer 10 : 104/768 type remove
Layer 11 : 54/768 type remove
Layer 12 : 82/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:48:51,990 [trainer.py] => Time:139.0882134437561
594 594
594 594
2025-12-10 13:48:54,465 [trainer.py] => Time:2.4742534160614014
2025-12-10 13:48:54,465 [inflora.py] => Exemplar size: 0
2025-12-10 13:48:54,465 [trainer.py] => CNN: {'total': np.float64(58.92), '00-01': np.float64(72.04), '02-03': np.float64(67.74), '04-05': np.float64(64.04), '06-07': np.float64(57.14), '08-09': np.float64(49.32), '10-11': np.float64(41.46), '12-13': np.float64(59.26), '14-15': np.float64(75.0), '16-17': np.float64(35.71), 'old': np.float64(62.02), 'new': np.float64(35.71)}
2025-12-10 13:48:54,465 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92)]
2025-12-10 13:48:54,465 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3)]
2025-12-10 13:48:54,465 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909]
2025-12-10 13:48:56,440 [trainer.py] => All params: 144526051
2025-12-10 13:48:56,452 [trainer.py] => Trainable params: 2044438
2025-12-10 13:48:56,452 [inflora.py] => Learning on 18-20
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.98.weight', 'image_encoder.blocks.4.attn.lora_B_k.99.weight', 'image_encoder.blocks.7.attn.lora_B_k.97.weight', 'classifier_pool.9.weight', 'classifier_pool.99.bias', 'image_encoder.blocks.1.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.97.weight', 'classifier_pool.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.95.weight', 'image_encoder.blocks.7.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.92.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.99.weight', 'classifier_pool.99.weight', 'image_encoder.blocks.2.attn.lora_B_v.98.weight', 'image_encoder.blocks.7.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_v.92.weight', 'image_encoder.blocks.8.attn.lora_B_v.91.weight', 'image_encoder.blocks.1.attn.lora_B_k.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.91.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_v.95.weight', 'image_encoder.blocks.3.attn.lora_B_k.92.weight', 'image_encoder.blocks.1.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_k.98.weight', 'classifier_pool.92.bias', 'image_encoder.blocks.11.attn.lora_B_v.90.weight', 'classifier_pool.93.bias', 'image_encoder.blocks.3.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_v.93.weight', 'image_encoder.blocks.6.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_k.95.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.96.weight', 'image_encoder.blocks.4.attn.lora_B_v.97.weight', 'image_encoder.blocks.9.attn.lora_B_k.99.weight', 'image_encoder.blocks.10.attn.lora_B_k.95.weight', 'image_encoder.blocks.10.attn.lora_B_v.92.weight', 'classifier_pool.95.weight', 'image_encoder.blocks.3.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.97.weight', 'image_encoder.blocks.2.attn.lora_B_k.96.weight', 'image_encoder.blocks.4.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_k.95.weight', 'classifier_pool.97.bias', 'image_encoder.blocks.8.attn.lora_B_v.97.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.93.weight', 'classifier_pool.91.weight', 'image_encoder.blocks.7.attn.lora_B_v.93.weight', 'image_encoder.blocks.2.attn.lora_B_v.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_v.96.weight', 'image_encoder.blocks.0.attn.lora_B_v.93.weight', 'image_encoder.blocks.5.attn.lora_B_v.98.weight', 'image_encoder.blocks.6.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.96.weight', 'classifier_pool.96.weight', 'classifier_pool.96.bias', 'image_encoder.blocks.6.attn.lora_B_k.92.weight', 'image_encoder.blocks.2.attn.lora_B_k.92.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_k.90.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.93.weight', 'image_encoder.blocks.2.attn.lora_B_v.97.weight', 'image_encoder.blocks.1.attn.lora_B_v.91.weight', 'image_encoder.blocks.11.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.96.weight', 'image_encoder.blocks.0.attn.lora_B_k.98.weight', 'image_encoder.blocks.11.attn.lora_B_v.92.weight', 'image_encoder.blocks.9.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_k.98.weight', 'image_encoder.blocks.9.attn.lora_B_v.93.weight', 'image_encoder.blocks.6.attn.lora_B_k.98.weight', 'image_encoder.blocks.8.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_k.92.weight', 'image_encoder.blocks.4.attn.lora_B_k.91.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.95.weight', 'image_encoder.blocks.6.attn.lora_B_v.94.weight', 'image_encoder.blocks.7.attn.lora_B_k.93.weight', 'image_encoder.blocks.7.attn.lora_B_k.99.weight', 'image_encoder.blocks.3.attn.lora_B_k.95.weight', 'image_encoder.blocks.7.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_v.96.weight', 'image_encoder.blocks.2.attn.lora_B_k.95.weight', 'image_encoder.blocks.7.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.97.weight', 'image_encoder.blocks.9.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_v.91.weight', 'image_encoder.blocks.1.attn.lora_B_k.90.weight', 'image_encoder.blocks.0.attn.lora_B_k.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.99.weight', 'image_encoder.blocks.3.attn.lora_B_k.97.weight', 'image_encoder.blocks.3.attn.lora_B_k.96.weight', 'image_encoder.blocks.5.attn.lora_B_v.95.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.97.weight', 'image_encoder.blocks.0.attn.lora_B_k.96.weight', 'image_encoder.blocks.7.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_k.99.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.96.weight', 'image_encoder.blocks.4.attn.lora_B_k.98.weight', 'image_encoder.blocks.8.attn.lora_B_k.94.weight', 'image_encoder.blocks.11.attn.lora_B_k.90.weight', 'image_encoder.blocks.7.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.96.weight', 'image_encoder.blocks.4.attn.lora_B_k.95.weight', 'image_encoder.blocks.5.attn.lora_B_v.92.weight', 'image_encoder.blocks.2.attn.lora_B_v.93.weight', 'image_encoder.blocks.10.attn.lora_B_v.98.weight', 'image_encoder.blocks.9.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.93.weight', 'classifier_pool.91.bias', 'image_encoder.blocks.4.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'classifier_pool.90.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.91.weight', 'image_encoder.blocks.3.attn.lora_B_v.91.weight', 'image_encoder.blocks.4.attn.lora_B_v.98.weight', 'image_encoder.blocks.7.attn.lora_B_k.92.weight', 'image_encoder.blocks.8.attn.lora_B_k.90.weight', 'image_encoder.blocks.9.attn.lora_B_k.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.96.weight', 'image_encoder.blocks.8.attn.lora_B_v.90.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.92.weight', 'image_encoder.blocks.8.attn.lora_B_k.91.weight', 'image_encoder.blocks.1.attn.lora_B_v.92.weight', 'classifier_pool.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.94.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.95.weight', 'image_encoder.blocks.4.attn.lora_B_k.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.92.weight', 'image_encoder.blocks.7.attn.lora_B_v.96.weight', 'image_encoder.blocks.7.attn.lora_B_v.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.97.weight', 'image_encoder.blocks.4.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_k.98.weight', 'image_encoder.blocks.8.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.91.weight', 'image_encoder.blocks.0.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_v.92.weight', 'classifier_pool.93.weight', 'image_encoder.blocks.6.attn.lora_B_k.90.weight', 'image_encoder.blocks.3.attn.lora_B_k.94.weight', 'image_encoder.blocks.10.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.91.weight', 'image_encoder.blocks.10.attn.lora_B_k.90.weight', 'image_encoder.blocks.1.attn.lora_B_k.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.97.weight', 'image_encoder.blocks.2.attn.lora_B_v.90.weight', 'image_encoder.blocks.0.attn.lora_B_v.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.93.weight', 'image_encoder.blocks.5.attn.lora_B_k.97.weight', 'image_encoder.blocks.5.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_v.93.weight', 'image_encoder.blocks.9.attn.lora_B_k.96.weight', 'image_encoder.blocks.0.attn.lora_B_v.99.weight', 'image_encoder.blocks.10.attn.lora_B_v.97.weight', 'image_encoder.blocks.9.attn.lora_B_k.90.weight', 'image_encoder.blocks.1.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_v.99.weight', 'image_encoder.blocks.5.attn.lora_B_k.94.weight', 'classifier_pool.98.bias', 'image_encoder.blocks.11.attn.lora_B_k.94.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_v.96.weight', 'image_encoder.blocks.11.attn.lora_B_v.99.weight', 'image_encoder.blocks.11.attn.lora_B_v.91.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_v.92.weight', 'image_encoder.blocks.10.attn.lora_B_k.99.weight', 'classifier_pool.90.bias', 'image_encoder.blocks.10.attn.lora_B_k.94.weight', 'image_encoder.blocks.8.attn.lora_B_v.95.weight', 'image_encoder.blocks.2.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_v.96.weight', 'image_encoder.blocks.0.attn.lora_B_k.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.98.weight', 'image_encoder.blocks.2.attn.lora_B_k.97.weight', 'image_encoder.blocks.0.attn.lora_B_k.93.weight', 'image_encoder.blocks.5.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_v.90.weight', 'image_encoder.blocks.6.attn.lora_B_v.98.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_k.97.weight', 'image_encoder.blocks.4.attn.lora_B_v.92.weight', 'image_encoder.blocks.3.attn.lora_B_k.90.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.90.weight', 'image_encoder.blocks.5.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.96.weight', 'classifier_pool.9.bias', 'classifier_pool.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.90.weight', 'image_encoder.blocks.2.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.91.weight', 'image_encoder.blocks.5.attn.lora_B_k.92.weight', 'image_encoder.blocks.0.attn.lora_B_k.94.weight', 'image_encoder.blocks.2.attn.lora_B_k.91.weight', 'image_encoder.blocks.5.attn.lora_B_v.93.weight', 'image_encoder.blocks.6.attn.lora_B_v.93.weight', 'image_encoder.blocks.8.attn.lora_B_k.93.weight', 'image_encoder.blocks.6.attn.lora_B_k.95.weight', 'image_encoder.blocks.8.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_k.96.weight', 'image_encoder.blocks.9.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.94.weight', 'image_encoder.blocks.8.attn.lora_B_v.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.91.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.99.weight', 'image_encoder.blocks.4.attn.lora_B_v.93.weight', 'image_encoder.blocks.11.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_k.94.weight', 'image_encoder.blocks.10.attn.lora_B_k.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.92.weight', 'classifier_pool.97.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_k.90.weight', 'image_encoder.blocks.6.attn.lora_B_k.97.weight', 'image_encoder.blocks.7.attn.lora_B_v.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.90.weight', 'image_encoder.blocks.10.attn.lora_B_k.93.weight', 'image_encoder.blocks.11.attn.lora_B_k.99.weight', 'image_encoder.blocks.3.attn.lora_B_v.98.weight', 'image_encoder.blocks.5.attn.lora_B_v.97.weight', 'image_encoder.blocks.0.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.93.weight', 'image_encoder.blocks.10.attn.lora_B_v.91.weight', 'image_encoder.blocks.5.attn.lora_B_k.93.weight', 'classifier_pool.95.bias', 'image_encoder.blocks.2.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.99.weight', 'classifier_pool.94.bias', 'image_encoder.blocks.11.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_v.93.weight'}
2025-12-10 13:50:46,382 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.032, Train_accy 98.60
Threshold:  0.9818
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 39/768 type remove
Layer 4 : 50/768 type remove
Layer 5 : 70/768 type remove
Layer 6 : 62/768 type remove
Layer 7 : 68/768 type remove
Layer 8 : 85/768 type remove
Layer 9 : 114/768 type remove
Layer 10 : 116/768 type remove
Layer 11 : 62/768 type remove
Layer 12 : 94/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:50:53,153 [trainer.py] => Time:116.70142459869385
650 650
650 650
2025-12-10 13:50:55,851 [trainer.py] => Time:2.6977694034576416
2025-12-10 13:50:55,852 [inflora.py] => Exemplar size: 0
2025-12-10 13:50:55,852 [trainer.py] => CNN: {'total': np.float64(57.23), '00-01': np.float64(75.27), '02-03': np.float64(66.13), '04-05': np.float64(61.8), '06-07': np.float64(64.29), '08-09': np.float64(46.58), '10-11': np.float64(41.46), '12-13': np.float64(57.41), '14-15': np.float64(67.86), '16-17': np.float64(21.43), '18-19': np.float64(62.5), 'old': np.float64(56.73), 'new': np.float64(62.5)}
2025-12-10 13:50:55,852 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23)]
2025-12-10 13:50:55,852 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62)]
2025-12-10 13:50:55,852 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923]
2025-12-10 13:50:56,976 [trainer.py] => All params: 144526051
2025-12-10 13:50:56,987 [trainer.py] => Trainable params: 2044438
2025-12-10 13:50:56,988 [inflora.py] => Learning on 20-22
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight'}
2025-12-10 13:53:30,134 [inflora.py] => Task 10, Epoch 50/50 => Loss 0.046, Train_accy 98.17
Threshold:  0.982
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 40/768 type remove
Layer 4 : 51/768 type remove
Layer 5 : 72/768 type remove
Layer 6 : 64/768 type remove
Layer 7 : 70/768 type remove
Layer 8 : 87/768 type remove
Layer 9 : 116/768 type remove
Layer 10 : 118/768 type remove
Layer 11 : 64/768 type remove
Layer 12 : 97/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:53:37,589 [trainer.py] => Time:160.6019241809845
741 741
741 741
2025-12-10 13:53:40,474 [trainer.py] => Time:2.884425640106201
2025-12-10 13:53:40,474 [inflora.py] => Exemplar size: 0
2025-12-10 13:53:40,474 [trainer.py] => CNN: {'total': np.float64(57.49), '00-01': np.float64(74.19), '02-03': np.float64(64.52), '04-05': np.float64(60.67), '06-07': np.float64(71.43), '08-09': np.float64(47.95), '10-11': np.float64(31.71), '12-13': np.float64(55.56), '14-15': np.float64(66.07), '16-17': np.float64(20.0), '18-19': np.float64(55.36), '20-21': np.float64(69.23), 'old': np.float64(55.85), 'new': np.float64(69.23)}
2025-12-10 13:53:40,474 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49)]
2025-12-10 13:53:40,475 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82)]
2025-12-10 13:53:40,475 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012]
2025-12-10 13:53:48,116 [trainer.py] => All params: 144526051
2025-12-10 13:53:48,128 [trainer.py] => Trainable params: 185858
2025-12-10 13:53:48,128 [inflora.py] => Learning on 22-24
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'classifier_pool.11.bias', 'classifier_pool.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight'}
2025-12-10 13:56:28,579 [inflora.py] => Task 11, Epoch 50/50 => Loss 0.029, Train_accy 99.27
Threshold:  0.9822
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 41/768 type remove
Layer 4 : 52/768 type remove
Layer 5 : 74/768 type remove
Layer 6 : 66/768 type remove
Layer 7 : 73/768 type remove
Layer 8 : 90/768 type remove
Layer 9 : 119/768 type remove
Layer 10 : 120/768 type remove
Layer 11 : 66/768 type remove
Layer 12 : 99/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:56:36,331 [trainer.py] => Time:168.2022271156311
869 869
869 869
2025-12-10 13:56:39,528 [trainer.py] => Time:3.1973414421081543
2025-12-10 13:56:39,528 [inflora.py] => Exemplar size: 0
2025-12-10 13:56:39,529 [trainer.py] => CNN: {'total': np.float64(57.19), '00-01': np.float64(70.97), '02-03': np.float64(64.52), '04-05': np.float64(62.92), '06-07': np.float64(67.86), '08-09': np.float64(46.58), '10-11': np.float64(31.71), '12-13': np.float64(50.0), '14-15': np.float64(62.5), '16-17': np.float64(22.86), '18-19': np.float64(57.14), '20-21': np.float64(69.23), '22-23': np.float64(60.16), 'old': np.float64(56.68), 'new': np.float64(60.16)}
2025-12-10 13:56:39,529 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19)]
2025-12-10 13:56:39,529 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55)]
2025-12-10 13:56:39,529 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211]
2025-12-10 13:56:47,398 [trainer.py] => All params: 144526051
2025-12-10 13:56:47,410 [trainer.py] => Trainable params: 185858
2025-12-10 13:56:47,410 [inflora.py] => Learning on 24-26
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight'}
2025-12-10 13:58:20,336 [inflora.py] => Task 12, Epoch 50/50 => Loss 0.059, Train_accy 98.71
Threshold:  0.9823999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 53/768 type remove
Layer 5 : 75/768 type remove
Layer 6 : 68/768 type remove
Layer 7 : 76/768 type remove
Layer 8 : 91/768 type remove
Layer 9 : 121/768 type remove
Layer 10 : 122/768 type remove
Layer 11 : 68/768 type remove
Layer 12 : 105/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:58:26,926 [trainer.py] => Time:99.51569318771362
918 918
918 918
2025-12-10 13:58:30,256 [trainer.py] => Time:3.330118179321289
2025-12-10 13:58:30,256 [inflora.py] => Exemplar size: 0
2025-12-10 13:58:30,256 [trainer.py] => CNN: {'total': np.float64(55.01), '00-01': np.float64(73.12), '02-03': np.float64(61.29), '04-05': np.float64(58.43), '06-07': np.float64(62.5), '08-09': np.float64(43.84), '10-11': np.float64(34.15), '12-13': np.float64(46.3), '14-15': np.float64(60.71), '16-17': np.float64(20.0), '18-19': np.float64(53.57), '20-21': np.float64(70.33), '22-23': np.float64(59.38), '24-25': np.float64(46.94), 'old': np.float64(55.47), 'new': np.float64(46.94)}
2025-12-10 13:58:30,256 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01)]
2025-12-10 13:58:30,256 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3)]
2025-12-10 13:58:30,256 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061]
2025-12-10 13:58:34,809 [trainer.py] => All params: 144526051
2025-12-10 13:58:34,820 [trainer.py] => Trainable params: 185858
2025-12-10 13:58:34,821 [inflora.py] => Learning on 26-28
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight'}
2025-12-10 14:01:40,817 [inflora.py] => Task 13, Epoch 50/50 => Loss 0.032, Train_accy 98.43
Threshold:  0.9826
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 54/768 type remove
Layer 5 : 77/768 type remove
Layer 6 : 69/768 type remove
Layer 7 : 78/768 type remove
Layer 8 : 93/768 type remove
Layer 9 : 123/768 type remove
Layer 10 : 124/768 type remove
Layer 11 : 69/768 type remove
Layer 12 : 115/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:01:49,315 [trainer.py] => Time:194.4946596622467
1033 1033
1033 1033
2025-12-10 14:01:53,015 [trainer.py] => Time:3.699558973312378
2025-12-10 14:01:53,015 [inflora.py] => Exemplar size: 0
2025-12-10 14:01:53,015 [trainer.py] => CNN: {'total': np.float64(57.79), '00-01': np.float64(74.19), '02-03': np.float64(59.68), '04-05': np.float64(57.3), '06-07': np.float64(57.14), '08-09': np.float64(46.58), '10-11': np.float64(29.27), '12-13': np.float64(48.15), '14-15': np.float64(60.71), '16-17': np.float64(20.0), '18-19': np.float64(55.36), '20-21': np.float64(76.92), '22-23': np.float64(58.59), '24-25': np.float64(42.86), '26-27': np.float64(79.13), 'old': np.float64(55.12), 'new': np.float64(79.13)}
2025-12-10 14:01:53,015 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79)]
2025-12-10 14:01:53,015 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32)]
2025-12-10 14:01:53,016 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833]
2025-12-10 14:01:58,925 [trainer.py] => All params: 144526051
2025-12-10 14:01:58,937 [trainer.py] => Trainable params: 185858
2025-12-10 14:01:58,937 [inflora.py] => Learning on 28-30
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight'}
2025-12-10 14:04:00,762 [inflora.py] => Task 14, Epoch 50/50 => Loss 0.013, Train_accy 99.24
Threshold:  0.9828
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 44/768 type remove
Layer 4 : 55/768 type remove
Layer 5 : 79/768 type remove
Layer 6 : 71/768 type remove
Layer 7 : 80/768 type remove
Layer 8 : 94/768 type remove
Layer 9 : 124/768 type remove
Layer 10 : 125/768 type remove
Layer 11 : 70/768 type remove
Layer 12 : 129/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:04:07,951 [trainer.py] => Time:129.01397705078125
1114 1114
1114 1114
2025-12-10 14:04:11,806 [trainer.py] => Time:3.8547582626342773
2025-12-10 14:04:11,806 [inflora.py] => Exemplar size: 0
2025-12-10 14:04:11,806 [trainer.py] => CNN: {'total': np.float64(54.13), '00-01': np.float64(68.82), '02-03': np.float64(53.23), '04-05': np.float64(56.18), '06-07': np.float64(57.14), '08-09': np.float64(39.73), '10-11': np.float64(31.71), '12-13': np.float64(48.15), '14-15': np.float64(33.93), '16-17': np.float64(11.43), '18-19': np.float64(51.79), '20-21': np.float64(70.33), '22-23': np.float64(57.81), '24-25': np.float64(38.78), '26-27': np.float64(78.26), '28-29': np.float64(65.43), 'old': np.float64(53.24), 'new': np.float64(65.43)}
2025-12-10 14:04:11,806 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13)]
2025-12-10 14:04:11,806 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42)]
2025-12-10 14:04:11,806 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858]
2025-12-10 14:04:16,064 [trainer.py] => All params: 144526051
2025-12-10 14:04:16,076 [trainer.py] => Trainable params: 185858
2025-12-10 14:04:16,076 [inflora.py] => Learning on 30-32
Parameters to be updated: {'classifier_pool.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight'}
2025-12-10 14:07:05,775 [inflora.py] => Task 15, Epoch 50/50 => Loss 0.016, Train_accy 99.76
Threshold:  0.983
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 45/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 80/768 type remove
Layer 6 : 72/768 type remove
Layer 7 : 81/768 type remove
Layer 8 : 95/768 type remove
Layer 9 : 125/768 type remove
Layer 10 : 126/768 type remove
Layer 11 : 71/768 type remove
Layer 12 : 133/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:07:14,273 [trainer.py] => Time:178.19772386550903
1224 1224
1224 1224
2025-12-10 14:07:18,477 [trainer.py] => Time:4.203711271286011
2025-12-10 14:07:18,478 [inflora.py] => Exemplar size: 0
2025-12-10 14:07:18,478 [trainer.py] => CNN: {'total': np.float64(53.1), '00-01': np.float64(68.82), '02-03': np.float64(51.61), '04-05': np.float64(50.56), '06-07': np.float64(55.36), '08-09': np.float64(30.14), '10-11': np.float64(26.83), '12-13': np.float64(42.59), '14-15': np.float64(41.07), '16-17': np.float64(12.86), '18-19': np.float64(53.57), '20-21': np.float64(73.63), '22-23': np.float64(57.03), '24-25': np.float64(32.65), '26-27': np.float64(73.04), '28-29': np.float64(64.2), '30-31': np.float64(61.82), 'old': np.float64(52.24), 'new': np.float64(61.82)}
2025-12-10 14:07:18,478 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1)]
2025-12-10 14:07:18,478 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0)]
2025-12-10 14:07:18,478 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392]
2025-12-10 14:07:25,798 [trainer.py] => All params: 144526051
2025-12-10 14:07:25,810 [trainer.py] => Trainable params: 185858
2025-12-10 14:07:25,810 [inflora.py] => Learning on 32-34
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight'}
2025-12-10 14:09:09,005 [inflora.py] => Task 16, Epoch 50/50 => Loss 0.059, Train_accy 95.48
Threshold:  0.9832
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 46/768 type remove
Layer 4 : 57/768 type remove
Layer 5 : 81/768 type remove
Layer 6 : 73/768 type remove
Layer 7 : 82/768 type remove
Layer 8 : 96/768 type remove
Layer 9 : 126/768 type remove
Layer 10 : 127/768 type remove
Layer 11 : 72/768 type remove
Layer 12 : 135/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:09:15,949 [trainer.py] => Time:110.13866376876831
1268 1268
1268 1268
2025-12-10 14:09:20,198 [trainer.py] => Time:4.248802661895752
2025-12-10 14:09:20,198 [inflora.py] => Exemplar size: 0
2025-12-10 14:09:20,198 [trainer.py] => CNN: {'total': np.float64(53.47), '00-01': np.float64(70.97), '02-03': np.float64(48.39), '04-05': np.float64(50.56), '06-07': np.float64(58.93), '08-09': np.float64(41.1), '10-11': np.float64(21.95), '12-13': np.float64(48.15), '14-15': np.float64(42.86), '16-17': np.float64(12.86), '18-19': np.float64(53.57), '20-21': np.float64(73.63), '22-23': np.float64(57.03), '24-25': np.float64(30.61), '26-27': np.float64(77.39), '28-29': np.float64(60.49), '30-31': np.float64(63.64), '32-33': np.float64(29.55), 'old': np.float64(54.33), 'new': np.float64(29.55)}
2025-12-10 14:09:20,199 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47)]
2025-12-10 14:09:20,199 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06)]
2025-12-10 14:09:20,199 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363]
2025-12-10 14:09:26,010 [trainer.py] => All params: 144526051
2025-12-10 14:09:26,022 [trainer.py] => Trainable params: 185858
2025-12-10 14:09:26,022 [inflora.py] => Learning on 34-36
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight'}
2025-12-10 14:10:57,646 [inflora.py] => Task 17, Epoch 50/50 => Loss 0.181, Train_accy 98.48
Threshold:  0.9833999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 47/768 type remove
Layer 4 : 58/768 type remove
Layer 5 : 82/768 type remove
Layer 6 : 74/768 type remove
Layer 7 : 83/768 type remove
Layer 8 : 97/768 type remove
Layer 9 : 127/768 type remove
Layer 10 : 128/768 type remove
Layer 11 : 73/768 type remove
Layer 12 : 139/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:11:04,549 [trainer.py] => Time:98.52690386772156
1299 1299
1299 1299
2025-12-10 14:11:08,904 [trainer.py] => Time:4.355051755905151
2025-12-10 14:11:08,905 [inflora.py] => Exemplar size: 0
2025-12-10 14:11:08,905 [trainer.py] => CNN: {'total': np.float64(53.5), '00-01': np.float64(75.27), '02-03': np.float64(48.39), '04-05': np.float64(49.44), '06-07': np.float64(58.93), '08-09': np.float64(45.21), '10-11': np.float64(24.39), '12-13': np.float64(48.15), '14-15': np.float64(44.64), '16-17': np.float64(12.86), '18-19': np.float64(48.21), '20-21': np.float64(74.73), '22-23': np.float64(54.69), '24-25': np.float64(32.65), '26-27': np.float64(75.65), '28-29': np.float64(60.49), '30-31': np.float64(63.64), '32-33': np.float64(25.0), '34-35': np.float64(54.84), 'old': np.float64(53.47), 'new': np.float64(54.84)}
2025-12-10 14:11:08,905 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5)]
2025-12-10 14:11:08,905 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23)]
2025-12-10 14:11:08,905 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254]
2025-12-10 14:11:14,628 [trainer.py] => All params: 144526051
2025-12-10 14:11:14,640 [trainer.py] => Trainable params: 185858
2025-12-10 14:11:14,640 [inflora.py] => Learning on 36-38
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight'}
2025-12-10 14:13:08,270 [inflora.py] => Task 18, Epoch 50/50 => Loss 0.028, Train_accy 99.12
Threshold:  0.9836
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 60/768 type remove
Layer 5 : 84/768 type remove
Layer 6 : 75/768 type remove
Layer 7 : 85/768 type remove
Layer 8 : 99/768 type remove
Layer 9 : 129/768 type remove
Layer 10 : 130/768 type remove
Layer 11 : 74/768 type remove
Layer 12 : 140/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:13:15,142 [trainer.py] => Time:120.50137758255005
1359 1359
1359 1359
2025-12-10 14:13:19,629 [trainer.py] => Time:4.487103700637817
2025-12-10 14:13:19,630 [inflora.py] => Exemplar size: 0
2025-12-10 14:13:19,630 [trainer.py] => CNN: {'total': np.float64(54.75), '00-01': np.float64(79.57), '02-03': np.float64(46.77), '04-05': np.float64(55.06), '06-07': np.float64(51.79), '08-09': np.float64(45.21), '10-11': np.float64(24.39), '12-13': np.float64(48.15), '14-15': np.float64(50.0), '16-17': np.float64(14.29), '18-19': np.float64(58.93), '20-21': np.float64(72.53), '22-23': np.float64(56.25), '24-25': np.float64(34.69), '26-27': np.float64(79.13), '28-29': np.float64(62.96), '30-31': np.float64(63.64), '32-33': np.float64(22.73), '34-35': np.float64(54.84), '36-37': np.float64(48.33), 'old': np.float64(55.04), 'new': np.float64(48.33)}
2025-12-10 14:13:19,630 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75)]
2025-12-10 14:13:19,630 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73)]
2025-12-10 14:13:19,630 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642]
2025-12-10 14:13:25,909 [trainer.py] => All params: 144526051
2025-12-10 14:13:25,921 [trainer.py] => Trainable params: 185858
2025-12-10 14:13:25,921 [inflora.py] => Learning on 38-40
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight'}
2025-12-10 14:15:12,324 [inflora.py] => Task 19, Epoch 50/50 => Loss 0.024, Train_accy 99.46
Threshold:  0.9838
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 49/768 type remove
Layer 4 : 61/768 type remove
Layer 5 : 86/768 type remove
Layer 6 : 77/768 type remove
Layer 7 : 88/768 type remove
Layer 8 : 103/768 type remove
Layer 9 : 135/768 type remove
Layer 10 : 136/768 type remove
Layer 11 : 80/768 type remove
Layer 12 : 146/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:15:19,217 [trainer.py] => Time:113.29527044296265
1400 1400
1400 1400
2025-12-10 14:15:23,798 [trainer.py] => Time:4.5815699100494385
2025-12-10 14:15:23,799 [inflora.py] => Exemplar size: 0
2025-12-10 14:15:23,799 [trainer.py] => CNN: {'total': np.float64(56.5), '00-01': np.float64(78.49), '02-03': np.float64(53.23), '04-05': np.float64(58.43), '06-07': np.float64(50.0), '08-09': np.float64(45.21), '10-11': np.float64(21.95), '12-13': np.float64(53.7), '14-15': np.float64(48.21), '16-17': np.float64(15.71), '18-19': np.float64(55.36), '20-21': np.float64(72.53), '22-23': np.float64(57.81), '24-25': np.float64(34.69), '26-27': np.float64(83.48), '28-29': np.float64(64.2), '30-31': np.float64(61.82), '32-33': np.float64(27.27), '34-35': np.float64(51.61), '36-37': np.float64(53.33), '38-39': np.float64(78.05), 'old': np.float64(55.85), 'new': np.float64(78.05)}
2025-12-10 14:15:23,799 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5)]
2025-12-10 14:15:23,799 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07)]
2025-12-10 14:15:23,799 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714]
2025-12-10 14:15:28,780 [trainer.py] => All params: 144526051
2025-12-10 14:15:28,792 [trainer.py] => Trainable params: 185858
2025-12-10 14:15:28,792 [inflora.py] => Learning on 40-42
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight'}
2025-12-10 14:17:06,371 [inflora.py] => Task 20, Epoch 50/50 => Loss 0.046, Train_accy 98.86
Threshold:  0.984
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 50/768 type remove
Layer 4 : 62/768 type remove
Layer 5 : 87/768 type remove
Layer 6 : 78/768 type remove
Layer 7 : 90/768 type remove
Layer 8 : 105/768 type remove
Layer 9 : 137/768 type remove
Layer 10 : 139/768 type remove
Layer 11 : 83/768 type remove
Layer 12 : 149/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:17:12,883 [trainer.py] => Time:104.09064960479736
1454 1454
1454 1454
2025-12-10 14:17:17,590 [trainer.py] => Time:4.706561088562012
2025-12-10 14:17:17,590 [inflora.py] => Exemplar size: 0
2025-12-10 14:17:17,590 [trainer.py] => CNN: {'total': np.float64(54.88), '00-01': np.float64(77.42), '02-03': np.float64(50.0), '04-05': np.float64(57.3), '06-07': np.float64(50.0), '08-09': np.float64(45.21), '10-11': np.float64(24.39), '12-13': np.float64(53.7), '14-15': np.float64(50.0), '16-17': np.float64(14.29), '18-19': np.float64(57.14), '20-21': np.float64(74.73), '22-23': np.float64(55.47), '24-25': np.float64(36.73), '26-27': np.float64(81.74), '28-29': np.float64(58.02), '30-31': np.float64(58.18), '32-33': np.float64(31.82), '34-35': np.float64(54.84), '36-37': np.float64(50.0), '38-39': np.float64(75.61), '40-41': np.float64(37.04), 'old': np.float64(55.57), 'new': np.float64(37.04)}
2025-12-10 14:17:17,590 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88)]
2025-12-10 14:17:17,590 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8)]
2025-12-10 14:17:17,590 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488]
2025-12-10 14:17:21,316 [trainer.py] => All params: 144526051
2025-12-10 14:17:21,329 [trainer.py] => Trainable params: 185858
2025-12-10 14:17:21,329 [inflora.py] => Learning on 42-44
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight'}
2025-12-10 14:18:50,461 [inflora.py] => Task 21, Epoch 50/50 => Loss 0.064, Train_accy 100.00
Threshold:  0.9842
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 51/768 type remove
Layer 4 : 63/768 type remove
Layer 5 : 88/768 type remove
Layer 6 : 79/768 type remove
Layer 7 : 92/768 type remove
Layer 8 : 107/768 type remove
Layer 9 : 140/768 type remove
Layer 10 : 141/768 type remove
Layer 11 : 86/768 type remove
Layer 12 : 153/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:18:57,096 [trainer.py] => Time:95.7667407989502
1486 1486
1486 1486
2025-12-10 14:19:01,924 [trainer.py] => Time:4.8274922370910645
2025-12-10 14:19:01,924 [inflora.py] => Exemplar size: 0
2025-12-10 14:19:01,924 [trainer.py] => CNN: {'total': np.float64(53.84), '00-01': np.float64(76.34), '02-03': np.float64(51.61), '04-05': np.float64(59.55), '06-07': np.float64(50.0), '08-09': np.float64(43.84), '10-11': np.float64(26.83), '12-13': np.float64(51.85), '14-15': np.float64(48.21), '16-17': np.float64(14.29), '18-19': np.float64(60.71), '20-21': np.float64(71.43), '22-23': np.float64(59.38), '24-25': np.float64(38.78), '26-27': np.float64(81.74), '28-29': np.float64(58.02), '30-31': np.float64(57.27), '32-33': np.float64(36.36), '34-35': np.float64(54.84), '36-37': np.float64(45.0), '38-39': np.float64(65.85), '40-41': np.float64(35.19), '42-43': np.float64(12.5), 'old': np.float64(54.75), 'new': np.float64(12.5)}
2025-12-10 14:19:01,924 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84)]
2025-12-10 14:19:01,924 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1)]
2025-12-10 14:19:01,924 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643]
2025-12-10 14:19:07,230 [trainer.py] => All params: 144526051
2025-12-10 14:19:07,242 [trainer.py] => Trainable params: 185858
2025-12-10 14:19:07,242 [inflora.py] => Learning on 44-46
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight'}
2025-12-10 14:20:36,533 [inflora.py] => Task 22, Epoch 50/50 => Loss 0.222, Train_accy 92.41
Threshold:  0.9843999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 23/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 64/768 type remove
Layer 5 : 89/768 type remove
Layer 6 : 80/768 type remove
Layer 7 : 93/768 type remove
Layer 8 : 109/768 type remove
Layer 9 : 143/768 type remove
Layer 10 : 143/768 type remove
Layer 11 : 88/768 type remove
Layer 12 : 155/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:20:42,744 [trainer.py] => Time:95.5012137889862
1513 1513
1513 1513
2025-12-10 14:20:47,688 [trainer.py] => Time:4.943835258483887
2025-12-10 14:20:47,688 [inflora.py] => Exemplar size: 0
2025-12-10 14:20:47,688 [trainer.py] => CNN: {'total': np.float64(51.88), '00-01': np.float64(75.27), '02-03': np.float64(48.39), '04-05': np.float64(59.55), '06-07': np.float64(53.57), '08-09': np.float64(42.47), '10-11': np.float64(24.39), '12-13': np.float64(53.7), '14-15': np.float64(50.0), '16-17': np.float64(15.71), '18-19': np.float64(55.36), '20-21': np.float64(67.03), '22-23': np.float64(57.03), '24-25': np.float64(38.78), '26-27': np.float64(77.39), '28-29': np.float64(58.02), '30-31': np.float64(59.09), '32-33': np.float64(27.27), '34-35': np.float64(48.39), '36-37': np.float64(43.33), '38-39': np.float64(78.05), '40-41': np.float64(37.04), '42-43': np.float64(9.38), '44-45': np.float64(0.0), 'old': np.float64(52.83), 'new': np.float64(0.0)}
2025-12-10 14:20:47,688 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88)]
2025-12-10 14:20:47,688 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84)]
2025-12-10 14:20:47,688 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684]
2025-12-10 14:20:52,065 [trainer.py] => All params: 144526051
2025-12-10 14:20:52,077 [trainer.py] => Trainable params: 185858
2025-12-10 14:20:52,078 [inflora.py] => Learning on 46-48
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight'}
2025-12-10 14:22:21,583 [inflora.py] => Task 23, Epoch 50/50 => Loss 0.033, Train_accy 99.28
Threshold:  0.9846
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 23/768 type remove
Layer 3 : 53/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 91/768 type remove
Layer 6 : 82/768 type remove
Layer 7 : 95/768 type remove
Layer 8 : 112/768 type remove
Layer 9 : 146/768 type remove
Layer 10 : 146/768 type remove
Layer 11 : 91/768 type remove
Layer 12 : 158/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:22:28,176 [trainer.py] => Time:96.09894800186157
1540 1540
1540 1540
2025-12-10 14:22:33,189 [trainer.py] => Time:5.012681484222412
2025-12-10 14:22:33,190 [inflora.py] => Exemplar size: 0
2025-12-10 14:22:33,190 [trainer.py] => CNN: {'total': np.float64(51.56), '00-01': np.float64(78.49), '02-03': np.float64(43.55), '04-05': np.float64(56.18), '06-07': np.float64(51.79), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(51.85), '14-15': np.float64(48.21), '16-17': np.float64(14.29), '18-19': np.float64(55.36), '20-21': np.float64(71.43), '22-23': np.float64(57.03), '24-25': np.float64(38.78), '26-27': np.float64(74.78), '28-29': np.float64(59.26), '30-31': np.float64(60.0), '32-33': np.float64(27.27), '34-35': np.float64(45.16), '36-37': np.float64(41.67), '38-39': np.float64(75.61), '40-41': np.float64(33.33), '42-43': np.float64(15.62), '44-45': np.float64(0.0), '46-47': np.float64(55.56), 'old': np.float64(51.49), 'new': np.float64(55.56)}
2025-12-10 14:22:33,190 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56)]
2025-12-10 14:22:33,190 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55)]
2025-12-10 14:22:33,190 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675]
2025-12-10 14:22:37,996 [trainer.py] => All params: 144526051
2025-12-10 14:22:38,009 [trainer.py] => Trainable params: 185858
2025-12-10 14:22:38,009 [inflora.py] => Learning on 48-50
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight'}
2025-12-10 14:24:43,882 [inflora.py] => Task 24, Epoch 50/50 => Loss 0.012, Train_accy 100.00
Threshold:  0.9848
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 54/768 type remove
Layer 4 : 67/768 type remove
Layer 5 : 93/768 type remove
Layer 6 : 84/768 type remove
Layer 7 : 97/768 type remove
Layer 8 : 114/768 type remove
Layer 9 : 148/768 type remove
Layer 10 : 148/768 type remove
Layer 11 : 94/768 type remove
Layer 12 : 161/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:24:50,888 [trainer.py] => Time:132.87886905670166
1634 1634
1634 1634
2025-12-10 14:24:56,091 [trainer.py] => Time:5.203012466430664
2025-12-10 14:24:56,091 [inflora.py] => Exemplar size: 0
2025-12-10 14:24:56,091 [trainer.py] => CNN: {'total': np.float64(52.26), '00-01': np.float64(73.12), '02-03': np.float64(45.16), '04-05': np.float64(56.18), '06-07': np.float64(51.79), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(46.3), '14-15': np.float64(44.64), '16-17': np.float64(15.71), '18-19': np.float64(57.14), '20-21': np.float64(69.23), '22-23': np.float64(57.81), '24-25': np.float64(36.73), '26-27': np.float64(80.0), '28-29': np.float64(55.56), '30-31': np.float64(61.82), '32-33': np.float64(25.0), '34-35': np.float64(51.61), '36-37': np.float64(43.33), '38-39': np.float64(70.73), '40-41': np.float64(27.78), '42-43': np.float64(15.62), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(71.28), 'old': np.float64(51.1), 'new': np.float64(71.28)}
2025-12-10 14:24:56,091 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26)]
2025-12-10 14:24:56,091 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8)]
2025-12-10 14:24:56,091 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275]
2025-12-10 14:25:00,265 [trainer.py] => All params: 144526051
2025-12-10 14:25:00,277 [trainer.py] => Trainable params: 185858
2025-12-10 14:25:00,277 [inflora.py] => Learning on 50-52
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight'}
2025-12-10 14:26:38,764 [inflora.py] => Task 25, Epoch 50/50 => Loss 0.028, Train_accy 98.30
Threshold:  0.985
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 55/768 type remove
Layer 4 : 68/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 86/768 type remove
Layer 7 : 99/768 type remove
Layer 8 : 117/768 type remove
Layer 9 : 150/768 type remove
Layer 10 : 150/768 type remove
Layer 11 : 97/768 type remove
Layer 12 : 163/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:26:45,366 [trainer.py] => Time:105.08895516395569
1671 1671
1671 1671
2025-12-10 14:26:50,705 [trainer.py] => Time:5.3389623165130615
2025-12-10 14:26:50,706 [inflora.py] => Exemplar size: 0
2025-12-10 14:26:50,706 [trainer.py] => CNN: {'total': np.float64(52.18), '00-01': np.float64(75.27), '02-03': np.float64(43.55), '04-05': np.float64(56.18), '06-07': np.float64(55.36), '08-09': np.float64(46.58), '10-11': np.float64(21.95), '12-13': np.float64(48.15), '14-15': np.float64(46.43), '16-17': np.float64(20.0), '18-19': np.float64(58.93), '20-21': np.float64(71.43), '22-23': np.float64(55.47), '24-25': np.float64(32.65), '26-27': np.float64(82.61), '28-29': np.float64(55.56), '30-31': np.float64(61.82), '32-33': np.float64(29.55), '34-35': np.float64(45.16), '36-37': np.float64(53.33), '38-39': np.float64(63.41), '40-41': np.float64(24.07), '42-43': np.float64(15.62), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(74.47), '50-51': np.float64(13.51), 'old': np.float64(53.06), 'new': np.float64(13.51)}
2025-12-10 14:26:50,706 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18)]
2025-12-10 14:26:50,706 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21)]
2025-12-10 14:26:50,706 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473]
2025-12-10 14:26:54,250 [trainer.py] => All params: 144526051
2025-12-10 14:26:54,261 [trainer.py] => Trainable params: 185858
2025-12-10 14:26:54,262 [inflora.py] => Learning on 52-54
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight'}
2025-12-10 14:28:29,710 [inflora.py] => Task 26, Epoch 50/50 => Loss 0.048, Train_accy 98.11
Threshold:  0.9852
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 56/768 type remove
Layer 4 : 70/768 type remove
Layer 5 : 97/768 type remove
Layer 6 : 88/768 type remove
Layer 7 : 102/768 type remove
Layer 8 : 119/768 type remove
Layer 9 : 152/768 type remove
Layer 10 : 152/768 type remove
Layer 11 : 99/768 type remove
Layer 12 : 165/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:28:36,485 [trainer.py] => Time:102.2234754562378
1707 1707
1707 1707
2025-12-10 14:28:41,937 [trainer.py] => Time:5.451976299285889
2025-12-10 14:28:41,938 [inflora.py] => Exemplar size: 0
2025-12-10 14:28:41,938 [trainer.py] => CNN: {'total': np.float64(53.25), '00-01': np.float64(77.42), '02-03': np.float64(43.55), '04-05': np.float64(59.55), '06-07': np.float64(57.14), '08-09': np.float64(42.47), '10-11': np.float64(24.39), '12-13': np.float64(51.85), '14-15': np.float64(46.43), '16-17': np.float64(25.71), '18-19': np.float64(53.57), '20-21': np.float64(71.43), '22-23': np.float64(61.72), '24-25': np.float64(32.65), '26-27': np.float64(85.22), '28-29': np.float64(56.79), '30-31': np.float64(60.0), '32-33': np.float64(36.36), '34-35': np.float64(48.39), '36-37': np.float64(48.33), '38-39': np.float64(63.41), '40-41': np.float64(27.78), '42-43': np.float64(15.62), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(71.28), '50-51': np.float64(16.22), '52-53': np.float64(55.56), 'old': np.float64(53.2), 'new': np.float64(55.56)}
2025-12-10 14:28:41,938 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25)]
2025-12-10 14:28:41,938 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02)]
2025-12-10 14:28:41,938 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616]
2025-12-10 14:28:49,212 [trainer.py] => All params: 144526051
2025-12-10 14:28:49,224 [trainer.py] => Trainable params: 185858
2025-12-10 14:28:49,224 [inflora.py] => Learning on 54-56
Parameters to be updated: {'classifier_pool.27.bias', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight'}
2025-12-10 14:30:23,579 [inflora.py] => Task 27, Epoch 50/50 => Loss 0.108, Train_accy 94.63
Threshold:  0.9853999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 57/768 type remove
Layer 4 : 71/768 type remove
Layer 5 : 98/768 type remove
Layer 6 : 89/768 type remove
Layer 7 : 104/768 type remove
Layer 8 : 122/768 type remove
Layer 9 : 155/768 type remove
Layer 10 : 155/768 type remove
Layer 11 : 101/768 type remove
Layer 12 : 167/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:30:30,127 [trainer.py] => Time:100.90243935585022
1732 1732
1732 1732
2025-12-10 14:30:35,632 [trainer.py] => Time:5.505146503448486
2025-12-10 14:30:35,632 [inflora.py] => Exemplar size: 0
2025-12-10 14:30:35,632 [trainer.py] => CNN: {'total': np.float64(52.37), '00-01': np.float64(78.49), '02-03': np.float64(41.94), '04-05': np.float64(60.67), '06-07': np.float64(62.5), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(48.15), '14-15': np.float64(46.43), '16-17': np.float64(30.0), '18-19': np.float64(51.79), '20-21': np.float64(71.43), '22-23': np.float64(61.72), '24-25': np.float64(28.57), '26-27': np.float64(82.61), '28-29': np.float64(55.56), '30-31': np.float64(59.09), '32-33': np.float64(29.55), '34-35': np.float64(48.39), '36-37': np.float64(46.67), '38-39': np.float64(65.85), '40-41': np.float64(25.93), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(72.34), '50-51': np.float64(18.92), '52-53': np.float64(58.33), '54-55': np.float64(0.0), 'old': np.float64(53.13), 'new': np.float64(0.0)}
2025-12-10 14:30:35,632 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37)]
2025-12-10 14:30:35,632 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73)]
2025-12-10 14:30:35,633 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945]
2025-12-10 14:30:44,687 [trainer.py] => All params: 144526051
2025-12-10 14:30:44,699 [trainer.py] => Trainable params: 185858
2025-12-10 14:30:44,699 [inflora.py] => Learning on 56-58
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight'}
2025-12-10 14:32:32,772 [inflora.py] => Task 28, Epoch 50/50 => Loss 0.018, Train_accy 99.49
Threshold:  0.9856
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 59/768 type remove
Layer 4 : 72/768 type remove
Layer 5 : 100/768 type remove
Layer 6 : 91/768 type remove
Layer 7 : 106/768 type remove
Layer 8 : 124/768 type remove
Layer 9 : 157/768 type remove
Layer 10 : 158/768 type remove
Layer 11 : 104/768 type remove
Layer 12 : 170/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:32:39,530 [trainer.py] => Time:114.83058571815491
1772 1772
1772 1772
2025-12-10 14:32:45,270 [trainer.py] => Time:5.740488529205322
2025-12-10 14:32:45,271 [inflora.py] => Exemplar size: 0
2025-12-10 14:32:45,271 [trainer.py] => CNN: {'total': np.float64(51.98), '00-01': np.float64(79.57), '02-03': np.float64(43.55), '04-05': np.float64(52.81), '06-07': np.float64(55.36), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(50.0), '14-15': np.float64(46.43), '16-17': np.float64(24.29), '18-19': np.float64(60.71), '20-21': np.float64(71.43), '22-23': np.float64(61.72), '24-25': np.float64(28.57), '26-27': np.float64(80.87), '28-29': np.float64(51.85), '30-31': np.float64(59.09), '32-33': np.float64(27.27), '34-35': np.float64(48.39), '36-37': np.float64(48.33), '38-39': np.float64(73.17), '40-41': np.float64(31.48), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(73.4), '50-51': np.float64(18.92), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(47.5), 'old': np.float64(52.08), 'new': np.float64(47.5)}
2025-12-10 14:32:45,271 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98)]
2025-12-10 14:32:45,271 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6)]
2025-12-10 14:32:45,271 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361]
2025-12-10 14:32:50,065 [trainer.py] => All params: 144526051
2025-12-10 14:32:50,077 [trainer.py] => Trainable params: 185858
2025-12-10 14:32:50,078 [inflora.py] => Learning on 58-60
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight'}
2025-12-10 14:34:38,523 [inflora.py] => Task 29, Epoch 50/50 => Loss 0.064, Train_accy 98.00
Threshold:  0.9858
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 60/768 type remove
Layer 4 : 73/768 type remove
Layer 5 : 101/768 type remove
Layer 6 : 92/768 type remove
Layer 7 : 108/768 type remove
Layer 8 : 125/768 type remove
Layer 9 : 158/768 type remove
Layer 10 : 159/768 type remove
Layer 11 : 105/768 type remove
Layer 12 : 173/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:34:45,790 [trainer.py] => Time:115.71239066123962
1827 1827
1827 1827
2025-12-10 14:34:51,619 [trainer.py] => Time:5.829111099243164
2025-12-10 14:34:51,619 [inflora.py] => Exemplar size: 0
2025-12-10 14:34:51,620 [trainer.py] => CNN: {'total': np.float64(50.36), '00-01': np.float64(74.19), '02-03': np.float64(48.39), '04-05': np.float64(52.81), '06-07': np.float64(64.29), '08-09': np.float64(46.58), '10-11': np.float64(24.39), '12-13': np.float64(48.15), '14-15': np.float64(42.86), '16-17': np.float64(21.43), '18-19': np.float64(55.36), '20-21': np.float64(67.03), '22-23': np.float64(60.16), '24-25': np.float64(30.61), '26-27': np.float64(75.65), '28-29': np.float64(51.85), '30-31': np.float64(53.64), '32-33': np.float64(31.82), '34-35': np.float64(48.39), '36-37': np.float64(46.67), '38-39': np.float64(65.85), '40-41': np.float64(35.19), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(76.6), '50-51': np.float64(32.43), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(50.0), '58-59': np.float64(21.82), 'old': np.float64(51.24), 'new': np.float64(21.82)}
2025-12-10 14:34:51,620 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36)]
2025-12-10 14:34:51,620 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24)]
2025-12-10 14:34:51,620 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869]
2025-12-10 14:34:59,567 [trainer.py] => All params: 144526051
2025-12-10 14:34:59,579 [trainer.py] => Trainable params: 185858
2025-12-10 14:34:59,580 [inflora.py] => Learning on 60-62
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight'}
2025-12-10 14:36:34,112 [inflora.py] => Task 30, Epoch 50/50 => Loss 0.038, Train_accy 99.38
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 27/768 type remove
Layer 3 : 61/768 type remove
Layer 4 : 74/768 type remove
Layer 5 : 103/768 type remove
Layer 6 : 93/768 type remove
Layer 7 : 110/768 type remove
Layer 8 : 127/768 type remove
Layer 9 : 160/768 type remove
Layer 10 : 160/768 type remove
Layer 11 : 106/768 type remove
Layer 12 : 175/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:36:40,623 [trainer.py] => Time:101.04301929473877
1868 1868
1868 1868
2025-12-10 14:36:46,580 [trainer.py] => Time:5.956969976425171
2025-12-10 14:36:46,580 [inflora.py] => Exemplar size: 0
2025-12-10 14:36:46,580 [trainer.py] => CNN: {'total': np.float64(50.91), '00-01': np.float64(73.12), '02-03': np.float64(46.77), '04-05': np.float64(53.93), '06-07': np.float64(64.29), '08-09': np.float64(45.21), '10-11': np.float64(26.83), '12-13': np.float64(50.0), '14-15': np.float64(41.07), '16-17': np.float64(22.86), '18-19': np.float64(57.14), '20-21': np.float64(67.03), '22-23': np.float64(60.16), '24-25': np.float64(30.61), '26-27': np.float64(77.39), '28-29': np.float64(51.85), '30-31': np.float64(51.82), '32-33': np.float64(34.09), '34-35': np.float64(48.39), '36-37': np.float64(48.33), '38-39': np.float64(68.29), '40-41': np.float64(35.19), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(74.47), '50-51': np.float64(37.84), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(52.5), '58-59': np.float64(23.64), '60-61': np.float64(68.29), 'old': np.float64(50.52), 'new': np.float64(68.29)}
2025-12-10 14:36:46,580 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91)]
2025-12-10 14:36:46,580 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18)]
2025-12-10 14:36:46,580 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685]
2025-12-10 14:36:51,851 [trainer.py] => All params: 144526051
2025-12-10 14:36:51,863 [trainer.py] => Trainable params: 185858
2025-12-10 14:36:51,863 [inflora.py] => Learning on 62-64
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'classifier_pool.31.bias', 'classifier_pool.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight'}
2025-12-10 14:39:38,269 [inflora.py] => Task 31, Epoch 50/50 => Loss 0.015, Train_accy 99.05
Threshold:  0.9862
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 62/768 type remove
Layer 4 : 76/768 type remove
Layer 5 : 106/768 type remove
Layer 6 : 95/768 type remove
Layer 7 : 113/768 type remove
Layer 8 : 129/768 type remove
Layer 9 : 162/768 type remove
Layer 10 : 162/768 type remove
Layer 11 : 108/768 type remove
Layer 12 : 177/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:39:46,493 [trainer.py] => Time:174.63041973114014
1974 1974
1974 1974
2025-12-10 14:39:52,653 [trainer.py] => Time:6.159755706787109
2025-12-10 14:39:52,654 [inflora.py] => Exemplar size: 0
2025-12-10 14:39:52,654 [trainer.py] => CNN: {'total': np.float64(50.56), '00-01': np.float64(70.97), '02-03': np.float64(50.0), '04-05': np.float64(49.44), '06-07': np.float64(58.93), '08-09': np.float64(46.58), '10-11': np.float64(29.27), '12-13': np.float64(50.0), '14-15': np.float64(42.86), '16-17': np.float64(21.43), '18-19': np.float64(60.71), '20-21': np.float64(68.13), '22-23': np.float64(57.81), '24-25': np.float64(32.65), '26-27': np.float64(76.52), '28-29': np.float64(54.32), '30-31': np.float64(51.82), '32-33': np.float64(27.27), '34-35': np.float64(54.84), '36-37': np.float64(48.33), '38-39': np.float64(63.41), '40-41': np.float64(38.89), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(73.4), '50-51': np.float64(32.43), '52-53': np.float64(58.33), '54-55': np.float64(0.0), '56-57': np.float64(50.0), '58-59': np.float64(16.36), '60-61': np.float64(53.66), '62-63': np.float64(59.43), 'old': np.float64(50.05), 'new': np.float64(59.43)}
2025-12-10 14:39:52,654 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56)]
2025-12-10 14:39:52,654 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44)]
2025-12-10 14:39:52,654 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545]
2025-12-10 14:40:00,321 [trainer.py] => All params: 144526051
2025-12-10 14:40:00,334 [trainer.py] => Trainable params: 185858
2025-12-10 14:40:00,334 [inflora.py] => Learning on 64-66
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'classifier_pool.32.bias', 'classifier_pool.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight'}
2025-12-10 14:41:25,561 [inflora.py] => Task 32, Epoch 50/50 => Loss 0.106, Train_accy 95.73
Threshold:  0.9863999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 63/768 type remove
Layer 4 : 78/768 type remove
Layer 5 : 109/768 type remove
Layer 6 : 99/768 type remove
Layer 7 : 116/768 type remove
Layer 8 : 131/768 type remove
Layer 9 : 164/768 type remove
Layer 10 : 163/768 type remove
Layer 11 : 109/768 type remove
Layer 12 : 180/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:41:31,991 [trainer.py] => Time:91.65777611732483
2004 2004
2004 2004
2025-12-10 14:41:38,239 [trainer.py] => Time:6.247703790664673
2025-12-10 14:41:38,240 [inflora.py] => Exemplar size: 0
2025-12-10 14:41:38,240 [trainer.py] => CNN: {'total': np.float64(50.1), '00-01': np.float64(72.04), '02-03': np.float64(53.23), '04-05': np.float64(52.81), '06-07': np.float64(60.71), '08-09': np.float64(46.58), '10-11': np.float64(29.27), '12-13': np.float64(53.7), '14-15': np.float64(46.43), '16-17': np.float64(22.86), '18-19': np.float64(58.93), '20-21': np.float64(65.93), '22-23': np.float64(57.03), '24-25': np.float64(36.73), '26-27': np.float64(77.39), '28-29': np.float64(59.26), '30-31': np.float64(50.0), '32-33': np.float64(36.36), '34-35': np.float64(61.29), '36-37': np.float64(48.33), '38-39': np.float64(60.98), '40-41': np.float64(38.89), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(25.93), '48-49': np.float64(72.34), '50-51': np.float64(29.73), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(12.73), '60-61': np.float64(53.66), '62-63': np.float64(53.77), '64-65': np.float64(6.67), 'old': np.float64(50.76), 'new': np.float64(6.67)}
2025-12-10 14:41:38,240 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1)]
2025-12-10 14:41:38,240 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81)]
2025-12-10 14:41:38,240 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016]
2025-12-10 14:41:43,499 [trainer.py] => All params: 144526051
2025-12-10 14:41:43,510 [trainer.py] => Trainable params: 185858
2025-12-10 14:41:43,511 [inflora.py] => Learning on 66-68
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight'}
2025-12-10 14:42:59,724 [inflora.py] => Task 33, Epoch 50/50 => Loss 0.090, Train_accy 96.15
Threshold:  0.9866
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 29/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 79/768 type remove
Layer 5 : 110/768 type remove
Layer 6 : 100/768 type remove
Layer 7 : 118/768 type remove
Layer 8 : 132/768 type remove
Layer 9 : 165/768 type remove
Layer 10 : 164/768 type remove
Layer 11 : 110/768 type remove
Layer 12 : 184/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:43:06,254 [trainer.py] => Time:82.74302697181702
2023 2023
2023 2023
2025-12-10 14:43:12,536 [trainer.py] => Time:6.282377481460571
2025-12-10 14:43:12,536 [inflora.py] => Exemplar size: 0
2025-12-10 14:43:12,537 [trainer.py] => CNN: {'total': np.float64(49.13), '00-01': np.float64(73.12), '02-03': np.float64(51.61), '04-05': np.float64(50.56), '06-07': np.float64(58.93), '08-09': np.float64(46.58), '10-11': np.float64(26.83), '12-13': np.float64(48.15), '14-15': np.float64(42.86), '16-17': np.float64(25.71), '18-19': np.float64(62.5), '20-21': np.float64(65.93), '22-23': np.float64(57.03), '24-25': np.float64(28.57), '26-27': np.float64(76.52), '28-29': np.float64(56.79), '30-31': np.float64(51.82), '32-33': np.float64(34.09), '34-35': np.float64(48.39), '36-37': np.float64(46.67), '38-39': np.float64(60.98), '40-41': np.float64(33.33), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(76.6), '50-51': np.float64(32.43), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(12.73), '60-61': np.float64(51.22), '62-63': np.float64(56.6), '64-65': np.float64(6.67), '66-67': np.float64(36.84), 'old': np.float64(49.25), 'new': np.float64(36.84)}
2025-12-10 14:43:12,537 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13)]
2025-12-10 14:43:12,537 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8)]
2025-12-10 14:43:12,537 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581]
2025-12-10 14:43:19,741 [trainer.py] => All params: 144526051
2025-12-10 14:43:19,753 [trainer.py] => Trainable params: 185858
2025-12-10 14:43:19,753 [inflora.py] => Learning on 68-70
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'classifier_pool.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight'}
2025-12-10 14:44:45,695 [inflora.py] => Task 34, Epoch 50/50 => Loss 0.049, Train_accy 99.21
Threshold:  0.9868
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 30/768 type remove
Layer 3 : 65/768 type remove
Layer 4 : 81/768 type remove
Layer 5 : 113/768 type remove
Layer 6 : 102/768 type remove
Layer 7 : 120/768 type remove
Layer 8 : 134/768 type remove
Layer 9 : 167/768 type remove
Layer 10 : 167/768 type remove
Layer 11 : 112/768 type remove
Layer 12 : 187/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:44:52,390 [trainer.py] => Time:92.63684749603271
2047 2047
2047 2047
2025-12-10 14:44:58,796 [trainer.py] => Time:6.406194448471069
2025-12-10 14:44:58,796 [inflora.py] => Exemplar size: 0
2025-12-10 14:44:58,796 [trainer.py] => CNN: {'total': np.float64(49.19), '00-01': np.float64(77.42), '02-03': np.float64(54.84), '04-05': np.float64(53.93), '06-07': np.float64(55.36), '08-09': np.float64(46.58), '10-11': np.float64(29.27), '12-13': np.float64(53.7), '14-15': np.float64(42.86), '16-17': np.float64(24.29), '18-19': np.float64(58.93), '20-21': np.float64(67.03), '22-23': np.float64(58.59), '24-25': np.float64(36.73), '26-27': np.float64(76.52), '28-29': np.float64(58.02), '30-31': np.float64(47.27), '32-33': np.float64(31.82), '34-35': np.float64(51.61), '36-37': np.float64(45.0), '38-39': np.float64(60.98), '40-41': np.float64(37.04), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(25.93), '48-49': np.float64(79.79), '50-51': np.float64(16.22), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(16.36), '60-61': np.float64(53.66), '62-63': np.float64(54.72), '64-65': np.float64(3.33), '66-67': np.float64(36.84), '68-69': np.float64(16.67), 'old': np.float64(49.58), 'new': np.float64(16.67)}
2025-12-10 14:44:58,796 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19)]
2025-12-10 14:44:58,796 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9)]
2025-12-10 14:44:58,797 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365]
2025-12-10 14:45:04,620 [trainer.py] => All params: 144526051
2025-12-10 14:45:04,632 [trainer.py] => Trainable params: 185858
2025-12-10 14:45:04,632 [inflora.py] => Learning on 70-72
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight'}
2025-12-10 14:47:02,257 [inflora.py] => Task 35, Epoch 50/50 => Loss 0.042, Train_accy 97.94
Threshold:  0.987
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 30/768 type remove
Layer 3 : 66/768 type remove
Layer 4 : 83/768 type remove
Layer 5 : 115/768 type remove
Layer 6 : 104/768 type remove
Layer 7 : 122/768 type remove
Layer 8 : 135/768 type remove
Layer 9 : 168/768 type remove
Layer 10 : 169/768 type remove
Layer 11 : 113/768 type remove
Layer 12 : 193/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:47:09,068 [trainer.py] => Time:124.43566632270813
2120 2120
2120 2120
2025-12-10 14:47:15,593 [trainer.py] => Time:6.52557897567749
2025-12-10 14:47:15,594 [inflora.py] => Exemplar size: 0
2025-12-10 14:47:15,594 [trainer.py] => CNN: {'total': np.float64(47.64), '00-01': np.float64(76.34), '02-03': np.float64(58.06), '04-05': np.float64(50.56), '06-07': np.float64(55.36), '08-09': np.float64(46.58), '10-11': np.float64(26.83), '12-13': np.float64(42.59), '14-15': np.float64(41.07), '16-17': np.float64(24.29), '18-19': np.float64(58.93), '20-21': np.float64(67.03), '22-23': np.float64(60.16), '24-25': np.float64(36.73), '26-27': np.float64(70.43), '28-29': np.float64(59.26), '30-31': np.float64(44.55), '32-33': np.float64(29.55), '34-35': np.float64(51.61), '36-37': np.float64(50.0), '38-39': np.float64(60.98), '40-41': np.float64(37.04), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(74.47), '50-51': np.float64(13.51), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(16.36), '60-61': np.float64(43.9), '62-63': np.float64(52.83), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(16.67), '70-71': np.float64(36.99), 'old': np.float64(48.02), 'new': np.float64(36.99)}
2025-12-10 14:47:15,594 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64)]
2025-12-10 14:47:15,594 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75)]
2025-12-10 14:47:15,594 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265]
2025-12-10 14:47:19,747 [trainer.py] => All params: 144526051
2025-12-10 14:47:19,759 [trainer.py] => Trainable params: 185858
2025-12-10 14:47:19,759 [inflora.py] => Learning on 72-74
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'classifier_pool.36.bias'}
2025-12-10 14:48:58,374 [inflora.py] => Task 36, Epoch 50/50 => Loss 0.021, Train_accy 98.83
Threshold:  0.9872
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 31/768 type remove
Layer 3 : 69/768 type remove
Layer 4 : 86/768 type remove
Layer 5 : 118/768 type remove
Layer 6 : 106/768 type remove
Layer 7 : 125/768 type remove
Layer 8 : 139/768 type remove
Layer 9 : 173/768 type remove
Layer 10 : 176/768 type remove
Layer 11 : 117/768 type remove
Layer 12 : 199/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:49:04,855 [trainer.py] => Time:105.09580492973328
2160 2160
2160 2160
2025-12-10 14:49:11,502 [trainer.py] => Time:6.646015882492065
2025-12-10 14:49:11,502 [inflora.py] => Exemplar size: 0
2025-12-10 14:49:11,502 [trainer.py] => CNN: {'total': np.float64(48.15), '00-01': np.float64(76.34), '02-03': np.float64(54.84), '04-05': np.float64(51.69), '06-07': np.float64(51.79), '08-09': np.float64(45.21), '10-11': np.float64(24.39), '12-13': np.float64(40.74), '14-15': np.float64(39.29), '16-17': np.float64(27.14), '18-19': np.float64(58.93), '20-21': np.float64(63.74), '22-23': np.float64(64.84), '24-25': np.float64(36.73), '26-27': np.float64(73.04), '28-29': np.float64(60.49), '30-31': np.float64(44.55), '32-33': np.float64(31.82), '34-35': np.float64(48.39), '36-37': np.float64(55.0), '38-39': np.float64(63.41), '40-41': np.float64(35.19), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(25.93), '48-49': np.float64(75.53), '50-51': np.float64(18.92), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(21.82), '60-61': np.float64(43.9), '62-63': np.float64(53.77), '64-65': np.float64(3.33), '66-67': np.float64(36.84), '68-69': np.float64(12.5), '70-71': np.float64(34.25), '72-73': np.float64(52.5), 'old': np.float64(48.07), 'new': np.float64(52.5)}
2025-12-10 14:49:11,502 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15)]
2025-12-10 14:49:11,502 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74)]
2025-12-10 14:49:11,502 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145]
2025-12-10 14:49:18,147 [trainer.py] => All params: 144526051
2025-12-10 14:49:18,159 [trainer.py] => Trainable params: 185858
2025-12-10 14:49:18,159 [inflora.py] => Learning on 74-76
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight'}
2025-12-10 14:51:53,664 [inflora.py] => Task 37, Epoch 50/50 => Loss 0.010, Train_accy 99.74
Threshold:  0.9873999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 32/768 type remove
Layer 3 : 70/768 type remove
Layer 4 : 88/768 type remove
Layer 5 : 120/768 type remove
Layer 6 : 108/768 type remove
Layer 7 : 127/768 type remove
Layer 8 : 141/768 type remove
Layer 9 : 176/768 type remove
Layer 10 : 180/768 type remove
Layer 11 : 120/768 type remove
Layer 12 : 209/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:52:01,441 [trainer.py] => Time:163.2815809249878
2238 2238
2238 2238
2025-12-10 14:52:08,325 [trainer.py] => Time:6.884268283843994
2025-12-10 14:52:08,326 [inflora.py] => Exemplar size: 0
2025-12-10 14:52:08,326 [trainer.py] => CNN: {'total': np.float64(45.58), '00-01': np.float64(64.52), '02-03': np.float64(46.77), '04-05': np.float64(47.19), '06-07': np.float64(48.21), '08-09': np.float64(41.1), '10-11': np.float64(19.51), '12-13': np.float64(33.33), '14-15': np.float64(41.07), '16-17': np.float64(20.0), '18-19': np.float64(58.93), '20-21': np.float64(51.65), '22-23': np.float64(60.94), '24-25': np.float64(34.69), '26-27': np.float64(70.43), '28-29': np.float64(56.79), '30-31': np.float64(41.82), '32-33': np.float64(38.64), '34-35': np.float64(45.16), '36-37': np.float64(53.33), '38-39': np.float64(68.29), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(67.02), '50-51': np.float64(5.41), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(18.18), '60-61': np.float64(43.9), '62-63': np.float64(50.94), '64-65': np.float64(3.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(34.25), '72-73': np.float64(52.5), '74-75': np.float64(75.64), 'old': np.float64(44.49), 'new': np.float64(75.64)}
2025-12-10 14:52:08,326 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58)]
2025-12-10 14:52:08,326 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84)]
2025-12-10 14:52:08,326 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963]
2025-12-10 14:52:14,709 [trainer.py] => All params: 144526051
2025-12-10 14:52:14,721 [trainer.py] => Trainable params: 185858
2025-12-10 14:52:14,721 [inflora.py] => Learning on 76-78
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight'}
2025-12-10 14:54:29,564 [inflora.py] => Task 38, Epoch 50/50 => Loss 0.037, Train_accy 98.38
Threshold:  0.9876
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 72/768 type remove
Layer 4 : 90/768 type remove
Layer 5 : 123/768 type remove
Layer 6 : 110/768 type remove
Layer 7 : 129/768 type remove
Layer 8 : 144/768 type remove
Layer 9 : 179/768 type remove
Layer 10 : 183/768 type remove
Layer 11 : 122/768 type remove
Layer 12 : 211/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:54:36,942 [trainer.py] => Time:142.2204144001007
2321 2321
2321 2321
2025-12-10 14:54:44,024 [trainer.py] => Time:7.082367181777954
2025-12-10 14:54:44,024 [inflora.py] => Exemplar size: 0
2025-12-10 14:54:44,024 [trainer.py] => CNN: {'total': np.float64(47.35), '00-01': np.float64(64.52), '02-03': np.float64(41.94), '04-05': np.float64(47.19), '06-07': np.float64(57.14), '08-09': np.float64(45.21), '10-11': np.float64(21.95), '12-13': np.float64(29.63), '14-15': np.float64(41.07), '16-17': np.float64(14.29), '18-19': np.float64(51.79), '20-21': np.float64(57.14), '22-23': np.float64(62.5), '24-25': np.float64(38.78), '26-27': np.float64(72.17), '28-29': np.float64(59.26), '30-31': np.float64(49.09), '32-33': np.float64(47.73), '34-35': np.float64(51.61), '36-37': np.float64(50.0), '38-39': np.float64(63.41), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(64.89), '50-51': np.float64(8.11), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(20.0), '60-61': np.float64(46.34), '62-63': np.float64(50.0), '64-65': np.float64(10.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(46.58), '72-73': np.float64(55.0), '74-75': np.float64(70.51), '76-77': np.float64(60.24), 'old': np.float64(46.87), 'new': np.float64(60.24)}
2025-12-10 14:54:44,025 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35)]
2025-12-10 14:54:44,025 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69)]
2025-12-10 14:54:44,025 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053]
2025-12-10 14:54:52,606 [trainer.py] => All params: 144526051
2025-12-10 14:54:52,618 [trainer.py] => Trainable params: 185858
2025-12-10 14:54:52,619 [inflora.py] => Learning on 78-80
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight'}
2025-12-10 14:57:15,181 [inflora.py] => Task 39, Epoch 50/50 => Loss 0.047, Train_accy 97.82
Threshold:  0.9878
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 73/768 type remove
Layer 4 : 91/768 type remove
Layer 5 : 124/768 type remove
Layer 6 : 111/768 type remove
Layer 7 : 131/768 type remove
Layer 8 : 146/768 type remove
Layer 9 : 181/768 type remove
Layer 10 : 185/768 type remove
Layer 11 : 123/768 type remove
Layer 12 : 215/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:57:23,145 [trainer.py] => Time:150.52684259414673
2403 2403
2403 2403
2025-12-10 14:57:30,575 [trainer.py] => Time:7.4292216300964355
2025-12-10 14:57:30,575 [inflora.py] => Exemplar size: 0
2025-12-10 14:57:30,575 [trainer.py] => CNN: {'total': np.float64(46.32), '00-01': np.float64(66.67), '02-03': np.float64(41.94), '04-05': np.float64(52.81), '06-07': np.float64(53.57), '08-09': np.float64(46.58), '10-11': np.float64(21.95), '12-13': np.float64(33.33), '14-15': np.float64(39.29), '16-17': np.float64(17.14), '18-19': np.float64(55.36), '20-21': np.float64(59.34), '22-23': np.float64(60.94), '24-25': np.float64(36.73), '26-27': np.float64(71.3), '28-29': np.float64(55.56), '30-31': np.float64(43.64), '32-33': np.float64(43.18), '34-35': np.float64(51.61), '36-37': np.float64(50.0), '38-39': np.float64(58.54), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(54.26), '50-51': np.float64(5.41), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(21.82), '60-61': np.float64(48.78), '62-63': np.float64(45.28), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(43.84), '72-73': np.float64(52.5), '74-75': np.float64(65.38), '76-77': np.float64(59.04), '78-79': np.float64(54.88), 'old': np.float64(46.01), 'new': np.float64(54.88)}
2025-12-10 14:57:30,575 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32)]
2025-12-10 14:57:30,575 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59)]
2025-12-10 14:57:30,576 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441]
2025-12-10 14:57:33,809 [trainer.py] => All params: 144526051
2025-12-10 14:57:33,821 [trainer.py] => Trainable params: 185858
2025-12-10 14:57:33,821 [inflora.py] => Learning on 80-82
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight'}
2025-12-10 14:59:42,325 [inflora.py] => Task 40, Epoch 50/50 => Loss 0.013, Train_accy 100.00
Threshold:  0.988
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 74/768 type remove
Layer 4 : 93/768 type remove
Layer 5 : 126/768 type remove
Layer 6 : 113/768 type remove
Layer 7 : 133/768 type remove
Layer 8 : 148/768 type remove
Layer 9 : 183/768 type remove
Layer 10 : 188/768 type remove
Layer 11 : 126/768 type remove
Layer 12 : 221/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:59:49,695 [trainer.py] => Time:135.87328934669495
2475 2475
2475 2475
2025-12-10 14:59:57,196 [trainer.py] => Time:7.501174449920654
2025-12-10 14:59:57,196 [inflora.py] => Exemplar size: 0
2025-12-10 14:59:57,197 [trainer.py] => CNN: {'total': np.float64(46.87), '00-01': np.float64(67.74), '02-03': np.float64(45.16), '04-05': np.float64(52.81), '06-07': np.float64(55.36), '08-09': np.float64(47.95), '10-11': np.float64(24.39), '12-13': np.float64(33.33), '14-15': np.float64(41.07), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(53.85), '22-23': np.float64(60.94), '24-25': np.float64(34.69), '26-27': np.float64(73.04), '28-29': np.float64(55.56), '30-31': np.float64(43.64), '32-33': np.float64(45.45), '34-35': np.float64(51.61), '36-37': np.float64(51.67), '38-39': np.float64(51.22), '40-41': np.float64(20.37), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(40.74), '48-49': np.float64(61.7), '50-51': np.float64(8.11), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(18.18), '60-61': np.float64(48.78), '62-63': np.float64(46.23), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(39.73), '72-73': np.float64(52.5), '74-75': np.float64(66.67), '76-77': np.float64(60.24), '78-79': np.float64(52.44), '80-81': np.float64(62.5), 'old': np.float64(46.4), 'new': np.float64(62.5)}
2025-12-10 14:59:57,197 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87)]
2025-12-10 14:59:57,197 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6)]
2025-12-10 14:59:57,197 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091]
2025-12-10 15:00:04,582 [trainer.py] => All params: 144526051
2025-12-10 15:00:04,595 [trainer.py] => Trainable params: 185858
2025-12-10 15:00:04,595 [inflora.py] => Learning on 82-84
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight'}
2025-12-10 15:02:52,355 [inflora.py] => Task 41, Epoch 50/50 => Loss 0.056, Train_accy 97.44
Threshold:  0.9882
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 75/768 type remove
Layer 4 : 95/768 type remove
Layer 5 : 128/768 type remove
Layer 6 : 115/768 type remove
Layer 7 : 135/768 type remove
Layer 8 : 150/768 type remove
Layer 9 : 185/768 type remove
Layer 10 : 191/768 type remove
Layer 11 : 128/768 type remove
Layer 12 : 230/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:03:00,639 [trainer.py] => Time:176.04322218894958
2593 2593
2593 2593
2025-12-10 15:03:08,500 [trainer.py] => Time:7.860990762710571
2025-12-10 15:03:08,500 [inflora.py] => Exemplar size: 0
2025-12-10 15:03:08,500 [trainer.py] => CNN: {'total': np.float64(46.28), '00-01': np.float64(65.59), '02-03': np.float64(37.1), '04-05': np.float64(47.19), '06-07': np.float64(57.14), '08-09': np.float64(43.84), '10-11': np.float64(29.27), '12-13': np.float64(35.19), '14-15': np.float64(42.86), '16-17': np.float64(10.0), '18-19': np.float64(57.14), '20-21': np.float64(50.55), '22-23': np.float64(59.38), '24-25': np.float64(32.65), '26-27': np.float64(70.43), '28-29': np.float64(54.32), '30-31': np.float64(51.82), '32-33': np.float64(47.73), '34-35': np.float64(45.16), '36-37': np.float64(48.33), '38-39': np.float64(56.1), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(57.45), '50-51': np.float64(13.51), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(16.36), '60-61': np.float64(48.78), '62-63': np.float64(48.11), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(39.73), '72-73': np.float64(52.5), '74-75': np.float64(69.23), '76-77': np.float64(60.24), '78-79': np.float64(56.1), '80-81': np.float64(68.06), '82-83': np.float64(31.36), 'old': np.float64(46.99), 'new': np.float64(31.36)}
2025-12-10 15:03:08,500 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28)]
2025-12-10 15:03:08,500 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76)]
2025-12-10 15:03:08,501 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998]
2025-12-10 15:03:18,520 [trainer.py] => All params: 144526051
2025-12-10 15:03:18,532 [trainer.py] => Trainable params: 185858
2025-12-10 15:03:18,532 [inflora.py] => Learning on 84-86
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight'}
2025-12-10 15:05:10,917 [inflora.py] => Task 42, Epoch 50/50 => Loss 0.049, Train_accy 99.08
Threshold:  0.9884
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 76/768 type remove
Layer 4 : 96/768 type remove
Layer 5 : 130/768 type remove
Layer 6 : 117/768 type remove
Layer 7 : 138/768 type remove
Layer 8 : 152/768 type remove
Layer 9 : 187/768 type remove
Layer 10 : 193/768 type remove
Layer 11 : 130/768 type remove
Layer 12 : 233/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:05:17,798 [trainer.py] => Time:119.26595163345337
2639 2639
2639 2639
2025-12-10 15:05:25,797 [trainer.py] => Time:7.99921178817749
2025-12-10 15:05:25,798 [inflora.py] => Exemplar size: 0
2025-12-10 15:05:25,798 [trainer.py] => CNN: {'total': np.float64(46.0), '00-01': np.float64(69.89), '02-03': np.float64(50.0), '04-05': np.float64(49.44), '06-07': np.float64(55.36), '08-09': np.float64(45.21), '10-11': np.float64(29.27), '12-13': np.float64(40.74), '14-15': np.float64(41.07), '16-17': np.float64(8.57), '18-19': np.float64(55.36), '20-21': np.float64(51.65), '22-23': np.float64(60.94), '24-25': np.float64(34.69), '26-27': np.float64(72.17), '28-29': np.float64(56.79), '30-31': np.float64(42.73), '32-33': np.float64(34.09), '34-35': np.float64(41.94), '36-37': np.float64(45.0), '38-39': np.float64(51.22), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(57.45), '50-51': np.float64(2.7), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(14.55), '60-61': np.float64(48.78), '62-63': np.float64(43.4), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(12.5), '70-71': np.float64(36.99), '72-73': np.float64(50.0), '74-75': np.float64(69.23), '76-77': np.float64(60.24), '78-79': np.float64(50.0), '80-81': np.float64(66.67), '82-83': np.float64(38.98), '84-85': np.float64(43.48), 'old': np.float64(46.05), 'new': np.float64(43.48)}
2025-12-10 15:05:25,798 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0)]
2025-12-10 15:05:25,798 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21)]
2025-12-10 15:05:25,798 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219]
2025-12-10 15:05:33,490 [trainer.py] => All params: 144526051
2025-12-10 15:05:33,502 [trainer.py] => Trainable params: 185858
2025-12-10 15:05:33,502 [inflora.py] => Learning on 86-88
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'classifier_pool.43.bias', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight'}
2025-12-10 15:08:10,994 [inflora.py] => Task 43, Epoch 50/50 => Loss 0.068, Train_accy 95.88
Threshold:  0.9886
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 78/768 type remove
Layer 4 : 99/768 type remove
Layer 5 : 133/768 type remove
Layer 6 : 119/768 type remove
Layer 7 : 141/768 type remove
Layer 8 : 155/768 type remove
Layer 9 : 190/768 type remove
Layer 10 : 199/768 type remove
Layer 11 : 132/768 type remove
Layer 12 : 236/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:08:18,676 [trainer.py] => Time:165.1732223033905
2726 2726
2726 2726
2025-12-10 15:08:26,965 [trainer.py] => Time:8.288996696472168
2025-12-10 15:08:26,965 [inflora.py] => Exemplar size: 0
2025-12-10 15:08:26,965 [trainer.py] => CNN: {'total': np.float64(46.74), '00-01': np.float64(65.59), '02-03': np.float64(43.55), '04-05': np.float64(52.81), '06-07': np.float64(57.14), '08-09': np.float64(47.95), '10-11': np.float64(26.83), '12-13': np.float64(35.19), '14-15': np.float64(33.93), '16-17': np.float64(10.0), '18-19': np.float64(67.86), '20-21': np.float64(51.65), '22-23': np.float64(62.5), '24-25': np.float64(34.69), '26-27': np.float64(71.3), '28-29': np.float64(58.02), '30-31': np.float64(44.55), '32-33': np.float64(31.82), '34-35': np.float64(41.94), '36-37': np.float64(45.0), '38-39': np.float64(56.1), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(57.45), '50-51': np.float64(18.92), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(21.82), '60-61': np.float64(46.34), '62-63': np.float64(44.34), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(41.1), '72-73': np.float64(57.5), '74-75': np.float64(70.51), '76-77': np.float64(56.63), '78-79': np.float64(50.0), '80-81': np.float64(69.44), '82-83': np.float64(41.53), '84-85': np.float64(47.83), '86-87': np.float64(45.98), 'old': np.float64(46.76), 'new': np.float64(45.98)}
2025-12-10 15:08:26,965 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74)]
2025-12-10 15:08:26,966 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18)]
2025-12-10 15:08:26,966 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456]
2025-12-10 15:08:33,257 [trainer.py] => All params: 144526051
2025-12-10 15:08:33,269 [trainer.py] => Trainable params: 185858
2025-12-10 15:08:33,270 [inflora.py] => Learning on 88-90
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight'}
2025-12-10 15:10:37,858 [inflora.py] => Task 44, Epoch 50/50 => Loss 0.041, Train_accy 98.06
Threshold:  0.9888
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 79/768 type remove
Layer 4 : 101/768 type remove
Layer 5 : 136/768 type remove
Layer 6 : 122/768 type remove
Layer 7 : 146/768 type remove
Layer 8 : 159/768 type remove
Layer 9 : 194/768 type remove
Layer 10 : 203/768 type remove
Layer 11 : 135/768 type remove
Layer 12 : 239/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:10:45,459 [trainer.py] => Time:132.18894290924072
2786 2786
2786 2786
2025-12-10 15:10:53,915 [trainer.py] => Time:8.456436395645142
2025-12-10 15:10:53,915 [inflora.py] => Exemplar size: 0
2025-12-10 15:10:53,916 [trainer.py] => CNN: {'total': np.float64(47.45), '00-01': np.float64(65.59), '02-03': np.float64(48.39), '04-05': np.float64(51.69), '06-07': np.float64(51.79), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(37.04), '14-15': np.float64(42.86), '16-17': np.float64(15.71), '18-19': np.float64(60.71), '20-21': np.float64(53.85), '22-23': np.float64(62.5), '24-25': np.float64(34.69), '26-27': np.float64(72.17), '28-29': np.float64(60.49), '30-31': np.float64(45.45), '32-33': np.float64(34.09), '34-35': np.float64(45.16), '36-37': np.float64(43.33), '38-39': np.float64(56.1), '40-41': np.float64(35.19), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(55.32), '50-51': np.float64(8.11), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(14.55), '60-61': np.float64(46.34), '62-63': np.float64(44.34), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(36.99), '72-73': np.float64(50.0), '74-75': np.float64(67.95), '76-77': np.float64(57.83), '78-79': np.float64(47.56), '80-81': np.float64(69.44), '82-83': np.float64(43.22), '84-85': np.float64(45.65), '86-87': np.float64(49.43), '88-89': np.float64(85.0), 'old': np.float64(46.63), 'new': np.float64(85.0)}
2025-12-10 15:10:53,916 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45)]
2025-12-10 15:10:53,916 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37)]
2025-12-10 15:10:53,916 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965]
2025-12-10 15:10:55,550 [trainer.py] => All params: 144526051
2025-12-10 15:10:55,562 [trainer.py] => Trainable params: 185858
2025-12-10 15:10:55,562 [inflora.py] => Learning on 90-92
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight'}
2025-12-10 15:12:26,349 [inflora.py] => Task 45, Epoch 50/50 => Loss 0.056, Train_accy 96.35
Threshold:  0.989
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 37/768 type remove
Layer 3 : 80/768 type remove
Layer 4 : 102/768 type remove
Layer 5 : 138/768 type remove
Layer 6 : 125/768 type remove
Layer 7 : 150/768 type remove
Layer 8 : 162/768 type remove
Layer 9 : 198/768 type remove
Layer 10 : 208/768 type remove
Layer 11 : 139/768 type remove
Layer 12 : 245/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:12:32,601 [trainer.py] => Time:97.03930616378784
2826 2826
2826 2826
2025-12-10 15:12:41,069 [trainer.py] => Time:8.46753740310669
2025-12-10 15:12:41,069 [inflora.py] => Exemplar size: 0
2025-12-10 15:12:41,070 [trainer.py] => CNN: {'total': np.float64(47.45), '00-01': np.float64(64.52), '02-03': np.float64(46.77), '04-05': np.float64(52.81), '06-07': np.float64(53.57), '08-09': np.float64(41.1), '10-11': np.float64(29.27), '12-13': np.float64(33.33), '14-15': np.float64(44.64), '16-17': np.float64(15.71), '18-19': np.float64(67.86), '20-21': np.float64(52.75), '22-23': np.float64(64.06), '24-25': np.float64(36.73), '26-27': np.float64(69.57), '28-29': np.float64(58.02), '30-31': np.float64(48.18), '32-33': np.float64(31.82), '34-35': np.float64(48.39), '36-37': np.float64(41.67), '38-39': np.float64(51.22), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(54.26), '50-51': np.float64(21.62), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(10.91), '60-61': np.float64(51.22), '62-63': np.float64(43.4), '64-65': np.float64(10.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(38.36), '72-73': np.float64(50.0), '74-75': np.float64(64.1), '76-77': np.float64(61.45), '78-79': np.float64(46.34), '80-81': np.float64(66.67), '82-83': np.float64(43.22), '84-85': np.float64(39.13), '86-87': np.float64(45.98), '88-89': np.float64(85.0), '90-91': np.float64(60.0), 'old': np.float64(47.27), 'new': np.float64(60.0)}
2025-12-10 15:12:41,070 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45)]
2025-12-10 15:12:41,070 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36)]
2025-12-10 15:12:41,070 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857]
2025-12-10 15:12:46,443 [trainer.py] => All params: 144526051
2025-12-10 15:12:46,455 [trainer.py] => Trainable params: 185858
2025-12-10 15:12:46,455 [inflora.py] => Learning on 92-94
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight'}
2025-12-10 15:14:30,156 [inflora.py] => Task 46, Epoch 50/50 => Loss 0.087, Train_accy 96.26
Threshold:  0.9892
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 38/768 type remove
Layer 3 : 81/768 type remove
Layer 4 : 103/768 type remove
Layer 5 : 140/768 type remove
Layer 6 : 127/768 type remove
Layer 7 : 154/768 type remove
Layer 8 : 165/768 type remove
Layer 9 : 202/768 type remove
Layer 10 : 212/768 type remove
Layer 11 : 143/768 type remove
Layer 12 : 257/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:14:36,739 [trainer.py] => Time:110.2840850353241
2875 2875
2875 2875
2025-12-10 15:14:45,410 [trainer.py] => Time:8.670581340789795
2025-12-10 15:14:45,410 [inflora.py] => Exemplar size: 0
2025-12-10 15:14:45,410 [trainer.py] => CNN: {'total': np.float64(45.84), '00-01': np.float64(67.74), '02-03': np.float64(46.77), '04-05': np.float64(51.69), '06-07': np.float64(51.79), '08-09': np.float64(43.84), '10-11': np.float64(29.27), '12-13': np.float64(35.19), '14-15': np.float64(35.71), '16-17': np.float64(14.29), '18-19': np.float64(60.71), '20-21': np.float64(49.45), '22-23': np.float64(63.28), '24-25': np.float64(34.69), '26-27': np.float64(66.09), '28-29': np.float64(59.26), '30-31': np.float64(36.36), '32-33': np.float64(31.82), '34-35': np.float64(41.94), '36-37': np.float64(36.67), '38-39': np.float64(48.78), '40-41': np.float64(33.33), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(60.64), '50-51': np.float64(2.7), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(50.0), '58-59': np.float64(12.73), '60-61': np.float64(51.22), '62-63': np.float64(42.45), '64-65': np.float64(13.33), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(38.36), '72-73': np.float64(45.0), '74-75': np.float64(64.1), '76-77': np.float64(56.63), '78-79': np.float64(51.22), '80-81': np.float64(68.06), '82-83': np.float64(44.07), '84-85': np.float64(41.3), '86-87': np.float64(43.68), '88-89': np.float64(85.0), '90-91': np.float64(57.5), '92-93': np.float64(14.29), 'old': np.float64(46.39), 'new': np.float64(14.29)}
2025-12-10 15:14:45,410 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84)]
2025-12-10 15:14:45,410 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35)]
2025-12-10 15:14:45,410 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522]
2025-12-10 15:14:50,218 [trainer.py] => All params: 144526051
2025-12-10 15:14:50,230 [trainer.py] => Trainable params: 185858
2025-12-10 15:14:50,230 [inflora.py] => Learning on 94-96
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'classifier_pool.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight'}
2025-12-10 15:16:56,783 [inflora.py] => Task 47, Epoch 50/50 => Loss 0.040, Train_accy 97.29
Threshold:  0.9894
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 38/768 type remove
Layer 3 : 82/768 type remove
Layer 4 : 105/768 type remove
Layer 5 : 142/768 type remove
Layer 6 : 130/768 type remove
Layer 7 : 156/768 type remove
Layer 8 : 168/768 type remove
Layer 9 : 205/768 type remove
Layer 10 : 216/768 type remove
Layer 11 : 147/768 type remove
Layer 12 : 260/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:17:04,149 [trainer.py] => Time:133.91829633712769
2946 2946
2946 2946
2025-12-10 15:17:12,969 [trainer.py] => Time:8.819960832595825
2025-12-10 15:17:12,969 [inflora.py] => Exemplar size: 0
2025-12-10 15:17:12,969 [trainer.py] => CNN: {'total': np.float64(46.23), '00-01': np.float64(70.97), '02-03': np.float64(46.77), '04-05': np.float64(53.93), '06-07': np.float64(48.21), '08-09': np.float64(42.47), '10-11': np.float64(29.27), '12-13': np.float64(37.04), '14-15': np.float64(39.29), '16-17': np.float64(14.29), '18-19': np.float64(58.93), '20-21': np.float64(52.75), '22-23': np.float64(64.84), '24-25': np.float64(30.61), '26-27': np.float64(69.57), '28-29': np.float64(55.56), '30-31': np.float64(37.27), '32-33': np.float64(34.09), '34-35': np.float64(45.16), '36-37': np.float64(36.67), '38-39': np.float64(46.34), '40-41': np.float64(35.19), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(64.89), '50-51': np.float64(13.51), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(10.91), '60-61': np.float64(51.22), '62-63': np.float64(45.28), '64-65': np.float64(10.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(38.36), '72-73': np.float64(47.5), '74-75': np.float64(60.26), '76-77': np.float64(56.63), '78-79': np.float64(50.0), '80-81': np.float64(63.89), '82-83': np.float64(47.46), '84-85': np.float64(41.3), '86-87': np.float64(44.83), '88-89': np.float64(86.67), '90-91': np.float64(55.0), '92-93': np.float64(14.29), '94-95': np.float64(43.66), 'old': np.float64(46.3), 'new': np.float64(43.66)}
2025-12-10 15:17:12,970 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23)]
2025-12-10 15:17:12,970 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44)]
2025-12-10 15:17:12,970 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264]
2025-12-10 15:17:17,471 [trainer.py] => All params: 144526051
2025-12-10 15:17:17,483 [trainer.py] => Trainable params: 185858
2025-12-10 15:17:17,483 [inflora.py] => Learning on 96-98
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight'}
2025-12-10 15:19:42,536 [inflora.py] => Task 48, Epoch 50/50 => Loss 0.106, Train_accy 95.87
Threshold:  0.9896
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 39/768 type remove
Layer 3 : 83/768 type remove
Layer 4 : 107/768 type remove
Layer 5 : 145/768 type remove
Layer 6 : 133/768 type remove
Layer 7 : 159/768 type remove
Layer 8 : 170/768 type remove
Layer 9 : 208/768 type remove
Layer 10 : 222/768 type remove
Layer 11 : 154/768 type remove
Layer 12 : 267/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:19:50,144 [trainer.py] => Time:152.6612594127655
3045 3045
3045 3045
2025-12-10 15:19:59,263 [trainer.py] => Time:9.118045806884766
2025-12-10 15:19:59,263 [inflora.py] => Exemplar size: 0
2025-12-10 15:19:59,263 [trainer.py] => CNN: {'total': np.float64(44.89), '00-01': np.float64(69.89), '02-03': np.float64(51.61), '04-05': np.float64(53.93), '06-07': np.float64(44.64), '08-09': np.float64(42.47), '10-11': np.float64(36.59), '12-13': np.float64(33.33), '14-15': np.float64(46.43), '16-17': np.float64(18.57), '18-19': np.float64(55.36), '20-21': np.float64(52.75), '22-23': np.float64(63.28), '24-25': np.float64(32.65), '26-27': np.float64(68.7), '28-29': np.float64(50.62), '30-31': np.float64(34.55), '32-33': np.float64(36.36), '34-35': np.float64(48.39), '36-37': np.float64(36.67), '38-39': np.float64(41.46), '40-41': np.float64(35.19), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(64.89), '50-51': np.float64(13.51), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(20.0), '60-61': np.float64(56.1), '62-63': np.float64(47.17), '64-65': np.float64(10.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(34.25), '72-73': np.float64(42.5), '74-75': np.float64(53.85), '76-77': np.float64(49.4), '78-79': np.float64(52.44), '80-81': np.float64(65.28), '82-83': np.float64(41.53), '84-85': np.float64(45.65), '86-87': np.float64(41.38), '88-89': np.float64(85.0), '90-91': np.float64(55.0), '92-93': np.float64(16.33), '94-95': np.float64(46.48), '96-97': np.float64(23.23), 'old': np.float64(45.62), 'new': np.float64(23.23)}
2025-12-10 15:19:59,264 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89)]
2025-12-10 15:19:59,264 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06)]
2025-12-10 15:19:59,264 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027]
2025-12-10 15:20:08,124 [trainer.py] => All params: 144526051
2025-12-10 15:20:08,136 [trainer.py] => Trainable params: 185858
2025-12-10 15:20:08,136 [inflora.py] => Learning on 98-100
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight'}
2025-12-10 15:22:28,495 [inflora.py] => Task 49, Epoch 50/50 => Loss 0.078, Train_accy 97.43
Threshold:  0.9898
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 40/768 type remove
Layer 3 : 84/768 type remove
Layer 4 : 109/768 type remove
Layer 5 : 147/768 type remove
Layer 6 : 136/768 type remove
Layer 7 : 162/768 type remove
Layer 8 : 172/768 type remove
Layer 9 : 212/768 type remove
Layer 10 : 227/768 type remove
Layer 11 : 159/768 type remove
Layer 12 : 274/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:22:35,919 [trainer.py] => Time:147.7828984260559
3119 3119
3119 3119
2025-12-10 15:22:45,274 [trainer.py] => Time:9.354724168777466
2025-12-10 15:22:45,275 [inflora.py] => Exemplar size: 0
2025-12-10 15:22:45,275 [trainer.py] => CNN: {'total': np.float64(44.31), '00-01': np.float64(62.37), '02-03': np.float64(40.32), '04-05': np.float64(50.56), '06-07': np.float64(50.0), '08-09': np.float64(43.84), '10-11': np.float64(36.59), '12-13': np.float64(35.19), '14-15': np.float64(46.43), '16-17': np.float64(17.14), '18-19': np.float64(53.57), '20-21': np.float64(56.04), '22-23': np.float64(60.16), '24-25': np.float64(34.69), '26-27': np.float64(68.7), '28-29': np.float64(49.38), '30-31': np.float64(44.55), '32-33': np.float64(34.09), '34-35': np.float64(51.61), '36-37': np.float64(41.67), '38-39': np.float64(43.9), '40-41': np.float64(40.74), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(56.38), '50-51': np.float64(21.62), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(50.0), '58-59': np.float64(18.18), '60-61': np.float64(53.66), '62-63': np.float64(45.28), '64-65': np.float64(6.67), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(30.14), '72-73': np.float64(42.5), '74-75': np.float64(60.26), '76-77': np.float64(53.01), '78-79': np.float64(50.0), '80-81': np.float64(65.28), '82-83': np.float64(39.83), '84-85': np.float64(45.65), '86-87': np.float64(39.08), '88-89': np.float64(86.67), '90-91': np.float64(50.0), '92-93': np.float64(18.37), '94-95': np.float64(39.44), '96-97': np.float64(18.18), '98-99': np.float64(33.78), 'old': np.float64(44.56), 'new': np.float64(33.78)}
2025-12-10 15:22:45,275 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31)]
2025-12-10 15:22:45,275 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77)]
2025-12-10 15:22:45,275 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826]
2025-12-10 15:22:52,863 [trainer.py] => All params: 144526051
2025-12-10 15:22:52,875 [trainer.py] => Trainable params: 185858
2025-12-10 15:22:52,875 [inflora.py] => Learning on 100-102
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.50.weight', 'image_encoder.blocks.10.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_k.50.weight', 'image_encoder.blocks.1.attn.lora_B_v.50.weight', 'image_encoder.blocks.0.attn.lora_B_k.50.weight', 'image_encoder.blocks.2.attn.lora_B_k.50.weight', 'classifier_pool.50.weight', 'image_encoder.blocks.3.attn.lora_B_v.50.weight', 'image_encoder.blocks.11.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_v.50.weight', 'image_encoder.blocks.6.attn.lora_B_k.50.weight', 'image_encoder.blocks.8.attn.lora_B_v.50.weight', 'image_encoder.blocks.5.attn.lora_B_v.50.weight', 'image_encoder.blocks.9.attn.lora_B_v.50.weight', 'image_encoder.blocks.6.attn.lora_B_v.50.weight', 'image_encoder.blocks.8.attn.lora_B_k.50.weight', 'image_encoder.blocks.5.attn.lora_B_k.50.weight', 'image_encoder.blocks.0.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.50.weight', 'classifier_pool.50.bias', 'image_encoder.blocks.1.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_k.50.weight', 'image_encoder.blocks.7.attn.lora_B_k.50.weight', 'image_encoder.blocks.3.attn.lora_B_k.50.weight', 'image_encoder.blocks.10.attn.lora_B_k.50.weight'}
2025-12-10 15:24:55,407 [inflora.py] => Task 50, Epoch 50/50 => Loss 0.049, Train_accy 97.69
Threshold:  0.99
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 40/768 type remove
Layer 3 : 85/768 type remove
Layer 4 : 110/768 type remove
Layer 5 : 148/768 type remove
Layer 6 : 137/768 type remove
Layer 7 : 163/768 type remove
Layer 8 : 173/768 type remove
Layer 9 : 213/768 type remove
Layer 10 : 228/768 type remove
Layer 11 : 161/768 type remove
Layer 12 : 277/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:25:02,587 [trainer.py] => Time:129.71159625053406
3179 3179
3179 3179
2025-12-10 15:25:12,047 [trainer.py] => Time:9.459953784942627
2025-12-10 15:25:12,047 [inflora.py] => Exemplar size: 0
2025-12-10 15:25:12,048 [trainer.py] => CNN: {'total': np.float64(44.07), '00-01': np.float64(62.37), '02-03': np.float64(37.1), '04-05': np.float64(48.31), '06-07': np.float64(50.0), '08-09': np.float64(45.21), '10-11': np.float64(34.15), '12-13': np.float64(33.33), '14-15': np.float64(42.86), '16-17': np.float64(18.57), '18-19': np.float64(60.71), '20-21': np.float64(52.75), '22-23': np.float64(60.94), '24-25': np.float64(42.86), '26-27': np.float64(68.7), '28-29': np.float64(44.44), '30-31': np.float64(46.36), '32-33': np.float64(31.82), '34-35': np.float64(48.39), '36-37': np.float64(43.33), '38-39': np.float64(43.9), '40-41': np.float64(38.89), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(51.06), '50-51': np.float64(18.92), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(23.64), '60-61': np.float64(60.98), '62-63': np.float64(37.74), '64-65': np.float64(6.67), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(31.51), '72-73': np.float64(50.0), '74-75': np.float64(58.97), '76-77': np.float64(50.6), '78-79': np.float64(51.22), '80-81': np.float64(63.89), '82-83': np.float64(36.44), '84-85': np.float64(41.3), '86-87': np.float64(40.23), '88-89': np.float64(86.67), '90-91': np.float64(50.0), '92-93': np.float64(16.33), '94-95': np.float64(39.44), '96-97': np.float64(22.22), '98-99': np.float64(28.38), '100-101': np.float64(61.67), 'old': np.float64(43.73), 'new': np.float64(61.67)}
2025-12-10 15:25:12,048 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07)]
2025-12-10 15:25:12,048 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88)]
2025-12-10 15:25:12,048 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327]
2025-12-10 15:25:18,493 [trainer.py] => All params: 144526051
2025-12-10 15:25:18,506 [trainer.py] => Trainable params: 185858
2025-12-10 15:25:18,508 [inflora.py] => Learning on 102-104
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.51.weight', 'image_encoder.blocks.11.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_v.51.weight', 'image_encoder.blocks.3.attn.lora_B_v.51.weight', 'classifier_pool.51.weight', 'image_encoder.blocks.10.attn.lora_B_k.51.weight', 'image_encoder.blocks.8.attn.lora_B_v.51.weight', 'image_encoder.blocks.0.attn.lora_B_k.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.51.weight', 'image_encoder.blocks.2.attn.lora_B_k.51.weight', 'image_encoder.blocks.0.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_k.51.weight', 'image_encoder.blocks.2.attn.lora_B_v.51.weight', 'image_encoder.blocks.3.attn.lora_B_k.51.weight', 'image_encoder.blocks.11.attn.lora_B_v.51.weight', 'image_encoder.blocks.9.attn.lora_B_v.51.weight', 'image_encoder.blocks.5.attn.lora_B_k.51.weight', 'image_encoder.blocks.4.attn.lora_B_v.51.weight', 'image_encoder.blocks.10.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_v.51.weight', 'image_encoder.blocks.6.attn.lora_B_v.51.weight', 'image_encoder.blocks.9.attn.lora_B_k.51.weight', 'image_encoder.blocks.6.attn.lora_B_k.51.weight', 'image_encoder.blocks.5.attn.lora_B_v.51.weight', 'classifier_pool.51.bias'}
2025-12-10 15:27:03,396 [inflora.py] => Task 51, Epoch 50/50 => Loss 0.039, Train_accy 98.51
Threshold:  0.9902
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 41/768 type remove
Layer 3 : 87/768 type remove
Layer 4 : 113/768 type remove
Layer 5 : 151/768 type remove
Layer 6 : 140/768 type remove
Layer 7 : 166/768 type remove
Layer 8 : 178/768 type remove
Layer 9 : 220/768 type remove
Layer 10 : 235/768 type remove
Layer 11 : 169/768 type remove
Layer 12 : 279/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:27:09,894 [trainer.py] => Time:111.3862795829773
3237 3237
3237 3237
2025-12-10 15:27:19,529 [trainer.py] => Time:9.634348392486572
2025-12-10 15:27:19,529 [inflora.py] => Exemplar size: 0
2025-12-10 15:27:19,529 [trainer.py] => CNN: {'total': np.float64(44.64), '00-01': np.float64(63.44), '02-03': np.float64(35.48), '04-05': np.float64(49.44), '06-07': np.float64(66.07), '08-09': np.float64(43.84), '10-11': np.float64(31.71), '12-13': np.float64(33.33), '14-15': np.float64(42.86), '16-17': np.float64(17.14), '18-19': np.float64(66.07), '20-21': np.float64(52.75), '22-23': np.float64(60.94), '24-25': np.float64(40.82), '26-27': np.float64(67.83), '28-29': np.float64(48.15), '30-31': np.float64(45.45), '32-33': np.float64(29.55), '34-35': np.float64(58.06), '36-37': np.float64(35.0), '38-39': np.float64(46.34), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(50.0), '50-51': np.float64(27.03), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(21.82), '60-61': np.float64(58.54), '62-63': np.float64(41.51), '64-65': np.float64(10.0), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(32.88), '72-73': np.float64(50.0), '74-75': np.float64(64.1), '76-77': np.float64(50.6), '78-79': np.float64(50.0), '80-81': np.float64(62.5), '82-83': np.float64(37.29), '84-85': np.float64(34.78), '86-87': np.float64(39.08), '88-89': np.float64(86.67), '90-91': np.float64(55.0), '92-93': np.float64(16.33), '94-95': np.float64(42.25), '96-97': np.float64(22.22), '98-99': np.float64(28.38), '100-101': np.float64(50.0), '102-103': np.float64(67.24), 'old': np.float64(44.23), 'new': np.float64(67.24)}
2025-12-10 15:27:19,529 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64)]
2025-12-10 15:27:19,529 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89)]
2025-12-10 15:27:19,530 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605]
2025-12-10 15:27:24,233 [trainer.py] => All params: 144526051
2025-12-10 15:27:24,245 [trainer.py] => Trainable params: 185858
2025-12-10 15:27:24,245 [inflora.py] => Learning on 104-106
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.52.weight', 'image_encoder.blocks.3.attn.lora_B_v.52.weight', 'image_encoder.blocks.8.attn.lora_B_v.52.weight', 'image_encoder.blocks.11.attn.lora_B_k.52.weight', 'image_encoder.blocks.9.attn.lora_B_k.52.weight', 'image_encoder.blocks.4.attn.lora_B_v.52.weight', 'image_encoder.blocks.7.attn.lora_B_k.52.weight', 'image_encoder.blocks.11.attn.lora_B_v.52.weight', 'classifier_pool.52.bias', 'image_encoder.blocks.0.attn.lora_B_v.52.weight', 'image_encoder.blocks.0.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.52.weight', 'image_encoder.blocks.8.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_v.52.weight', 'image_encoder.blocks.6.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_v.52.weight', 'image_encoder.blocks.6.attn.lora_B_k.52.weight', 'image_encoder.blocks.4.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_k.52.weight', 'image_encoder.blocks.1.attn.lora_B_k.52.weight', 'classifier_pool.52.weight', 'image_encoder.blocks.1.attn.lora_B_v.52.weight', 'image_encoder.blocks.2.attn.lora_B_k.52.weight', 'image_encoder.blocks.2.attn.lora_B_v.52.weight', 'image_encoder.blocks.3.attn.lora_B_k.52.weight'}
2025-12-10 15:29:53,162 [inflora.py] => Task 52, Epoch 50/50 => Loss 0.147, Train_accy 93.26
Threshold:  0.9904
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 89/768 type remove
Layer 4 : 118/768 type remove
Layer 5 : 157/768 type remove
Layer 6 : 146/768 type remove
Layer 7 : 173/768 type remove
Layer 8 : 184/768 type remove
Layer 9 : 231/768 type remove
Layer 10 : 249/768 type remove
Layer 11 : 180/768 type remove
Layer 12 : 282/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:30:00,834 [trainer.py] => Time:156.58891677856445
3331 3331
3331 3331
2025-12-10 15:30:10,786 [trainer.py] => Time:9.9519944190979
2025-12-10 15:30:10,786 [inflora.py] => Exemplar size: 0
2025-12-10 15:30:10,786 [trainer.py] => CNN: {'total': np.float64(43.26), '00-01': np.float64(67.74), '02-03': np.float64(30.65), '04-05': np.float64(46.07), '06-07': np.float64(64.29), '08-09': np.float64(47.95), '10-11': np.float64(34.15), '12-13': np.float64(29.63), '14-15': np.float64(42.86), '16-17': np.float64(17.14), '18-19': np.float64(67.86), '20-21': np.float64(56.04), '22-23': np.float64(58.59), '24-25': np.float64(32.65), '26-27': np.float64(66.09), '28-29': np.float64(49.38), '30-31': np.float64(42.73), '32-33': np.float64(27.27), '34-35': np.float64(48.39), '36-37': np.float64(26.67), '38-39': np.float64(46.34), '40-41': np.float64(35.19), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(44.68), '50-51': np.float64(21.62), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(23.64), '60-61': np.float64(60.98), '62-63': np.float64(36.79), '64-65': np.float64(13.33), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(45.0), '74-75': np.float64(61.54), '76-77': np.float64(49.4), '78-79': np.float64(48.78), '80-81': np.float64(63.89), '82-83': np.float64(34.75), '84-85': np.float64(32.61), '86-87': np.float64(35.63), '88-89': np.float64(86.67), '90-91': np.float64(55.0), '92-93': np.float64(14.29), '94-95': np.float64(49.3), '96-97': np.float64(22.22), '98-99': np.float64(35.14), '100-101': np.float64(51.67), '102-103': np.float64(72.41), '104-105': np.float64(18.09), 'old': np.float64(43.99), 'new': np.float64(18.09)}
2025-12-10 15:30:10,787 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26)]
2025-12-10 15:30:10,787 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5)]
2025-12-10 15:30:10,787 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873]
2025-12-10 15:30:13,868 [trainer.py] => All params: 144526051
2025-12-10 15:30:13,880 [trainer.py] => Trainable params: 185858
2025-12-10 15:30:13,880 [inflora.py] => Learning on 106-108
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.53.weight', 'image_encoder.blocks.9.attn.lora_B_k.53.weight', 'classifier_pool.53.bias', 'image_encoder.blocks.0.attn.lora_B_k.53.weight', 'image_encoder.blocks.11.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_k.53.weight', 'image_encoder.blocks.3.attn.lora_B_v.53.weight', 'image_encoder.blocks.4.attn.lora_B_k.53.weight', 'image_encoder.blocks.6.attn.lora_B_v.53.weight', 'image_encoder.blocks.5.attn.lora_B_k.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.53.weight', 'image_encoder.blocks.8.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_v.53.weight', 'image_encoder.blocks.3.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_v.53.weight', 'image_encoder.blocks.6.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_v.53.weight', 'image_encoder.blocks.9.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_k.53.weight', 'image_encoder.blocks.5.attn.lora_B_v.53.weight', 'classifier_pool.53.weight', 'image_encoder.blocks.0.attn.lora_B_v.53.weight', 'image_encoder.blocks.11.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.53.weight', 'image_encoder.blocks.7.attn.lora_B_k.53.weight'}
2025-12-10 15:31:50,940 [inflora.py] => Task 53, Epoch 50/50 => Loss 0.129, Train_accy 94.30
Threshold:  0.9906
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 91/768 type remove
Layer 4 : 119/768 type remove
Layer 5 : 158/768 type remove
Layer 6 : 148/768 type remove
Layer 7 : 175/768 type remove
Layer 8 : 186/768 type remove
Layer 9 : 233/768 type remove
Layer 10 : 251/768 type remove
Layer 11 : 183/768 type remove
Layer 12 : 287/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:31:57,708 [trainer.py] => Time:103.82716751098633
3370 3370
3370 3370
2025-12-10 15:32:07,668 [trainer.py] => Time:9.959831953048706
2025-12-10 15:32:07,668 [inflora.py] => Exemplar size: 0
2025-12-10 15:32:07,668 [trainer.py] => CNN: {'total': np.float64(43.74), '00-01': np.float64(68.82), '02-03': np.float64(32.26), '04-05': np.float64(44.94), '06-07': np.float64(64.29), '08-09': np.float64(42.47), '10-11': np.float64(29.27), '12-13': np.float64(31.48), '14-15': np.float64(41.07), '16-17': np.float64(20.0), '18-19': np.float64(67.86), '20-21': np.float64(51.65), '22-23': np.float64(59.38), '24-25': np.float64(32.65), '26-27': np.float64(60.87), '28-29': np.float64(49.38), '30-31': np.float64(40.0), '32-33': np.float64(22.73), '34-35': np.float64(51.61), '36-37': np.float64(25.0), '38-39': np.float64(53.66), '40-41': np.float64(33.33), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(47.87), '50-51': np.float64(27.03), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(20.0), '60-61': np.float64(63.41), '62-63': np.float64(38.68), '64-65': np.float64(10.0), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(42.5), '74-75': np.float64(62.82), '76-77': np.float64(54.22), '78-79': np.float64(50.0), '80-81': np.float64(63.89), '82-83': np.float64(41.53), '84-85': np.float64(39.13), '86-87': np.float64(39.08), '88-89': np.float64(86.67), '90-91': np.float64(57.5), '92-93': np.float64(12.24), '94-95': np.float64(52.11), '96-97': np.float64(23.23), '98-99': np.float64(35.14), '100-101': np.float64(53.33), '102-103': np.float64(70.69), '104-105': np.float64(20.21), '106-107': np.float64(51.28), 'old': np.float64(43.65), 'new': np.float64(51.28)}
2025-12-10 15:32:07,668 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74)]
2025-12-10 15:32:07,668 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58)]
2025-12-10 15:32:07,669 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283]
2025-12-10 15:32:14,271 [trainer.py] => All params: 144526051
2025-12-10 15:32:14,283 [trainer.py] => Trainable params: 185858
2025-12-10 15:32:14,283 [inflora.py] => Learning on 108-110
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.54.weight', 'image_encoder.blocks.4.attn.lora_B_v.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.54.weight', 'image_encoder.blocks.4.attn.lora_B_k.54.weight', 'image_encoder.blocks.3.attn.lora_B_v.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.54.weight', 'image_encoder.blocks.11.attn.lora_B_k.54.weight', 'image_encoder.blocks.8.attn.lora_B_k.54.weight', 'image_encoder.blocks.10.attn.lora_B_k.54.weight', 'image_encoder.blocks.6.attn.lora_B_k.54.weight', 'classifier_pool.54.weight', 'image_encoder.blocks.10.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_v.54.weight', 'image_encoder.blocks.1.attn.lora_B_v.54.weight', 'image_encoder.blocks.9.attn.lora_B_v.54.weight', 'image_encoder.blocks.6.attn.lora_B_v.54.weight', 'image_encoder.blocks.5.attn.lora_B_k.54.weight', 'image_encoder.blocks.7.attn.lora_B_k.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.54.weight', 'image_encoder.blocks.3.attn.lora_B_k.54.weight', 'image_encoder.blocks.5.attn.lora_B_v.54.weight', 'image_encoder.blocks.2.attn.lora_B_v.54.weight', 'classifier_pool.54.bias', 'image_encoder.blocks.7.attn.lora_B_v.54.weight', 'image_encoder.blocks.2.attn.lora_B_k.54.weight'}
2025-12-10 15:34:12,577 [inflora.py] => Task 54, Epoch 50/50 => Loss 0.024, Train_accy 99.60
Threshold:  0.9908
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 92/768 type remove
Layer 4 : 120/768 type remove
Layer 5 : 159/768 type remove
Layer 6 : 150/768 type remove
Layer 7 : 177/768 type remove
Layer 8 : 188/768 type remove
Layer 9 : 235/768 type remove
Layer 10 : 255/768 type remove
Layer 11 : 186/768 type remove
Layer 12 : 299/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:34:19,272 [trainer.py] => Time:124.98953938484192
3437 3437
3437 3437
2025-12-10 15:34:29,447 [trainer.py] => Time:10.174566507339478
2025-12-10 15:34:29,448 [inflora.py] => Exemplar size: 0
2025-12-10 15:34:29,448 [trainer.py] => CNN: {'total': np.float64(43.93), '00-01': np.float64(72.04), '02-03': np.float64(35.48), '04-05': np.float64(49.44), '06-07': np.float64(57.14), '08-09': np.float64(41.1), '10-11': np.float64(29.27), '12-13': np.float64(29.63), '14-15': np.float64(35.71), '16-17': np.float64(15.71), '18-19': np.float64(60.71), '20-21': np.float64(54.95), '22-23': np.float64(63.28), '24-25': np.float64(34.69), '26-27': np.float64(65.22), '28-29': np.float64(48.15), '30-31': np.float64(39.09), '32-33': np.float64(29.55), '34-35': np.float64(45.16), '36-37': np.float64(30.0), '38-39': np.float64(58.54), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(51.06), '50-51': np.float64(29.73), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(18.18), '60-61': np.float64(56.1), '62-63': np.float64(34.91), '64-65': np.float64(10.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(32.88), '72-73': np.float64(42.5), '74-75': np.float64(61.54), '76-77': np.float64(50.6), '78-79': np.float64(50.0), '80-81': np.float64(68.06), '82-83': np.float64(42.37), '84-85': np.float64(36.96), '86-87': np.float64(35.63), '88-89': np.float64(83.33), '90-91': np.float64(55.0), '92-93': np.float64(12.24), '94-95': np.float64(52.11), '96-97': np.float64(25.25), '98-99': np.float64(31.08), '100-101': np.float64(53.33), '102-103': np.float64(72.41), '104-105': np.float64(21.28), '106-107': np.float64(43.59), '108-109': np.float64(62.69), 'old': np.float64(43.56), 'new': np.float64(62.69)}
2025-12-10 15:34:29,448 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93)]
2025-12-10 15:34:29,448 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93)]
2025-12-10 15:34:29,448 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736]
2025-12-10 15:34:34,510 [trainer.py] => All params: 144526051
2025-12-10 15:34:34,522 [trainer.py] => Trainable params: 185858
2025-12-10 15:34:34,522 [inflora.py] => Learning on 110-112
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.55.weight', 'image_encoder.blocks.8.attn.lora_B_k.55.weight', 'image_encoder.blocks.7.attn.lora_B_k.55.weight', 'image_encoder.blocks.9.attn.lora_B_v.55.weight', 'image_encoder.blocks.10.attn.lora_B_k.55.weight', 'image_encoder.blocks.2.attn.lora_B_k.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_k.55.weight', 'image_encoder.blocks.11.attn.lora_B_v.55.weight', 'image_encoder.blocks.1.attn.lora_B_k.55.weight', 'image_encoder.blocks.5.attn.lora_B_k.55.weight', 'image_encoder.blocks.7.attn.lora_B_v.55.weight', 'image_encoder.blocks.11.attn.lora_B_k.55.weight', 'image_encoder.blocks.10.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_k.55.weight', 'image_encoder.blocks.3.attn.lora_B_v.55.weight', 'image_encoder.blocks.6.attn.lora_B_k.55.weight', 'classifier_pool.55.bias', 'image_encoder.blocks.0.attn.lora_B_k.55.weight', 'image_encoder.blocks.1.attn.lora_B_v.55.weight', 'image_encoder.blocks.0.attn.lora_B_v.55.weight', 'image_encoder.blocks.6.attn.lora_B_v.55.weight', 'classifier_pool.55.weight', 'image_encoder.blocks.3.attn.lora_B_k.55.weight', 'image_encoder.blocks.8.attn.lora_B_v.55.weight', 'image_encoder.blocks.2.attn.lora_B_v.55.weight'}
2025-12-10 15:36:47,142 [inflora.py] => Task 55, Epoch 50/50 => Loss 0.039, Train_accy 99.31
Threshold:  0.991
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 94/768 type remove
Layer 4 : 123/768 type remove
Layer 5 : 161/768 type remove
Layer 6 : 153/768 type remove
Layer 7 : 180/768 type remove
Layer 8 : 193/768 type remove
Layer 9 : 243/768 type remove
Layer 10 : 266/768 type remove
Layer 11 : 194/768 type remove
Layer 12 : 305/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:36:54,562 [trainer.py] => Time:140.04041075706482
3497 3497
3497 3497
2025-12-10 15:37:04,977 [trainer.py] => Time:10.41444444656372
2025-12-10 15:37:04,978 [inflora.py] => Exemplar size: 0
2025-12-10 15:37:04,978 [trainer.py] => CNN: {'total': np.float64(43.29), '00-01': np.float64(70.97), '02-03': np.float64(30.65), '04-05': np.float64(48.31), '06-07': np.float64(58.93), '08-09': np.float64(36.99), '10-11': np.float64(26.83), '12-13': np.float64(31.48), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(62.5), '20-21': np.float64(48.35), '22-23': np.float64(62.5), '24-25': np.float64(28.57), '26-27': np.float64(64.35), '28-29': np.float64(50.62), '30-31': np.float64(39.09), '32-33': np.float64(22.73), '34-35': np.float64(48.39), '36-37': np.float64(33.33), '38-39': np.float64(53.66), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(47.87), '50-51': np.float64(18.92), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(20.0), '60-61': np.float64(53.66), '62-63': np.float64(37.74), '64-65': np.float64(10.0), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(35.62), '72-73': np.float64(45.0), '74-75': np.float64(56.41), '76-77': np.float64(49.4), '78-79': np.float64(45.12), '80-81': np.float64(61.11), '82-83': np.float64(42.37), '84-85': np.float64(39.13), '86-87': np.float64(37.93), '88-89': np.float64(83.33), '90-91': np.float64(60.0), '92-93': np.float64(14.29), '94-95': np.float64(42.25), '96-97': np.float64(23.23), '98-99': np.float64(32.43), '100-101': np.float64(53.33), '102-103': np.float64(70.69), '104-105': np.float64(21.28), '106-107': np.float64(46.15), '108-109': np.float64(58.21), '110-111': np.float64(56.67), 'old': np.float64(43.06), 'new': np.float64(56.67)}
2025-12-10 15:37:04,978 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29)]
2025-12-10 15:37:04,978 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51)]
2025-12-10 15:37:04,978 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191]
2025-12-10 15:37:09,296 [trainer.py] => All params: 144526051
2025-12-10 15:37:09,308 [trainer.py] => Trainable params: 185858
2025-12-10 15:37:09,308 [inflora.py] => Learning on 112-114
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.56.weight', 'image_encoder.blocks.10.attn.lora_B_k.56.weight', 'image_encoder.blocks.4.attn.lora_B_k.56.weight', 'image_encoder.blocks.6.attn.lora_B_v.56.weight', 'image_encoder.blocks.9.attn.lora_B_k.56.weight', 'image_encoder.blocks.3.attn.lora_B_v.56.weight', 'image_encoder.blocks.1.attn.lora_B_k.56.weight', 'image_encoder.blocks.8.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.56.weight', 'classifier_pool.56.weight', 'classifier_pool.56.bias', 'image_encoder.blocks.10.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_v.56.weight', 'image_encoder.blocks.9.attn.lora_B_v.56.weight', 'image_encoder.blocks.4.attn.lora_B_v.56.weight', 'image_encoder.blocks.5.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_v.56.weight', 'image_encoder.blocks.3.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.56.weight', 'image_encoder.blocks.2.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.56.weight', 'image_encoder.blocks.5.attn.lora_B_v.56.weight', 'image_encoder.blocks.8.attn.lora_B_k.56.weight'}
2025-12-10 15:38:50,077 [inflora.py] => Task 56, Epoch 50/50 => Loss 0.044, Train_accy 98.88
Threshold:  0.9912
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 95/768 type remove
Layer 4 : 125/768 type remove
Layer 5 : 164/768 type remove
Layer 6 : 159/768 type remove
Layer 7 : 187/768 type remove
Layer 8 : 202/768 type remove
Layer 9 : 257/768 type remove
Layer 10 : 285/768 type remove
Layer 11 : 207/768 type remove
Layer 12 : 317/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:38:56,945 [trainer.py] => Time:107.63676023483276
3549 3549
3549 3549
2025-12-10 15:39:07,427 [trainer.py] => Time:10.482415676116943
2025-12-10 15:39:07,428 [inflora.py] => Exemplar size: 0
2025-12-10 15:39:07,428 [trainer.py] => CNN: {'total': np.float64(43.48), '00-01': np.float64(69.89), '02-03': np.float64(33.87), '04-05': np.float64(48.31), '06-07': np.float64(48.21), '08-09': np.float64(36.99), '10-11': np.float64(29.27), '12-13': np.float64(35.19), '14-15': np.float64(39.29), '16-17': np.float64(17.14), '18-19': np.float64(58.93), '20-21': np.float64(54.95), '22-23': np.float64(63.28), '24-25': np.float64(28.57), '26-27': np.float64(61.74), '28-29': np.float64(50.62), '30-31': np.float64(34.55), '32-33': np.float64(36.36), '34-35': np.float64(45.16), '36-37': np.float64(36.67), '38-39': np.float64(53.66), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(51.06), '50-51': np.float64(16.22), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(16.36), '60-61': np.float64(53.66), '62-63': np.float64(39.62), '64-65': np.float64(10.0), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(42.5), '74-75': np.float64(56.41), '76-77': np.float64(51.81), '78-79': np.float64(40.24), '80-81': np.float64(68.06), '82-83': np.float64(41.53), '84-85': np.float64(36.96), '86-87': np.float64(37.93), '88-89': np.float64(83.33), '90-91': np.float64(52.5), '92-93': np.float64(12.24), '94-95': np.float64(46.48), '96-97': np.float64(24.24), '98-99': np.float64(35.14), '100-101': np.float64(48.33), '102-103': np.float64(67.24), '104-105': np.float64(14.89), '106-107': np.float64(43.59), '108-109': np.float64(61.19), '110-111': np.float64(51.67), '112-113': np.float64(84.62), 'old': np.float64(42.87), 'new': np.float64(84.62)}
2025-12-10 15:39:07,428 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48)]
2025-12-10 15:39:07,428 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63)]
2025-12-10 15:39:07,428 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584]
2025-12-10 15:39:13,081 [trainer.py] => All params: 144526051
2025-12-10 15:39:13,093 [trainer.py] => Trainable params: 185858
2025-12-10 15:39:13,093 [inflora.py] => Learning on 114-116
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.57.weight', 'image_encoder.blocks.11.attn.lora_B_k.57.weight', 'image_encoder.blocks.5.attn.lora_B_k.57.weight', 'image_encoder.blocks.1.attn.lora_B_k.57.weight', 'image_encoder.blocks.0.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_k.57.weight', 'image_encoder.blocks.4.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_v.57.weight', 'image_encoder.blocks.3.attn.lora_B_v.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.57.weight', 'image_encoder.blocks.0.attn.lora_B_k.57.weight', 'image_encoder.blocks.6.attn.lora_B_v.57.weight', 'image_encoder.blocks.9.attn.lora_B_k.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.57.weight', 'image_encoder.blocks.8.attn.lora_B_v.57.weight', 'classifier_pool.57.bias', 'image_encoder.blocks.10.attn.lora_B_k.57.weight', 'image_encoder.blocks.2.attn.lora_B_v.57.weight', 'image_encoder.blocks.9.attn.lora_B_v.57.weight', 'image_encoder.blocks.1.attn.lora_B_v.57.weight', 'image_encoder.blocks.8.attn.lora_B_k.57.weight', 'image_encoder.blocks.3.attn.lora_B_k.57.weight', 'classifier_pool.57.weight', 'image_encoder.blocks.4.attn.lora_B_k.57.weight', 'image_encoder.blocks.6.attn.lora_B_k.57.weight', 'image_encoder.blocks.10.attn.lora_B_v.57.weight'}
2025-12-10 15:40:50,328 [inflora.py] => Task 57, Epoch 50/50 => Loss 0.055, Train_accy 98.67
Threshold:  0.9914
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
Skip Updating DualGPM for layer: 3
Skip Updating DualGPM for layer: 4
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 95/768 type remove
Layer 4 : 125/768 type remove
Layer 5 : 165/768 type remove
Layer 6 : 161/768 type remove
Layer 7 : 190/768 type remove
Layer 8 : 205/768 type remove
Layer 9 : 260/768 type remove
Layer 10 : 288/768 type remove
Layer 11 : 210/768 type remove
Layer 12 : 322/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:40:57,253 [trainer.py] => Time:104.16020369529724
3589 3589
3589 3589
2025-12-10 15:41:07,935 [trainer.py] => Time:10.681865453720093
2025-12-10 15:41:07,936 [inflora.py] => Exemplar size: 0
2025-12-10 15:41:07,936 [trainer.py] => CNN: {'total': np.float64(43.44), '00-01': np.float64(74.19), '02-03': np.float64(37.1), '04-05': np.float64(48.31), '06-07': np.float64(53.57), '08-09': np.float64(39.73), '10-11': np.float64(29.27), '12-13': np.float64(33.33), '14-15': np.float64(42.86), '16-17': np.float64(11.43), '18-19': np.float64(58.93), '20-21': np.float64(54.95), '22-23': np.float64(63.28), '24-25': np.float64(24.49), '26-27': np.float64(63.48), '28-29': np.float64(50.62), '30-31': np.float64(40.0), '32-33': np.float64(29.55), '34-35': np.float64(41.94), '36-37': np.float64(40.0), '38-39': np.float64(53.66), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(52.13), '50-51': np.float64(16.22), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(12.73), '60-61': np.float64(51.22), '62-63': np.float64(37.74), '64-65': np.float64(13.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(34.25), '72-73': np.float64(42.5), '74-75': np.float64(57.69), '76-77': np.float64(53.01), '78-79': np.float64(34.15), '80-81': np.float64(63.89), '82-83': np.float64(38.98), '84-85': np.float64(41.3), '86-87': np.float64(39.08), '88-89': np.float64(83.33), '90-91': np.float64(52.5), '92-93': np.float64(12.24), '94-95': np.float64(43.66), '96-97': np.float64(24.24), '98-99': np.float64(35.14), '100-101': np.float64(45.0), '102-103': np.float64(68.97), '104-105': np.float64(17.02), '106-107': np.float64(38.46), '108-109': np.float64(58.21), '110-111': np.float64(51.67), '112-113': np.float64(84.62), '114-115': np.float64(47.5), 'old': np.float64(43.39), 'new': np.float64(47.5)}
2025-12-10 15:41:07,936 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44)]
2025-12-10 15:41:07,936 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54)]
2025-12-10 15:41:07,936 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915]
2025-12-10 15:41:11,409 [trainer.py] => All params: 144526051
2025-12-10 15:41:11,421 [trainer.py] => Trainable params: 185858
2025-12-10 15:41:11,421 [inflora.py] => Learning on 116-118
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.58.weight', 'classifier_pool.58.weight', 'image_encoder.blocks.1.attn.lora_B_k.58.weight', 'image_encoder.blocks.2.attn.lora_B_k.58.weight', 'image_encoder.blocks.10.attn.lora_B_k.58.weight', 'image_encoder.blocks.10.attn.lora_B_v.58.weight', 'image_encoder.blocks.2.attn.lora_B_v.58.weight', 'image_encoder.blocks.6.attn.lora_B_k.58.weight', 'classifier_pool.58.bias', 'image_encoder.blocks.6.attn.lora_B_v.58.weight', 'image_encoder.blocks.7.attn.lora_B_k.58.weight', 'image_encoder.blocks.5.attn.lora_B_v.58.weight', 'image_encoder.blocks.0.attn.lora_B_v.58.weight', 'image_encoder.blocks.4.attn.lora_B_k.58.weight', 'image_encoder.blocks.9.attn.lora_B_k.58.weight', 'image_encoder.blocks.11.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_v.58.weight', 'image_encoder.blocks.7.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_v.58.weight', 'image_encoder.blocks.1.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.58.weight', 'image_encoder.blocks.4.attn.lora_B_v.58.weight', 'image_encoder.blocks.0.attn.lora_B_k.58.weight', 'image_encoder.blocks.11.attn.lora_B_v.58.weight', 'image_encoder.blocks.9.attn.lora_B_v.58.weight'}
2025-12-10 15:42:45,845 [inflora.py] => Task 58, Epoch 50/50 => Loss 0.017, Train_accy 99.31
Threshold:  0.9916
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 96/768 type remove
Layer 4 : 126/768 type remove
Layer 5 : 168/768 type remove
Layer 6 : 167/768 type remove
Layer 7 : 195/768 type remove
Layer 8 : 211/768 type remove
Layer 9 : 271/768 type remove
Layer 10 : 306/768 type remove
Layer 11 : 222/768 type remove
Layer 12 : 345/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:42:52,684 [trainer.py] => Time:101.26309847831726
3625 3625
3625 3625
2025-12-10 15:43:03,392 [trainer.py] => Time:10.707171440124512
2025-12-10 15:43:03,392 [inflora.py] => Exemplar size: 0
2025-12-10 15:43:03,392 [trainer.py] => CNN: {'total': np.float64(43.2), '00-01': np.float64(75.27), '02-03': np.float64(35.48), '04-05': np.float64(47.19), '06-07': np.float64(51.79), '08-09': np.float64(38.36), '10-11': np.float64(29.27), '12-13': np.float64(25.93), '14-15': np.float64(41.07), '16-17': np.float64(11.43), '18-19': np.float64(62.5), '20-21': np.float64(58.24), '22-23': np.float64(61.72), '24-25': np.float64(24.49), '26-27': np.float64(65.22), '28-29': np.float64(51.85), '30-31': np.float64(38.18), '32-33': np.float64(27.27), '34-35': np.float64(45.16), '36-37': np.float64(41.67), '38-39': np.float64(58.54), '40-41': np.float64(33.33), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(45.74), '50-51': np.float64(18.92), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(10.91), '60-61': np.float64(46.34), '62-63': np.float64(38.68), '64-65': np.float64(13.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(32.88), '72-73': np.float64(40.0), '74-75': np.float64(60.26), '76-77': np.float64(54.22), '78-79': np.float64(31.71), '80-81': np.float64(61.11), '82-83': np.float64(37.29), '84-85': np.float64(45.65), '86-87': np.float64(34.48), '88-89': np.float64(85.0), '90-91': np.float64(55.0), '92-93': np.float64(14.29), '94-95': np.float64(38.03), '96-97': np.float64(24.24), '98-99': np.float64(36.49), '100-101': np.float64(43.33), '102-103': np.float64(68.97), '104-105': np.float64(17.02), '106-107': np.float64(38.46), '108-109': np.float64(52.24), '110-111': np.float64(46.67), '112-113': np.float64(86.54), '114-115': np.float64(47.5), '116-117': np.float64(72.22), 'old': np.float64(42.91), 'new': np.float64(72.22)}
2025-12-10 15:43:03,393 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2)]
2025-12-10 15:43:03,393 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48)]
2025-12-10 15:43:03,393 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655]
2025-12-10 15:43:08,447 [trainer.py] => All params: 144526051
2025-12-10 15:43:08,459 [trainer.py] => Trainable params: 185858
2025-12-10 15:43:08,459 [inflora.py] => Learning on 118-120
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.59.weight', 'image_encoder.blocks.0.attn.lora_B_v.59.weight', 'image_encoder.blocks.9.attn.lora_B_v.59.weight', 'image_encoder.blocks.5.attn.lora_B_k.59.weight', 'image_encoder.blocks.8.attn.lora_B_k.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.59.weight', 'image_encoder.blocks.3.attn.lora_B_k.59.weight', 'image_encoder.blocks.2.attn.lora_B_k.59.weight', 'image_encoder.blocks.9.attn.lora_B_k.59.weight', 'image_encoder.blocks.7.attn.lora_B_k.59.weight', 'image_encoder.blocks.10.attn.lora_B_v.59.weight', 'image_encoder.blocks.6.attn.lora_B_v.59.weight', 'image_encoder.blocks.11.attn.lora_B_k.59.weight', 'image_encoder.blocks.7.attn.lora_B_v.59.weight', 'classifier_pool.59.weight', 'image_encoder.blocks.6.attn.lora_B_k.59.weight', 'image_encoder.blocks.2.attn.lora_B_v.59.weight', 'image_encoder.blocks.5.attn.lora_B_v.59.weight', 'image_encoder.blocks.0.attn.lora_B_k.59.weight', 'image_encoder.blocks.11.attn.lora_B_v.59.weight', 'image_encoder.blocks.10.attn.lora_B_k.59.weight', 'classifier_pool.59.bias', 'image_encoder.blocks.4.attn.lora_B_k.59.weight', 'image_encoder.blocks.1.attn.lora_B_k.59.weight', 'image_encoder.blocks.8.attn.lora_B_v.59.weight'}
2025-12-10 15:45:01,540 [inflora.py] => Task 59, Epoch 50/50 => Loss 0.064, Train_accy 97.36
Threshold:  0.9918
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 97/768 type remove
Layer 4 : 127/768 type remove
Layer 5 : 170/768 type remove
Layer 6 : 171/768 type remove
Layer 7 : 200/768 type remove
Layer 8 : 217/768 type remove
Layer 9 : 282/768 type remove
Layer 10 : 321/768 type remove
Layer 11 : 239/768 type remove
Layer 12 : 384/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:45:08,446 [trainer.py] => Time:119.9865448474884
3677 3677
3677 3677
2025-12-10 15:45:19,265 [trainer.py] => Time:10.81889295578003
2025-12-10 15:45:19,266 [inflora.py] => Exemplar size: 0
2025-12-10 15:45:19,266 [trainer.py] => CNN: {'total': np.float64(43.21), '00-01': np.float64(73.12), '02-03': np.float64(38.71), '04-05': np.float64(43.82), '06-07': np.float64(50.0), '08-09': np.float64(38.36), '10-11': np.float64(29.27), '12-13': np.float64(25.93), '14-15': np.float64(41.07), '16-17': np.float64(11.43), '18-19': np.float64(62.5), '20-21': np.float64(54.95), '22-23': np.float64(60.94), '24-25': np.float64(24.49), '26-27': np.float64(69.57), '28-29': np.float64(53.09), '30-31': np.float64(37.27), '32-33': np.float64(22.73), '34-35': np.float64(48.39), '36-37': np.float64(43.33), '38-39': np.float64(53.66), '40-41': np.float64(33.33), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(42.55), '50-51': np.float64(16.22), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(12.73), '60-61': np.float64(41.46), '62-63': np.float64(40.57), '64-65': np.float64(13.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(34.25), '72-73': np.float64(45.0), '74-75': np.float64(58.97), '76-77': np.float64(55.42), '78-79': np.float64(31.71), '80-81': np.float64(63.89), '82-83': np.float64(35.59), '84-85': np.float64(50.0), '86-87': np.float64(41.38), '88-89': np.float64(86.67), '90-91': np.float64(52.5), '92-93': np.float64(14.29), '94-95': np.float64(38.03), '96-97': np.float64(22.22), '98-99': np.float64(36.49), '100-101': np.float64(43.33), '102-103': np.float64(72.41), '104-105': np.float64(18.09), '106-107': np.float64(43.59), '108-109': np.float64(56.72), '110-111': np.float64(50.0), '112-113': np.float64(80.77), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(34.62), 'old': np.float64(43.34), 'new': np.float64(34.62)}
2025-12-10 15:45:19,266 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21)]
2025-12-10 15:45:19,266 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65)]
2025-12-10 15:45:19,266 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345]
2025-12-10 15:45:30,587 [trainer.py] => All params: 144526051
2025-12-10 15:45:30,599 [trainer.py] => Trainable params: 185858
2025-12-10 15:45:30,599 [inflora.py] => Learning on 120-122
Parameters to be updated: {'classifier_pool.60.bias', 'image_encoder.blocks.8.attn.lora_B_k.60.weight', 'image_encoder.blocks.3.attn.lora_B_k.60.weight', 'image_encoder.blocks.6.attn.lora_B_k.60.weight', 'image_encoder.blocks.0.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_v.60.weight', 'classifier_pool.60.weight', 'image_encoder.blocks.3.attn.lora_B_v.60.weight', 'image_encoder.blocks.10.attn.lora_B_v.60.weight', 'image_encoder.blocks.9.attn.lora_B_v.60.weight', 'image_encoder.blocks.4.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_k.60.weight', 'image_encoder.blocks.4.attn.lora_B_k.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.60.weight', 'image_encoder.blocks.7.attn.lora_B_k.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.60.weight', 'image_encoder.blocks.8.attn.lora_B_v.60.weight', 'image_encoder.blocks.1.attn.lora_B_v.60.weight', 'image_encoder.blocks.7.attn.lora_B_v.60.weight', 'image_encoder.blocks.9.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.60.weight', 'image_encoder.blocks.0.attn.lora_B_v.60.weight', 'image_encoder.blocks.6.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.60.weight', 'image_encoder.blocks.2.attn.lora_B_k.60.weight', 'image_encoder.blocks.1.attn.lora_B_k.60.weight'}
2025-12-10 15:47:25,686 [inflora.py] => Task 60, Epoch 50/50 => Loss 0.016, Train_accy 99.58
Threshold:  0.992
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 98/768 type remove
Layer 4 : 129/768 type remove
Layer 5 : 172/768 type remove
Layer 6 : 175/768 type remove
Layer 7 : 205/768 type remove
Layer 8 : 224/768 type remove
Layer 9 : 291/768 type remove
Layer 10 : 331/768 type remove
Layer 11 : 245/768 type remove
Layer 12 : 377/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:47:32,417 [trainer.py] => Time:121.81799244880676
3738 3738
3738 3738
2025-12-10 15:47:43,459 [trainer.py] => Time:11.041695833206177
2025-12-10 15:47:43,460 [inflora.py] => Exemplar size: 0
2025-12-10 15:47:43,460 [trainer.py] => CNN: {'total': np.float64(43.71), '00-01': np.float64(73.12), '02-03': np.float64(43.55), '04-05': np.float64(44.94), '06-07': np.float64(51.79), '08-09': np.float64(38.36), '10-11': np.float64(26.83), '12-13': np.float64(27.78), '14-15': np.float64(37.5), '16-17': np.float64(14.29), '18-19': np.float64(64.29), '20-21': np.float64(54.95), '22-23': np.float64(60.94), '24-25': np.float64(28.57), '26-27': np.float64(69.57), '28-29': np.float64(54.32), '30-31': np.float64(39.09), '32-33': np.float64(18.18), '34-35': np.float64(45.16), '36-37': np.float64(46.67), '38-39': np.float64(51.22), '40-41': np.float64(37.04), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(44.68), '50-51': np.float64(24.32), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(14.55), '60-61': np.float64(43.9), '62-63': np.float64(42.45), '64-65': np.float64(13.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(28.77), '72-73': np.float64(47.5), '74-75': np.float64(60.26), '76-77': np.float64(54.22), '78-79': np.float64(31.71), '80-81': np.float64(62.5), '82-83': np.float64(28.81), '84-85': np.float64(47.83), '86-87': np.float64(43.68), '88-89': np.float64(83.33), '90-91': np.float64(52.5), '92-93': np.float64(14.29), '94-95': np.float64(32.39), '96-97': np.float64(20.2), '98-99': np.float64(35.14), '100-101': np.float64(48.33), '102-103': np.float64(68.97), '104-105': np.float64(20.21), '106-107': np.float64(41.03), '108-109': np.float64(55.22), '110-111': np.float64(48.33), '112-113': np.float64(78.85), '114-115': np.float64(47.5), '116-117': np.float64(77.78), '118-119': np.float64(36.54), '120-121': np.float64(63.93), 'old': np.float64(43.38), 'new': np.float64(63.93)}
2025-12-10 15:47:43,460 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71)]
2025-12-10 15:47:43,460 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72)]
2025-12-10 15:47:43,460 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455]
2025-12-10 15:47:50,336 [trainer.py] => All params: 144526051
2025-12-10 15:47:50,348 [trainer.py] => Trainable params: 185858
2025-12-10 15:47:50,348 [inflora.py] => Learning on 122-124
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.61.weight', 'image_encoder.blocks.7.attn.lora_B_v.61.weight', 'image_encoder.blocks.1.attn.lora_B_v.61.weight', 'image_encoder.blocks.9.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_v.61.weight', 'image_encoder.blocks.1.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.61.weight', 'classifier_pool.61.bias', 'image_encoder.blocks.5.attn.lora_B_k.61.weight', 'image_encoder.blocks.2.attn.lora_B_v.61.weight', 'image_encoder.blocks.2.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.61.weight', 'classifier_pool.61.weight', 'image_encoder.blocks.6.attn.lora_B_k.61.weight', 'image_encoder.blocks.0.attn.lora_B_k.61.weight', 'image_encoder.blocks.10.attn.lora_B_v.61.weight', 'image_encoder.blocks.6.attn.lora_B_v.61.weight', 'image_encoder.blocks.4.attn.lora_B_k.61.weight', 'image_encoder.blocks.8.attn.lora_B_k.61.weight', 'image_encoder.blocks.10.attn.lora_B_k.61.weight', 'image_encoder.blocks.8.attn.lora_B_v.61.weight', 'image_encoder.blocks.7.attn.lora_B_k.61.weight', 'image_encoder.blocks.11.attn.lora_B_k.61.weight', 'image_encoder.blocks.9.attn.lora_B_k.61.weight', 'image_encoder.blocks.11.attn.lora_B_v.61.weight', 'image_encoder.blocks.4.attn.lora_B_v.61.weight'}
2025-12-10 15:49:47,584 [inflora.py] => Task 61, Epoch 50/50 => Loss 0.067, Train_accy 97.52
Threshold:  0.9922
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 100/768 type remove
Layer 4 : 131/768 type remove
Layer 5 : 175/768 type remove
Layer 6 : 179/768 type remove
Layer 7 : 209/768 type remove
Layer 8 : 230/768 type remove
Layer 9 : 298/768 type remove
Layer 10 : 341/768 type remove
Layer 11 : 253/768 type remove
Layer 12 : 356/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:49:55,326 [trainer.py] => Time:124.97812271118164
3794 3794
3794 3794
2025-12-10 15:50:06,533 [trainer.py] => Time:11.207353115081787
2025-12-10 15:50:06,534 [inflora.py] => Exemplar size: 0
2025-12-10 15:50:06,534 [trainer.py] => CNN: {'total': np.float64(43.33), '00-01': np.float64(77.42), '02-03': np.float64(40.32), '04-05': np.float64(46.07), '06-07': np.float64(44.64), '08-09': np.float64(38.36), '10-11': np.float64(26.83), '12-13': np.float64(29.63), '14-15': np.float64(39.29), '16-17': np.float64(12.86), '18-19': np.float64(62.5), '20-21': np.float64(56.04), '22-23': np.float64(59.38), '24-25': np.float64(28.57), '26-27': np.float64(73.04), '28-29': np.float64(55.56), '30-31': np.float64(37.27), '32-33': np.float64(20.45), '34-35': np.float64(41.94), '36-37': np.float64(46.67), '38-39': np.float64(46.34), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(51.06), '50-51': np.float64(16.22), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(18.18), '60-61': np.float64(43.9), '62-63': np.float64(38.68), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(34.25), '72-73': np.float64(45.0), '74-75': np.float64(58.97), '76-77': np.float64(56.63), '78-79': np.float64(30.49), '80-81': np.float64(59.72), '82-83': np.float64(30.51), '84-85': np.float64(45.65), '86-87': np.float64(40.23), '88-89': np.float64(85.0), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(30.99), '96-97': np.float64(20.2), '98-99': np.float64(35.14), '100-101': np.float64(43.33), '102-103': np.float64(68.97), '104-105': np.float64(19.15), '106-107': np.float64(41.03), '108-109': np.float64(58.21), '110-111': np.float64(46.67), '112-113': np.float64(82.69), '114-115': np.float64(42.5), '116-117': np.float64(80.56), '118-119': np.float64(38.46), '120-121': np.float64(65.57), '122-123': np.float64(37.5), 'old': np.float64(43.42), 'new': np.float64(37.5)}
2025-12-10 15:50:06,534 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33)]
2025-12-10 15:50:06,534 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68)]
2025-12-10 15:50:06,534 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767]
2025-12-10 15:50:11,814 [trainer.py] => All params: 144526051
2025-12-10 15:50:11,826 [trainer.py] => Trainable params: 185858
2025-12-10 15:50:11,826 [inflora.py] => Learning on 124-126
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.62.weight', 'image_encoder.blocks.0.attn.lora_B_v.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.62.weight', 'classifier_pool.62.weight', 'image_encoder.blocks.7.attn.lora_B_k.62.weight', 'image_encoder.blocks.9.attn.lora_B_v.62.weight', 'image_encoder.blocks.4.attn.lora_B_v.62.weight', 'image_encoder.blocks.10.attn.lora_B_v.62.weight', 'image_encoder.blocks.10.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_k.62.weight', 'image_encoder.blocks.2.attn.lora_B_v.62.weight', 'image_encoder.blocks.8.attn.lora_B_v.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.62.weight', 'classifier_pool.62.bias', 'image_encoder.blocks.0.attn.lora_B_k.62.weight', 'image_encoder.blocks.3.attn.lora_B_v.62.weight', 'image_encoder.blocks.11.attn.lora_B_k.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.62.weight', 'image_encoder.blocks.2.attn.lora_B_k.62.weight', 'image_encoder.blocks.3.attn.lora_B_k.62.weight', 'image_encoder.blocks.6.attn.lora_B_v.62.weight', 'image_encoder.blocks.6.attn.lora_B_k.62.weight', 'image_encoder.blocks.9.attn.lora_B_k.62.weight', 'image_encoder.blocks.5.attn.lora_B_k.62.weight', 'image_encoder.blocks.5.attn.lora_B_v.62.weight', 'image_encoder.blocks.7.attn.lora_B_v.62.weight'}
2025-12-10 15:52:18,208 [inflora.py] => Task 62, Epoch 50/50 => Loss 0.072, Train_accy 96.62
Threshold:  0.9924
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 101/768 type remove
Layer 4 : 132/768 type remove
Layer 5 : 176/768 type remove
Layer 6 : 180/768 type remove
Layer 7 : 211/768 type remove
Layer 8 : 231/768 type remove
Layer 9 : 299/768 type remove
Layer 10 : 342/768 type remove
Layer 11 : 254/768 type remove
Layer 12 : 343/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:52:25,814 [trainer.py] => Time:133.98797583580017
3843 3843
3843 3843
2025-12-10 15:52:37,341 [trainer.py] => Time:11.526796340942383
2025-12-10 15:52:37,341 [inflora.py] => Exemplar size: 0
2025-12-10 15:52:37,342 [trainer.py] => CNN: {'total': np.float64(42.26), '00-01': np.float64(76.34), '02-03': np.float64(41.94), '04-05': np.float64(48.31), '06-07': np.float64(51.79), '08-09': np.float64(42.47), '10-11': np.float64(24.39), '12-13': np.float64(33.33), '14-15': np.float64(39.29), '16-17': np.float64(17.14), '18-19': np.float64(57.14), '20-21': np.float64(53.85), '22-23': np.float64(59.38), '24-25': np.float64(30.61), '26-27': np.float64(71.3), '28-29': np.float64(60.49), '30-31': np.float64(32.73), '32-33': np.float64(18.18), '34-35': np.float64(38.71), '36-37': np.float64(40.0), '38-39': np.float64(39.02), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(48.94), '50-51': np.float64(16.22), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(10.91), '60-61': np.float64(46.34), '62-63': np.float64(29.25), '64-65': np.float64(10.0), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(34.25), '72-73': np.float64(47.5), '74-75': np.float64(51.28), '76-77': np.float64(55.42), '78-79': np.float64(28.05), '80-81': np.float64(58.33), '82-83': np.float64(28.81), '84-85': np.float64(41.3), '86-87': np.float64(36.78), '88-89': np.float64(81.67), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(29.58), '96-97': np.float64(20.2), '98-99': np.float64(32.43), '100-101': np.float64(40.0), '102-103': np.float64(67.24), '104-105': np.float64(17.02), '106-107': np.float64(38.46), '108-109': np.float64(62.69), '110-111': np.float64(48.33), '112-113': np.float64(84.62), '114-115': np.float64(50.0), '116-117': np.float64(72.22), '118-119': np.float64(42.31), '120-121': np.float64(68.85), '122-123': np.float64(32.14), '124-125': np.float64(40.82), 'old': np.float64(42.28), 'new': np.float64(40.82)}
2025-12-10 15:52:37,342 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26)]
2025-12-10 15:52:37,342 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47)]
2025-12-10 15:52:37,342 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166]
2025-12-10 15:52:44,566 [trainer.py] => All params: 144526051
2025-12-10 15:52:44,578 [trainer.py] => Trainable params: 185858
2025-12-10 15:52:44,578 [inflora.py] => Learning on 126-128
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.63.weight', 'image_encoder.blocks.0.attn.lora_B_k.63.weight', 'classifier_pool.63.weight', 'image_encoder.blocks.5.attn.lora_B_k.63.weight', 'image_encoder.blocks.3.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.63.weight', 'image_encoder.blocks.0.attn.lora_B_v.63.weight', 'image_encoder.blocks.10.attn.lora_B_k.63.weight', 'image_encoder.blocks.3.attn.lora_B_v.63.weight', 'image_encoder.blocks.11.attn.lora_B_k.63.weight', 'image_encoder.blocks.9.attn.lora_B_k.63.weight', 'image_encoder.blocks.10.attn.lora_B_v.63.weight', 'image_encoder.blocks.6.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.63.weight', 'image_encoder.blocks.1.attn.lora_B_v.63.weight', 'image_encoder.blocks.5.attn.lora_B_v.63.weight', 'image_encoder.blocks.6.attn.lora_B_v.63.weight', 'classifier_pool.63.bias', 'image_encoder.blocks.8.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_v.63.weight', 'image_encoder.blocks.11.attn.lora_B_v.63.weight', 'image_encoder.blocks.8.attn.lora_B_k.63.weight', 'image_encoder.blocks.9.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_v.63.weight', 'image_encoder.blocks.7.attn.lora_B_k.63.weight'}
2025-12-10 15:54:36,130 [inflora.py] => Task 63, Epoch 50/50 => Loss 0.104, Train_accy 96.30
Threshold:  0.9926
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 102/768 type remove
Layer 4 : 133/768 type remove
Layer 5 : 177/768 type remove
Layer 6 : 182/768 type remove
Layer 7 : 216/768 type remove
Layer 8 : 234/768 type remove
Layer 9 : 302/768 type remove
Layer 10 : 346/768 type remove
Layer 11 : 259/768 type remove
Layer 12 : 329/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:54:42,847 [trainer.py] => Time:118.26851415634155
3896 3896
3896 3896
2025-12-10 15:54:54,313 [trainer.py] => Time:11.466313123703003
2025-12-10 15:54:54,314 [inflora.py] => Exemplar size: 0
2025-12-10 15:54:54,314 [trainer.py] => CNN: {'total': np.float64(41.89), '00-01': np.float64(75.27), '02-03': np.float64(40.32), '04-05': np.float64(50.56), '06-07': np.float64(44.64), '08-09': np.float64(41.1), '10-11': np.float64(31.71), '12-13': np.float64(35.19), '14-15': np.float64(41.07), '16-17': np.float64(21.43), '18-19': np.float64(60.71), '20-21': np.float64(57.14), '22-23': np.float64(60.16), '24-25': np.float64(28.57), '26-27': np.float64(69.57), '28-29': np.float64(59.26), '30-31': np.float64(35.45), '32-33': np.float64(18.18), '34-35': np.float64(35.48), '36-37': np.float64(36.67), '38-39': np.float64(39.02), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(44.68), '50-51': np.float64(16.22), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(14.55), '60-61': np.float64(46.34), '62-63': np.float64(28.3), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(35.62), '72-73': np.float64(50.0), '74-75': np.float64(55.13), '76-77': np.float64(55.42), '78-79': np.float64(30.49), '80-81': np.float64(56.94), '82-83': np.float64(26.27), '84-85': np.float64(43.48), '86-87': np.float64(40.23), '88-89': np.float64(81.67), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(35.21), '96-97': np.float64(17.17), '98-99': np.float64(29.73), '100-101': np.float64(38.33), '102-103': np.float64(65.52), '104-105': np.float64(18.09), '106-107': np.float64(41.03), '108-109': np.float64(61.19), '110-111': np.float64(50.0), '112-113': np.float64(86.54), '114-115': np.float64(50.0), '116-117': np.float64(75.0), '118-119': np.float64(38.46), '120-121': np.float64(63.93), '122-123': np.float64(33.93), '124-125': np.float64(38.78), '126-127': np.float64(13.21), 'old': np.float64(42.28), 'new': np.float64(13.21)}
2025-12-10 15:54:54,314 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89)]
2025-12-10 15:54:54,314 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59)]
2025-12-10 15:54:54,314 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875]
2025-12-10 15:54:59,009 [trainer.py] => All params: 144526051
2025-12-10 15:54:59,021 [trainer.py] => Trainable params: 185858
2025-12-10 15:54:59,021 [inflora.py] => Learning on 128-130
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.64.weight', 'image_encoder.blocks.7.attn.lora_B_k.64.weight', 'classifier_pool.64.bias', 'image_encoder.blocks.3.attn.lora_B_k.64.weight', 'image_encoder.blocks.8.attn.lora_B_v.64.weight', 'image_encoder.blocks.6.attn.lora_B_v.64.weight', 'classifier_pool.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.64.weight', 'image_encoder.blocks.9.attn.lora_B_v.64.weight', 'image_encoder.blocks.1.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_v.64.weight', 'image_encoder.blocks.10.attn.lora_B_k.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.64.weight', 'image_encoder.blocks.5.attn.lora_B_k.64.weight', 'image_encoder.blocks.11.attn.lora_B_k.64.weight', 'image_encoder.blocks.4.attn.lora_B_k.64.weight', 'image_encoder.blocks.6.attn.lora_B_k.64.weight', 'image_encoder.blocks.0.attn.lora_B_v.64.weight', 'image_encoder.blocks.9.attn.lora_B_k.64.weight', 'image_encoder.blocks.10.attn.lora_B_v.64.weight', 'image_encoder.blocks.4.attn.lora_B_v.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.64.weight', 'image_encoder.blocks.1.attn.lora_B_k.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_k.64.weight'}
2025-12-10 15:57:03,163 [inflora.py] => Task 64, Epoch 50/50 => Loss 0.061, Train_accy 96.93
Threshold:  0.9928
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 47/768 type remove
Layer 3 : 103/768 type remove
Layer 4 : 135/768 type remove
Layer 5 : 179/768 type remove
Layer 6 : 185/768 type remove
Layer 7 : 219/768 type remove
Layer 8 : 236/768 type remove
Layer 9 : 303/768 type remove
Layer 10 : 347/768 type remove
Layer 11 : 260/768 type remove
Layer 12 : 326/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:57:11,018 [trainer.py] => Time:131.99690794944763
3975 3975
3975 3975
2025-12-10 15:57:23,106 [trainer.py] => Time:12.088016271591187
2025-12-10 15:57:23,106 [inflora.py] => Exemplar size: 0
2025-12-10 15:57:23,107 [trainer.py] => CNN: {'total': np.float64(41.71), '00-01': np.float64(81.72), '02-03': np.float64(37.1), '04-05': np.float64(51.69), '06-07': np.float64(55.36), '08-09': np.float64(42.47), '10-11': np.float64(26.83), '12-13': np.float64(31.48), '14-15': np.float64(39.29), '16-17': np.float64(20.0), '18-19': np.float64(60.71), '20-21': np.float64(53.85), '22-23': np.float64(60.16), '24-25': np.float64(26.53), '26-27': np.float64(69.57), '28-29': np.float64(55.56), '30-31': np.float64(37.27), '32-33': np.float64(13.64), '34-35': np.float64(35.48), '36-37': np.float64(38.33), '38-39': np.float64(39.02), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(47.87), '50-51': np.float64(27.03), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(9.09), '60-61': np.float64(43.9), '62-63': np.float64(32.08), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(36.99), '72-73': np.float64(47.5), '74-75': np.float64(53.85), '76-77': np.float64(56.63), '78-79': np.float64(29.27), '80-81': np.float64(62.5), '82-83': np.float64(32.2), '84-85': np.float64(41.3), '86-87': np.float64(33.33), '88-89': np.float64(75.0), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(30.99), '96-97': np.float64(15.15), '98-99': np.float64(27.03), '100-101': np.float64(43.33), '102-103': np.float64(63.79), '104-105': np.float64(18.09), '106-107': np.float64(43.59), '108-109': np.float64(62.69), '110-111': np.float64(48.33), '112-113': np.float64(88.46), '114-115': np.float64(52.5), '116-117': np.float64(77.78), '118-119': np.float64(44.23), '120-121': np.float64(68.85), '122-123': np.float64(30.36), '124-125': np.float64(42.86), '126-127': np.float64(11.32), '128-129': np.float64(24.05), 'old': np.float64(42.07), 'new': np.float64(24.05)}
2025-12-10 15:57:23,107 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71)]
2025-12-10 15:57:23,107 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57)]
2025-12-10 15:57:23,107 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576]
2025-12-10 15:57:30,362 [trainer.py] => All params: 144526051
2025-12-10 15:57:30,374 [trainer.py] => Trainable params: 185858
2025-12-10 15:57:30,374 [inflora.py] => Learning on 130-132
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.65.weight', 'image_encoder.blocks.9.attn.lora_B_v.65.weight', 'image_encoder.blocks.4.attn.lora_B_v.65.weight', 'image_encoder.blocks.7.attn.lora_B_v.65.weight', 'image_encoder.blocks.11.attn.lora_B_k.65.weight', 'image_encoder.blocks.6.attn.lora_B_v.65.weight', 'image_encoder.blocks.2.attn.lora_B_v.65.weight', 'image_encoder.blocks.3.attn.lora_B_k.65.weight', 'image_encoder.blocks.11.attn.lora_B_v.65.weight', 'image_encoder.blocks.3.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_v.65.weight', 'image_encoder.blocks.10.attn.lora_B_k.65.weight', 'image_encoder.blocks.10.attn.lora_B_v.65.weight', 'image_encoder.blocks.2.attn.lora_B_k.65.weight', 'image_encoder.blocks.6.attn.lora_B_k.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.65.weight', 'image_encoder.blocks.4.attn.lora_B_k.65.weight', 'image_encoder.blocks.8.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_k.65.weight', 'image_encoder.blocks.8.attn.lora_B_k.65.weight', 'image_encoder.blocks.5.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.65.weight', 'classifier_pool.65.bias', 'classifier_pool.65.weight', 'image_encoder.blocks.7.attn.lora_B_k.65.weight'}
2025-12-10 15:59:13,247 [inflora.py] => Task 65, Epoch 50/50 => Loss 0.023, Train_accy 99.34
Threshold:  0.993
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 48/768 type remove
Layer 3 : 104/768 type remove
Layer 4 : 139/768 type remove
Layer 5 : 183/768 type remove
Layer 6 : 191/768 type remove
Layer 7 : 225/768 type remove
Layer 8 : 241/768 type remove
Layer 9 : 309/768 type remove
Layer 10 : 356/768 type remove
Layer 11 : 267/768 type remove
Layer 12 : 321/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:59:20,089 [trainer.py] => Time:109.71487259864807
4020 4020
4020 4020
2025-12-10 15:59:31,955 [trainer.py] => Time:11.866478204727173
2025-12-10 15:59:31,956 [inflora.py] => Exemplar size: 0
2025-12-10 15:59:31,956 [trainer.py] => CNN: {'total': np.float64(41.42), '00-01': np.float64(77.42), '02-03': np.float64(32.26), '04-05': np.float64(50.56), '06-07': np.float64(53.57), '08-09': np.float64(42.47), '10-11': np.float64(24.39), '12-13': np.float64(35.19), '14-15': np.float64(39.29), '16-17': np.float64(25.71), '18-19': np.float64(58.93), '20-21': np.float64(54.95), '22-23': np.float64(61.72), '24-25': np.float64(26.53), '26-27': np.float64(70.43), '28-29': np.float64(58.02), '30-31': np.float64(39.09), '32-33': np.float64(13.64), '34-35': np.float64(38.71), '36-37': np.float64(36.67), '38-39': np.float64(41.46), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(45.74), '50-51': np.float64(21.62), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(9.09), '60-61': np.float64(41.46), '62-63': np.float64(32.08), '64-65': np.float64(13.33), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(34.25), '72-73': np.float64(52.5), '74-75': np.float64(50.0), '76-77': np.float64(57.83), '78-79': np.float64(25.61), '80-81': np.float64(56.94), '82-83': np.float64(29.66), '84-85': np.float64(45.65), '86-87': np.float64(33.33), '88-89': np.float64(76.67), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(18.18), '98-99': np.float64(25.68), '100-101': np.float64(41.67), '102-103': np.float64(63.79), '104-105': np.float64(15.96), '106-107': np.float64(41.03), '108-109': np.float64(61.19), '110-111': np.float64(48.33), '112-113': np.float64(88.46), '114-115': np.float64(55.0), '116-117': np.float64(69.44), '118-119': np.float64(44.23), '120-121': np.float64(70.49), '122-123': np.float64(32.14), '124-125': np.float64(40.82), '126-127': np.float64(15.09), '128-129': np.float64(25.32), '130-131': np.float64(35.56), 'old': np.float64(41.48), 'new': np.float64(35.56)}
2025-12-10 15:59:31,956 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42)]
2025-12-10 15:59:31,956 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7)]
2025-12-10 15:59:31,956 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837]
2025-12-10 15:59:37,383 [trainer.py] => All params: 144526051
2025-12-10 15:59:37,395 [trainer.py] => Trainable params: 185858
2025-12-10 15:59:37,395 [inflora.py] => Learning on 132-134
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.66.weight', 'image_encoder.blocks.8.attn.lora_B_k.66.weight', 'image_encoder.blocks.4.attn.lora_B_v.66.weight', 'image_encoder.blocks.3.attn.lora_B_k.66.weight', 'image_encoder.blocks.9.attn.lora_B_k.66.weight', 'image_encoder.blocks.8.attn.lora_B_v.66.weight', 'image_encoder.blocks.1.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_v.66.weight', 'image_encoder.blocks.2.attn.lora_B_v.66.weight', 'image_encoder.blocks.0.attn.lora_B_k.66.weight', 'image_encoder.blocks.2.attn.lora_B_k.66.weight', 'image_encoder.blocks.7.attn.lora_B_k.66.weight', 'image_encoder.blocks.11.attn.lora_B_v.66.weight', 'image_encoder.blocks.1.attn.lora_B_k.66.weight', 'classifier_pool.66.weight', 'image_encoder.blocks.5.attn.lora_B_v.66.weight', 'image_encoder.blocks.5.attn.lora_B_k.66.weight', 'image_encoder.blocks.7.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.66.weight', 'image_encoder.blocks.10.attn.lora_B_v.66.weight', 'classifier_pool.66.bias', 'image_encoder.blocks.11.attn.lora_B_k.66.weight', 'image_encoder.blocks.4.attn.lora_B_k.66.weight', 'image_encoder.blocks.6.attn.lora_B_v.66.weight', 'image_encoder.blocks.9.attn.lora_B_v.66.weight'}
2025-12-10 16:01:19,770 [inflora.py] => Task 66, Epoch 50/50 => Loss 0.014, Train_accy 98.83
Threshold:  0.9932
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 48/768 type remove
Layer 3 : 106/768 type remove
Layer 4 : 143/768 type remove
Layer 5 : 188/768 type remove
Layer 6 : 198/768 type remove
Layer 7 : 233/768 type remove
Layer 8 : 251/768 type remove
Layer 9 : 326/768 type remove
Layer 10 : 373/768 type remove
Layer 11 : 279/768 type remove
Layer 12 : 309/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:01:27,625 [trainer.py] => Time:110.22990131378174
4054 4054
4054 4054
2025-12-10 16:01:39,492 [trainer.py] => Time:11.867660999298096
2025-12-10 16:01:39,493 [inflora.py] => Exemplar size: 0
2025-12-10 16:01:39,493 [trainer.py] => CNN: {'total': np.float64(40.21), '00-01': np.float64(72.04), '02-03': np.float64(33.87), '04-05': np.float64(48.31), '06-07': np.float64(51.79), '08-09': np.float64(38.36), '10-11': np.float64(24.39), '12-13': np.float64(33.33), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(57.14), '20-21': np.float64(53.85), '22-23': np.float64(61.72), '24-25': np.float64(26.53), '26-27': np.float64(71.3), '28-29': np.float64(51.85), '30-31': np.float64(33.64), '32-33': np.float64(13.64), '34-35': np.float64(35.48), '36-37': np.float64(43.33), '38-39': np.float64(43.9), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(46.81), '50-51': np.float64(18.92), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(36.79), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(31.51), '72-73': np.float64(45.0), '74-75': np.float64(55.13), '76-77': np.float64(59.04), '78-79': np.float64(20.73), '80-81': np.float64(52.78), '82-83': np.float64(31.36), '84-85': np.float64(47.83), '86-87': np.float64(29.89), '88-89': np.float64(76.67), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(26.76), '96-97': np.float64(17.17), '98-99': np.float64(22.97), '100-101': np.float64(41.67), '102-103': np.float64(56.9), '104-105': np.float64(17.02), '106-107': np.float64(38.46), '108-109': np.float64(56.72), '110-111': np.float64(48.33), '112-113': np.float64(82.69), '114-115': np.float64(55.0), '116-117': np.float64(72.22), '118-119': np.float64(30.77), '120-121': np.float64(68.85), '122-123': np.float64(32.14), '124-125': np.float64(42.86), '126-127': np.float64(15.09), '128-129': np.float64(25.32), '130-131': np.float64(35.56), '132-133': np.float64(38.24), 'old': np.float64(40.22), 'new': np.float64(38.24)}
2025-12-10 16:01:39,493 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21)]
2025-12-10 16:01:39,493 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66)]
2025-12-10 16:01:39,494 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327]
2025-12-10 16:01:44,306 [trainer.py] => All params: 144526051
2025-12-10 16:01:44,318 [trainer.py] => Trainable params: 185858
2025-12-10 16:01:44,318 [inflora.py] => Learning on 134-136
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.67.weight', 'image_encoder.blocks.9.attn.lora_B_v.67.weight', 'image_encoder.blocks.3.attn.lora_B_k.67.weight', 'image_encoder.blocks.0.attn.lora_B_k.67.weight', 'image_encoder.blocks.7.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.67.weight', 'classifier_pool.67.weight', 'image_encoder.blocks.8.attn.lora_B_k.67.weight', 'image_encoder.blocks.3.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_k.67.weight', 'image_encoder.blocks.6.attn.lora_B_k.67.weight', 'image_encoder.blocks.9.attn.lora_B_k.67.weight', 'image_encoder.blocks.5.attn.lora_B_k.67.weight', 'image_encoder.blocks.10.attn.lora_B_k.67.weight', 'image_encoder.blocks.5.attn.lora_B_v.67.weight', 'image_encoder.blocks.10.attn.lora_B_v.67.weight', 'image_encoder.blocks.7.attn.lora_B_v.67.weight', 'classifier_pool.67.bias', 'image_encoder.blocks.2.attn.lora_B_v.67.weight', 'image_encoder.blocks.4.attn.lora_B_v.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.67.weight', 'image_encoder.blocks.6.attn.lora_B_v.67.weight', 'image_encoder.blocks.1.attn.lora_B_k.67.weight', 'image_encoder.blocks.0.attn.lora_B_v.67.weight'}
2025-12-10 16:04:15,565 [inflora.py] => Task 67, Epoch 50/50 => Loss 0.071, Train_accy 96.42
Threshold:  0.9934
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 50/768 type remove
Layer 3 : 109/768 type remove
Layer 4 : 147/768 type remove
Layer 5 : 193/768 type remove
Layer 6 : 202/768 type remove
Layer 7 : 238/768 type remove
Layer 8 : 256/768 type remove
Layer 9 : 334/768 type remove
Layer 10 : 381/768 type remove
Layer 11 : 287/768 type remove
Layer 12 : 306/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:04:23,619 [trainer.py] => Time:159.3011119365692
4132 4132
4132 4132
2025-12-10 16:04:35,819 [trainer.py] => Time:12.199458360671997
2025-12-10 16:04:35,819 [inflora.py] => Exemplar size: 0
2025-12-10 16:04:35,819 [trainer.py] => CNN: {'total': np.float64(39.84), '00-01': np.float64(72.04), '02-03': np.float64(35.48), '04-05': np.float64(46.07), '06-07': np.float64(51.79), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(31.48), '14-15': np.float64(35.71), '16-17': np.float64(17.14), '18-19': np.float64(60.71), '20-21': np.float64(56.04), '22-23': np.float64(64.84), '24-25': np.float64(20.41), '26-27': np.float64(66.96), '28-29': np.float64(51.85), '30-31': np.float64(34.55), '32-33': np.float64(11.36), '34-35': np.float64(29.03), '36-37': np.float64(46.67), '38-39': np.float64(46.34), '40-41': np.float64(27.78), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(47.87), '50-51': np.float64(18.92), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(10.91), '60-61': np.float64(41.46), '62-63': np.float64(33.96), '64-65': np.float64(10.0), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(28.77), '72-73': np.float64(42.5), '74-75': np.float64(53.85), '76-77': np.float64(51.81), '78-79': np.float64(21.95), '80-81': np.float64(54.17), '82-83': np.float64(29.66), '84-85': np.float64(47.83), '86-87': np.float64(28.74), '88-89': np.float64(78.33), '90-91': np.float64(50.0), '92-93': np.float64(14.29), '94-95': np.float64(26.76), '96-97': np.float64(20.2), '98-99': np.float64(27.03), '100-101': np.float64(41.67), '102-103': np.float64(56.9), '104-105': np.float64(14.89), '106-107': np.float64(35.9), '108-109': np.float64(49.25), '110-111': np.float64(51.67), '112-113': np.float64(82.69), '114-115': np.float64(50.0), '116-117': np.float64(72.22), '118-119': np.float64(32.69), '120-121': np.float64(67.21), '122-123': np.float64(32.14), '124-125': np.float64(46.94), '126-127': np.float64(11.32), '128-129': np.float64(25.32), '130-131': np.float64(37.78), '132-133': np.float64(29.41), '134-135': np.float64(47.44), 'old': np.float64(39.69), 'new': np.float64(47.44)}
2025-12-10 16:04:35,819 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84)]
2025-12-10 16:04:35,819 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47)]
2025-12-10 16:04:35,820 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757]
2025-12-10 16:04:43,438 [trainer.py] => All params: 144526051
2025-12-10 16:04:43,450 [trainer.py] => Trainable params: 185858
2025-12-10 16:04:43,451 [inflora.py] => Learning on 136-138
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.68.weight', 'image_encoder.blocks.3.attn.lora_B_v.68.weight', 'image_encoder.blocks.10.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_k.68.weight', 'image_encoder.blocks.5.attn.lora_B_k.68.weight', 'image_encoder.blocks.5.attn.lora_B_v.68.weight', 'image_encoder.blocks.2.attn.lora_B_k.68.weight', 'image_encoder.blocks.2.attn.lora_B_v.68.weight', 'image_encoder.blocks.0.attn.lora_B_k.68.weight', 'image_encoder.blocks.1.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.68.weight', 'image_encoder.blocks.11.attn.lora_B_v.68.weight', 'classifier_pool.68.bias', 'image_encoder.blocks.9.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_k.68.weight', 'image_encoder.blocks.8.attn.lora_B_v.68.weight', 'image_encoder.blocks.9.attn.lora_B_v.68.weight', 'image_encoder.blocks.1.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_v.68.weight', 'classifier_pool.68.weight', 'image_encoder.blocks.8.attn.lora_B_k.68.weight', 'image_encoder.blocks.3.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.68.weight', 'image_encoder.blocks.10.attn.lora_B_k.68.weight'}
2025-12-10 16:06:31,232 [inflora.py] => Task 68, Epoch 50/50 => Loss 0.028, Train_accy 99.52
Threshold:  0.9936
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 113/768 type remove
Layer 4 : 151/768 type remove
Layer 5 : 198/768 type remove
Layer 6 : 208/768 type remove
Layer 7 : 245/768 type remove
Layer 8 : 265/768 type remove
Layer 9 : 348/768 type remove
Layer 10 : 368/768 type retain
Layer 11 : 306/768 type remove
Layer 12 : 300/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:06:38,424 [trainer.py] => Time:114.97341823577881
4181 4181
4181 4181
2025-12-10 16:06:50,762 [trainer.py] => Time:12.337349891662598
2025-12-10 16:06:50,762 [inflora.py] => Exemplar size: 0
2025-12-10 16:06:50,762 [trainer.py] => CNN: {'total': np.float64(39.8), '00-01': np.float64(70.97), '02-03': np.float64(35.48), '04-05': np.float64(48.31), '06-07': np.float64(44.64), '08-09': np.float64(36.99), '10-11': np.float64(21.95), '12-13': np.float64(31.48), '14-15': np.float64(33.93), '16-17': np.float64(20.0), '18-19': np.float64(60.71), '20-21': np.float64(56.04), '22-23': np.float64(65.62), '24-25': np.float64(22.45), '26-27': np.float64(67.83), '28-29': np.float64(56.79), '30-31': np.float64(34.55), '32-33': np.float64(13.64), '34-35': np.float64(38.71), '36-37': np.float64(45.0), '38-39': np.float64(46.34), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(48.94), '50-51': np.float64(18.92), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(7.27), '60-61': np.float64(46.34), '62-63': np.float64(33.96), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(35.62), '72-73': np.float64(45.0), '74-75': np.float64(53.85), '76-77': np.float64(46.99), '78-79': np.float64(23.17), '80-81': np.float64(55.56), '82-83': np.float64(24.58), '84-85': np.float64(52.17), '86-87': np.float64(28.74), '88-89': np.float64(75.0), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(16.16), '98-99': np.float64(24.32), '100-101': np.float64(45.0), '102-103': np.float64(58.62), '104-105': np.float64(17.02), '106-107': np.float64(33.33), '108-109': np.float64(52.24), '110-111': np.float64(48.33), '112-113': np.float64(80.77), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(34.62), '120-121': np.float64(62.3), '122-123': np.float64(23.21), '124-125': np.float64(40.82), '126-127': np.float64(11.32), '128-129': np.float64(20.25), '130-131': np.float64(37.78), '132-133': np.float64(32.35), '134-135': np.float64(48.72), '136-137': np.float64(61.22), 'old': np.float64(39.55), 'new': np.float64(61.22)}
2025-12-10 16:06:50,763 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8)]
2025-12-10 16:06:50,763 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65)]
2025-12-10 16:06:50,763 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523]
2025-12-10 16:06:58,330 [trainer.py] => All params: 144526051
2025-12-10 16:06:58,342 [trainer.py] => Trainable params: 185858
2025-12-10 16:06:58,342 [inflora.py] => Learning on 138-140
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.69.weight', 'image_encoder.blocks.8.attn.lora_B_k.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.69.weight', 'image_encoder.blocks.11.attn.lora_B_k.69.weight', 'image_encoder.blocks.2.attn.lora_B_k.69.weight', 'image_encoder.blocks.2.attn.lora_B_v.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.69.weight', 'image_encoder.blocks.11.attn.lora_B_v.69.weight', 'image_encoder.blocks.1.attn.lora_B_v.69.weight', 'image_encoder.blocks.0.attn.lora_B_v.69.weight', 'image_encoder.blocks.5.attn.lora_B_k.69.weight', 'image_encoder.blocks.10.attn.lora_B_v.69.weight', 'image_encoder.blocks.3.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_k.69.weight', 'image_encoder.blocks.4.attn.lora_B_k.69.weight', 'image_encoder.blocks.3.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_k.69.weight', 'image_encoder.blocks.7.attn.lora_B_k.69.weight', 'image_encoder.blocks.5.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.69.weight', 'image_encoder.blocks.1.attn.lora_B_k.69.weight', 'classifier_pool.69.bias', 'image_encoder.blocks.7.attn.lora_B_v.69.weight', 'classifier_pool.69.weight', 'image_encoder.blocks.4.attn.lora_B_v.69.weight'}
2025-12-10 16:08:52,647 [inflora.py] => Task 69, Epoch 50/50 => Loss 0.057, Train_accy 97.62
Threshold:  0.9938
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 55/768 type remove
Layer 3 : 118/768 type remove
Layer 4 : 157/768 type remove
Layer 5 : 206/768 type remove
Layer 6 : 218/768 type remove
Layer 7 : 254/768 type remove
Layer 8 : 277/768 type remove
Layer 9 : 369/768 type remove
Layer 10 : 339/768 type retain
Layer 11 : 336/768 type remove
Layer 12 : 295/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:09:00,081 [trainer.py] => Time:121.7392225265503
4234 4234
4234 4234
2025-12-10 16:09:12,607 [trainer.py] => Time:12.525769710540771
2025-12-10 16:09:12,607 [inflora.py] => Exemplar size: 0
2025-12-10 16:09:12,608 [trainer.py] => CNN: {'total': np.float64(40.03), '00-01': np.float64(69.89), '02-03': np.float64(33.87), '04-05': np.float64(47.19), '06-07': np.float64(41.07), '08-09': np.float64(38.36), '10-11': np.float64(26.83), '12-13': np.float64(31.48), '14-15': np.float64(39.29), '16-17': np.float64(12.86), '18-19': np.float64(58.93), '20-21': np.float64(57.14), '22-23': np.float64(63.28), '24-25': np.float64(20.41), '26-27': np.float64(66.96), '28-29': np.float64(58.02), '30-31': np.float64(35.45), '32-33': np.float64(13.64), '34-35': np.float64(35.48), '36-37': np.float64(43.33), '38-39': np.float64(48.78), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(50.0), '50-51': np.float64(16.22), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(7.27), '60-61': np.float64(43.9), '62-63': np.float64(33.02), '64-65': np.float64(10.0), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(31.51), '72-73': np.float64(40.0), '74-75': np.float64(56.41), '76-77': np.float64(49.4), '78-79': np.float64(26.83), '80-81': np.float64(52.78), '82-83': np.float64(27.12), '84-85': np.float64(45.65), '86-87': np.float64(26.44), '88-89': np.float64(75.0), '90-91': np.float64(47.5), '92-93': np.float64(14.29), '94-95': np.float64(26.76), '96-97': np.float64(18.18), '98-99': np.float64(28.38), '100-101': np.float64(43.33), '102-103': np.float64(58.62), '104-105': np.float64(17.02), '106-107': np.float64(33.33), '108-109': np.float64(55.22), '110-111': np.float64(51.67), '112-113': np.float64(78.85), '114-115': np.float64(45.0), '116-117': np.float64(77.78), '118-119': np.float64(34.62), '120-121': np.float64(62.3), '122-123': np.float64(25.0), '124-125': np.float64(40.82), '126-127': np.float64(11.32), '128-129': np.float64(22.78), '130-131': np.float64(40.0), '132-133': np.float64(32.35), '134-135': np.float64(47.44), '136-137': np.float64(67.35), '138-139': np.float64(47.17), 'old': np.float64(39.94), 'new': np.float64(47.17)}
2025-12-10 16:09:12,608 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03)]
2025-12-10 16:09:12,608 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51)]
2025-12-10 16:09:12,608 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613]
2025-12-10 16:09:16,311 [trainer.py] => All params: 144526051
2025-12-10 16:09:16,323 [trainer.py] => Trainable params: 185858
2025-12-10 16:09:16,323 [inflora.py] => Learning on 140-142
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.70.weight', 'image_encoder.blocks.5.attn.lora_B_k.70.weight', 'image_encoder.blocks.4.attn.lora_B_v.70.weight', 'classifier_pool.70.weight', 'image_encoder.blocks.2.attn.lora_B_v.70.weight', 'image_encoder.blocks.0.attn.lora_B_v.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.70.weight', 'image_encoder.blocks.9.attn.lora_B_k.70.weight', 'image_encoder.blocks.7.attn.lora_B_v.70.weight', 'image_encoder.blocks.9.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.70.weight', 'image_encoder.blocks.1.attn.lora_B_v.70.weight', 'image_encoder.blocks.11.attn.lora_B_k.70.weight', 'image_encoder.blocks.6.attn.lora_B_k.70.weight', 'image_encoder.blocks.10.attn.lora_B_k.70.weight', 'classifier_pool.70.bias', 'image_encoder.blocks.4.attn.lora_B_k.70.weight', 'image_encoder.blocks.8.attn.lora_B_v.70.weight', 'image_encoder.blocks.2.attn.lora_B_k.70.weight', 'image_encoder.blocks.6.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_k.70.weight', 'image_encoder.blocks.7.attn.lora_B_k.70.weight', 'image_encoder.blocks.0.attn.lora_B_k.70.weight', 'image_encoder.blocks.10.attn.lora_B_v.70.weight', 'image_encoder.blocks.8.attn.lora_B_k.70.weight'}
2025-12-10 16:11:00,635 [inflora.py] => Task 70, Epoch 50/50 => Loss 0.074, Train_accy 97.94
Threshold:  0.994
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 55/768 type remove
Layer 3 : 119/768 type remove
Layer 4 : 158/768 type remove
Layer 5 : 207/768 type remove
Layer 6 : 219/768 type remove
Layer 7 : 255/768 type remove
Layer 8 : 278/768 type remove
Layer 9 : 370/768 type remove
Layer 10 : 337/768 type retain
Layer 11 : 338/768 type remove
Layer 12 : 290/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:11:07,944 [trainer.py] => Time:111.62122845649719
4281 4281
4281 4281
2025-12-10 16:11:20,713 [trainer.py] => Time:12.76850700378418
2025-12-10 16:11:20,713 [inflora.py] => Exemplar size: 0
2025-12-10 16:11:20,714 [trainer.py] => CNN: {'total': np.float64(39.45), '00-01': np.float64(68.82), '02-03': np.float64(33.87), '04-05': np.float64(48.31), '06-07': np.float64(53.57), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(38.89), '14-15': np.float64(37.5), '16-17': np.float64(11.43), '18-19': np.float64(58.93), '20-21': np.float64(54.95), '22-23': np.float64(64.06), '24-25': np.float64(20.41), '26-27': np.float64(67.83), '28-29': np.float64(61.73), '30-31': np.float64(37.27), '32-33': np.float64(4.55), '34-35': np.float64(41.94), '36-37': np.float64(45.0), '38-39': np.float64(43.9), '40-41': np.float64(27.78), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(50.0), '50-51': np.float64(13.51), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(9.09), '60-61': np.float64(39.02), '62-63': np.float64(31.13), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(28.77), '72-73': np.float64(47.5), '74-75': np.float64(55.13), '76-77': np.float64(54.22), '78-79': np.float64(23.17), '80-81': np.float64(51.39), '82-83': np.float64(20.34), '84-85': np.float64(52.17), '86-87': np.float64(21.84), '88-89': np.float64(66.67), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(12.12), '98-99': np.float64(25.68), '100-101': np.float64(36.67), '102-103': np.float64(56.9), '104-105': np.float64(19.15), '106-107': np.float64(33.33), '108-109': np.float64(53.73), '110-111': np.float64(48.33), '112-113': np.float64(76.92), '114-115': np.float64(50.0), '116-117': np.float64(66.67), '118-119': np.float64(40.38), '120-121': np.float64(62.3), '122-123': np.float64(32.14), '124-125': np.float64(40.82), '126-127': np.float64(13.21), '128-129': np.float64(20.25), '130-131': np.float64(44.44), '132-133': np.float64(32.35), '134-135': np.float64(47.44), '136-137': np.float64(67.35), '138-139': np.float64(39.62), '140-141': np.float64(34.04), 'old': np.float64(39.51), 'new': np.float64(34.04)}
2025-12-10 16:11:20,714 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45)]
2025-12-10 16:11:20,714 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66)]
2025-12-10 16:11:20,714 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897]
2025-12-10 16:11:24,779 [trainer.py] => All params: 144526051
2025-12-10 16:11:24,790 [trainer.py] => Trainable params: 185858
2025-12-10 16:11:24,790 [inflora.py] => Learning on 142-144
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.71.weight', 'image_encoder.blocks.4.attn.lora_B_k.71.weight', 'image_encoder.blocks.2.attn.lora_B_k.71.weight', 'image_encoder.blocks.3.attn.lora_B_k.71.weight', 'image_encoder.blocks.11.attn.lora_B_v.71.weight', 'image_encoder.blocks.7.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.71.weight', 'image_encoder.blocks.6.attn.lora_B_k.71.weight', 'image_encoder.blocks.7.attn.lora_B_k.71.weight', 'image_encoder.blocks.5.attn.lora_B_k.71.weight', 'image_encoder.blocks.9.attn.lora_B_k.71.weight', 'image_encoder.blocks.1.attn.lora_B_v.71.weight', 'image_encoder.blocks.0.attn.lora_B_k.71.weight', 'image_encoder.blocks.2.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.71.weight', 'image_encoder.blocks.10.attn.lora_B_v.71.weight', 'classifier_pool.71.weight', 'image_encoder.blocks.3.attn.lora_B_v.71.weight', 'classifier_pool.71.bias', 'image_encoder.blocks.4.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_k.71.weight', 'image_encoder.blocks.5.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_k.71.weight', 'image_encoder.blocks.10.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_v.71.weight', 'image_encoder.blocks.0.attn.lora_B_v.71.weight'}
2025-12-10 16:13:05,145 [inflora.py] => Task 71, Epoch 50/50 => Loss 0.057, Train_accy 98.10
Threshold:  0.9942
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 56/768 type remove
Layer 3 : 120/768 type remove
Layer 4 : 161/768 type remove
Layer 5 : 211/768 type remove
Layer 6 : 223/768 type remove
Layer 7 : 262/768 type remove
Layer 8 : 290/768 type remove
Layer 9 : 382/768 type remove
Layer 10 : 324/768 type retain
Layer 11 : 352/768 type remove
Layer 12 : 286/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:13:12,240 [trainer.py] => Time:107.44947528839111
4329 4329
4329 4329
2025-12-10 16:13:24,922 [trainer.py] => Time:12.68142580986023
2025-12-10 16:13:24,922 [inflora.py] => Exemplar size: 0
2025-12-10 16:13:24,922 [trainer.py] => CNN: {'total': np.float64(39.04), '00-01': np.float64(65.59), '02-03': np.float64(35.48), '04-05': np.float64(47.19), '06-07': np.float64(48.21), '08-09': np.float64(38.36), '10-11': np.float64(17.07), '12-13': np.float64(35.19), '14-15': np.float64(41.07), '16-17': np.float64(12.86), '18-19': np.float64(57.14), '20-21': np.float64(52.75), '22-23': np.float64(63.28), '24-25': np.float64(18.37), '26-27': np.float64(67.83), '28-29': np.float64(60.49), '30-31': np.float64(38.18), '32-33': np.float64(2.27), '34-35': np.float64(41.94), '36-37': np.float64(45.0), '38-39': np.float64(43.9), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(48.94), '50-51': np.float64(13.51), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(10.91), '60-61': np.float64(39.02), '62-63': np.float64(32.08), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(28.77), '72-73': np.float64(47.5), '74-75': np.float64(53.85), '76-77': np.float64(50.6), '78-79': np.float64(23.17), '80-81': np.float64(52.78), '82-83': np.float64(23.73), '84-85': np.float64(50.0), '86-87': np.float64(24.14), '88-89': np.float64(66.67), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(10.1), '98-99': np.float64(24.32), '100-101': np.float64(38.33), '102-103': np.float64(56.9), '104-105': np.float64(18.09), '106-107': np.float64(30.77), '108-109': np.float64(56.72), '110-111': np.float64(46.67), '112-113': np.float64(78.85), '114-115': np.float64(55.0), '116-117': np.float64(58.33), '118-119': np.float64(46.15), '120-121': np.float64(63.93), '122-123': np.float64(32.14), '124-125': np.float64(34.69), '126-127': np.float64(11.32), '128-129': np.float64(18.99), '130-131': np.float64(44.44), '132-133': np.float64(29.41), '134-135': np.float64(44.87), '136-137': np.float64(63.27), '138-139': np.float64(35.85), '140-141': np.float64(34.04), '142-143': np.float64(58.33), 'old': np.float64(38.82), 'new': np.float64(58.33)}
2025-12-10 16:13:24,923 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04)]
2025-12-10 16:13:24,923 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54)]
2025-12-10 16:13:24,923 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036]
2025-12-10 16:13:31,640 [trainer.py] => All params: 144526051
2025-12-10 16:13:31,652 [trainer.py] => Trainable params: 185858
2025-12-10 16:13:31,652 [inflora.py] => Learning on 144-146
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_v.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.72.weight', 'image_encoder.blocks.5.attn.lora_B_v.72.weight', 'image_encoder.blocks.1.attn.lora_B_v.72.weight', 'image_encoder.blocks.9.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_k.72.weight', 'classifier_pool.72.bias', 'image_encoder.blocks.4.attn.lora_B_k.72.weight', 'image_encoder.blocks.2.attn.lora_B_v.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_k.72.weight', 'image_encoder.blocks.6.attn.lora_B_k.72.weight', 'image_encoder.blocks.10.attn.lora_B_k.72.weight', 'image_encoder.blocks.4.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_v.72.weight', 'image_encoder.blocks.7.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_v.72.weight', 'image_encoder.blocks.10.attn.lora_B_v.72.weight', 'image_encoder.blocks.1.attn.lora_B_k.72.weight', 'image_encoder.blocks.6.attn.lora_B_v.72.weight', 'image_encoder.blocks.9.attn.lora_B_v.72.weight', 'classifier_pool.72.weight', 'image_encoder.blocks.5.attn.lora_B_k.72.weight', 'image_encoder.blocks.2.attn.lora_B_k.72.weight'}
2025-12-10 16:15:15,773 [inflora.py] => Task 72, Epoch 50/50 => Loss 0.028, Train_accy 98.94
Threshold:  0.9944
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 121/768 type remove
Layer 4 : 162/768 type remove
Layer 5 : 213/768 type remove
Layer 6 : 227/768 type remove
Layer 7 : 267/768 type remove
Layer 8 : 293/768 type remove
Layer 9 : 383/768 type retain
Layer 10 : 320/768 type retain
Layer 11 : 359/768 type remove
Layer 12 : 281/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:15:23,038 [trainer.py] => Time:111.38635110855103
4375 4375
4375 4375
2025-12-10 16:15:36,004 [trainer.py] => Time:12.965359926223755
2025-12-10 16:15:36,004 [inflora.py] => Exemplar size: 0
2025-12-10 16:15:36,005 [trainer.py] => CNN: {'total': np.float64(38.19), '00-01': np.float64(64.52), '02-03': np.float64(33.87), '04-05': np.float64(43.82), '06-07': np.float64(51.79), '08-09': np.float64(41.1), '10-11': np.float64(14.63), '12-13': np.float64(37.04), '14-15': np.float64(37.5), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(52.75), '22-23': np.float64(62.5), '24-25': np.float64(20.41), '26-27': np.float64(71.3), '28-29': np.float64(59.26), '30-31': np.float64(37.27), '32-33': np.float64(4.55), '34-35': np.float64(41.94), '36-37': np.float64(46.67), '38-39': np.float64(41.46), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(43.62), '50-51': np.float64(13.51), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(9.09), '60-61': np.float64(39.02), '62-63': np.float64(30.19), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(27.4), '72-73': np.float64(45.0), '74-75': np.float64(56.41), '76-77': np.float64(53.01), '78-79': np.float64(17.07), '80-81': np.float64(52.78), '82-83': np.float64(17.8), '84-85': np.float64(50.0), '86-87': np.float64(17.24), '88-89': np.float64(65.0), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(11.11), '98-99': np.float64(24.32), '100-101': np.float64(38.33), '102-103': np.float64(55.17), '104-105': np.float64(22.34), '106-107': np.float64(30.77), '108-109': np.float64(44.78), '110-111': np.float64(41.67), '112-113': np.float64(78.85), '114-115': np.float64(50.0), '116-117': np.float64(55.56), '118-119': np.float64(42.31), '120-121': np.float64(62.3), '122-123': np.float64(35.71), '124-125': np.float64(36.73), '126-127': np.float64(13.21), '128-129': np.float64(16.46), '130-131': np.float64(40.0), '132-133': np.float64(35.29), '134-135': np.float64(38.46), '136-137': np.float64(61.22), '138-139': np.float64(39.62), '140-141': np.float64(29.79), '142-143': np.float64(62.5), '144-145': np.float64(43.48), 'old': np.float64(38.14), 'new': np.float64(43.48)}
2025-12-10 16:15:36,005 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19)]
2025-12-10 16:15:36,005 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5)]
2025-12-10 16:15:36,005 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714]
2025-12-10 16:15:43,753 [trainer.py] => All params: 144526051
2025-12-10 16:15:43,765 [trainer.py] => Trainable params: 185858
2025-12-10 16:15:43,765 [inflora.py] => Learning on 146-148
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.73.weight', 'classifier_pool.73.weight', 'image_encoder.blocks.4.attn.lora_B_v.73.weight', 'image_encoder.blocks.11.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_v.73.weight', 'image_encoder.blocks.8.attn.lora_B_v.73.weight', 'image_encoder.blocks.10.attn.lora_B_k.73.weight', 'image_encoder.blocks.7.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_v.73.weight', 'image_encoder.blocks.1.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.73.weight', 'image_encoder.blocks.2.attn.lora_B_k.73.weight', 'image_encoder.blocks.4.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_v.73.weight', 'image_encoder.blocks.3.attn.lora_B_v.73.weight', 'classifier_pool.73.bias', 'image_encoder.blocks.10.attn.lora_B_v.73.weight', 'image_encoder.blocks.3.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_k.73.weight', 'image_encoder.blocks.8.attn.lora_B_k.73.weight', 'image_encoder.blocks.2.attn.lora_B_v.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.73.weight', 'image_encoder.blocks.1.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_k.73.weight'}
2025-12-10 16:17:18,650 [inflora.py] => Task 73, Epoch 50/50 => Loss 0.081, Train_accy 97.08
Threshold:  0.9946
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 122/768 type remove
Layer 4 : 163/768 type remove
Layer 5 : 214/768 type remove
Layer 6 : 229/768 type remove
Layer 7 : 272/768 type remove
Layer 8 : 298/768 type remove
Layer 9 : 376/768 type retain
Layer 10 : 315/768 type retain
Layer 11 : 370/768 type remove
Layer 12 : 271/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:17:26,031 [trainer.py] => Time:102.26610207557678
4402 4402
4402 4402
2025-12-10 16:17:38,944 [trainer.py] => Time:12.912473917007446
2025-12-10 16:17:38,945 [inflora.py] => Exemplar size: 0
2025-12-10 16:17:38,945 [trainer.py] => CNN: {'total': np.float64(37.78), '00-01': np.float64(63.44), '02-03': np.float64(30.65), '04-05': np.float64(42.7), '06-07': np.float64(55.36), '08-09': np.float64(39.73), '10-11': np.float64(14.63), '12-13': np.float64(37.04), '14-15': np.float64(35.71), '16-17': np.float64(10.0), '18-19': np.float64(58.93), '20-21': np.float64(51.65), '22-23': np.float64(62.5), '24-25': np.float64(22.45), '26-27': np.float64(69.57), '28-29': np.float64(59.26), '30-31': np.float64(37.27), '32-33': np.float64(2.27), '34-35': np.float64(45.16), '36-37': np.float64(51.67), '38-39': np.float64(41.46), '40-41': np.float64(25.93), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(40.43), '50-51': np.float64(10.81), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(9.09), '60-61': np.float64(39.02), '62-63': np.float64(28.3), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(4.17), '70-71': np.float64(23.29), '72-73': np.float64(42.5), '74-75': np.float64(52.56), '76-77': np.float64(51.81), '78-79': np.float64(18.29), '80-81': np.float64(54.17), '82-83': np.float64(20.34), '84-85': np.float64(50.0), '86-87': np.float64(13.79), '88-89': np.float64(63.33), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(11.11), '98-99': np.float64(25.68), '100-101': np.float64(40.0), '102-103': np.float64(53.45), '104-105': np.float64(19.15), '106-107': np.float64(30.77), '108-109': np.float64(44.78), '110-111': np.float64(41.67), '112-113': np.float64(76.92), '114-115': np.float64(47.5), '116-117': np.float64(55.56), '118-119': np.float64(36.54), '120-121': np.float64(60.66), '122-123': np.float64(32.14), '124-125': np.float64(38.78), '126-127': np.float64(13.21), '128-129': np.float64(16.46), '130-131': np.float64(37.78), '132-133': np.float64(29.41), '134-135': np.float64(41.03), '136-137': np.float64(61.22), '138-139': np.float64(39.62), '140-141': np.float64(36.17), '142-143': np.float64(62.5), '144-145': np.float64(45.65), '146-147': np.float64(55.56), 'old': np.float64(37.67), 'new': np.float64(55.56)}
2025-12-10 16:17:38,945 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78)]
2025-12-10 16:17:38,945 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71)]
2025-12-10 16:17:38,945 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406]
2025-12-10 16:17:47,593 [trainer.py] => All params: 144526051
2025-12-10 16:17:47,605 [trainer.py] => Trainable params: 185858
2025-12-10 16:17:47,605 [inflora.py] => Learning on 148-150
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.74.weight', 'image_encoder.blocks.0.attn.lora_B_v.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.74.weight', 'image_encoder.blocks.6.attn.lora_B_v.74.weight', 'image_encoder.blocks.0.attn.lora_B_k.74.weight', 'image_encoder.blocks.1.attn.lora_B_k.74.weight', 'image_encoder.blocks.7.attn.lora_B_k.74.weight', 'classifier_pool.74.weight', 'image_encoder.blocks.2.attn.lora_B_k.74.weight', 'image_encoder.blocks.3.attn.lora_B_v.74.weight', 'image_encoder.blocks.5.attn.lora_B_k.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.74.weight', 'image_encoder.blocks.11.attn.lora_B_v.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.74.weight', 'image_encoder.blocks.9.attn.lora_B_k.74.weight', 'image_encoder.blocks.8.attn.lora_B_v.74.weight', 'image_encoder.blocks.7.attn.lora_B_v.74.weight', 'image_encoder.blocks.8.attn.lora_B_k.74.weight', 'image_encoder.blocks.1.attn.lora_B_v.74.weight', 'image_encoder.blocks.11.attn.lora_B_k.74.weight', 'image_encoder.blocks.10.attn.lora_B_v.74.weight', 'image_encoder.blocks.4.attn.lora_B_k.74.weight', 'classifier_pool.74.bias', 'image_encoder.blocks.5.attn.lora_B_v.74.weight', 'image_encoder.blocks.3.attn.lora_B_k.74.weight', 'image_encoder.blocks.9.attn.lora_B_v.74.weight'}
2025-12-10 16:19:13,710 [inflora.py] => Task 74, Epoch 50/50 => Loss 0.035, Train_accy 98.40
Threshold:  0.9948
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 123/768 type remove
Layer 4 : 164/768 type remove
Layer 5 : 215/768 type remove
Layer 6 : 231/768 type remove
Layer 7 : 276/768 type remove
Layer 8 : 301/768 type remove
Layer 9 : 373/768 type retain
Layer 10 : 311/768 type retain
Layer 11 : 377/768 type remove
Layer 12 : 266/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:19:19,933 [trainer.py] => Time:92.32796144485474
4435 4435
4435 4435
2025-12-10 16:19:32,926 [trainer.py] => Time:12.993034601211548
2025-12-10 16:19:32,927 [inflora.py] => Exemplar size: 0
2025-12-10 16:19:32,927 [trainer.py] => CNN: {'total': np.float64(38.13), '00-01': np.float64(67.74), '02-03': np.float64(29.03), '04-05': np.float64(43.82), '06-07': np.float64(55.36), '08-09': np.float64(41.1), '10-11': np.float64(14.63), '12-13': np.float64(33.33), '14-15': np.float64(33.93), '16-17': np.float64(10.0), '18-19': np.float64(53.57), '20-21': np.float64(50.55), '22-23': np.float64(62.5), '24-25': np.float64(22.45), '26-27': np.float64(72.17), '28-29': np.float64(54.32), '30-31': np.float64(35.45), '32-33': np.float64(2.27), '34-35': np.float64(41.94), '36-37': np.float64(51.67), '38-39': np.float64(36.59), '40-41': np.float64(24.07), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(42.55), '50-51': np.float64(24.32), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(33.02), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(42.5), '74-75': np.float64(52.56), '76-77': np.float64(49.4), '78-79': np.float64(18.29), '80-81': np.float64(56.94), '82-83': np.float64(23.73), '84-85': np.float64(50.0), '86-87': np.float64(12.64), '88-89': np.float64(61.67), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(29.58), '96-97': np.float64(11.11), '98-99': np.float64(25.68), '100-101': np.float64(40.0), '102-103': np.float64(56.9), '104-105': np.float64(21.28), '106-107': np.float64(25.64), '108-109': np.float64(44.78), '110-111': np.float64(43.33), '112-113': np.float64(78.85), '114-115': np.float64(55.0), '116-117': np.float64(58.33), '118-119': np.float64(36.54), '120-121': np.float64(62.3), '122-123': np.float64(39.29), '124-125': np.float64(38.78), '126-127': np.float64(9.43), '128-129': np.float64(17.72), '130-131': np.float64(40.0), '132-133': np.float64(26.47), '134-135': np.float64(37.18), '136-137': np.float64(63.27), '138-139': np.float64(45.28), '140-141': np.float64(42.55), '142-143': np.float64(62.5), '144-145': np.float64(43.48), '146-147': np.float64(51.85), '148-149': np.float64(18.18), 'old': np.float64(38.28), 'new': np.float64(18.18)}
2025-12-10 16:19:32,927 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13)]
2025-12-10 16:19:32,927 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85)]
2025-12-10 16:19:32,928 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301]
2025-12-10 16:19:41,185 [trainer.py] => All params: 144526051
2025-12-10 16:19:41,197 [trainer.py] => Trainable params: 185858
2025-12-10 16:19:41,197 [inflora.py] => Learning on 150-152
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_v.75.weight', 'image_encoder.blocks.9.attn.lora_B_k.75.weight', 'image_encoder.blocks.0.attn.lora_B_v.75.weight', 'image_encoder.blocks.9.attn.lora_B_v.75.weight', 'image_encoder.blocks.11.attn.lora_B_k.75.weight', 'image_encoder.blocks.6.attn.lora_B_v.75.weight', 'image_encoder.blocks.5.attn.lora_B_v.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.75.weight', 'image_encoder.blocks.7.attn.lora_B_v.75.weight', 'image_encoder.blocks.8.attn.lora_B_k.75.weight', 'image_encoder.blocks.11.attn.lora_B_v.75.weight', 'image_encoder.blocks.5.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.75.weight', 'image_encoder.blocks.7.attn.lora_B_k.75.weight', 'image_encoder.blocks.2.attn.lora_B_k.75.weight', 'image_encoder.blocks.1.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_v.75.weight', 'classifier_pool.75.weight', 'image_encoder.blocks.4.attn.lora_B_k.75.weight', 'classifier_pool.75.bias', 'image_encoder.blocks.6.attn.lora_B_k.75.weight', 'image_encoder.blocks.1.attn.lora_B_v.75.weight', 'image_encoder.blocks.8.attn.lora_B_v.75.weight'}
2025-12-10 16:21:12,952 [inflora.py] => Task 75, Epoch 50/50 => Loss 0.099, Train_accy 96.38
Threshold:  0.995
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 124/768 type remove
Layer 4 : 165/768 type remove
Layer 5 : 216/768 type remove
Layer 6 : 233/768 type remove
Layer 7 : 281/768 type remove
Layer 8 : 306/768 type remove
Layer 9 : 368/768 type retain
Layer 10 : 307/768 type retain
Layer 11 : 380/768 type remove
Layer 12 : 262/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:21:19,597 [trainer.py] => Time:98.39975595474243
4473 4473
4473 4473
2025-12-10 16:21:32,745 [trainer.py] => Time:13.147910833358765
2025-12-10 16:21:32,745 [inflora.py] => Exemplar size: 0
2025-12-10 16:21:32,745 [trainer.py] => CNN: {'total': np.float64(38.14), '00-01': np.float64(69.89), '02-03': np.float64(30.65), '04-05': np.float64(46.07), '06-07': np.float64(53.57), '08-09': np.float64(43.84), '10-11': np.float64(19.51), '12-13': np.float64(35.19), '14-15': np.float64(35.71), '16-17': np.float64(10.0), '18-19': np.float64(53.57), '20-21': np.float64(53.85), '22-23': np.float64(60.16), '24-25': np.float64(22.45), '26-27': np.float64(71.3), '28-29': np.float64(54.32), '30-31': np.float64(35.45), '32-33': np.float64(2.27), '34-35': np.float64(45.16), '36-37': np.float64(51.67), '38-39': np.float64(36.59), '40-41': np.float64(22.22), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(42.55), '50-51': np.float64(21.62), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(5.45), '60-61': np.float64(41.46), '62-63': np.float64(33.02), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(30.14), '72-73': np.float64(40.0), '74-75': np.float64(52.56), '76-77': np.float64(46.99), '78-79': np.float64(20.73), '80-81': np.float64(59.72), '82-83': np.float64(22.88), '84-85': np.float64(50.0), '86-87': np.float64(17.24), '88-89': np.float64(63.33), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(30.99), '96-97': np.float64(10.1), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(63.79), '104-105': np.float64(20.21), '106-107': np.float64(25.64), '108-109': np.float64(44.78), '110-111': np.float64(43.33), '112-113': np.float64(76.92), '114-115': np.float64(52.5), '116-117': np.float64(61.11), '118-119': np.float64(38.46), '120-121': np.float64(63.93), '122-123': np.float64(35.71), '124-125': np.float64(38.78), '126-127': np.float64(9.43), '128-129': np.float64(17.72), '130-131': np.float64(37.78), '132-133': np.float64(29.41), '134-135': np.float64(32.05), '136-137': np.float64(61.22), '138-139': np.float64(50.94), '140-141': np.float64(34.04), '142-143': np.float64(66.67), '144-145': np.float64(43.48), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(5.26), 'old': np.float64(38.42), 'new': np.float64(5.26)}
2025-12-10 16:21:32,746 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14)]
2025-12-10 16:21:32,746 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84)]
2025-12-10 16:21:32,746 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685]
2025-12-10 16:21:39,412 [trainer.py] => All params: 144526051
2025-12-10 16:21:39,423 [trainer.py] => Trainable params: 185858
2025-12-10 16:21:39,424 [inflora.py] => Learning on 152-154
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.76.weight', 'image_encoder.blocks.3.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.76.weight', 'image_encoder.blocks.10.attn.lora_B_k.76.weight', 'image_encoder.blocks.8.attn.lora_B_k.76.weight', 'image_encoder.blocks.3.attn.lora_B_v.76.weight', 'image_encoder.blocks.9.attn.lora_B_v.76.weight', 'image_encoder.blocks.11.attn.lora_B_k.76.weight', 'image_encoder.blocks.5.attn.lora_B_v.76.weight', 'classifier_pool.76.bias', 'image_encoder.blocks.8.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_k.76.weight', 'image_encoder.blocks.6.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_v.76.weight', 'image_encoder.blocks.2.attn.lora_B_k.76.weight', 'classifier_pool.76.weight', 'image_encoder.blocks.1.attn.lora_B_v.76.weight', 'image_encoder.blocks.11.attn.lora_B_v.76.weight', 'image_encoder.blocks.5.attn.lora_B_k.76.weight', 'image_encoder.blocks.6.attn.lora_B_k.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.76.weight', 'image_encoder.blocks.4.attn.lora_B_v.76.weight', 'image_encoder.blocks.9.attn.lora_B_k.76.weight', 'image_encoder.blocks.4.attn.lora_B_k.76.weight'}
2025-12-10 16:23:27,652 [inflora.py] => Task 76, Epoch 50/50 => Loss 0.055, Train_accy 98.16
Threshold:  0.9952
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 59/768 type remove
Layer 3 : 125/768 type remove
Layer 4 : 166/768 type remove
Layer 5 : 218/768 type remove
Layer 6 : 236/768 type remove
Layer 7 : 286/768 type remove
Layer 8 : 310/768 type remove
Layer 9 : 363/768 type retain
Layer 10 : 304/768 type retain
Layer 11 : 384/768 type remove
Layer 12 : 257/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:23:34,783 [trainer.py] => Time:115.35900211334229
4510 4510
4510 4510
2025-12-10 16:23:48,139 [trainer.py] => Time:13.35660696029663
2025-12-10 16:23:48,140 [inflora.py] => Exemplar size: 0
2025-12-10 16:23:48,140 [trainer.py] => CNN: {'total': np.float64(38.16), '00-01': np.float64(69.89), '02-03': np.float64(30.65), '04-05': np.float64(43.82), '06-07': np.float64(60.71), '08-09': np.float64(41.1), '10-11': np.float64(14.63), '12-13': np.float64(33.33), '14-15': np.float64(37.5), '16-17': np.float64(11.43), '18-19': np.float64(55.36), '20-21': np.float64(50.55), '22-23': np.float64(60.94), '24-25': np.float64(22.45), '26-27': np.float64(73.91), '28-29': np.float64(60.49), '30-31': np.float64(34.55), '32-33': np.float64(4.55), '34-35': np.float64(45.16), '36-37': np.float64(53.33), '38-39': np.float64(39.02), '40-41': np.float64(22.22), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(40.43), '50-51': np.float64(24.32), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(9.09), '60-61': np.float64(41.46), '62-63': np.float64(32.08), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(30.14), '72-73': np.float64(42.5), '74-75': np.float64(47.44), '76-77': np.float64(49.4), '78-79': np.float64(17.07), '80-81': np.float64(58.33), '82-83': np.float64(22.88), '84-85': np.float64(52.17), '86-87': np.float64(13.79), '88-89': np.float64(60.0), '90-91': np.float64(47.5), '92-93': np.float64(14.29), '94-95': np.float64(28.17), '96-97': np.float64(15.15), '98-99': np.float64(24.32), '100-101': np.float64(41.67), '102-103': np.float64(56.9), '104-105': np.float64(17.02), '106-107': np.float64(30.77), '108-109': np.float64(50.75), '110-111': np.float64(43.33), '112-113': np.float64(80.77), '114-115': np.float64(55.0), '116-117': np.float64(61.11), '118-119': np.float64(36.54), '120-121': np.float64(62.3), '122-123': np.float64(35.71), '124-125': np.float64(42.86), '126-127': np.float64(13.21), '128-129': np.float64(18.99), '130-131': np.float64(37.78), '132-133': np.float64(29.41), '134-135': np.float64(38.46), '136-137': np.float64(59.18), '138-139': np.float64(47.17), '140-141': np.float64(29.79), '142-143': np.float64(68.75), '144-145': np.float64(32.61), '146-147': np.float64(51.85), '148-149': np.float64(24.24), '150-151': np.float64(2.63), '152-153': np.float64(27.03), 'old': np.float64(38.25), 'new': np.float64(27.03)}
2025-12-10 16:23:48,140 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16)]
2025-12-10 16:23:48,140 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81)]
2025-12-10 16:23:48,140 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183]
2025-12-10 16:23:54,217 [trainer.py] => All params: 144526051
2025-12-10 16:23:54,229 [trainer.py] => Trainable params: 185858
2025-12-10 16:23:54,229 [inflora.py] => Learning on 154-156
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.77.weight', 'classifier_pool.77.bias', 'image_encoder.blocks.1.attn.lora_B_v.77.weight', 'image_encoder.blocks.7.attn.lora_B_k.77.weight', 'image_encoder.blocks.7.attn.lora_B_v.77.weight', 'image_encoder.blocks.8.attn.lora_B_k.77.weight', 'image_encoder.blocks.11.attn.lora_B_k.77.weight', 'image_encoder.blocks.0.attn.lora_B_k.77.weight', 'image_encoder.blocks.9.attn.lora_B_k.77.weight', 'image_encoder.blocks.10.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_v.77.weight', 'image_encoder.blocks.11.attn.lora_B_v.77.weight', 'image_encoder.blocks.6.attn.lora_B_k.77.weight', 'image_encoder.blocks.6.attn.lora_B_v.77.weight', 'image_encoder.blocks.3.attn.lora_B_k.77.weight', 'image_encoder.blocks.5.attn.lora_B_v.77.weight', 'image_encoder.blocks.5.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_k.77.weight', 'image_encoder.blocks.8.attn.lora_B_v.77.weight', 'image_encoder.blocks.9.attn.lora_B_v.77.weight', 'image_encoder.blocks.3.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.77.weight', 'classifier_pool.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.77.weight', 'image_encoder.blocks.0.attn.lora_B_v.77.weight', 'image_encoder.blocks.10.attn.lora_B_k.77.weight'}
2025-12-10 16:25:40,476 [inflora.py] => Task 77, Epoch 50/50 => Loss 0.017, Train_accy 99.53
Threshold:  0.9954
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 64/768 type remove
Layer 3 : 135/768 type remove
Layer 4 : 179/768 type remove
Layer 5 : 234/768 type remove
Layer 6 : 254/768 type remove
Layer 7 : 299/768 type remove
Layer 8 : 322/768 type remove
Layer 9 : 353/768 type retain
Layer 10 : 296/768 type retain
Layer 11 : 377/768 type retain
Layer 12 : 252/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:25:47,793 [trainer.py] => Time:113.56383347511292
4560 4560
4560 4560
2025-12-10 16:26:01,281 [trainer.py] => Time:13.487827062606812
2025-12-10 16:26:01,281 [inflora.py] => Exemplar size: 0
2025-12-10 16:26:01,281 [trainer.py] => CNN: {'total': np.float64(38.36), '00-01': np.float64(73.12), '02-03': np.float64(27.42), '04-05': np.float64(44.94), '06-07': np.float64(53.57), '08-09': np.float64(42.47), '10-11': np.float64(17.07), '12-13': np.float64(35.19), '14-15': np.float64(33.93), '16-17': np.float64(8.57), '18-19': np.float64(55.36), '20-21': np.float64(49.45), '22-23': np.float64(59.38), '24-25': np.float64(18.37), '26-27': np.float64(73.04), '28-29': np.float64(60.49), '30-31': np.float64(40.0), '32-33': np.float64(4.55), '34-35': np.float64(41.94), '36-37': np.float64(53.33), '38-39': np.float64(43.9), '40-41': np.float64(25.93), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(37.23), '50-51': np.float64(29.73), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(7.27), '60-61': np.float64(43.9), '62-63': np.float64(34.91), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(40.0), '74-75': np.float64(47.44), '76-77': np.float64(48.19), '78-79': np.float64(13.41), '80-81': np.float64(61.11), '82-83': np.float64(21.19), '84-85': np.float64(50.0), '86-87': np.float64(17.24), '88-89': np.float64(68.33), '90-91': np.float64(47.5), '92-93': np.float64(16.33), '94-95': np.float64(35.21), '96-97': np.float64(13.13), '98-99': np.float64(32.43), '100-101': np.float64(45.0), '102-103': np.float64(58.62), '104-105': np.float64(17.02), '106-107': np.float64(30.77), '108-109': np.float64(58.21), '110-111': np.float64(41.67), '112-113': np.float64(78.85), '114-115': np.float64(62.5), '116-117': np.float64(63.89), '118-119': np.float64(44.23), '120-121': np.float64(63.93), '122-123': np.float64(33.93), '124-125': np.float64(34.69), '126-127': np.float64(11.32), '128-129': np.float64(18.99), '130-131': np.float64(40.0), '132-133': np.float64(26.47), '134-135': np.float64(33.33), '136-137': np.float64(63.27), '138-139': np.float64(52.83), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(28.26), '146-147': np.float64(48.15), '148-149': np.float64(21.21), '150-151': np.float64(2.63), '152-153': np.float64(24.32), '154-155': np.float64(10.0), 'old': np.float64(38.67), 'new': np.float64(10.0)}
2025-12-10 16:26:01,282 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36)]
2025-12-10 16:26:01,282 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72)]
2025-12-10 16:26:01,282 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945]
2025-12-10 16:26:05,119 [trainer.py] => All params: 144526051
2025-12-10 16:26:05,131 [trainer.py] => Trainable params: 185858
2025-12-10 16:26:05,131 [inflora.py] => Learning on 156-158
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.78.weight', 'image_encoder.blocks.9.attn.lora_B_v.78.weight', 'image_encoder.blocks.1.attn.lora_B_k.78.weight', 'image_encoder.blocks.6.attn.lora_B_k.78.weight', 'image_encoder.blocks.3.attn.lora_B_k.78.weight', 'image_encoder.blocks.7.attn.lora_B_v.78.weight', 'image_encoder.blocks.4.attn.lora_B_v.78.weight', 'image_encoder.blocks.0.attn.lora_B_k.78.weight', 'image_encoder.blocks.4.attn.lora_B_k.78.weight', 'image_encoder.blocks.8.attn.lora_B_k.78.weight', 'classifier_pool.78.weight', 'classifier_pool.78.bias', 'image_encoder.blocks.5.attn.lora_B_k.78.weight', 'image_encoder.blocks.8.attn.lora_B_v.78.weight', 'image_encoder.blocks.7.attn.lora_B_k.78.weight', 'image_encoder.blocks.10.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.78.weight', 'image_encoder.blocks.6.attn.lora_B_v.78.weight', 'image_encoder.blocks.2.attn.lora_B_k.78.weight', 'image_encoder.blocks.2.attn.lora_B_v.78.weight', 'image_encoder.blocks.5.attn.lora_B_v.78.weight', 'image_encoder.blocks.10.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.78.weight', 'image_encoder.blocks.3.attn.lora_B_v.78.weight', 'image_encoder.blocks.0.attn.lora_B_v.78.weight'}
2025-12-10 16:28:00,139 [inflora.py] => Task 78, Epoch 50/50 => Loss 0.032, Train_accy 98.69
Threshold:  0.9956
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 64/768 type remove
Layer 3 : 136/768 type remove
Layer 4 : 180/768 type remove
Layer 5 : 236/768 type remove
Layer 6 : 257/768 type remove
Layer 7 : 304/768 type remove
Layer 8 : 329/768 type remove
Layer 9 : 348/768 type retain
Layer 10 : 292/768 type retain
Layer 11 : 372/768 type retain
Layer 12 : 248/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:28:07,329 [trainer.py] => Time:122.1985034942627
4616 4616
4616 4616
2025-12-10 16:28:20,922 [trainer.py] => Time:13.592111587524414
2025-12-10 16:28:20,922 [inflora.py] => Exemplar size: 0
2025-12-10 16:28:20,923 [trainer.py] => CNN: {'total': np.float64(38.11), '00-01': np.float64(74.19), '02-03': np.float64(29.03), '04-05': np.float64(47.19), '06-07': np.float64(33.93), '08-09': np.float64(41.1), '10-11': np.float64(17.07), '12-13': np.float64(38.89), '14-15': np.float64(35.71), '16-17': np.float64(8.57), '18-19': np.float64(57.14), '20-21': np.float64(57.14), '22-23': np.float64(59.38), '24-25': np.float64(12.24), '26-27': np.float64(69.57), '28-29': np.float64(58.02), '30-31': np.float64(34.55), '32-33': np.float64(2.27), '34-35': np.float64(38.71), '36-37': np.float64(50.0), '38-39': np.float64(43.9), '40-41': np.float64(25.93), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(37.23), '50-51': np.float64(24.32), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(41.46), '62-63': np.float64(32.08), '64-65': np.float64(6.67), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(32.5), '74-75': np.float64(46.15), '76-77': np.float64(44.58), '78-79': np.float64(18.29), '80-81': np.float64(62.5), '82-83': np.float64(22.88), '84-85': np.float64(47.83), '86-87': np.float64(21.84), '88-89': np.float64(78.33), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(38.03), '96-97': np.float64(11.11), '98-99': np.float64(29.73), '100-101': np.float64(43.33), '102-103': np.float64(58.62), '104-105': np.float64(17.02), '106-107': np.float64(30.77), '108-109': np.float64(50.75), '110-111': np.float64(40.0), '112-113': np.float64(78.85), '114-115': np.float64(60.0), '116-117': np.float64(69.44), '118-119': np.float64(42.31), '120-121': np.float64(62.3), '122-123': np.float64(25.0), '124-125': np.float64(40.82), '126-127': np.float64(5.66), '128-129': np.float64(21.52), '130-131': np.float64(35.56), '132-133': np.float64(26.47), '134-135': np.float64(35.9), '136-137': np.float64(65.31), '138-139': np.float64(62.26), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(41.3), '146-147': np.float64(55.56), '148-149': np.float64(21.21), '150-151': np.float64(5.26), '152-153': np.float64(27.03), '154-155': np.float64(12.0), '156-157': np.float64(48.21), 'old': np.float64(37.98), 'new': np.float64(48.21)}
2025-12-10 16:28:20,923 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11)]
2025-12-10 16:28:20,923 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65)]
2025-12-10 16:28:20,923 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437]
2025-12-10 16:28:29,592 [trainer.py] => All params: 144526051
2025-12-10 16:28:29,604 [trainer.py] => Trainable params: 185858
2025-12-10 16:28:29,604 [inflora.py] => Learning on 158-160
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_k.79.weight', 'image_encoder.blocks.2.attn.lora_B_v.79.weight', 'image_encoder.blocks.7.attn.lora_B_v.79.weight', 'image_encoder.blocks.10.attn.lora_B_v.79.weight', 'image_encoder.blocks.1.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_k.79.weight', 'image_encoder.blocks.8.attn.lora_B_v.79.weight', 'image_encoder.blocks.10.attn.lora_B_k.79.weight', 'image_encoder.blocks.1.attn.lora_B_v.79.weight', 'image_encoder.blocks.0.attn.lora_B_v.79.weight', 'classifier_pool.79.bias', 'classifier_pool.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.79.weight', 'image_encoder.blocks.3.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.79.weight', 'image_encoder.blocks.7.attn.lora_B_k.79.weight', 'image_encoder.blocks.3.attn.lora_B_k.79.weight', 'image_encoder.blocks.5.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_v.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.79.weight', 'image_encoder.blocks.2.attn.lora_B_k.79.weight', 'image_encoder.blocks.9.attn.lora_B_v.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.79.weight', 'image_encoder.blocks.9.attn.lora_B_k.79.weight'}
2025-12-10 16:30:43,912 [inflora.py] => Task 79, Epoch 50/50 => Loss 0.048, Train_accy 98.60
Threshold:  0.9958
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 65/768 type remove
Layer 3 : 137/768 type remove
Layer 4 : 181/768 type remove
Layer 5 : 239/768 type remove
Layer 6 : 261/768 type remove
Layer 7 : 307/768 type remove
Layer 8 : 335/768 type remove
Layer 9 : 346/768 type retain
Layer 10 : 291/768 type retain
Layer 11 : 370/768 type retain
Layer 12 : 245/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:30:51,515 [trainer.py] => Time:141.91124534606934
4679 4679
4679 4679
2025-12-10 16:31:05,244 [trainer.py] => Time:13.728693723678589
2025-12-10 16:31:05,244 [inflora.py] => Exemplar size: 0
2025-12-10 16:31:05,244 [trainer.py] => CNN: {'total': np.float64(37.66), '00-01': np.float64(73.12), '02-03': np.float64(27.42), '04-05': np.float64(42.7), '06-07': np.float64(37.5), '08-09': np.float64(42.47), '10-11': np.float64(17.07), '12-13': np.float64(42.59), '14-15': np.float64(37.5), '16-17': np.float64(17.14), '18-19': np.float64(60.71), '20-21': np.float64(52.75), '22-23': np.float64(57.03), '24-25': np.float64(16.33), '26-27': np.float64(71.3), '28-29': np.float64(60.49), '30-31': np.float64(37.27), '32-33': np.float64(2.27), '34-35': np.float64(25.81), '36-37': np.float64(48.33), '38-39': np.float64(41.46), '40-41': np.float64(22.22), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(38.3), '50-51': np.float64(18.92), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(3.64), '60-61': np.float64(43.9), '62-63': np.float64(29.25), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(40.0), '74-75': np.float64(38.46), '76-77': np.float64(48.19), '78-79': np.float64(18.29), '80-81': np.float64(61.11), '82-83': np.float64(19.49), '84-85': np.float64(47.83), '86-87': np.float64(19.54), '88-89': np.float64(75.0), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(43.66), '96-97': np.float64(11.11), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(58.62), '104-105': np.float64(15.96), '106-107': np.float64(30.77), '108-109': np.float64(47.76), '110-111': np.float64(38.33), '112-113': np.float64(80.77), '114-115': np.float64(60.0), '116-117': np.float64(58.33), '118-119': np.float64(42.31), '120-121': np.float64(63.93), '122-123': np.float64(30.36), '124-125': np.float64(34.69), '126-127': np.float64(7.55), '128-129': np.float64(17.72), '130-131': np.float64(42.22), '132-133': np.float64(29.41), '134-135': np.float64(34.62), '136-137': np.float64(61.22), '138-139': np.float64(50.94), '140-141': np.float64(25.53), '142-143': np.float64(68.75), '144-145': np.float64(50.0), '146-147': np.float64(55.56), '148-149': np.float64(21.21), '150-151': np.float64(5.26), '152-153': np.float64(18.92), '154-155': np.float64(10.0), '156-157': np.float64(48.21), '158-159': np.float64(44.44), 'old': np.float64(37.56), 'new': np.float64(44.44)}
2025-12-10 16:31:05,245 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66)]
2025-12-10 16:31:05,245 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6)]
2025-12-10 16:31:05,245 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439]
2025-12-10 16:31:10,444 [trainer.py] => All params: 144526051
2025-12-10 16:31:10,456 [trainer.py] => Trainable params: 185858
2025-12-10 16:31:10,456 [inflora.py] => Learning on 160-162
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.80.weight', 'classifier_pool.80.bias', 'image_encoder.blocks.11.attn.lora_B_k.80.weight', 'image_encoder.blocks.9.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_v.80.weight', 'image_encoder.blocks.5.attn.lora_B_v.80.weight', 'image_encoder.blocks.6.attn.lora_B_k.80.weight', 'image_encoder.blocks.9.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_k.80.weight', 'image_encoder.blocks.7.attn.lora_B_k.80.weight', 'image_encoder.blocks.2.attn.lora_B_k.80.weight', 'image_encoder.blocks.2.attn.lora_B_v.80.weight', 'image_encoder.blocks.4.attn.lora_B_v.80.weight', 'classifier_pool.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.80.weight', 'image_encoder.blocks.5.attn.lora_B_k.80.weight', 'image_encoder.blocks.6.attn.lora_B_v.80.weight', 'image_encoder.blocks.4.attn.lora_B_k.80.weight', 'image_encoder.blocks.7.attn.lora_B_v.80.weight', 'image_encoder.blocks.11.attn.lora_B_v.80.weight', 'image_encoder.blocks.0.attn.lora_B_k.80.weight', 'image_encoder.blocks.1.attn.lora_B_k.80.weight', 'image_encoder.blocks.1.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.80.weight', 'image_encoder.blocks.10.attn.lora_B_v.80.weight', 'image_encoder.blocks.0.attn.lora_B_v.80.weight'}
2025-12-10 16:32:53,157 [inflora.py] => Task 80, Epoch 50/50 => Loss 0.015, Train_accy 99.45
Threshold:  0.996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 69/768 type remove
Layer 3 : 145/768 type remove
Layer 4 : 194/768 type remove
Layer 5 : 256/768 type remove
Layer 6 : 278/768 type remove
Layer 7 : 322/768 type remove
Layer 8 : 350/768 type remove
Layer 9 : 329/768 type retain
Layer 10 : 275/768 type retain
Layer 11 : 360/768 type retain
Layer 12 : 239/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:33:00,302 [trainer.py] => Time:109.84597587585449
4725 4725
4725 4725
2025-12-10 16:33:14,563 [trainer.py] => Time:14.260937213897705
2025-12-10 16:33:14,564 [inflora.py] => Exemplar size: 0
2025-12-10 16:33:14,564 [trainer.py] => CNN: {'total': np.float64(37.82), '00-01': np.float64(65.59), '02-03': np.float64(30.65), '04-05': np.float64(42.7), '06-07': np.float64(41.07), '08-09': np.float64(39.73), '10-11': np.float64(17.07), '12-13': np.float64(37.04), '14-15': np.float64(35.71), '16-17': np.float64(20.0), '18-19': np.float64(58.93), '20-21': np.float64(54.95), '22-23': np.float64(57.03), '24-25': np.float64(14.29), '26-27': np.float64(70.43), '28-29': np.float64(59.26), '30-31': np.float64(38.18), '32-33': np.float64(4.55), '34-35': np.float64(35.48), '36-37': np.float64(48.33), '38-39': np.float64(46.34), '40-41': np.float64(22.22), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(35.11), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(5.45), '60-61': np.float64(43.9), '62-63': np.float64(31.13), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(37.5), '74-75': np.float64(42.31), '76-77': np.float64(50.6), '78-79': np.float64(17.07), '80-81': np.float64(59.72), '82-83': np.float64(22.03), '84-85': np.float64(47.83), '86-87': np.float64(19.54), '88-89': np.float64(73.33), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(36.62), '96-97': np.float64(11.11), '98-99': np.float64(29.73), '100-101': np.float64(36.67), '102-103': np.float64(56.9), '104-105': np.float64(14.89), '106-107': np.float64(28.21), '108-109': np.float64(49.25), '110-111': np.float64(40.0), '112-113': np.float64(80.77), '114-115': np.float64(62.5), '116-117': np.float64(63.89), '118-119': np.float64(44.23), '120-121': np.float64(67.21), '122-123': np.float64(30.36), '124-125': np.float64(34.69), '126-127': np.float64(9.43), '128-129': np.float64(20.25), '130-131': np.float64(40.0), '132-133': np.float64(29.41), '134-135': np.float64(38.46), '136-137': np.float64(59.18), '138-139': np.float64(52.83), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(43.48), '146-147': np.float64(55.56), '148-149': np.float64(24.24), '150-151': np.float64(2.63), '152-153': np.float64(21.62), '154-155': np.float64(10.0), '156-157': np.float64(48.21), '158-159': np.float64(39.68), '160-161': np.float64(39.13), 'old': np.float64(37.81), 'new': np.float64(39.13)}
2025-12-10 16:33:14,564 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82)]
2025-12-10 16:33:14,564 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58)]
2025-12-10 16:33:14,564 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984]
2025-12-10 16:33:23,793 [trainer.py] => All params: 144526051
2025-12-10 16:33:23,805 [trainer.py] => Trainable params: 185858
2025-12-10 16:33:23,805 [inflora.py] => Learning on 162-164
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.81.weight', 'image_encoder.blocks.5.attn.lora_B_v.81.weight', 'image_encoder.blocks.7.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_k.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.81.weight', 'image_encoder.blocks.5.attn.lora_B_k.81.weight', 'image_encoder.blocks.10.attn.lora_B_v.81.weight', 'image_encoder.blocks.6.attn.lora_B_k.81.weight', 'image_encoder.blocks.3.attn.lora_B_k.81.weight', 'image_encoder.blocks.1.attn.lora_B_k.81.weight', 'image_encoder.blocks.8.attn.lora_B_k.81.weight', 'image_encoder.blocks.3.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_v.81.weight', 'image_encoder.blocks.4.attn.lora_B_k.81.weight', 'classifier_pool.81.weight', 'image_encoder.blocks.9.attn.lora_B_k.81.weight', 'image_encoder.blocks.8.attn.lora_B_v.81.weight', 'image_encoder.blocks.4.attn.lora_B_v.81.weight', 'image_encoder.blocks.1.attn.lora_B_v.81.weight', 'image_encoder.blocks.6.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_k.81.weight', 'classifier_pool.81.bias', 'image_encoder.blocks.7.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_k.81.weight'}
2025-12-10 16:35:36,525 [inflora.py] => Task 81, Epoch 50/50 => Loss 0.071, Train_accy 96.99
Threshold:  0.9962
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 15/768 type remove
Layer 2 : 70/768 type remove
Layer 3 : 146/768 type remove
Layer 4 : 196/768 type remove
Layer 5 : 258/768 type remove
Layer 6 : 283/768 type remove
Layer 7 : 330/768 type remove
Layer 8 : 363/768 type remove
Layer 9 : 313/768 type retain
Layer 10 : 267/768 type retain
Layer 11 : 350/768 type retain
Layer 12 : 232/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:35:44,257 [trainer.py] => Time:140.45178985595703
4786 4786
4786 4786
2025-12-10 16:35:58,388 [trainer.py] => Time:14.130924940109253
2025-12-10 16:35:58,388 [inflora.py] => Exemplar size: 0
2025-12-10 16:35:58,388 [trainer.py] => CNN: {'total': np.float64(37.84), '00-01': np.float64(69.89), '02-03': np.float64(30.65), '04-05': np.float64(43.82), '06-07': np.float64(37.5), '08-09': np.float64(39.73), '10-11': np.float64(17.07), '12-13': np.float64(37.04), '14-15': np.float64(39.29), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(56.04), '22-23': np.float64(58.59), '24-25': np.float64(10.2), '26-27': np.float64(70.43), '28-29': np.float64(55.56), '30-31': np.float64(36.36), '32-33': np.float64(4.55), '34-35': np.float64(25.81), '36-37': np.float64(50.0), '38-39': np.float64(46.34), '40-41': np.float64(22.22), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(40.43), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(5.45), '60-61': np.float64(41.46), '62-63': np.float64(30.19), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(40.0), '74-75': np.float64(42.31), '76-77': np.float64(46.99), '78-79': np.float64(15.85), '80-81': np.float64(58.33), '82-83': np.float64(22.88), '84-85': np.float64(47.83), '86-87': np.float64(25.29), '88-89': np.float64(76.67), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(38.03), '96-97': np.float64(11.11), '98-99': np.float64(29.73), '100-101': np.float64(33.33), '102-103': np.float64(53.45), '104-105': np.float64(14.89), '106-107': np.float64(20.51), '108-109': np.float64(49.25), '110-111': np.float64(41.67), '112-113': np.float64(80.77), '114-115': np.float64(57.5), '116-117': np.float64(63.89), '118-119': np.float64(48.08), '120-121': np.float64(63.93), '122-123': np.float64(25.0), '124-125': np.float64(32.65), '126-127': np.float64(9.43), '128-129': np.float64(20.25), '130-131': np.float64(40.0), '132-133': np.float64(32.35), '134-135': np.float64(34.62), '136-137': np.float64(59.18), '138-139': np.float64(62.26), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(32.61), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(7.89), '152-153': np.float64(27.03), '154-155': np.float64(12.0), '156-157': np.float64(48.21), '158-159': np.float64(41.27), '160-161': np.float64(36.96), '162-163': np.float64(55.74), 'old': np.float64(37.61), 'new': np.float64(55.74)}
2025-12-10 16:35:58,389 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84)]
2025-12-10 16:35:58,389 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7)]
2025-12-10 16:35:58,389 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936]
2025-12-10 16:36:01,668 [trainer.py] => All params: 144526051
2025-12-10 16:36:01,680 [trainer.py] => Trainable params: 185858
2025-12-10 16:36:01,680 [inflora.py] => Learning on 164-166
Parameters to be updated: {'classifier_pool.82.weight', 'image_encoder.blocks.1.attn.lora_B_k.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.82.weight', 'image_encoder.blocks.0.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.82.weight', 'image_encoder.blocks.3.attn.lora_B_k.82.weight', 'image_encoder.blocks.8.attn.lora_B_k.82.weight', 'image_encoder.blocks.3.attn.lora_B_v.82.weight', 'image_encoder.blocks.9.attn.lora_B_k.82.weight', 'image_encoder.blocks.9.attn.lora_B_v.82.weight', 'image_encoder.blocks.0.attn.lora_B_k.82.weight', 'image_encoder.blocks.10.attn.lora_B_v.82.weight', 'image_encoder.blocks.5.attn.lora_B_v.82.weight', 'image_encoder.blocks.1.attn.lora_B_v.82.weight', 'image_encoder.blocks.11.attn.lora_B_v.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.82.weight', 'classifier_pool.82.bias', 'image_encoder.blocks.6.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_v.82.weight', 'image_encoder.blocks.8.attn.lora_B_v.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.82.weight', 'image_encoder.blocks.5.attn.lora_B_k.82.weight'}
2025-12-10 16:38:14,617 [inflora.py] => Task 82, Epoch 50/50 => Loss 0.018, Train_accy 99.30
Threshold:  0.9964
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 72/768 type remove
Layer 3 : 151/768 type remove
Layer 4 : 202/768 type remove
Layer 5 : 267/768 type remove
Layer 6 : 290/768 type remove
Layer 7 : 338/768 type remove
Layer 8 : 377/768 type remove
Layer 9 : 300/768 type retain
Layer 10 : 256/768 type retain
Layer 11 : 343/768 type retain
Layer 12 : 228/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:38:22,127 [trainer.py] => Time:140.44680500030518
4876 4876
4876 4876
2025-12-10 16:38:36,572 [trainer.py] => Time:14.445011854171753
2025-12-10 16:38:36,572 [inflora.py] => Exemplar size: 0
2025-12-10 16:38:36,573 [trainer.py] => CNN: {'total': np.float64(38.54), '00-01': np.float64(74.19), '02-03': np.float64(30.65), '04-05': np.float64(40.45), '06-07': np.float64(30.36), '08-09': np.float64(41.1), '10-11': np.float64(17.07), '12-13': np.float64(37.04), '14-15': np.float64(41.07), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(59.34), '22-23': np.float64(58.59), '24-25': np.float64(12.24), '26-27': np.float64(67.83), '28-29': np.float64(64.2), '30-31': np.float64(39.09), '32-33': np.float64(4.55), '34-35': np.float64(29.03), '36-37': np.float64(50.0), '38-39': np.float64(41.46), '40-41': np.float64(27.78), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(39.36), '50-51': np.float64(16.22), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(5.45), '60-61': np.float64(39.02), '62-63': np.float64(31.13), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(37.5), '74-75': np.float64(38.46), '76-77': np.float64(43.37), '78-79': np.float64(18.29), '80-81': np.float64(62.5), '82-83': np.float64(23.73), '84-85': np.float64(45.65), '86-87': np.float64(26.44), '88-89': np.float64(78.33), '90-91': np.float64(40.0), '92-93': np.float64(16.33), '94-95': np.float64(45.07), '96-97': np.float64(9.09), '98-99': np.float64(29.73), '100-101': np.float64(35.0), '102-103': np.float64(48.28), '104-105': np.float64(17.02), '106-107': np.float64(12.82), '108-109': np.float64(58.21), '110-111': np.float64(40.0), '112-113': np.float64(78.85), '114-115': np.float64(57.5), '116-117': np.float64(63.89), '118-119': np.float64(46.15), '120-121': np.float64(67.21), '122-123': np.float64(19.64), '124-125': np.float64(34.69), '126-127': np.float64(7.55), '128-129': np.float64(17.72), '130-131': np.float64(37.78), '132-133': np.float64(23.53), '134-135': np.float64(30.77), '136-137': np.float64(63.27), '138-139': np.float64(62.26), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(39.13), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(7.89), '152-153': np.float64(35.14), '154-155': np.float64(12.0), '156-157': np.float64(53.57), '158-159': np.float64(34.92), '160-161': np.float64(39.13), '162-163': np.float64(55.74), '164-165': np.float64(63.33), 'old': np.float64(38.07), 'new': np.float64(63.33)}
2025-12-10 16:38:36,573 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54)]
2025-12-10 16:38:36,573 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61)]
2025-12-10 16:38:36,573 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255]
2025-12-10 16:38:41,302 [trainer.py] => All params: 144526051
2025-12-10 16:38:41,314 [trainer.py] => Trainable params: 185858
2025-12-10 16:38:41,314 [inflora.py] => Learning on 166-168
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.83.weight', 'image_encoder.blocks.3.attn.lora_B_v.83.weight', 'image_encoder.blocks.3.attn.lora_B_k.83.weight', 'image_encoder.blocks.4.attn.lora_B_k.83.weight', 'image_encoder.blocks.6.attn.lora_B_v.83.weight', 'image_encoder.blocks.4.attn.lora_B_v.83.weight', 'image_encoder.blocks.10.attn.lora_B_k.83.weight', 'image_encoder.blocks.11.attn.lora_B_k.83.weight', 'image_encoder.blocks.5.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_v.83.weight', 'image_encoder.blocks.2.attn.lora_B_v.83.weight', 'image_encoder.blocks.10.attn.lora_B_v.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.83.weight', 'image_encoder.blocks.7.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_v.83.weight', 'image_encoder.blocks.6.attn.lora_B_k.83.weight', 'image_encoder.blocks.1.attn.lora_B_k.83.weight', 'classifier_pool.83.bias', 'image_encoder.blocks.8.attn.lora_B_v.83.weight', 'image_encoder.blocks.2.attn.lora_B_k.83.weight', 'image_encoder.blocks.8.attn.lora_B_k.83.weight', 'image_encoder.blocks.1.attn.lora_B_v.83.weight', 'classifier_pool.83.weight', 'image_encoder.blocks.7.attn.lora_B_v.83.weight', 'image_encoder.blocks.5.attn.lora_B_k.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.83.weight'}
2025-12-10 16:40:45,179 [inflora.py] => Task 83, Epoch 50/50 => Loss 0.022, Train_accy 98.44
Threshold:  0.9966
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 73/768 type remove
Layer 3 : 156/768 type remove
Layer 4 : 211/768 type remove
Layer 5 : 279/768 type remove
Layer 6 : 299/768 type remove
Layer 7 : 345/768 type remove
Layer 8 : 381/768 type remove
Layer 9 : 294/768 type retain
Layer 10 : 246/768 type retain
Layer 11 : 332/768 type retain
Layer 12 : 225/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:40:52,769 [trainer.py] => Time:131.4549596309662
4933 4933
4933 4933
2025-12-10 16:41:07,318 [trainer.py] => Time:14.549126625061035
2025-12-10 16:41:07,319 [inflora.py] => Exemplar size: 0
2025-12-10 16:41:07,319 [trainer.py] => CNN: {'total': np.float64(38.56), '00-01': np.float64(74.19), '02-03': np.float64(29.03), '04-05': np.float64(44.94), '06-07': np.float64(35.71), '08-09': np.float64(39.73), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(42.86), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(59.34), '22-23': np.float64(60.16), '24-25': np.float64(14.29), '26-27': np.float64(65.22), '28-29': np.float64(65.43), '30-31': np.float64(38.18), '32-33': np.float64(4.55), '34-35': np.float64(32.26), '36-37': np.float64(45.0), '38-39': np.float64(39.02), '40-41': np.float64(27.78), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(39.36), '50-51': np.float64(16.22), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(43.9), '62-63': np.float64(29.25), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(35.0), '74-75': np.float64(34.62), '76-77': np.float64(44.58), '78-79': np.float64(19.51), '80-81': np.float64(62.5), '82-83': np.float64(23.73), '84-85': np.float64(39.13), '86-87': np.float64(22.99), '88-89': np.float64(78.33), '90-91': np.float64(40.0), '92-93': np.float64(16.33), '94-95': np.float64(45.07), '96-97': np.float64(10.1), '98-99': np.float64(28.38), '100-101': np.float64(35.0), '102-103': np.float64(50.0), '104-105': np.float64(18.09), '106-107': np.float64(15.38), '108-109': np.float64(64.18), '110-111': np.float64(45.0), '112-113': np.float64(80.77), '114-115': np.float64(55.0), '116-117': np.float64(61.11), '118-119': np.float64(50.0), '120-121': np.float64(65.57), '122-123': np.float64(21.43), '124-125': np.float64(30.61), '126-127': np.float64(5.66), '128-129': np.float64(16.46), '130-131': np.float64(40.0), '132-133': np.float64(26.47), '134-135': np.float64(30.77), '136-137': np.float64(63.27), '138-139': np.float64(62.26), '140-141': np.float64(27.66), '142-143': np.float64(68.75), '144-145': np.float64(26.09), '146-147': np.float64(48.15), '148-149': np.float64(18.18), '150-151': np.float64(5.26), '152-153': np.float64(35.14), '154-155': np.float64(10.0), '156-157': np.float64(53.57), '158-159': np.float64(39.68), '160-161': np.float64(36.96), '162-163': np.float64(52.46), '164-165': np.float64(64.44), '166-167': np.float64(61.4), 'old': np.float64(38.29), 'new': np.float64(61.4)}
2025-12-10 16:41:07,319 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56)]
2025-12-10 16:41:07,319 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46)]
2025-12-10 16:41:07,320 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768]
2025-12-10 16:41:11,601 [trainer.py] => All params: 144526051
2025-12-10 16:41:11,612 [trainer.py] => Trainable params: 185858
2025-12-10 16:41:11,612 [inflora.py] => Learning on 168-170
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.84.weight', 'image_encoder.blocks.1.attn.lora_B_v.84.weight', 'classifier_pool.84.bias', 'image_encoder.blocks.8.attn.lora_B_v.84.weight', 'image_encoder.blocks.1.attn.lora_B_k.84.weight', 'image_encoder.blocks.9.attn.lora_B_k.84.weight', 'classifier_pool.84.weight', 'image_encoder.blocks.3.attn.lora_B_v.84.weight', 'image_encoder.blocks.3.attn.lora_B_k.84.weight', 'image_encoder.blocks.2.attn.lora_B_k.84.weight', 'image_encoder.blocks.4.attn.lora_B_v.84.weight', 'image_encoder.blocks.0.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_k.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.84.weight', 'image_encoder.blocks.0.attn.lora_B_k.84.weight', 'image_encoder.blocks.8.attn.lora_B_k.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.84.weight', 'image_encoder.blocks.10.attn.lora_B_v.84.weight', 'image_encoder.blocks.6.attn.lora_B_v.84.weight', 'image_encoder.blocks.6.attn.lora_B_k.84.weight', 'image_encoder.blocks.2.attn.lora_B_v.84.weight', 'image_encoder.blocks.4.attn.lora_B_k.84.weight'}
2025-12-10 16:43:38,466 [inflora.py] => Task 84, Epoch 50/50 => Loss 0.040, Train_accy 98.82
Threshold:  0.9968
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 74/768 type remove
Layer 3 : 158/768 type remove
Layer 4 : 216/768 type remove
Layer 5 : 288/768 type remove
Layer 6 : 308/768 type remove
Layer 7 : 353/768 type remove
Layer 8 : 379/768 type retain
Layer 9 : 285/768 type retain
Layer 10 : 235/768 type retain
Layer 11 : 321/768 type retain
Layer 12 : 218/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:43:46,950 [trainer.py] => Time:155.3372187614441
5005 5005
5005 5005
2025-12-10 16:44:01,760 [trainer.py] => Time:14.810027122497559
2025-12-10 16:44:01,761 [inflora.py] => Exemplar size: 0
2025-12-10 16:44:01,761 [trainer.py] => CNN: {'total': np.float64(38.08), '00-01': np.float64(74.19), '02-03': np.float64(30.65), '04-05': np.float64(46.07), '06-07': np.float64(28.57), '08-09': np.float64(39.73), '10-11': np.float64(17.07), '12-13': np.float64(37.04), '14-15': np.float64(41.07), '16-17': np.float64(18.57), '18-19': np.float64(53.57), '20-21': np.float64(60.44), '22-23': np.float64(57.81), '24-25': np.float64(14.29), '26-27': np.float64(66.09), '28-29': np.float64(61.73), '30-31': np.float64(38.18), '32-33': np.float64(6.82), '34-35': np.float64(25.81), '36-37': np.float64(46.67), '38-39': np.float64(39.02), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(39.36), '50-51': np.float64(13.51), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(5.45), '60-61': np.float64(41.46), '62-63': np.float64(26.42), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(32.5), '74-75': np.float64(34.62), '76-77': np.float64(43.37), '78-79': np.float64(21.95), '80-81': np.float64(62.5), '82-83': np.float64(24.58), '84-85': np.float64(39.13), '86-87': np.float64(28.74), '88-89': np.float64(76.67), '90-91': np.float64(37.5), '92-93': np.float64(14.29), '94-95': np.float64(43.66), '96-97': np.float64(8.08), '98-99': np.float64(29.73), '100-101': np.float64(35.0), '102-103': np.float64(50.0), '104-105': np.float64(18.09), '106-107': np.float64(10.26), '108-109': np.float64(59.7), '110-111': np.float64(41.67), '112-113': np.float64(78.85), '114-115': np.float64(55.0), '116-117': np.float64(58.33), '118-119': np.float64(42.31), '120-121': np.float64(63.93), '122-123': np.float64(21.43), '124-125': np.float64(32.65), '126-127': np.float64(5.66), '128-129': np.float64(15.19), '130-131': np.float64(40.0), '132-133': np.float64(26.47), '134-135': np.float64(30.77), '136-137': np.float64(57.14), '138-139': np.float64(64.15), '140-141': np.float64(27.66), '142-143': np.float64(66.67), '144-145': np.float64(26.09), '146-147': np.float64(48.15), '148-149': np.float64(18.18), '150-151': np.float64(5.26), '152-153': np.float64(40.54), '154-155': np.float64(10.0), '156-157': np.float64(55.36), '158-159': np.float64(31.75), '160-161': np.float64(39.13), '162-163': np.float64(49.18), '164-165': np.float64(62.22), '166-167': np.float64(61.4), '168-169': np.float64(45.83), 'old': np.float64(37.97), 'new': np.float64(45.83)}
2025-12-10 16:44:01,761 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08)]
2025-12-10 16:44:01,761 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56)]
2025-12-10 16:44:01,761 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981]
2025-12-10 16:44:13,233 [trainer.py] => All params: 144526051
2025-12-10 16:44:13,245 [trainer.py] => Trainable params: 185858
2025-12-10 16:44:13,245 [inflora.py] => Learning on 170-172
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.85.weight', 'image_encoder.blocks.7.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_v.85.weight', 'image_encoder.blocks.11.attn.lora_B_v.85.weight', 'image_encoder.blocks.3.attn.lora_B_k.85.weight', 'image_encoder.blocks.1.attn.lora_B_k.85.weight', 'image_encoder.blocks.2.attn.lora_B_v.85.weight', 'image_encoder.blocks.6.attn.lora_B_v.85.weight', 'image_encoder.blocks.0.attn.lora_B_k.85.weight', 'image_encoder.blocks.4.attn.lora_B_k.85.weight', 'image_encoder.blocks.6.attn.lora_B_k.85.weight', 'image_encoder.blocks.11.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.85.weight', 'image_encoder.blocks.5.attn.lora_B_k.85.weight', 'classifier_pool.85.weight', 'image_encoder.blocks.8.attn.lora_B_v.85.weight', 'image_encoder.blocks.4.attn.lora_B_v.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.85.weight', 'image_encoder.blocks.8.attn.lora_B_k.85.weight', 'image_encoder.blocks.5.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_k.85.weight', 'image_encoder.blocks.2.attn.lora_B_k.85.weight', 'image_encoder.blocks.7.attn.lora_B_k.85.weight', 'image_encoder.blocks.0.attn.lora_B_v.85.weight', 'classifier_pool.85.bias'}
2025-12-10 16:46:10,215 [inflora.py] => Task 85, Epoch 50/50 => Loss 0.025, Train_accy 98.73
Threshold:  0.997
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 77/768 type remove
Layer 3 : 163/768 type remove
Layer 4 : 220/768 type remove
Layer 5 : 296/768 type remove
Layer 6 : 315/768 type remove
Layer 7 : 360/768 type remove
Layer 8 : 370/768 type retain
Layer 9 : 277/768 type retain
Layer 10 : 229/768 type retain
Layer 11 : 309/768 type retain
Layer 12 : 200/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:46:18,075 [trainer.py] => Time:124.82964491844177
5072 5072
5072 5072
2025-12-10 16:46:33,089 [trainer.py] => Time:15.013567686080933
2025-12-10 16:46:33,089 [inflora.py] => Exemplar size: 0
2025-12-10 16:46:33,090 [trainer.py] => CNN: {'total': np.float64(38.31), '00-01': np.float64(73.12), '02-03': np.float64(30.65), '04-05': np.float64(47.19), '06-07': np.float64(28.57), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(38.89), '14-15': np.float64(35.71), '16-17': np.float64(14.29), '18-19': np.float64(55.36), '20-21': np.float64(60.44), '22-23': np.float64(58.59), '24-25': np.float64(12.24), '26-27': np.float64(64.35), '28-29': np.float64(64.2), '30-31': np.float64(39.09), '32-33': np.float64(0.0), '34-35': np.float64(25.81), '36-37': np.float64(46.67), '38-39': np.float64(43.9), '40-41': np.float64(33.33), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(39.36), '50-51': np.float64(13.51), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(5.45), '60-61': np.float64(43.9), '62-63': np.float64(29.25), '64-65': np.float64(10.0), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(37.5), '74-75': np.float64(34.62), '76-77': np.float64(40.96), '78-79': np.float64(23.17), '80-81': np.float64(62.5), '82-83': np.float64(25.42), '84-85': np.float64(39.13), '86-87': np.float64(25.29), '88-89': np.float64(76.67), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(45.07), '96-97': np.float64(10.1), '98-99': np.float64(29.73), '100-101': np.float64(33.33), '102-103': np.float64(44.83), '104-105': np.float64(20.21), '106-107': np.float64(10.26), '108-109': np.float64(55.22), '110-111': np.float64(45.0), '112-113': np.float64(82.69), '114-115': np.float64(55.0), '116-117': np.float64(58.33), '118-119': np.float64(44.23), '120-121': np.float64(60.66), '122-123': np.float64(21.43), '124-125': np.float64(32.65), '126-127': np.float64(3.77), '128-129': np.float64(12.66), '130-131': np.float64(37.78), '132-133': np.float64(26.47), '134-135': np.float64(30.77), '136-137': np.float64(59.18), '138-139': np.float64(62.26), '140-141': np.float64(29.79), '142-143': np.float64(70.83), '144-145': np.float64(26.09), '146-147': np.float64(51.85), '148-149': np.float64(18.18), '150-151': np.float64(7.89), '152-153': np.float64(43.24), '154-155': np.float64(10.0), '156-157': np.float64(55.36), '158-159': np.float64(31.75), '160-161': np.float64(36.96), '162-163': np.float64(50.82), '164-165': np.float64(61.11), '166-167': np.float64(61.4), '168-169': np.float64(43.06), '170-171': np.float64(38.81), 'old': np.float64(38.3), 'new': np.float64(38.81)}
2025-12-10 16:46:33,090 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31)]
2025-12-10 16:46:33,090 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7)]
2025-12-10 16:46:33,090 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918]
2025-12-10 16:46:39,691 [trainer.py] => All params: 144526051
2025-12-10 16:46:39,703 [trainer.py] => Trainable params: 185858
2025-12-10 16:46:39,703 [inflora.py] => Learning on 172-174
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.86.weight', 'image_encoder.blocks.5.attn.lora_B_k.86.weight', 'image_encoder.blocks.6.attn.lora_B_v.86.weight', 'image_encoder.blocks.4.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_v.86.weight', 'image_encoder.blocks.6.attn.lora_B_k.86.weight', 'image_encoder.blocks.5.attn.lora_B_v.86.weight', 'image_encoder.blocks.3.attn.lora_B_v.86.weight', 'image_encoder.blocks.4.attn.lora_B_v.86.weight', 'image_encoder.blocks.0.attn.lora_B_k.86.weight', 'image_encoder.blocks.11.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_k.86.weight', 'image_encoder.blocks.8.attn.lora_B_v.86.weight', 'image_encoder.blocks.9.attn.lora_B_k.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_k.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.86.weight', 'image_encoder.blocks.11.attn.lora_B_v.86.weight', 'image_encoder.blocks.0.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_v.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.86.weight', 'image_encoder.blocks.7.attn.lora_B_v.86.weight', 'classifier_pool.86.weight', 'classifier_pool.86.bias', 'image_encoder.blocks.7.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_v.86.weight'}
2025-12-10 16:48:30,219 [inflora.py] => Task 86, Epoch 50/50 => Loss 0.070, Train_accy 97.55
Threshold:  0.9972
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 77/768 type remove
Layer 3 : 164/768 type remove
Layer 4 : 221/768 type remove
Layer 5 : 298/768 type remove
Layer 6 : 317/768 type remove
Layer 7 : 362/768 type remove
Layer 8 : 368/768 type retain
Layer 9 : 276/768 type retain
Layer 10 : 228/768 type retain
Layer 11 : 307/768 type retain
Layer 12 : 180/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:48:37,975 [trainer.py] => Time:118.27191305160522
5125 5125
5125 5125
2025-12-10 16:48:53,120 [trainer.py] => Time:15.144777536392212
2025-12-10 16:48:53,120 [inflora.py] => Exemplar size: 0
2025-12-10 16:48:53,120 [trainer.py] => CNN: {'total': np.float64(37.85), '00-01': np.float64(75.27), '02-03': np.float64(30.65), '04-05': np.float64(46.07), '06-07': np.float64(26.79), '08-09': np.float64(38.36), '10-11': np.float64(21.95), '12-13': np.float64(38.89), '14-15': np.float64(37.5), '16-17': np.float64(14.29), '18-19': np.float64(50.0), '20-21': np.float64(59.34), '22-23': np.float64(58.59), '24-25': np.float64(16.33), '26-27': np.float64(66.09), '28-29': np.float64(61.73), '30-31': np.float64(36.36), '32-33': np.float64(9.09), '34-35': np.float64(29.03), '36-37': np.float64(48.33), '38-39': np.float64(43.9), '40-41': np.float64(33.33), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(40.43), '50-51': np.float64(13.51), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(5.45), '60-61': np.float64(43.9), '62-63': np.float64(30.19), '64-65': np.float64(6.67), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(35.0), '74-75': np.float64(37.18), '76-77': np.float64(38.55), '78-79': np.float64(24.39), '80-81': np.float64(65.28), '82-83': np.float64(23.73), '84-85': np.float64(32.61), '86-87': np.float64(26.44), '88-89': np.float64(76.67), '90-91': np.float64(37.5), '92-93': np.float64(16.33), '94-95': np.float64(42.25), '96-97': np.float64(10.1), '98-99': np.float64(31.08), '100-101': np.float64(35.0), '102-103': np.float64(48.28), '104-105': np.float64(19.15), '106-107': np.float64(12.82), '108-109': np.float64(52.24), '110-111': np.float64(40.0), '112-113': np.float64(78.85), '114-115': np.float64(55.0), '116-117': np.float64(52.78), '118-119': np.float64(44.23), '120-121': np.float64(60.66), '122-123': np.float64(23.21), '124-125': np.float64(32.65), '126-127': np.float64(3.77), '128-129': np.float64(12.66), '130-131': np.float64(35.56), '132-133': np.float64(20.59), '134-135': np.float64(28.21), '136-137': np.float64(59.18), '138-139': np.float64(60.38), '140-141': np.float64(27.66), '142-143': np.float64(68.75), '144-145': np.float64(30.43), '146-147': np.float64(48.15), '148-149': np.float64(15.15), '150-151': np.float64(7.89), '152-153': np.float64(35.14), '154-155': np.float64(12.0), '156-157': np.float64(51.79), '158-159': np.float64(30.16), '160-161': np.float64(36.96), '162-163': np.float64(54.1), '164-165': np.float64(60.0), '166-167': np.float64(59.65), '168-169': np.float64(45.83), '170-171': np.float64(40.3), '172-173': np.float64(28.3), 'old': np.float64(37.95), 'new': np.float64(28.3)}
2025-12-10 16:48:53,121 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85)]
2025-12-10 16:48:53,121 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8)]
2025-12-10 16:48:53,121 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927]
2025-12-10 16:48:59,595 [trainer.py] => All params: 144526051
2025-12-10 16:48:59,607 [trainer.py] => Trainable params: 185858
2025-12-10 16:48:59,607 [inflora.py] => Learning on 174-176
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.87.weight', 'image_encoder.blocks.5.attn.lora_B_k.87.weight', 'image_encoder.blocks.9.attn.lora_B_k.87.weight', 'image_encoder.blocks.4.attn.lora_B_k.87.weight', 'classifier_pool.87.bias', 'image_encoder.blocks.4.attn.lora_B_v.87.weight', 'image_encoder.blocks.2.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_v.87.weight', 'image_encoder.blocks.9.attn.lora_B_v.87.weight', 'image_encoder.blocks.7.attn.lora_B_k.87.weight', 'image_encoder.blocks.10.attn.lora_B_v.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.87.weight', 'image_encoder.blocks.7.attn.lora_B_v.87.weight', 'image_encoder.blocks.2.attn.lora_B_k.87.weight', 'classifier_pool.87.weight', 'image_encoder.blocks.1.attn.lora_B_v.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_k.87.weight', 'image_encoder.blocks.6.attn.lora_B_k.87.weight', 'image_encoder.blocks.11.attn.lora_B_k.87.weight', 'image_encoder.blocks.6.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.87.weight', 'image_encoder.blocks.11.attn.lora_B_v.87.weight', 'image_encoder.blocks.3.attn.lora_B_k.87.weight', 'image_encoder.blocks.5.attn.lora_B_v.87.weight', 'image_encoder.blocks.8.attn.lora_B_v.87.weight'}
2025-12-10 16:51:03,868 [inflora.py] => Task 87, Epoch 50/50 => Loss 0.015, Train_accy 99.61
Threshold:  0.9974
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 78/768 type remove
Layer 3 : 165/768 type remove
Layer 4 : 225/768 type remove
Layer 5 : 304/768 type remove
Layer 6 : 322/768 type remove
Layer 7 : 367/768 type remove
Layer 8 : 360/768 type retain
Layer 9 : 273/768 type retain
Layer 10 : 226/768 type retain
Layer 11 : 304/768 type retain
Layer 12 : 177/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:51:11,249 [trainer.py] => Time:131.6415765285492
5201 5201
5201 5201
2025-12-10 16:51:26,658 [trainer.py] => Time:15.408776760101318
2025-12-10 16:51:26,658 [inflora.py] => Exemplar size: 0
2025-12-10 16:51:26,658 [trainer.py] => CNN: {'total': np.float64(37.22), '00-01': np.float64(73.12), '02-03': np.float64(30.65), '04-05': np.float64(43.82), '06-07': np.float64(26.79), '08-09': np.float64(38.36), '10-11': np.float64(21.95), '12-13': np.float64(33.33), '14-15': np.float64(33.93), '16-17': np.float64(11.43), '18-19': np.float64(50.0), '20-21': np.float64(58.24), '22-23': np.float64(57.81), '24-25': np.float64(14.29), '26-27': np.float64(66.09), '28-29': np.float64(62.96), '30-31': np.float64(37.27), '32-33': np.float64(4.55), '34-35': np.float64(32.26), '36-37': np.float64(48.33), '38-39': np.float64(43.9), '40-41': np.float64(33.33), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(38.3), '50-51': np.float64(13.51), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(5.45), '60-61': np.float64(41.46), '62-63': np.float64(25.47), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(35.0), '74-75': np.float64(35.9), '76-77': np.float64(37.35), '78-79': np.float64(23.17), '80-81': np.float64(59.72), '82-83': np.float64(22.88), '84-85': np.float64(36.96), '86-87': np.float64(26.44), '88-89': np.float64(76.67), '90-91': np.float64(37.5), '92-93': np.float64(16.33), '94-95': np.float64(40.85), '96-97': np.float64(9.09), '98-99': np.float64(28.38), '100-101': np.float64(35.0), '102-103': np.float64(46.55), '104-105': np.float64(19.15), '106-107': np.float64(10.26), '108-109': np.float64(50.75), '110-111': np.float64(41.67), '112-113': np.float64(80.77), '114-115': np.float64(55.0), '116-117': np.float64(52.78), '118-119': np.float64(44.23), '120-121': np.float64(59.02), '122-123': np.float64(23.21), '124-125': np.float64(32.65), '126-127': np.float64(3.77), '128-129': np.float64(13.92), '130-131': np.float64(31.11), '132-133': np.float64(26.47), '134-135': np.float64(29.49), '136-137': np.float64(57.14), '138-139': np.float64(58.49), '140-141': np.float64(34.04), '142-143': np.float64(68.75), '144-145': np.float64(30.43), '146-147': np.float64(44.44), '148-149': np.float64(15.15), '150-151': np.float64(7.89), '152-153': np.float64(32.43), '154-155': np.float64(10.0), '156-157': np.float64(51.79), '158-159': np.float64(26.98), '160-161': np.float64(36.96), '162-163': np.float64(54.1), '164-165': np.float64(58.89), '166-167': np.float64(59.65), '168-169': np.float64(45.83), '170-171': np.float64(37.31), '172-173': np.float64(28.3), '174-175': np.float64(50.0), 'old': np.float64(37.03), 'new': np.float64(50.0)}
2025-12-10 16:51:26,659 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22)]
2025-12-10 16:51:26,659 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79)]
2025-12-10 16:51:26,659 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024]
2025-12-10 16:51:30,719 [trainer.py] => All params: 144526051
2025-12-10 16:51:30,731 [trainer.py] => Trainable params: 185858
2025-12-10 16:51:30,731 [inflora.py] => Learning on 176-178
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.88.weight', 'image_encoder.blocks.3.attn.lora_B_v.88.weight', 'image_encoder.blocks.11.attn.lora_B_v.88.weight', 'image_encoder.blocks.0.attn.lora_B_k.88.weight', 'image_encoder.blocks.7.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_k.88.weight', 'image_encoder.blocks.6.attn.lora_B_k.88.weight', 'image_encoder.blocks.9.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_k.88.weight', 'image_encoder.blocks.4.attn.lora_B_v.88.weight', 'classifier_pool.88.weight', 'image_encoder.blocks.8.attn.lora_B_k.88.weight', 'image_encoder.blocks.9.attn.lora_B_v.88.weight', 'image_encoder.blocks.10.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_k.88.weight', 'image_encoder.blocks.7.attn.lora_B_k.88.weight', 'image_encoder.blocks.8.attn.lora_B_v.88.weight', 'image_encoder.blocks.1.attn.lora_B_v.88.weight', 'image_encoder.blocks.11.attn.lora_B_k.88.weight', 'image_encoder.blocks.0.attn.lora_B_v.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.88.weight', 'image_encoder.blocks.2.attn.lora_B_k.88.weight', 'image_encoder.blocks.2.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_v.88.weight', 'classifier_pool.88.bias'}
2025-12-10 16:53:43,779 [inflora.py] => Task 88, Epoch 50/50 => Loss 0.082, Train_accy 96.10
Threshold:  0.9976
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 19/768 type remove
Layer 2 : 80/768 type remove
Layer 3 : 169/768 type remove
Layer 4 : 231/768 type remove
Layer 5 : 313/768 type remove
Layer 6 : 330/768 type remove
Layer 7 : 378/768 type remove
Layer 8 : 348/768 type retain
Layer 9 : 260/768 type retain
Layer 10 : 214/768 type retain
Layer 11 : 285/768 type retain
Layer 12 : 165/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:53:51,616 [trainer.py] => Time:140.88537049293518
5276 5276
5276 5276
2025-12-10 16:54:07,250 [trainer.py] => Time:15.633125066757202
2025-12-10 16:54:07,250 [inflora.py] => Exemplar size: 0
2025-12-10 16:54:07,251 [trainer.py] => CNN: {'total': np.float64(38.15), '00-01': np.float64(73.12), '02-03': np.float64(27.42), '04-05': np.float64(47.19), '06-07': np.float64(32.14), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(33.33), '14-15': np.float64(35.71), '16-17': np.float64(14.29), '18-19': np.float64(48.21), '20-21': np.float64(59.34), '22-23': np.float64(58.59), '24-25': np.float64(16.33), '26-27': np.float64(64.35), '28-29': np.float64(61.73), '30-31': np.float64(40.0), '32-33': np.float64(2.27), '34-35': np.float64(35.48), '36-37': np.float64(48.33), '38-39': np.float64(43.9), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(39.36), '50-51': np.float64(10.81), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(5.45), '60-61': np.float64(39.02), '62-63': np.float64(24.53), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(37.5), '74-75': np.float64(38.46), '76-77': np.float64(43.37), '78-79': np.float64(21.95), '80-81': np.float64(59.72), '82-83': np.float64(25.42), '84-85': np.float64(39.13), '86-87': np.float64(24.14), '88-89': np.float64(76.67), '90-91': np.float64(40.0), '92-93': np.float64(16.33), '94-95': np.float64(40.85), '96-97': np.float64(11.11), '98-99': np.float64(29.73), '100-101': np.float64(35.0), '102-103': np.float64(50.0), '104-105': np.float64(19.15), '106-107': np.float64(10.26), '108-109': np.float64(55.22), '110-111': np.float64(46.67), '112-113': np.float64(84.62), '114-115': np.float64(55.0), '116-117': np.float64(61.11), '118-119': np.float64(40.38), '120-121': np.float64(60.66), '122-123': np.float64(23.21), '124-125': np.float64(32.65), '126-127': np.float64(5.66), '128-129': np.float64(16.46), '130-131': np.float64(35.56), '132-133': np.float64(32.35), '134-135': np.float64(32.05), '136-137': np.float64(57.14), '138-139': np.float64(47.17), '140-141': np.float64(31.91), '142-143': np.float64(58.33), '144-145': np.float64(30.43), '146-147': np.float64(48.15), '148-149': np.float64(18.18), '150-151': np.float64(10.53), '152-153': np.float64(35.14), '154-155': np.float64(10.0), '156-157': np.float64(50.0), '158-159': np.float64(31.75), '160-161': np.float64(36.96), '162-163': np.float64(54.1), '164-165': np.float64(62.22), '166-167': np.float64(59.65), '168-169': np.float64(44.44), '170-171': np.float64(35.82), '172-173': np.float64(30.19), '174-175': np.float64(50.0), '176-177': np.float64(58.67), 'old': np.float64(37.86), 'new': np.float64(58.67)}
2025-12-10 16:54:07,251 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15)]
2025-12-10 16:54:07,252 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98)]
2025-12-10 16:54:07,252 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735]
2025-12-10 16:54:11,784 [trainer.py] => All params: 144526051
2025-12-10 16:54:11,796 [trainer.py] => Trainable params: 185858
2025-12-10 16:54:11,796 [inflora.py] => Learning on 178-180
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.89.weight', 'classifier_pool.89.weight', 'image_encoder.blocks.7.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.89.weight', 'image_encoder.blocks.3.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_v.89.weight', 'image_encoder.blocks.10.attn.lora_B_k.89.weight', 'image_encoder.blocks.0.attn.lora_B_v.89.weight', 'image_encoder.blocks.9.attn.lora_B_k.89.weight', 'image_encoder.blocks.7.attn.lora_B_v.89.weight', 'image_encoder.blocks.3.attn.lora_B_v.89.weight', 'image_encoder.blocks.8.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_k.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.89.weight', 'image_encoder.blocks.2.attn.lora_B_k.89.weight', 'image_encoder.blocks.0.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_v.89.weight', 'image_encoder.blocks.2.attn.lora_B_v.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_k.89.weight', 'classifier_pool.89.bias', 'image_encoder.blocks.9.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_k.89.weight'}
2025-12-10 16:55:55,216 [inflora.py] => Task 89, Epoch 50/50 => Loss 0.067, Train_accy 98.28
Threshold:  0.9978
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 28/768 type remove
Layer 2 : 84/768 type remove
Layer 3 : 176/768 type remove
Layer 4 : 237/768 type remove
Layer 5 : 321/768 type remove
Layer 6 : 341/768 type remove
Layer 7 : 375/768 type retain
Layer 8 : 329/768 type retain
Layer 9 : 241/768 type retain
Layer 10 : 196/768 type retain
Layer 11 : 260/768 type retain
Layer 12 : 153/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:56:02,162 [trainer.py] => Time:110.36597108840942
5306 5306
5306 5306
2025-12-10 16:56:17,822 [trainer.py] => Time:15.659748792648315
2025-12-10 16:56:17,823 [inflora.py] => Exemplar size: 0
2025-12-10 16:56:17,823 [trainer.py] => CNN: {'total': np.float64(38.33), '00-01': np.float64(74.19), '02-03': np.float64(27.42), '04-05': np.float64(46.07), '06-07': np.float64(30.36), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(33.33), '14-15': np.float64(42.86), '16-17': np.float64(17.14), '18-19': np.float64(46.43), '20-21': np.float64(60.44), '22-23': np.float64(57.81), '24-25': np.float64(14.29), '26-27': np.float64(64.35), '28-29': np.float64(60.49), '30-31': np.float64(40.0), '32-33': np.float64(2.27), '34-35': np.float64(32.26), '36-37': np.float64(46.67), '38-39': np.float64(41.46), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(40.43), '50-51': np.float64(10.81), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(5.45), '60-61': np.float64(39.02), '62-63': np.float64(26.42), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(37.5), '74-75': np.float64(39.74), '76-77': np.float64(42.17), '78-79': np.float64(24.39), '80-81': np.float64(61.11), '82-83': np.float64(25.42), '84-85': np.float64(41.3), '86-87': np.float64(25.29), '88-89': np.float64(76.67), '90-91': np.float64(40.0), '92-93': np.float64(16.33), '94-95': np.float64(36.62), '96-97': np.float64(10.1), '98-99': np.float64(28.38), '100-101': np.float64(31.67), '102-103': np.float64(51.72), '104-105': np.float64(19.15), '106-107': np.float64(12.82), '108-109': np.float64(56.72), '110-111': np.float64(46.67), '112-113': np.float64(84.62), '114-115': np.float64(52.5), '116-117': np.float64(61.11), '118-119': np.float64(38.46), '120-121': np.float64(59.02), '122-123': np.float64(25.0), '124-125': np.float64(32.65), '126-127': np.float64(5.66), '128-129': np.float64(16.46), '130-131': np.float64(35.56), '132-133': np.float64(29.41), '134-135': np.float64(32.05), '136-137': np.float64(57.14), '138-139': np.float64(47.17), '140-141': np.float64(31.91), '142-143': np.float64(58.33), '144-145': np.float64(28.26), '146-147': np.float64(48.15), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(35.14), '154-155': np.float64(12.0), '156-157': np.float64(50.0), '158-159': np.float64(31.75), '160-161': np.float64(36.96), '162-163': np.float64(55.74), '164-165': np.float64(60.0), '166-167': np.float64(57.89), '168-169': np.float64(43.06), '170-171': np.float64(37.31), '172-173': np.float64(33.96), '174-175': np.float64(50.0), '176-177': np.float64(58.67), '178-179': np.float64(60.0), 'old': np.float64(38.21), 'new': np.float64(60.0)}
2025-12-10 16:56:17,823 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33)]
2025-12-10 16:56:17,823 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0)]
2025-12-10 16:56:17,823 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347]
2025-12-10 16:56:23,646 [trainer.py] => All params: 144526051
2025-12-10 16:56:23,657 [trainer.py] => Trainable params: 185858
2025-12-10 16:56:23,657 [inflora.py] => Learning on 180-182
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.90.weight', 'image_encoder.blocks.6.attn.lora_B_v.90.weight', 'image_encoder.blocks.0.attn.lora_B_k.90.weight', 'image_encoder.blocks.3.attn.lora_B_k.90.weight', 'image_encoder.blocks.10.attn.lora_B_v.90.weight', 'image_encoder.blocks.7.attn.lora_B_k.90.weight', 'image_encoder.blocks.1.attn.lora_B_v.90.weight', 'image_encoder.blocks.7.attn.lora_B_v.90.weight', 'image_encoder.blocks.1.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.90.weight', 'image_encoder.blocks.9.attn.lora_B_v.90.weight', 'image_encoder.blocks.6.attn.lora_B_k.90.weight', 'image_encoder.blocks.10.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_k.90.weight', 'image_encoder.blocks.2.attn.lora_B_v.90.weight', 'image_encoder.blocks.0.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_v.90.weight', 'image_encoder.blocks.3.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_v.90.weight', 'image_encoder.blocks.9.attn.lora_B_k.90.weight', 'image_encoder.blocks.2.attn.lora_B_k.90.weight', 'classifier_pool.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.90.weight', 'classifier_pool.90.bias'}
2025-12-10 16:59:06,826 [inflora.py] => Task 90, Epoch 50/50 => Loss 0.125, Train_accy 96.01
Threshold:  0.998
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 28/768 type remove
Layer 2 : 86/768 type remove
Layer 3 : 183/768 type remove
Layer 4 : 248/768 type remove
Layer 5 : 334/768 type remove
Layer 6 : 355/768 type remove
Layer 7 : 362/768 type retain
Layer 8 : 320/768 type retain
Layer 9 : 232/768 type retain
Layer 10 : 183/768 type retain
Layer 11 : 242/768 type retain
Layer 12 : 145/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 16:59:15,194 [trainer.py] => Time:171.53660988807678
5400 5400
5400 5400
2025-12-10 16:59:31,226 [trainer.py] => Time:16.03137969970703
2025-12-10 16:59:31,226 [inflora.py] => Exemplar size: 0
2025-12-10 16:59:31,226 [trainer.py] => CNN: {'total': np.float64(38.54), '00-01': np.float64(73.12), '02-03': np.float64(25.81), '04-05': np.float64(44.94), '06-07': np.float64(32.14), '08-09': np.float64(41.1), '10-11': np.float64(24.39), '12-13': np.float64(27.78), '14-15': np.float64(41.07), '16-17': np.float64(20.0), '18-19': np.float64(44.64), '20-21': np.float64(57.14), '22-23': np.float64(59.38), '24-25': np.float64(14.29), '26-27': np.float64(64.35), '28-29': np.float64(61.73), '30-31': np.float64(39.09), '32-33': np.float64(2.27), '34-35': np.float64(25.81), '36-37': np.float64(45.0), '38-39': np.float64(43.9), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(41.49), '50-51': np.float64(8.11), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(27.36), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(30.14), '72-73': np.float64(35.0), '74-75': np.float64(41.03), '76-77': np.float64(44.58), '78-79': np.float64(23.17), '80-81': np.float64(59.72), '82-83': np.float64(27.97), '84-85': np.float64(41.3), '86-87': np.float64(27.59), '88-89': np.float64(75.0), '90-91': np.float64(40.0), '92-93': np.float64(16.33), '94-95': np.float64(38.03), '96-97': np.float64(9.09), '98-99': np.float64(28.38), '100-101': np.float64(31.67), '102-103': np.float64(48.28), '104-105': np.float64(22.34), '106-107': np.float64(12.82), '108-109': np.float64(61.19), '110-111': np.float64(36.67), '112-113': np.float64(80.77), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(42.31), '120-121': np.float64(57.38), '122-123': np.float64(21.43), '124-125': np.float64(32.65), '126-127': np.float64(3.77), '128-129': np.float64(16.46), '130-131': np.float64(33.33), '132-133': np.float64(29.41), '134-135': np.float64(34.62), '136-137': np.float64(55.1), '138-139': np.float64(52.83), '140-141': np.float64(31.91), '142-143': np.float64(60.42), '144-145': np.float64(32.61), '146-147': np.float64(48.15), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(37.84), '154-155': np.float64(10.0), '156-157': np.float64(50.0), '158-159': np.float64(33.33), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(60.0), '166-167': np.float64(56.14), '168-169': np.float64(48.61), '170-171': np.float64(38.81), '172-173': np.float64(32.08), '174-175': np.float64(51.32), '176-177': np.float64(58.67), '178-179': np.float64(63.33), '180-181': np.float64(40.43), 'old': np.float64(38.5), 'new': np.float64(40.43)}
2025-12-10 16:59:31,226 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54)]
2025-12-10 16:59:31,227 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87)]
2025-12-10 16:59:31,227 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407]
2025-12-10 16:59:36,453 [trainer.py] => All params: 144526051
2025-12-10 16:59:36,465 [trainer.py] => Trainable params: 185858
2025-12-10 16:59:36,465 [inflora.py] => Learning on 182-184
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.91.weight', 'image_encoder.blocks.1.attn.lora_B_v.91.weight', 'image_encoder.blocks.8.attn.lora_B_v.91.weight', 'image_encoder.blocks.4.attn.lora_B_k.91.weight', 'image_encoder.blocks.5.attn.lora_B_k.91.weight', 'image_encoder.blocks.11.attn.lora_B_k.91.weight', 'image_encoder.blocks.4.attn.lora_B_v.91.weight', 'image_encoder.blocks.7.attn.lora_B_v.91.weight', 'image_encoder.blocks.1.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_v.91.weight', 'image_encoder.blocks.5.attn.lora_B_v.91.weight', 'image_encoder.blocks.0.attn.lora_B_k.91.weight', 'image_encoder.blocks.0.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.91.weight', 'image_encoder.blocks.9.attn.lora_B_v.91.weight', 'classifier_pool.91.bias', 'image_encoder.blocks.3.attn.lora_B_k.91.weight', 'image_encoder.blocks.10.attn.lora_B_v.91.weight', 'image_encoder.blocks.10.attn.lora_B_k.91.weight', 'image_encoder.blocks.3.attn.lora_B_v.91.weight', 'image_encoder.blocks.11.attn.lora_B_v.91.weight', 'classifier_pool.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.91.weight', 'image_encoder.blocks.6.attn.lora_B_k.91.weight'}
2025-12-10 17:01:24,383 [inflora.py] => Task 91, Epoch 50/50 => Loss 0.064, Train_accy 97.95
Threshold:  0.9982
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 31/768 type remove
Layer 2 : 91/768 type remove
Layer 3 : 195/768 type remove
Layer 4 : 263/768 type remove
Layer 5 : 349/768 type remove
Layer 6 : 370/768 type remove
Layer 7 : 345/768 type retain
Layer 8 : 301/768 type retain
Layer 9 : 210/768 type retain
Layer 10 : 163/768 type retain
Layer 11 : 219/768 type retain
Layer 12 : 128/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:01:31,420 [trainer.py] => Time:114.95583939552307
5441 5441
5441 5441
2025-12-10 17:01:47,503 [trainer.py] => Time:16.082542419433594
2025-12-10 17:01:47,504 [inflora.py] => Exemplar size: 0
2025-12-10 17:01:47,504 [trainer.py] => CNN: {'total': np.float64(38.5), '00-01': np.float64(72.04), '02-03': np.float64(24.19), '04-05': np.float64(44.94), '06-07': np.float64(35.71), '08-09': np.float64(38.36), '10-11': np.float64(24.39), '12-13': np.float64(27.78), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(46.43), '20-21': np.float64(63.74), '22-23': np.float64(59.38), '24-25': np.float64(14.29), '26-27': np.float64(66.09), '28-29': np.float64(59.26), '30-31': np.float64(40.0), '32-33': np.float64(2.27), '34-35': np.float64(29.03), '36-37': np.float64(45.0), '38-39': np.float64(43.9), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(41.49), '50-51': np.float64(8.11), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(36.59), '62-63': np.float64(27.36), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(40.0), '74-75': np.float64(41.03), '76-77': np.float64(44.58), '78-79': np.float64(24.39), '80-81': np.float64(61.11), '82-83': np.float64(27.97), '84-85': np.float64(41.3), '86-87': np.float64(27.59), '88-89': np.float64(76.67), '90-91': np.float64(37.5), '92-93': np.float64(16.33), '94-95': np.float64(36.62), '96-97': np.float64(9.09), '98-99': np.float64(28.38), '100-101': np.float64(31.67), '102-103': np.float64(46.55), '104-105': np.float64(20.21), '106-107': np.float64(15.38), '108-109': np.float64(61.19), '110-111': np.float64(38.33), '112-113': np.float64(80.77), '114-115': np.float64(52.5), '116-117': np.float64(58.33), '118-119': np.float64(42.31), '120-121': np.float64(57.38), '122-123': np.float64(21.43), '124-125': np.float64(34.69), '126-127': np.float64(3.77), '128-129': np.float64(16.46), '130-131': np.float64(35.56), '132-133': np.float64(26.47), '134-135': np.float64(34.62), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(29.79), '142-143': np.float64(64.58), '144-145': np.float64(32.61), '146-147': np.float64(48.15), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(35.14), '154-155': np.float64(10.0), '156-157': np.float64(48.21), '158-159': np.float64(34.92), '160-161': np.float64(36.96), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(47.22), '170-171': np.float64(41.79), '172-173': np.float64(32.08), '174-175': np.float64(53.95), '176-177': np.float64(58.67), '178-179': np.float64(56.67), '180-181': np.float64(38.3), '182-183': np.float64(21.95), 'old': np.float64(38.63), 'new': np.float64(21.95)}
2025-12-10 17:01:47,504 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5)]
2025-12-10 17:01:47,504 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79)]
2025-12-10 17:01:47,504 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894]
2025-12-10 17:01:53,705 [trainer.py] => All params: 144526051
2025-12-10 17:01:53,717 [trainer.py] => Trainable params: 185858
2025-12-10 17:01:53,717 [inflora.py] => Learning on 184-186
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.92.weight', 'image_encoder.blocks.2.attn.lora_B_k.92.weight', 'image_encoder.blocks.3.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_v.92.weight', 'image_encoder.blocks.11.attn.lora_B_k.92.weight', 'classifier_pool.92.weight', 'image_encoder.blocks.4.attn.lora_B_v.92.weight', 'image_encoder.blocks.11.attn.lora_B_v.92.weight', 'image_encoder.blocks.6.attn.lora_B_v.92.weight', 'image_encoder.blocks.4.attn.lora_B_k.92.weight', 'image_encoder.blocks.10.attn.lora_B_k.92.weight', 'image_encoder.blocks.7.attn.lora_B_v.92.weight', 'image_encoder.blocks.5.attn.lora_B_k.92.weight', 'image_encoder.blocks.9.attn.lora_B_k.92.weight', 'image_encoder.blocks.3.attn.lora_B_k.92.weight', 'image_encoder.blocks.8.attn.lora_B_k.92.weight', 'classifier_pool.92.bias', 'image_encoder.blocks.9.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.92.weight', 'image_encoder.blocks.0.attn.lora_B_k.92.weight', 'image_encoder.blocks.10.attn.lora_B_v.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.92.weight', 'image_encoder.blocks.0.attn.lora_B_v.92.weight', 'image_encoder.blocks.7.attn.lora_B_k.92.weight', 'image_encoder.blocks.8.attn.lora_B_v.92.weight', 'image_encoder.blocks.2.attn.lora_B_v.92.weight'}
2025-12-10 17:03:40,442 [inflora.py] => Task 92, Epoch 50/50 => Loss 0.064, Train_accy 97.83
Threshold:  0.9984
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 31/768 type remove
Layer 2 : 91/768 type remove
Layer 3 : 196/768 type remove
Layer 4 : 266/768 type remove
Layer 5 : 356/768 type remove
Layer 6 : 378/768 type remove
Layer 7 : 334/768 type retain
Layer 8 : 286/768 type retain
Layer 9 : 197/768 type retain
Layer 10 : 153/768 type retain
Layer 11 : 205/768 type retain
Layer 12 : 119/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:03:48,927 [trainer.py] => Time:115.21061372756958
5480 5480
5480 5480
2025-12-10 17:04:05,138 [trainer.py] => Time:16.210365295410156
2025-12-10 17:04:05,139 [inflora.py] => Exemplar size: 0
2025-12-10 17:04:05,139 [trainer.py] => CNN: {'total': np.float64(38.39), '00-01': np.float64(69.89), '02-03': np.float64(25.81), '04-05': np.float64(43.82), '06-07': np.float64(32.14), '08-09': np.float64(36.99), '10-11': np.float64(21.95), '12-13': np.float64(29.63), '14-15': np.float64(41.07), '16-17': np.float64(17.14), '18-19': np.float64(46.43), '20-21': np.float64(63.74), '22-23': np.float64(60.94), '24-25': np.float64(12.24), '26-27': np.float64(66.09), '28-29': np.float64(60.49), '30-31': np.float64(40.0), '32-33': np.float64(2.27), '34-35': np.float64(29.03), '36-37': np.float64(48.33), '38-39': np.float64(43.9), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(40.43), '50-51': np.float64(8.11), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(36.59), '62-63': np.float64(27.36), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(40.0), '74-75': np.float64(43.59), '76-77': np.float64(43.37), '78-79': np.float64(24.39), '80-81': np.float64(61.11), '82-83': np.float64(27.97), '84-85': np.float64(43.48), '86-87': np.float64(26.44), '88-89': np.float64(75.0), '90-91': np.float64(37.5), '92-93': np.float64(16.33), '94-95': np.float64(36.62), '96-97': np.float64(7.07), '98-99': np.float64(28.38), '100-101': np.float64(31.67), '102-103': np.float64(51.72), '104-105': np.float64(19.15), '106-107': np.float64(17.95), '108-109': np.float64(55.22), '110-111': np.float64(35.0), '112-113': np.float64(80.77), '114-115': np.float64(52.5), '116-117': np.float64(58.33), '118-119': np.float64(42.31), '120-121': np.float64(57.38), '122-123': np.float64(25.0), '124-125': np.float64(34.69), '126-127': np.float64(3.77), '128-129': np.float64(15.19), '130-131': np.float64(40.0), '132-133': np.float64(32.35), '134-135': np.float64(35.9), '136-137': np.float64(57.14), '138-139': np.float64(50.94), '140-141': np.float64(31.91), '142-143': np.float64(62.5), '144-145': np.float64(34.78), '146-147': np.float64(48.15), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(35.14), '154-155': np.float64(10.0), '156-157': np.float64(44.64), '158-159': np.float64(33.33), '160-161': np.float64(36.96), '162-163': np.float64(54.1), '164-165': np.float64(55.56), '166-167': np.float64(56.14), '168-169': np.float64(47.22), '170-171': np.float64(41.79), '172-173': np.float64(32.08), '174-175': np.float64(50.0), '176-177': np.float64(58.67), '178-179': np.float64(60.0), '180-181': np.float64(38.3), '182-183': np.float64(21.95), '184-185': np.float64(48.72), 'old': np.float64(38.32), 'new': np.float64(48.72)}
2025-12-10 17:04:05,139 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39)]
2025-12-10 17:04:05,139 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8)]
2025-12-10 17:04:05,139 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409]
2025-12-10 17:04:12,376 [trainer.py] => All params: 144526051
2025-12-10 17:04:12,388 [trainer.py] => Trainable params: 185858
2025-12-10 17:04:12,388 [inflora.py] => Learning on 186-188
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_v.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.93.weight', 'image_encoder.blocks.7.attn.lora_B_k.93.weight', 'image_encoder.blocks.8.attn.lora_B_k.93.weight', 'image_encoder.blocks.6.attn.lora_B_v.93.weight', 'image_encoder.blocks.5.attn.lora_B_v.93.weight', 'classifier_pool.93.bias', 'image_encoder.blocks.6.attn.lora_B_k.93.weight', 'image_encoder.blocks.3.attn.lora_B_k.93.weight', 'image_encoder.blocks.4.attn.lora_B_v.93.weight', 'classifier_pool.93.weight', 'image_encoder.blocks.11.attn.lora_B_v.93.weight', 'image_encoder.blocks.10.attn.lora_B_v.93.weight', 'image_encoder.blocks.2.attn.lora_B_v.93.weight', 'image_encoder.blocks.8.attn.lora_B_v.93.weight', 'image_encoder.blocks.10.attn.lora_B_k.93.weight', 'image_encoder.blocks.3.attn.lora_B_v.93.weight', 'image_encoder.blocks.1.attn.lora_B_k.93.weight', 'image_encoder.blocks.5.attn.lora_B_k.93.weight', 'image_encoder.blocks.2.attn.lora_B_k.93.weight', 'image_encoder.blocks.7.attn.lora_B_v.93.weight', 'image_encoder.blocks.11.attn.lora_B_k.93.weight', 'image_encoder.blocks.0.attn.lora_B_v.93.weight', 'image_encoder.blocks.1.attn.lora_B_v.93.weight'}
2025-12-10 17:07:03,002 [inflora.py] => Task 93, Epoch 50/50 => Loss 0.052, Train_accy 98.35
Threshold:  0.9986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 31/768 type remove
Layer 2 : 96/768 type remove
Layer 3 : 209/768 type remove
Layer 4 : 285/768 type remove
Layer 5 : 378/768 type remove
Layer 6 : 367/768 type retain
Layer 7 : 311/768 type retain
Layer 8 : 263/768 type retain
Layer 9 : 177/768 type retain
Layer 10 : 133/768 type retain
Layer 11 : 180/768 type retain
Layer 12 : 107/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:07:11,478 [trainer.py] => Time:179.09071016311646
5592 5592
5592 5592
2025-12-10 17:07:28,012 [trainer.py] => Time:16.532933712005615
2025-12-10 17:07:28,012 [inflora.py] => Exemplar size: 0
2025-12-10 17:07:28,012 [trainer.py] => CNN: {'total': np.float64(38.61), '00-01': np.float64(74.19), '02-03': np.float64(25.81), '04-05': np.float64(47.19), '06-07': np.float64(32.14), '08-09': np.float64(39.73), '10-11': np.float64(24.39), '12-13': np.float64(29.63), '14-15': np.float64(42.86), '16-17': np.float64(22.86), '18-19': np.float64(42.86), '20-21': np.float64(64.84), '22-23': np.float64(60.16), '24-25': np.float64(12.24), '26-27': np.float64(64.35), '28-29': np.float64(60.49), '30-31': np.float64(38.18), '32-33': np.float64(9.09), '34-35': np.float64(25.81), '36-37': np.float64(45.0), '38-39': np.float64(39.02), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(35.11), '50-51': np.float64(5.41), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(5.45), '60-61': np.float64(39.02), '62-63': np.float64(27.36), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(40.0), '74-75': np.float64(41.03), '76-77': np.float64(44.58), '78-79': np.float64(23.17), '80-81': np.float64(61.11), '82-83': np.float64(26.27), '84-85': np.float64(39.13), '86-87': np.float64(31.03), '88-89': np.float64(80.0), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(35.21), '96-97': np.float64(10.1), '98-99': np.float64(27.03), '100-101': np.float64(30.0), '102-103': np.float64(46.55), '104-105': np.float64(19.15), '106-107': np.float64(15.38), '108-109': np.float64(58.21), '110-111': np.float64(38.33), '112-113': np.float64(80.77), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(46.15), '120-121': np.float64(54.1), '122-123': np.float64(23.21), '124-125': np.float64(34.69), '126-127': np.float64(1.89), '128-129': np.float64(13.92), '130-131': np.float64(42.22), '132-133': np.float64(32.35), '134-135': np.float64(30.77), '136-137': np.float64(51.02), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(64.58), '144-145': np.float64(26.09), '146-147': np.float64(51.85), '148-149': np.float64(24.24), '150-151': np.float64(10.53), '152-153': np.float64(37.84), '154-155': np.float64(10.0), '156-157': np.float64(48.21), '158-159': np.float64(39.68), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(44.44), '170-171': np.float64(37.31), '172-173': np.float64(32.08), '174-175': np.float64(50.0), '176-177': np.float64(56.0), '178-179': np.float64(60.0), '180-181': np.float64(42.55), '182-183': np.float64(24.39), '184-185': np.float64(48.72), '186-187': np.float64(51.79), 'old': np.float64(38.34), 'new': np.float64(51.79)}
2025-12-10 17:07:28,013 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61)]
2025-12-10 17:07:28,013 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01)]
2025-12-10 17:07:28,013 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084]
2025-12-10 17:07:34,636 [trainer.py] => All params: 144526051
2025-12-10 17:07:34,647 [trainer.py] => Trainable params: 185858
2025-12-10 17:07:34,648 [inflora.py] => Learning on 188-190
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_v.94.weight', 'classifier_pool.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_k.94.weight', 'image_encoder.blocks.6.attn.lora_B_v.94.weight', 'image_encoder.blocks.2.attn.lora_B_k.94.weight', 'image_encoder.blocks.0.attn.lora_B_k.94.weight', 'image_encoder.blocks.8.attn.lora_B_v.94.weight', 'image_encoder.blocks.11.attn.lora_B_v.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.94.weight', 'image_encoder.blocks.5.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_k.94.weight', 'image_encoder.blocks.8.attn.lora_B_k.94.weight', 'image_encoder.blocks.6.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_v.94.weight', 'image_encoder.blocks.9.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_v.94.weight', 'image_encoder.blocks.5.attn.lora_B_k.94.weight', 'image_encoder.blocks.11.attn.lora_B_k.94.weight', 'image_encoder.blocks.9.attn.lora_B_k.94.weight', 'image_encoder.blocks.2.attn.lora_B_v.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.94.weight', 'classifier_pool.94.bias', 'image_encoder.blocks.10.attn.lora_B_k.94.weight'}
2025-12-10 17:10:07,358 [inflora.py] => Task 94, Epoch 50/50 => Loss 0.062, Train_accy 97.79
Threshold:  0.9988
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 31/768 type remove
Layer 2 : 101/768 type remove
Layer 3 : 221/768 type remove
Layer 4 : 302/768 type remove
Layer 5 : 371/768 type retain
Layer 6 : 348/768 type retain
Layer 7 : 292/768 type retain
Layer 8 : 241/768 type retain
Layer 9 : 159/768 type retain
Layer 10 : 116/768 type retain
Layer 11 : 158/768 type retain
Layer 12 : 95/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:10:16,910 [trainer.py] => Time:162.2621626853943
5683 5683
5683 5683
2025-12-10 17:10:33,745 [trainer.py] => Time:16.83494806289673
2025-12-10 17:10:33,745 [inflora.py] => Exemplar size: 0
2025-12-10 17:10:33,746 [trainer.py] => CNN: {'total': np.float64(39.36), '00-01': np.float64(74.19), '02-03': np.float64(25.81), '04-05': np.float64(48.31), '06-07': np.float64(33.93), '08-09': np.float64(39.73), '10-11': np.float64(24.39), '12-13': np.float64(27.78), '14-15': np.float64(42.86), '16-17': np.float64(24.29), '18-19': np.float64(42.86), '20-21': np.float64(65.93), '22-23': np.float64(61.72), '24-25': np.float64(18.37), '26-27': np.float64(63.48), '28-29': np.float64(60.49), '30-31': np.float64(40.0), '32-33': np.float64(4.55), '34-35': np.float64(25.81), '36-37': np.float64(46.67), '38-39': np.float64(41.46), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(36.17), '50-51': np.float64(8.11), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(41.46), '62-63': np.float64(24.53), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(28.77), '72-73': np.float64(37.5), '74-75': np.float64(42.31), '76-77': np.float64(46.99), '78-79': np.float64(25.61), '80-81': np.float64(61.11), '82-83': np.float64(26.27), '84-85': np.float64(45.65), '86-87': np.float64(26.44), '88-89': np.float64(80.0), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(35.21), '96-97': np.float64(11.11), '98-99': np.float64(25.68), '100-101': np.float64(30.0), '102-103': np.float64(44.83), '104-105': np.float64(17.02), '106-107': np.float64(17.95), '108-109': np.float64(61.19), '110-111': np.float64(40.0), '112-113': np.float64(78.85), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(44.23), '120-121': np.float64(52.46), '122-123': np.float64(25.0), '124-125': np.float64(34.69), '126-127': np.float64(7.55), '128-129': np.float64(15.19), '130-131': np.float64(37.78), '132-133': np.float64(32.35), '134-135': np.float64(32.05), '136-137': np.float64(55.1), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(64.58), '144-145': np.float64(21.74), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(37.84), '154-155': np.float64(10.0), '156-157': np.float64(50.0), '158-159': np.float64(34.92), '160-161': np.float64(34.78), '162-163': np.float64(54.1), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(44.44), '170-171': np.float64(38.81), '172-173': np.float64(33.96), '174-175': np.float64(51.32), '176-177': np.float64(56.0), '178-179': np.float64(60.0), '180-181': np.float64(42.55), '182-183': np.float64(24.39), '184-185': np.float64(48.72), '186-187': np.float64(51.79), '188-189': np.float64(65.93), 'old': np.float64(38.93), 'new': np.float64(65.93)}
2025-12-10 17:10:33,746 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36)]
2025-12-10 17:10:33,746 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09)]
2025-12-10 17:10:33,746 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266]
2025-12-10 17:10:42,260 [trainer.py] => All params: 144526051
2025-12-10 17:10:42,272 [trainer.py] => Trainable params: 185858
2025-12-10 17:10:42,272 [inflora.py] => Learning on 190-192
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.95.weight', 'image_encoder.blocks.0.attn.lora_B_k.95.weight', 'image_encoder.blocks.4.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_k.95.weight', 'image_encoder.blocks.7.attn.lora_B_v.95.weight', 'image_encoder.blocks.1.attn.lora_B_k.95.weight', 'image_encoder.blocks.3.attn.lora_B_k.95.weight', 'image_encoder.blocks.6.attn.lora_B_k.95.weight', 'image_encoder.blocks.0.attn.lora_B_v.95.weight', 'image_encoder.blocks.3.attn.lora_B_v.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_v.95.weight', 'image_encoder.blocks.7.attn.lora_B_k.95.weight', 'image_encoder.blocks.10.attn.lora_B_v.95.weight', 'image_encoder.blocks.6.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_k.95.weight', 'image_encoder.blocks.10.attn.lora_B_k.95.weight', 'image_encoder.blocks.4.attn.lora_B_k.95.weight', 'classifier_pool.95.weight', 'image_encoder.blocks.1.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_k.95.weight', 'classifier_pool.95.bias', 'image_encoder.blocks.8.attn.lora_B_v.95.weight'}
2025-12-10 17:13:15,343 [inflora.py] => Task 95, Epoch 50/50 => Loss 0.049, Train_accy 98.61
Threshold:  0.999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 31/768 type remove
Layer 2 : 104/768 type remove
Layer 3 : 222/768 type remove
Layer 4 : 304/768 type remove
Layer 5 : 367/768 type retain
Layer 6 : 342/768 type retain
Layer 7 : 281/768 type retain
Layer 8 : 233/768 type retain
Layer 9 : 152/768 type retain
Layer 10 : 111/768 type retain
Layer 11 : 153/768 type retain
Layer 12 : 92/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:13:23,523 [trainer.py] => Time:161.25082564353943
5771 5771
5771 5771
2025-12-10 17:13:40,718 [trainer.py] => Time:17.19506001472473
2025-12-10 17:13:40,719 [inflora.py] => Exemplar size: 0
2025-12-10 17:13:40,719 [trainer.py] => CNN: {'total': np.float64(39.72), '00-01': np.float64(74.19), '02-03': np.float64(25.81), '04-05': np.float64(47.19), '06-07': np.float64(37.5), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(31.48), '14-15': np.float64(42.86), '16-17': np.float64(21.43), '18-19': np.float64(42.86), '20-21': np.float64(63.74), '22-23': np.float64(62.5), '24-25': np.float64(18.37), '26-27': np.float64(63.48), '28-29': np.float64(60.49), '30-31': np.float64(39.09), '32-33': np.float64(2.27), '34-35': np.float64(25.81), '36-37': np.float64(48.33), '38-39': np.float64(48.78), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(38.3), '50-51': np.float64(10.81), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(24.53), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(37.5), '74-75': np.float64(42.31), '76-77': np.float64(44.58), '78-79': np.float64(23.17), '80-81': np.float64(62.5), '82-83': np.float64(27.12), '84-85': np.float64(45.65), '86-87': np.float64(26.44), '88-89': np.float64(78.33), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(36.62), '96-97': np.float64(11.11), '98-99': np.float64(24.32), '100-101': np.float64(33.33), '102-103': np.float64(46.55), '104-105': np.float64(19.15), '106-107': np.float64(17.95), '108-109': np.float64(58.21), '110-111': np.float64(38.33), '112-113': np.float64(78.85), '114-115': np.float64(52.5), '116-117': np.float64(63.89), '118-119': np.float64(46.15), '120-121': np.float64(50.82), '122-123': np.float64(25.0), '124-125': np.float64(34.69), '126-127': np.float64(7.55), '128-129': np.float64(13.92), '130-131': np.float64(42.22), '132-133': np.float64(35.29), '134-135': np.float64(33.33), '136-137': np.float64(57.14), '138-139': np.float64(49.06), '140-141': np.float64(34.04), '142-143': np.float64(64.58), '144-145': np.float64(23.91), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(13.16), '152-153': np.float64(37.84), '154-155': np.float64(8.0), '156-157': np.float64(48.21), '158-159': np.float64(36.51), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(54.39), '168-169': np.float64(44.44), '170-171': np.float64(38.81), '172-173': np.float64(32.08), '174-175': np.float64(50.0), '176-177': np.float64(56.0), '178-179': np.float64(63.33), '180-181': np.float64(41.49), '182-183': np.float64(21.95), '184-185': np.float64(48.72), '186-187': np.float64(51.79), '188-189': np.float64(65.93), '190-191': np.float64(62.5), 'old': np.float64(39.36), 'new': np.float64(62.5)}
2025-12-10 17:13:40,719 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36), np.float64(39.72)]
2025-12-10 17:13:40,719 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09), np.float64(96.08)]
2025-12-10 17:13:40,719 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266, 0.3973314850112632]
2025-12-10 17:13:47,333 [trainer.py] => All params: 144526051
2025-12-10 17:13:47,345 [trainer.py] => Trainable params: 185858
2025-12-10 17:13:47,345 [inflora.py] => Learning on 192-194
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_k.96.weight', 'image_encoder.blocks.0.attn.lora_B_v.96.weight', 'image_encoder.blocks.8.attn.lora_B_v.96.weight', 'image_encoder.blocks.7.attn.lora_B_v.96.weight', 'image_encoder.blocks.7.attn.lora_B_k.96.weight', 'image_encoder.blocks.9.attn.lora_B_v.96.weight', 'image_encoder.blocks.5.attn.lora_B_k.96.weight', 'image_encoder.blocks.1.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_k.96.weight', 'image_encoder.blocks.0.attn.lora_B_k.96.weight', 'classifier_pool.96.bias', 'image_encoder.blocks.4.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_k.96.weight', 'image_encoder.blocks.8.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.96.weight', 'image_encoder.blocks.9.attn.lora_B_k.96.weight', 'image_encoder.blocks.2.attn.lora_B_k.96.weight', 'image_encoder.blocks.4.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.96.weight', 'image_encoder.blocks.1.attn.lora_B_v.96.weight', 'image_encoder.blocks.2.attn.lora_B_v.96.weight', 'classifier_pool.96.weight'}
2025-12-10 17:15:41,005 [inflora.py] => Task 96, Epoch 50/50 => Loss 0.045, Train_accy 98.58
Threshold:  0.9992
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 52/768 type remove
Layer 2 : 116/768 type remove
Layer 3 : 249/768 type remove
Layer 4 : 336/768 type remove
Layer 5 : 331/768 type retain
Layer 6 : 304/768 type retain
Layer 7 : 250/768 type retain
Layer 8 : 200/768 type retain
Layer 9 : 126/768 type retain
Layer 10 : 87/768 type retain
Layer 11 : 117/768 type retain
Layer 12 : 72/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:15:50,110 [trainer.py] => Time:122.7647933959961
5825 5825
5825 5825
2025-12-10 17:16:07,301 [trainer.py] => Time:17.191238403320312
2025-12-10 17:16:07,302 [inflora.py] => Exemplar size: 0
2025-12-10 17:16:07,302 [trainer.py] => CNN: {'total': np.float64(39.14), '00-01': np.float64(74.19), '02-03': np.float64(25.81), '04-05': np.float64(47.19), '06-07': np.float64(37.5), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(31.48), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(42.86), '20-21': np.float64(64.84), '22-23': np.float64(60.16), '24-25': np.float64(16.33), '26-27': np.float64(63.48), '28-29': np.float64(58.02), '30-31': np.float64(38.18), '32-33': np.float64(6.82), '34-35': np.float64(25.81), '36-37': np.float64(50.0), '38-39': np.float64(46.34), '40-41': np.float64(29.63), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(38.3), '50-51': np.float64(10.81), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(24.53), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(32.5), '74-75': np.float64(41.03), '76-77': np.float64(46.99), '78-79': np.float64(26.83), '80-81': np.float64(63.89), '82-83': np.float64(24.58), '84-85': np.float64(39.13), '86-87': np.float64(25.29), '88-89': np.float64(80.0), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(36.62), '96-97': np.float64(11.11), '98-99': np.float64(24.32), '100-101': np.float64(31.67), '102-103': np.float64(46.55), '104-105': np.float64(17.02), '106-107': np.float64(17.95), '108-109': np.float64(56.72), '110-111': np.float64(38.33), '112-113': np.float64(78.85), '114-115': np.float64(52.5), '116-117': np.float64(61.11), '118-119': np.float64(44.23), '120-121': np.float64(52.46), '122-123': np.float64(25.0), '124-125': np.float64(34.69), '126-127': np.float64(3.77), '128-129': np.float64(13.92), '130-131': np.float64(37.78), '132-133': np.float64(32.35), '134-135': np.float64(32.05), '136-137': np.float64(57.14), '138-139': np.float64(50.94), '140-141': np.float64(31.91), '142-143': np.float64(64.58), '144-145': np.float64(21.74), '146-147': np.float64(48.15), '148-149': np.float64(18.18), '150-151': np.float64(13.16), '152-153': np.float64(37.84), '154-155': np.float64(8.0), '156-157': np.float64(50.0), '158-159': np.float64(39.68), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(44.44), '170-171': np.float64(40.3), '172-173': np.float64(32.08), '174-175': np.float64(51.32), '176-177': np.float64(56.0), '178-179': np.float64(63.33), '180-181': np.float64(42.55), '182-183': np.float64(24.39), '184-185': np.float64(48.72), '186-187': np.float64(51.79), '188-189': np.float64(64.84), '190-191': np.float64(61.36), '192-193': np.float64(12.96), 'old': np.float64(39.39), 'new': np.float64(12.96)}
2025-12-10 17:16:07,302 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36), np.float64(39.72), np.float64(39.14)]
2025-12-10 17:16:07,302 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09), np.float64(96.08), np.float64(96.17)]
2025-12-10 17:16:07,303 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266, 0.3973314850112632, 0.3917596566523605]
2025-12-10 17:16:10,606 [trainer.py] => All params: 144526051
2025-12-10 17:16:10,618 [trainer.py] => Trainable params: 185858
2025-12-10 17:16:10,618 [inflora.py] => Learning on 194-196
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.97.weight', 'image_encoder.blocks.2.attn.lora_B_k.97.weight', 'image_encoder.blocks.11.attn.lora_B_k.97.weight', 'image_encoder.blocks.2.attn.lora_B_v.97.weight', 'image_encoder.blocks.1.attn.lora_B_k.97.weight', 'image_encoder.blocks.3.attn.lora_B_v.97.weight', 'image_encoder.blocks.0.attn.lora_B_k.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.97.weight', 'image_encoder.blocks.9.attn.lora_B_v.97.weight', 'image_encoder.blocks.3.attn.lora_B_k.97.weight', 'image_encoder.blocks.9.attn.lora_B_k.97.weight', 'image_encoder.blocks.11.attn.lora_B_v.97.weight', 'classifier_pool.97.weight', 'image_encoder.blocks.10.attn.lora_B_k.97.weight', 'image_encoder.blocks.1.attn.lora_B_v.97.weight', 'image_encoder.blocks.7.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_k.97.weight', 'image_encoder.blocks.4.attn.lora_B_v.97.weight', 'image_encoder.blocks.5.attn.lora_B_k.97.weight', 'image_encoder.blocks.4.attn.lora_B_k.97.weight', 'image_encoder.blocks.5.attn.lora_B_v.97.weight', 'image_encoder.blocks.10.attn.lora_B_v.97.weight', 'classifier_pool.97.bias', 'image_encoder.blocks.8.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_v.97.weight', 'image_encoder.blocks.0.attn.lora_B_v.97.weight'}
2025-12-10 17:17:41,930 [inflora.py] => Task 97, Epoch 50/50 => Loss 0.103, Train_accy 95.42
Threshold:  0.9994
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 60/768 type remove
Layer 2 : 119/768 type remove
Layer 3 : 252/768 type remove
Layer 4 : 343/768 type remove
Layer 5 : 322/768 type retain
Layer 6 : 291/768 type retain
Layer 7 : 233/768 type retain
Layer 8 : 187/768 type retain
Layer 9 : 112/768 type retain
Layer 10 : 75/768 type retain
Layer 11 : 99/768 type retain
Layer 12 : 58/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:17:50,245 [trainer.py] => Time:99.62660193443298
5854 5854
5854 5854
2025-12-10 17:18:07,484 [trainer.py] => Time:17.239187717437744
2025-12-10 17:18:07,485 [inflora.py] => Exemplar size: 0
2025-12-10 17:18:07,485 [trainer.py] => CNN: {'total': np.float64(39.22), '00-01': np.float64(75.27), '02-03': np.float64(25.81), '04-05': np.float64(47.19), '06-07': np.float64(35.71), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(31.48), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(42.86), '20-21': np.float64(64.84), '22-23': np.float64(60.94), '24-25': np.float64(14.29), '26-27': np.float64(64.35), '28-29': np.float64(55.56), '30-31': np.float64(36.36), '32-33': np.float64(6.82), '34-35': np.float64(22.58), '36-37': np.float64(48.33), '38-39': np.float64(46.34), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(37.23), '50-51': np.float64(10.81), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(25.47), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(28.77), '72-73': np.float64(32.5), '74-75': np.float64(42.31), '76-77': np.float64(46.99), '78-79': np.float64(23.17), '80-81': np.float64(63.89), '82-83': np.float64(24.58), '84-85': np.float64(39.13), '86-87': np.float64(29.89), '88-89': np.float64(81.67), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(36.62), '96-97': np.float64(14.14), '98-99': np.float64(24.32), '100-101': np.float64(31.67), '102-103': np.float64(46.55), '104-105': np.float64(19.15), '106-107': np.float64(17.95), '108-109': np.float64(58.21), '110-111': np.float64(38.33), '112-113': np.float64(78.85), '114-115': np.float64(52.5), '116-117': np.float64(61.11), '118-119': np.float64(46.15), '120-121': np.float64(52.46), '122-123': np.float64(25.0), '124-125': np.float64(36.73), '126-127': np.float64(1.89), '128-129': np.float64(13.92), '130-131': np.float64(37.78), '132-133': np.float64(35.29), '134-135': np.float64(33.33), '136-137': np.float64(57.14), '138-139': np.float64(54.72), '140-141': np.float64(29.79), '142-143': np.float64(66.67), '144-145': np.float64(23.91), '146-147': np.float64(48.15), '148-149': np.float64(18.18), '150-151': np.float64(13.16), '152-153': np.float64(37.84), '154-155': np.float64(8.0), '156-157': np.float64(48.21), '158-159': np.float64(39.68), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(44.44), '170-171': np.float64(38.81), '172-173': np.float64(33.96), '174-175': np.float64(51.32), '176-177': np.float64(56.0), '178-179': np.float64(60.0), '180-181': np.float64(41.49), '182-183': np.float64(24.39), '184-185': np.float64(48.72), '186-187': np.float64(51.79), '188-189': np.float64(64.84), '190-191': np.float64(61.36), '192-193': np.float64(12.96), '194-195': np.float64(24.14), 'old': np.float64(39.3), 'new': np.float64(24.14)}
2025-12-10 17:18:07,485 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36), np.float64(39.72), np.float64(39.14), np.float64(39.22)]
2025-12-10 17:18:07,485 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09), np.float64(96.08), np.float64(96.17), np.float64(95.99)]
2025-12-10 17:18:07,485 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266, 0.3973314850112632, 0.3917596566523605, 0.3925521011274342]
2025-12-10 17:18:10,078 [trainer.py] => All params: 144526051
2025-12-10 17:18:10,090 [trainer.py] => Trainable params: 185858
2025-12-10 17:18:10,090 [inflora.py] => Learning on 196-198
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.98.weight', 'image_encoder.blocks.2.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.98.weight', 'image_encoder.blocks.0.attn.lora_B_k.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.98.weight', 'image_encoder.blocks.7.attn.lora_B_k.98.weight', 'image_encoder.blocks.9.attn.lora_B_v.98.weight', 'image_encoder.blocks.6.attn.lora_B_k.98.weight', 'image_encoder.blocks.7.attn.lora_B_v.98.weight', 'image_encoder.blocks.2.attn.lora_B_v.98.weight', 'classifier_pool.98.weight', 'image_encoder.blocks.8.attn.lora_B_k.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_k.98.weight', 'image_encoder.blocks.9.attn.lora_B_k.98.weight', 'image_encoder.blocks.8.attn.lora_B_v.98.weight', 'image_encoder.blocks.4.attn.lora_B_k.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_v.98.weight', 'classifier_pool.98.bias', 'image_encoder.blocks.5.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_k.98.weight', 'image_encoder.blocks.4.attn.lora_B_v.98.weight', 'image_encoder.blocks.5.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.98.weight'}
2025-12-10 17:20:26,311 [inflora.py] => Task 98, Epoch 50/50 => Loss 0.032, Train_accy 99.00
Threshold:  0.9996
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 60/768 type remove
Layer 2 : 136/768 type remove
Layer 3 : 291/768 type remove
Layer 4 : 368/768 type retain
Layer 5 : 265/768 type retain
Layer 6 : 236/768 type retain
Layer 7 : 184/768 type retain
Layer 8 : 144/768 type retain
Layer 9 : 77/768 type retain
Layer 10 : 46/768 type retain
Layer 11 : 63/768 type retain
Layer 12 : 33/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:20:34,496 [trainer.py] => Time:144.40660548210144
5936 5936
5936 5936
2025-12-10 17:20:52,061 [trainer.py] => Time:17.564886808395386
2025-12-10 17:20:52,062 [inflora.py] => Exemplar size: 0
2025-12-10 17:20:52,062 [trainer.py] => CNN: {'total': np.float64(39.15), '00-01': np.float64(76.34), '02-03': np.float64(24.19), '04-05': np.float64(47.19), '06-07': np.float64(37.5), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(29.63), '14-15': np.float64(42.86), '16-17': np.float64(20.0), '18-19': np.float64(42.86), '20-21': np.float64(63.74), '22-23': np.float64(60.94), '24-25': np.float64(14.29), '26-27': np.float64(65.22), '28-29': np.float64(50.62), '30-31': np.float64(39.09), '32-33': np.float64(11.36), '34-35': np.float64(25.81), '36-37': np.float64(48.33), '38-39': np.float64(46.34), '40-41': np.float64(31.48), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(37.23), '50-51': np.float64(8.11), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(24.53), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(4.17), '70-71': np.float64(27.4), '72-73': np.float64(32.5), '74-75': np.float64(43.59), '76-77': np.float64(45.78), '78-79': np.float64(24.39), '80-81': np.float64(65.28), '82-83': np.float64(25.42), '84-85': np.float64(41.3), '86-87': np.float64(28.74), '88-89': np.float64(78.33), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(36.62), '96-97': np.float64(14.14), '98-99': np.float64(25.68), '100-101': np.float64(31.67), '102-103': np.float64(44.83), '104-105': np.float64(18.09), '106-107': np.float64(20.51), '108-109': np.float64(55.22), '110-111': np.float64(36.67), '112-113': np.float64(75.0), '114-115': np.float64(52.5), '116-117': np.float64(61.11), '118-119': np.float64(44.23), '120-121': np.float64(54.1), '122-123': np.float64(26.79), '124-125': np.float64(34.69), '126-127': np.float64(1.89), '128-129': np.float64(12.66), '130-131': np.float64(40.0), '132-133': np.float64(35.29), '134-135': np.float64(32.05), '136-137': np.float64(55.1), '138-139': np.float64(56.6), '140-141': np.float64(34.04), '142-143': np.float64(66.67), '144-145': np.float64(23.91), '146-147': np.float64(44.44), '148-149': np.float64(21.21), '150-151': np.float64(13.16), '152-153': np.float64(37.84), '154-155': np.float64(10.0), '156-157': np.float64(46.43), '158-159': np.float64(39.68), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(56.67), '166-167': np.float64(56.14), '168-169': np.float64(43.06), '170-171': np.float64(38.81), '172-173': np.float64(32.08), '174-175': np.float64(52.63), '176-177': np.float64(56.0), '178-179': np.float64(60.0), '180-181': np.float64(40.43), '182-183': np.float64(24.39), '184-185': np.float64(48.72), '186-187': np.float64(48.21), '188-189': np.float64(65.93), '190-191': np.float64(59.09), '192-193': np.float64(14.81), '194-195': np.float64(24.14), '196-197': np.float64(45.12), 'old': np.float64(39.07), 'new': np.float64(45.12)}
2025-12-10 17:20:52,062 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36), np.float64(39.72), np.float64(39.14), np.float64(39.22), np.float64(39.15)]
2025-12-10 17:20:52,063 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09), np.float64(96.08), np.float64(96.17), np.float64(95.99), np.float64(96.18)]
2025-12-10 17:20:52,063 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266, 0.3973314850112632, 0.3917596566523605, 0.3925521011274342, 0.39167789757412397]
2025-12-10 17:20:57,910 [trainer.py] => All params: 144526051
2025-12-10 17:20:57,922 [trainer.py] => Trainable params: 185858
2025-12-10 17:20:57,922 [inflora.py] => Learning on 198-200
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.99.weight', 'classifier_pool.99.bias', 'image_encoder.blocks.11.attn.lora_B_v.99.weight', 'image_encoder.blocks.5.attn.lora_B_k.99.weight', 'image_encoder.blocks.3.attn.lora_B_k.99.weight', 'image_encoder.blocks.5.attn.lora_B_v.99.weight', 'image_encoder.blocks.3.attn.lora_B_v.99.weight', 'image_encoder.blocks.6.attn.lora_B_v.99.weight', 'classifier_pool.99.weight', 'image_encoder.blocks.7.attn.lora_B_k.99.weight', 'image_encoder.blocks.10.attn.lora_B_v.99.weight', 'image_encoder.blocks.1.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_k.99.weight', 'image_encoder.blocks.7.attn.lora_B_v.99.weight', 'image_encoder.blocks.1.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_k.99.weight', 'image_encoder.blocks.2.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_k.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.99.weight', 'image_encoder.blocks.8.attn.lora_B_v.99.weight', 'image_encoder.blocks.2.attn.lora_B_k.99.weight', 'image_encoder.blocks.8.attn.lora_B_k.99.weight', 'image_encoder.blocks.10.attn.lora_B_k.99.weight', 'image_encoder.blocks.4.attn.lora_B_v.99.weight'}
2025-12-10 17:23:10,210 [inflora.py] => Task 99, Epoch 50/50 => Loss 0.036, Train_accy 98.92
Threshold:  0.9998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 69/768 type remove
Layer 2 : 153/768 type remove
Layer 3 : 328/768 type remove
Layer 4 : 319/768 type retain
Layer 5 : 216/768 type retain
Layer 6 : 190/768 type retain
Layer 7 : 141/768 type retain
Layer 8 : 105/768 type retain
Layer 9 : 50/768 type retain
Layer 10 : 29/768 type retain
Layer 11 : 40/768 type retain
Layer 12 : 19/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 17:23:19,810 [trainer.py] => Time:141.88743948936462
6000 6000
6000 6000
2025-12-10 17:23:37,585 [trainer.py] => Time:17.77486824989319
2025-12-10 17:23:37,585 [inflora.py] => Exemplar size: 0
2025-12-10 17:23:37,585 [trainer.py] => CNN: {'total': np.float64(38.73), '00-01': np.float64(74.19), '02-03': np.float64(25.81), '04-05': np.float64(46.07), '06-07': np.float64(37.5), '08-09': np.float64(41.1), '10-11': np.float64(21.95), '12-13': np.float64(29.63), '14-15': np.float64(41.07), '16-17': np.float64(18.57), '18-19': np.float64(42.86), '20-21': np.float64(64.84), '22-23': np.float64(60.16), '24-25': np.float64(16.33), '26-27': np.float64(65.22), '28-29': np.float64(49.38), '30-31': np.float64(38.18), '32-33': np.float64(6.82), '34-35': np.float64(29.03), '36-37': np.float64(48.33), '38-39': np.float64(46.34), '40-41': np.float64(33.33), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(36.17), '50-51': np.float64(8.11), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(24.53), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(4.17), '70-71': np.float64(27.4), '72-73': np.float64(32.5), '74-75': np.float64(44.87), '76-77': np.float64(45.78), '78-79': np.float64(24.39), '80-81': np.float64(62.5), '82-83': np.float64(25.42), '84-85': np.float64(41.3), '86-87': np.float64(27.59), '88-89': np.float64(78.33), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(33.8), '96-97': np.float64(13.13), '98-99': np.float64(24.32), '100-101': np.float64(31.67), '102-103': np.float64(44.83), '104-105': np.float64(15.96), '106-107': np.float64(17.95), '108-109': np.float64(58.21), '110-111': np.float64(35.0), '112-113': np.float64(75.0), '114-115': np.float64(52.5), '116-117': np.float64(63.89), '118-119': np.float64(42.31), '120-121': np.float64(54.1), '122-123': np.float64(25.0), '124-125': np.float64(34.69), '126-127': np.float64(3.77), '128-129': np.float64(11.39), '130-131': np.float64(40.0), '132-133': np.float64(32.35), '134-135': np.float64(32.05), '136-137': np.float64(55.1), '138-139': np.float64(54.72), '140-141': np.float64(29.79), '142-143': np.float64(66.67), '144-145': np.float64(23.91), '146-147': np.float64(44.44), '148-149': np.float64(21.21), '150-151': np.float64(13.16), '152-153': np.float64(37.84), '154-155': np.float64(10.0), '156-157': np.float64(46.43), '158-159': np.float64(38.1), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(54.44), '166-167': np.float64(56.14), '168-169': np.float64(43.06), '170-171': np.float64(38.81), '172-173': np.float64(32.08), '174-175': np.float64(51.32), '176-177': np.float64(54.67), '178-179': np.float64(53.33), '180-181': np.float64(40.43), '182-183': np.float64(24.39), '184-185': np.float64(46.15), '186-187': np.float64(49.11), '188-189': np.float64(64.84), '190-191': np.float64(59.09), '192-193': np.float64(14.81), '194-195': np.float64(24.14), '196-197': np.float64(45.12), '198-199': np.float64(39.06), 'old': np.float64(38.73), 'new': np.float64(39.06)}
2025-12-10 17:23:37,586 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(78.71), np.float64(70.9), np.float64(71.33), np.float64(65.68), np.float64(62.8), np.float64(61.75), np.float64(61.07), np.float64(58.92), np.float64(57.23), np.float64(57.49), np.float64(57.19), np.float64(55.01), np.float64(57.79), np.float64(54.13), np.float64(53.1), np.float64(53.47), np.float64(53.5), np.float64(54.75), np.float64(56.5), np.float64(54.88), np.float64(53.84), np.float64(51.88), np.float64(51.56), np.float64(52.26), np.float64(52.18), np.float64(53.25), np.float64(52.37), np.float64(51.98), np.float64(50.36), np.float64(50.91), np.float64(50.56), np.float64(50.1), np.float64(49.13), np.float64(49.19), np.float64(47.64), np.float64(48.15), np.float64(45.58), np.float64(47.35), np.float64(46.32), np.float64(46.87), np.float64(46.28), np.float64(46.0), np.float64(46.74), np.float64(47.45), np.float64(47.45), np.float64(45.84), np.float64(46.23), np.float64(44.89), np.float64(44.31), np.float64(44.07), np.float64(44.64), np.float64(43.26), np.float64(43.74), np.float64(43.93), np.float64(43.29), np.float64(43.48), np.float64(43.44), np.float64(43.2), np.float64(43.21), np.float64(43.71), np.float64(43.33), np.float64(42.26), np.float64(41.89), np.float64(41.71), np.float64(41.42), np.float64(40.21), np.float64(39.84), np.float64(39.8), np.float64(40.03), np.float64(39.45), np.float64(39.04), np.float64(38.19), np.float64(37.78), np.float64(38.13), np.float64(38.14), np.float64(38.16), np.float64(38.36), np.float64(38.11), np.float64(37.66), np.float64(37.82), np.float64(37.84), np.float64(38.54), np.float64(38.56), np.float64(38.08), np.float64(38.31), np.float64(37.85), np.float64(37.22), np.float64(38.15), np.float64(38.33), np.float64(38.54), np.float64(38.5), np.float64(38.39), np.float64(38.61), np.float64(39.36), np.float64(39.72), np.float64(39.14), np.float64(39.22), np.float64(39.15), np.float64(38.73)]
2025-12-10 17:23:37,586 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(95.67), np.float64(95.98), np.float64(95.17), np.float64(95.51), np.float64(95.23), np.float64(96.3), np.float64(96.62), np.float64(95.82), np.float64(96.55), np.float64(96.3), np.float64(96.32), np.float64(95.42), np.float64(96.0), np.float64(96.06), np.float64(96.23), np.float64(95.73), np.float64(96.07), np.float64(95.8), np.float64(96.1), np.float64(94.84), np.float64(94.55), np.float64(94.8), np.float64(95.21), np.float64(96.02), np.float64(95.73), np.float64(95.6), np.float64(95.24), np.float64(95.18), np.float64(95.44), np.float64(95.81), np.float64(95.8), np.float64(95.9), np.float64(95.75), np.float64(95.74), np.float64(95.84), np.float64(95.69), np.float64(95.59), np.float64(95.6), np.float64(95.76), np.float64(96.21), np.float64(96.18), np.float64(96.37), np.float64(96.36), np.float64(96.35), np.float64(96.44), np.float64(96.06), np.float64(95.77), np.float64(95.88), np.float64(95.89), np.float64(95.5), np.float64(95.58), np.float64(95.93), np.float64(95.51), np.float64(95.63), np.float64(95.54), np.float64(95.48), np.float64(95.65), np.float64(95.72), np.float64(95.68), np.float64(95.47), np.float64(95.59), np.float64(95.57), np.float64(95.7), np.float64(95.66), np.float64(95.47), np.float64(95.65), np.float64(95.51), np.float64(95.66), np.float64(95.54), np.float64(95.5), np.float64(95.71), np.float64(95.85), np.float64(95.84), np.float64(95.81), np.float64(95.72), np.float64(95.65), np.float64(95.6), np.float64(95.58), np.float64(95.7), np.float64(95.61), np.float64(95.46), np.float64(95.56), np.float64(95.7), np.float64(95.8), np.float64(95.79), np.float64(95.98), np.float64(96.0), np.float64(95.87), np.float64(95.79), np.float64(95.8), np.float64(96.01), np.float64(96.09), np.float64(96.08), np.float64(96.17), np.float64(95.99), np.float64(96.18), np.float64(96.15)]
2025-12-10 17:23:37,586 [trainer.py] => CNN top1 task curve: [1.0, 0.7935483870967742, 0.7090163934426229, 0.7133333333333334, 0.6568364611260054, 0.6280193236714976, 0.6175213675213675, 0.6106870229007634, 0.5909090909090909, 0.5723076923076923, 0.5748987854251012, 0.5742232451093211, 0.55119825708061, 0.5779283639883833, 0.5430879712746858, 0.5318627450980392, 0.5354889589905363, 0.5350269438029254, 0.5489330389992642, 0.5664285714285714, 0.5495185694635488, 0.5390309555854643, 0.5208195637805684, 0.5175324675324675, 0.5238678090575275, 0.5230400957510473, 0.5330990041007616, 0.5242494226327945, 0.5203160270880361, 0.5041050903119869, 0.5096359743040685, 0.5055724417426545, 0.500998003992016, 0.4913494809688581, 0.49193942354665365, 0.47641509433962265, 0.48148148148148145, 0.45621090259159963, 0.47436449806118053, 0.4631710362047441, 0.4690909090909091, 0.4631700732741998, 0.460401667298219, 0.4684519442406456, 0.4748743718592965, 0.4748761500353857, 0.4587826086956522, 0.46334012219959264, 0.44991789819376027, 0.4430907342096826, 0.4410191884240327, 0.44701884460920605, 0.43320324226958873, 0.43768545994065283, 0.4396275821937736, 0.4332284815556191, 0.4353338968723584, 0.43494009473390915, 0.4322758620689655, 0.43268969268425345, 0.4376672017121455, 0.43384290985767, 0.42310694769711166, 0.41940451745379875, 0.41786163522012576, 0.41492537313432837, 0.4030587074494327, 0.398838334946757, 0.39823008849557523, 0.4008030231459613, 0.39476757766876897, 0.39039039039039036, 0.38194285714285714, 0.37823716492503406, 0.381510710259301, 0.38162307176391685, 0.38181818181818183, 0.38421052631578945, 0.38128249566724437, 0.376789912374439, 0.3784126984126984, 0.3786042624320936, 0.3855619360131255, 0.3857693087370768, 0.381018981018981, 0.3832807570977918, 0.3789268292682927, 0.372620649875024, 0.38172858225928735, 0.38352808141726347, 0.3857407407407407, 0.38540709428413894, 0.3841240875912409, 0.38626609442060084, 0.3938060883336266, 0.3973314850112632, 0.3917596566523605, 0.3925521011274342, 0.39167789757412397, 0.3875]
