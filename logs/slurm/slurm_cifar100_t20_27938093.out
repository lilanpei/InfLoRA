logs/cifar100/5_5_sip/InfLoRA/adam/10/0.95_1.0-0.0005/42
2025-12-10 13:29:03,848 [trainer.py] => config: configs/cifar100_20tasks_inflora_seed42.json
2025-12-10 13:29:03,849 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 13:29:03,850 [trainer.py] => prefix: reproduce
2025-12-10 13:29:03,850 [trainer.py] => dataset: cifar100
2025-12-10 13:29:03,850 [trainer.py] => data_path: data/
2025-12-10 13:29:03,850 [trainer.py] => memory_size: 0
2025-12-10 13:29:03,850 [trainer.py] => memory_per_class: 0
2025-12-10 13:29:03,850 [trainer.py] => fixed_memory: True
2025-12-10 13:29:03,850 [trainer.py] => shuffle: False
2025-12-10 13:29:03,850 [trainer.py] => init_cls: 5
2025-12-10 13:29:03,850 [trainer.py] => increment: 5
2025-12-10 13:29:03,850 [trainer.py] => model_name: InfLoRA
2025-12-10 13:29:03,850 [trainer.py] => net_type: sip
2025-12-10 13:29:03,850 [trainer.py] => embd_dim: 768
2025-12-10 13:29:03,850 [trainer.py] => num_heads: 12
2025-12-10 13:29:03,850 [trainer.py] => total_sessions: 20
2025-12-10 13:29:03,850 [trainer.py] => seed: 42
2025-12-10 13:29:03,850 [trainer.py] => EPSILON: 1e-08
2025-12-10 13:29:03,850 [trainer.py] => init_epoch: 20
2025-12-10 13:29:03,850 [trainer.py] => optim: adam
2025-12-10 13:29:03,850 [trainer.py] => init_lr: 0.0005
2025-12-10 13:29:03,850 [trainer.py] => init_lr_decay: 0.1
2025-12-10 13:29:03,850 [trainer.py] => init_weight_decay: 0.0
2025-12-10 13:29:03,850 [trainer.py] => epochs: 20
2025-12-10 13:29:03,851 [trainer.py] => lrate: 0.0005
2025-12-10 13:29:03,851 [trainer.py] => lrate_decay: 0.1
2025-12-10 13:29:03,851 [trainer.py] => batch_size: 128
2025-12-10 13:29:03,851 [trainer.py] => weight_decay: 0.0
2025-12-10 13:29:03,851 [trainer.py] => rank: 10
2025-12-10 13:29:03,851 [trainer.py] => lamb: 0.95
2025-12-10 13:29:03,851 [trainer.py] => lame: 1.0
2025-12-10 13:29:03,851 [trainer.py] => num_workers: 8
2025-12-10 13:29:05,687 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 962, unexpected 0
2025-12-10 13:29:08,150 [trainer.py] => All params: 114881051
2025-12-10 13:29:08,153 [trainer.py] => Trainable params: 114881051
2025-12-10 13:29:08,153 [inflora.py] => Learning on 0-5
Parameters to be updated: {'classifier_pool.0.bias', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight'}
2025-12-10 13:34:05,283 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.086, Train_accy 96.96
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 12/768 type remove
Layer 6 : 13/768 type remove
Layer 7 : 12/768 type remove
Layer 8 : 16/768 type remove
Layer 9 : 19/768 type remove
Layer 10 : 13/768 type remove
Layer 11 : 5/768 type remove
Layer 12 : 5/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:34:21,102 [trainer.py] => Time:312.94922637939453
500 500
500 500
2025-12-10 13:34:22,981 [trainer.py] => Time:1.878267765045166
2025-12-10 13:34:22,981 [inflora.py] => Exemplar size: 0
2025-12-10 13:34:22,981 [trainer.py] => CNN: {'total': np.float64(99.6), '00-04': np.float64(99.6), 'old': 0, 'new': np.float64(99.6)}
2025-12-10 13:34:22,981 [trainer.py] => CNN top1 curve: [np.float64(99.6)]
2025-12-10 13:34:22,981 [trainer.py] => CNN top1 with task curve: [np.float64(99.6)]
2025-12-10 13:34:22,981 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 13:34:23,476 [trainer.py] => All params: 114881051
2025-12-10 13:34:23,479 [trainer.py] => Trainable params: 188165
2025-12-10 13:34:23,480 [inflora.py] => Learning on 5-10
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'classifier_pool.19.bias', 'classifier_pool.13.bias', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'classifier_pool.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight'}
2025-12-10 13:39:12,692 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.069, Train_accy 97.64
Threshold:  0.9524999999999999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 17/768 type remove
Layer 6 : 18/768 type remove
Layer 7 : 18/768 type remove
Layer 8 : 25/768 type remove
Layer 9 : 30/768 type remove
Layer 10 : 25/768 type remove
Layer 11 : 12/768 type remove
Layer 12 : 13/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:39:29,940 [trainer.py] => Time:306.46080589294434
1000 1000
1000 1000
2025-12-10 13:39:33,039 [trainer.py] => Time:3.0986368656158447
2025-12-10 13:39:33,039 [inflora.py] => Exemplar size: 0
2025-12-10 13:39:33,039 [trainer.py] => CNN: {'total': np.float64(99.3), '00-04': np.float64(99.0), '05-09': np.float64(99.6), 'old': np.float64(99.0), 'new': np.float64(99.6)}
2025-12-10 13:39:33,040 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3)]
2025-12-10 13:39:33,040 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6)]
2025-12-10 13:39:33,040 [trainer.py] => CNN top1 task curve: [1.0, 0.997]
2025-12-10 13:39:33,551 [trainer.py] => All params: 114881051
2025-12-10 13:39:33,554 [trainer.py] => Trainable params: 2069815
2025-12-10 13:39:33,554 [inflora.py] => Learning on 10-15
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight'}
2025-12-10 13:44:23,083 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.065, Train_accy 97.72
Threshold:  0.955
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 18/768 type remove
Layer 6 : 20/768 type remove
Layer 7 : 21/768 type remove
Layer 8 : 28/768 type remove
Layer 9 : 39/768 type remove
Layer 10 : 38/768 type remove
Layer 11 : 18/768 type remove
Layer 12 : 25/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:44:40,386 [trainer.py] => Time:306.8313903808594
1500 1500
1500 1500
2025-12-10 13:44:44,776 [trainer.py] => Time:4.390356779098511
2025-12-10 13:44:44,777 [inflora.py] => Exemplar size: 0
2025-12-10 13:44:44,777 [trainer.py] => CNN: {'total': np.float64(95.93), '00-04': np.float64(97.6), '05-09': np.float64(99.0), '10-14': np.float64(91.2), 'old': np.float64(98.3), 'new': np.float64(91.2)}
2025-12-10 13:44:44,777 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93)]
2025-12-10 13:44:44,777 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6)]
2025-12-10 13:44:44,777 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962]
2025-12-10 13:44:45,306 [trainer.py] => All params: 114881051
2025-12-10 13:44:45,308 [trainer.py] => Trainable params: 188165
2025-12-10 13:44:45,309 [inflora.py] => Learning on 15-20
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'classifier_pool.3.weight'}
2025-12-10 13:49:34,924 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.082, Train_accy 97.20
Threshold:  0.9575
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 16/768 type remove
Layer 5 : 20/768 type remove
Layer 6 : 24/768 type remove
Layer 7 : 26/768 type remove
Layer 8 : 34/768 type remove
Layer 9 : 50/768 type remove
Layer 10 : 49/768 type remove
Layer 11 : 22/768 type remove
Layer 12 : 29/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:49:52,407 [trainer.py] => Time:307.09854435920715
2000 2000
2000 2000
2025-12-10 13:49:58,061 [trainer.py] => Time:5.65342378616333
2025-12-10 13:49:58,061 [inflora.py] => Exemplar size: 0
2025-12-10 13:49:58,061 [trainer.py] => CNN: {'total': np.float64(95.8), '00-04': np.float64(97.0), '05-09': np.float64(97.6), '10-14': np.float64(92.6), '15-19': np.float64(96.0), 'old': np.float64(95.73), 'new': np.float64(96.0)}
2025-12-10 13:49:58,061 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8)]
2025-12-10 13:49:58,061 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5)]
2025-12-10 13:49:58,061 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605]
2025-12-10 13:49:58,587 [trainer.py] => All params: 114881051
2025-12-10 13:49:58,590 [trainer.py] => Trainable params: 188165
2025-12-10 13:49:58,590 [inflora.py] => Learning on 20-25
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight'}
2025-12-10 13:54:48,382 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.035, Train_accy 99.08
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 17/768 type remove
Layer 5 : 24/768 type remove
Layer 6 : 28/768 type remove
Layer 7 : 31/768 type remove
Layer 8 : 43/768 type remove
Layer 9 : 58/768 type remove
Layer 10 : 54/768 type remove
Layer 11 : 26/768 type remove
Layer 12 : 34/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:55:06,013 [trainer.py] => Time:307.4229121208191
2500 2500
2500 2500
2025-12-10 13:55:12,951 [trainer.py] => Time:6.937432289123535
2025-12-10 13:55:12,951 [inflora.py] => Exemplar size: 0
2025-12-10 13:55:12,951 [trainer.py] => CNN: {'total': np.float64(94.32), '00-04': np.float64(96.8), '05-09': np.float64(95.6), '10-14': np.float64(89.6), '15-19': np.float64(95.6), '20-24': np.float64(94.0), 'old': np.float64(94.4), 'new': np.float64(94.0)}
2025-12-10 13:55:12,951 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32)]
2025-12-10 13:55:12,951 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6)]
2025-12-10 13:55:12,951 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444]
2025-12-10 13:55:19,225 [trainer.py] => All params: 114881051
2025-12-10 13:55:19,227 [trainer.py] => Trainable params: 188165
2025-12-10 13:55:19,228 [inflora.py] => Learning on 25-30
Parameters to be updated: {'classifier_pool.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight'}
2025-12-10 14:00:09,424 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.113, Train_accy 95.88
Threshold:  0.9624999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 19/768 type remove
Layer 5 : 27/768 type remove
Layer 6 : 31/768 type remove
Layer 7 : 33/768 type remove
Layer 8 : 46/768 type remove
Layer 9 : 64/768 type remove
Layer 10 : 62/768 type remove
Layer 11 : 30/768 type remove
Layer 12 : 39/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:00:27,018 [trainer.py] => Time:307.79093384742737
3000 3000
3000 3000
2025-12-10 14:00:35,203 [trainer.py] => Time:8.184554815292358
2025-12-10 14:00:35,203 [inflora.py] => Exemplar size: 0
2025-12-10 14:00:35,204 [trainer.py] => CNN: {'total': np.float64(92.93), '00-04': np.float64(95.8), '05-09': np.float64(95.4), '10-14': np.float64(90.0), '15-19': np.float64(94.8), '20-24': np.float64(92.6), '25-29': np.float64(89.0), 'old': np.float64(93.72), 'new': np.float64(89.0)}
2025-12-10 14:00:35,204 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93)]
2025-12-10 14:00:35,204 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5)]
2025-12-10 14:00:35,204 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931]
2025-12-10 14:00:39,220 [trainer.py] => All params: 114881051
2025-12-10 14:00:39,223 [trainer.py] => Trainable params: 188165
2025-12-10 14:00:39,223 [inflora.py] => Learning on 30-35
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight'}
2025-12-10 14:05:29,448 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.074, Train_accy 97.44
Threshold:  0.965
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 35/768 type remove
Layer 7 : 39/768 type remove
Layer 8 : 54/768 type remove
Layer 9 : 75/768 type remove
Layer 10 : 73/768 type remove
Layer 11 : 35/768 type remove
Layer 12 : 44/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:05:47,018 [trainer.py] => Time:307.795779466629
3500 3500
3500 3500
2025-12-10 14:05:56,477 [trainer.py] => Time:9.458691596984863
2025-12-10 14:05:56,478 [inflora.py] => Exemplar size: 0
2025-12-10 14:05:56,478 [trainer.py] => CNN: {'total': np.float64(92.6), '00-04': np.float64(95.0), '05-09': np.float64(94.4), '10-14': np.float64(88.8), '15-19': np.float64(93.2), '20-24': np.float64(91.4), '25-29': np.float64(89.0), '30-34': np.float64(96.4), 'old': np.float64(91.97), 'new': np.float64(96.4)}
2025-12-10 14:05:56,478 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6)]
2025-12-10 14:05:56,478 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43)]
2025-12-10 14:05:56,478 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572]
2025-12-10 14:05:59,301 [trainer.py] => All params: 114881051
2025-12-10 14:05:59,304 [trainer.py] => Trainable params: 188165
2025-12-10 14:05:59,304 [inflora.py] => Learning on 35-40
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'classifier_pool.7.weight'}
2025-12-10 14:10:49,588 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.054, Train_accy 98.12
Threshold:  0.9675
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 31/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 44/768 type remove
Layer 8 : 63/768 type remove
Layer 9 : 93/768 type remove
Layer 10 : 90/768 type remove
Layer 11 : 44/768 type remove
Layer 12 : 49/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:11:07,494 [trainer.py] => Time:308.1899583339691
4000 4000
4000 4000
2025-12-10 14:11:18,210 [trainer.py] => Time:10.715980768203735
2025-12-10 14:11:18,210 [inflora.py] => Exemplar size: 0
2025-12-10 14:11:18,210 [trainer.py] => CNN: {'total': np.float64(90.82), '00-04': np.float64(94.8), '05-09': np.float64(93.4), '10-14': np.float64(84.6), '15-19': np.float64(91.6), '20-24': np.float64(90.8), '25-29': np.float64(87.6), '30-34': np.float64(94.2), '35-39': np.float64(89.6), 'old': np.float64(91.0), 'new': np.float64(89.6)}
2025-12-10 14:11:18,210 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82)]
2025-12-10 14:11:18,210 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58)]
2025-12-10 14:11:18,211 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095]
2025-12-10 14:11:21,963 [trainer.py] => All params: 114881051
2025-12-10 14:11:21,966 [trainer.py] => Trainable params: 188165
2025-12-10 14:11:21,966 [inflora.py] => Learning on 40-45
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight'}
2025-12-10 14:16:12,463 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.090, Train_accy 96.80
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 23/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 34/768 type remove
Layer 6 : 42/768 type remove
Layer 7 : 47/768 type remove
Layer 8 : 67/768 type remove
Layer 9 : 99/768 type remove
Layer 10 : 99/768 type remove
Layer 11 : 51/768 type remove
Layer 12 : 53/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:16:30,095 [trainer.py] => Time:308.1287567615509
4500 4500
4500 4500
2025-12-10 14:16:42,057 [trainer.py] => Time:11.96201229095459
2025-12-10 14:16:42,057 [inflora.py] => Exemplar size: 0
2025-12-10 14:16:42,057 [trainer.py] => CNN: {'total': np.float64(90.51), '00-04': np.float64(94.6), '05-09': np.float64(93.0), '10-14': np.float64(84.8), '15-19': np.float64(91.2), '20-24': np.float64(89.2), '25-29': np.float64(86.6), '30-34': np.float64(94.2), '35-39': np.float64(88.2), '40-44': np.float64(92.8), 'old': np.float64(90.22), 'new': np.float64(92.8)}
2025-12-10 14:16:42,057 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51)]
2025-12-10 14:16:42,057 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38)]
2025-12-10 14:16:42,057 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889]
2025-12-10 14:16:44,679 [trainer.py] => All params: 114881051
2025-12-10 14:16:44,682 [trainer.py] => Trainable params: 188165
2025-12-10 14:16:44,682 [inflora.py] => Learning on 45-50
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight'}
2025-12-10 14:21:34,950 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.057, Train_accy 98.16
Threshold:  0.9724999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 37/768 type remove
Layer 6 : 46/768 type remove
Layer 7 : 52/768 type remove
Layer 8 : 74/768 type remove
Layer 9 : 107/768 type remove
Layer 10 : 109/768 type remove
Layer 11 : 57/768 type remove
Layer 12 : 59/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:21:52,236 [trainer.py] => Time:307.5535399913788
5000 5000
5000 5000
2025-12-10 14:22:05,503 [trainer.py] => Time:13.267067909240723
2025-12-10 14:22:05,504 [inflora.py] => Exemplar size: 0
2025-12-10 14:22:05,504 [trainer.py] => CNN: {'total': np.float64(89.16), '00-04': np.float64(93.6), '05-09': np.float64(92.6), '10-14': np.float64(80.8), '15-19': np.float64(90.2), '20-24': np.float64(86.2), '25-29': np.float64(84.2), '30-34': np.float64(93.4), '35-39': np.float64(87.2), '40-44': np.float64(93.2), '45-49': np.float64(90.2), 'old': np.float64(89.04), 'new': np.float64(90.2)}
2025-12-10 14:22:05,504 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16)]
2025-12-10 14:22:05,504 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4)]
2025-12-10 14:22:05,504 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928]
2025-12-10 14:22:14,242 [trainer.py] => All params: 114881051
2025-12-10 14:22:14,245 [trainer.py] => Trainable params: 188165
2025-12-10 14:22:14,245 [inflora.py] => Learning on 50-55
Parameters to be updated: {'classifier_pool.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'classifier_pool.10.bias'}
2025-12-10 14:27:04,856 [inflora.py] => Task 10, Epoch 20/20 => Loss 0.032, Train_accy 98.80
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 25/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 40/768 type remove
Layer 6 : 52/768 type remove
Layer 7 : 59/768 type remove
Layer 8 : 88/768 type remove
Layer 9 : 120/768 type remove
Layer 10 : 118/768 type remove
Layer 11 : 64/768 type remove
Layer 12 : 65/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:27:22,528 [trainer.py] => Time:308.28298354148865
5500 5500
5500 5500
2025-12-10 14:27:37,098 [trainer.py] => Time:14.57032322883606
2025-12-10 14:27:37,098 [inflora.py] => Exemplar size: 0
2025-12-10 14:27:37,099 [trainer.py] => CNN: {'total': np.float64(87.62), '00-04': np.float64(91.2), '05-09': np.float64(91.8), '10-14': np.float64(80.0), '15-19': np.float64(88.4), '20-24': np.float64(87.2), '25-29': np.float64(84.4), '30-34': np.float64(92.0), '35-39': np.float64(86.2), '40-44': np.float64(92.4), '45-49': np.float64(86.4), '50-54': np.float64(83.8), 'old': np.float64(88.0), 'new': np.float64(83.8)}
2025-12-10 14:27:37,099 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62)]
2025-12-10 14:27:37,099 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45)]
2025-12-10 14:27:37,099 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455]
2025-12-10 14:27:38,785 [trainer.py] => All params: 114881051
2025-12-10 14:27:38,788 [trainer.py] => Trainable params: 188165
2025-12-10 14:27:38,788 [inflora.py] => Learning on 55-60
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight'}
2025-12-10 14:32:29,622 [inflora.py] => Task 11, Epoch 20/20 => Loss 0.076, Train_accy 97.44
Threshold:  0.9775
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 26/768 type remove
Layer 4 : 32/768 type remove
Layer 5 : 43/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 66/768 type remove
Layer 8 : 97/768 type remove
Layer 9 : 129/768 type remove
Layer 10 : 126/768 type remove
Layer 11 : 69/768 type remove
Layer 12 : 71/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:32:47,004 [trainer.py] => Time:308.21516704559326
6000 6000
6000 6000
2025-12-10 14:33:02,831 [trainer.py] => Time:15.827061414718628
2025-12-10 14:33:02,831 [inflora.py] => Exemplar size: 0
2025-12-10 14:33:02,831 [trainer.py] => CNN: {'total': np.float64(86.35), '00-04': np.float64(91.6), '05-09': np.float64(92.0), '10-14': np.float64(78.4), '15-19': np.float64(88.4), '20-24': np.float64(86.0), '25-29': np.float64(83.0), '30-34': np.float64(91.6), '35-39': np.float64(88.2), '40-44': np.float64(91.8), '45-49': np.float64(85.6), '50-54': np.float64(85.0), '55-59': np.float64(74.6), 'old': np.float64(87.42), 'new': np.float64(74.6)}
2025-12-10 14:33:02,831 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35)]
2025-12-10 14:33:02,831 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38)]
2025-12-10 14:33:02,831 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645]
2025-12-10 14:33:04,634 [trainer.py] => All params: 114881051
2025-12-10 14:33:04,636 [trainer.py] => Trainable params: 188165
2025-12-10 14:33:04,637 [inflora.py] => Learning on 60-65
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight'}
2025-12-10 14:37:55,688 [inflora.py] => Task 12, Epoch 20/20 => Loss 0.062, Train_accy 97.48
Threshold:  0.98
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 37/768 type remove
Layer 5 : 48/768 type remove
Layer 6 : 64/768 type remove
Layer 7 : 78/768 type remove
Layer 8 : 111/768 type remove
Layer 9 : 155/768 type remove
Layer 10 : 154/768 type remove
Layer 11 : 87/768 type remove
Layer 12 : 88/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:38:13,428 [trainer.py] => Time:308.79135060310364
6500 6500
6500 6500
2025-12-10 14:38:30,544 [trainer.py] => Time:17.11578392982483
2025-12-10 14:38:30,544 [inflora.py] => Exemplar size: 0
2025-12-10 14:38:30,544 [trainer.py] => CNN: {'total': np.float64(85.63), '00-04': np.float64(90.6), '05-09': np.float64(91.0), '10-14': np.float64(75.8), '15-19': np.float64(88.6), '20-24': np.float64(82.6), '25-29': np.float64(84.0), '30-34': np.float64(92.6), '35-39': np.float64(89.6), '40-44': np.float64(92.2), '45-49': np.float64(83.8), '50-54': np.float64(86.0), '55-59': np.float64(72.4), '60-64': np.float64(84.0), 'old': np.float64(85.77), 'new': np.float64(84.0)}
2025-12-10 14:38:30,544 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63)]
2025-12-10 14:38:30,544 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37)]
2025-12-10 14:38:30,544 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307]
2025-12-10 14:38:34,322 [trainer.py] => All params: 114881051
2025-12-10 14:38:34,325 [trainer.py] => Trainable params: 188165
2025-12-10 14:38:34,325 [inflora.py] => Learning on 65-70
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight'}
2025-12-10 14:43:25,286 [inflora.py] => Task 13, Epoch 20/20 => Loss 0.092, Train_accy 96.96
Threshold:  0.9824999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 39/768 type remove
Layer 5 : 51/768 type remove
Layer 6 : 69/768 type remove
Layer 7 : 85/768 type remove
Layer 8 : 119/768 type remove
Layer 9 : 169/768 type remove
Layer 10 : 168/768 type remove
Layer 11 : 95/768 type remove
Layer 12 : 94/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:43:42,955 [trainer.py] => Time:308.6293168067932
7000 7000
7000 7000
2025-12-10 14:44:01,347 [trainer.py] => Time:18.39189839363098
2025-12-10 14:44:01,347 [inflora.py] => Exemplar size: 0
2025-12-10 14:44:01,347 [trainer.py] => CNN: {'total': np.float64(85.0), '00-04': np.float64(91.0), '05-09': np.float64(91.0), '10-14': np.float64(75.8), '15-19': np.float64(88.8), '20-24': np.float64(78.4), '25-29': np.float64(82.4), '30-34': np.float64(91.0), '35-39': np.float64(87.6), '40-44': np.float64(92.4), '45-49': np.float64(80.6), '50-54': np.float64(86.0), '55-59': np.float64(71.4), '60-64': np.float64(84.0), '65-69': np.float64(89.6), 'old': np.float64(84.65), 'new': np.float64(89.6)}
2025-12-10 14:44:01,347 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0)]
2025-12-10 14:44:01,347 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36)]
2025-12-10 14:44:01,347 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429]
2025-12-10 14:44:06,097 [trainer.py] => All params: 114881051
2025-12-10 14:44:06,100 [trainer.py] => Trainable params: 188165
2025-12-10 14:44:06,100 [inflora.py] => Learning on 70-75
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'classifier_pool.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight'}
2025-12-10 14:48:57,720 [inflora.py] => Task 14, Epoch 20/20 => Loss 0.095, Train_accy 96.68
Threshold:  0.985
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 32/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 56/768 type remove
Layer 6 : 78/768 type remove
Layer 7 : 96/768 type remove
Layer 8 : 136/768 type remove
Layer 9 : 188/768 type remove
Layer 10 : 183/768 type remove
Layer 11 : 105/768 type remove
Layer 12 : 105/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:49:15,162 [trainer.py] => Time:309.0619833469391
7500 7500
7500 7500
2025-12-10 14:49:34,948 [trainer.py] => Time:19.785569429397583
2025-12-10 14:49:34,948 [inflora.py] => Exemplar size: 0
2025-12-10 14:49:34,948 [trainer.py] => CNN: {'total': np.float64(82.84), '00-04': np.float64(90.2), '05-09': np.float64(90.8), '10-14': np.float64(74.2), '15-19': np.float64(89.0), '20-24': np.float64(76.2), '25-29': np.float64(84.2), '30-34': np.float64(90.0), '35-39': np.float64(88.2), '40-44': np.float64(93.6), '45-49': np.float64(79.4), '50-54': np.float64(83.0), '55-59': np.float64(72.6), '60-64': np.float64(82.6), '65-69': np.float64(88.6), '70-74': np.float64(60.0), 'old': np.float64(84.47), 'new': np.float64(60.0)}
2025-12-10 14:49:34,948 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84)]
2025-12-10 14:49:34,948 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17)]
2025-12-10 14:49:34,948 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333]
2025-12-10 14:49:35,663 [trainer.py] => All params: 114881051
2025-12-10 14:49:35,666 [trainer.py] => Trainable params: 188165
2025-12-10 14:49:35,666 [inflora.py] => Learning on 75-80
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight'}
2025-12-10 14:54:26,791 [inflora.py] => Task 15, Epoch 20/20 => Loss 0.078, Train_accy 97.04
Threshold:  0.9875
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 36/768 type remove
Layer 4 : 50/768 type remove
Layer 5 : 66/768 type remove
Layer 6 : 94/768 type remove
Layer 7 : 115/768 type remove
Layer 8 : 174/768 type remove
Layer 9 : 231/768 type remove
Layer 10 : 229/768 type remove
Layer 11 : 122/768 type remove
Layer 12 : 111/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:54:44,174 [trainer.py] => Time:308.50826501846313
8000 8000
8000 8000
2025-12-10 14:55:05,178 [trainer.py] => Time:21.00324535369873
2025-12-10 14:55:05,178 [inflora.py] => Exemplar size: 0
2025-12-10 14:55:05,178 [trainer.py] => CNN: {'total': np.float64(83.86), '00-04': np.float64(90.4), '05-09': np.float64(93.6), '10-14': np.float64(76.4), '15-19': np.float64(89.6), '20-24': np.float64(78.8), '25-29': np.float64(83.0), '30-34': np.float64(89.6), '35-39': np.float64(86.0), '40-44': np.float64(93.4), '45-49': np.float64(80.0), '50-54': np.float64(82.6), '55-59': np.float64(73.2), '60-64': np.float64(79.2), '65-69': np.float64(88.0), '70-74': np.float64(61.6), '75-79': np.float64(96.4), 'old': np.float64(83.03), 'new': np.float64(96.4)}
2025-12-10 14:55:05,178 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84), np.float64(83.86)]
2025-12-10 14:55:05,178 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17), np.float64(99.24)]
2025-12-10 14:55:05,178 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333, 0.839125]
2025-12-10 14:55:06,870 [trainer.py] => All params: 114881051
2025-12-10 14:55:06,873 [trainer.py] => Trainable params: 188165
2025-12-10 14:55:06,873 [inflora.py] => Learning on 80-85
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight'}
2025-12-10 14:59:58,840 [inflora.py] => Task 16, Epoch 20/20 => Loss 0.040, Train_accy 98.52
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 57/768 type remove
Layer 5 : 75/768 type remove
Layer 6 : 110/768 type remove
Layer 7 : 134/768 type remove
Layer 8 : 201/768 type remove
Layer 9 : 267/768 type remove
Layer 10 : 278/768 type remove
Layer 11 : 171/768 type remove
Layer 12 : 141/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:00:16,673 [trainer.py] => Time:309.79975295066833
8500 8500
8500 8500
2025-12-10 15:00:38,985 [trainer.py] => Time:22.312228679656982
2025-12-10 15:00:38,986 [inflora.py] => Exemplar size: 0
2025-12-10 15:00:38,986 [trainer.py] => CNN: {'total': np.float64(82.76), '00-04': np.float64(91.0), '05-09': np.float64(93.8), '10-14': np.float64(74.6), '15-19': np.float64(90.8), '20-24': np.float64(78.8), '25-29': np.float64(81.8), '30-34': np.float64(90.2), '35-39': np.float64(83.4), '40-44': np.float64(91.4), '45-49': np.float64(79.2), '50-54': np.float64(82.6), '55-59': np.float64(69.4), '60-64': np.float64(78.8), '65-69': np.float64(87.4), '70-74': np.float64(58.6), '75-79': np.float64(94.8), '80-84': np.float64(80.4), 'old': np.float64(82.91), 'new': np.float64(80.4)}
2025-12-10 15:00:38,986 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84), np.float64(83.86), np.float64(82.76)]
2025-12-10 15:00:38,986 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17), np.float64(99.24), np.float64(99.36)]
2025-12-10 15:00:38,986 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333, 0.839125, 0.828]
2025-12-10 15:00:42,179 [trainer.py] => All params: 114881051
2025-12-10 15:00:42,182 [trainer.py] => Trainable params: 188165
2025-12-10 15:00:42,182 [inflora.py] => Learning on 85-90
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight'}
2025-12-10 15:05:33,618 [inflora.py] => Task 17, Epoch 20/20 => Loss 0.080, Train_accy 96.84
Threshold:  0.9924999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 68/768 type remove
Layer 5 : 91/768 type remove
Layer 6 : 134/768 type remove
Layer 7 : 163/768 type remove
Layer 8 : 227/768 type remove
Layer 9 : 301/768 type remove
Layer 10 : 326/768 type remove
Layer 11 : 224/768 type remove
Layer 12 : 168/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:05:51,166 [trainer.py] => Time:308.9842143058777
9000 9000
9000 9000
2025-12-10 15:06:14,774 [trainer.py] => Time:23.60785746574402
2025-12-10 15:06:14,775 [inflora.py] => Exemplar size: 0
2025-12-10 15:06:14,775 [trainer.py] => CNN: {'total': np.float64(82.99), '00-04': np.float64(90.2), '05-09': np.float64(93.6), '10-14': np.float64(76.6), '15-19': np.float64(90.4), '20-24': np.float64(78.8), '25-29': np.float64(82.2), '30-34': np.float64(90.2), '35-39': np.float64(82.6), '40-44': np.float64(90.0), '45-49': np.float64(77.4), '50-54': np.float64(84.2), '55-59': np.float64(68.4), '60-64': np.float64(79.0), '65-69': np.float64(87.4), '70-74': np.float64(60.0), '75-79': np.float64(94.8), '80-84': np.float64(78.2), '85-89': np.float64(89.8), 'old': np.float64(82.59), 'new': np.float64(89.8)}
2025-12-10 15:06:14,775 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84), np.float64(83.86), np.float64(82.76), np.float64(82.99)]
2025-12-10 15:06:14,775 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17), np.float64(99.24), np.float64(99.36), np.float64(99.29)]
2025-12-10 15:06:14,775 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333, 0.839125, 0.828, 0.8303333333333334]
2025-12-10 15:06:17,561 [trainer.py] => All params: 114881051
2025-12-10 15:06:17,564 [trainer.py] => Trainable params: 188165
2025-12-10 15:06:17,564 [inflora.py] => Learning on 90-95
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.6.attn.lora_B_k.18.weight'}
2025-12-10 15:11:09,348 [inflora.py] => Task 18, Epoch 20/20 => Loss 0.064, Train_accy 97.68
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 29/768 type remove
Layer 3 : 54/768 type remove
Layer 4 : 80/768 type remove
Layer 5 : 109/768 type remove
Layer 6 : 160/768 type remove
Layer 7 : 195/768 type remove
Layer 8 : 275/768 type remove
Layer 9 : 358/768 type remove
Layer 10 : 381/768 type retain
Layer 11 : 282/768 type remove
Layer 12 : 235/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:11:26,944 [trainer.py] => Time:309.3798146247864
9500 9500
9500 9500
2025-12-10 15:11:51,816 [trainer.py] => Time:24.8723623752594
2025-12-10 15:11:51,817 [inflora.py] => Exemplar size: 0
2025-12-10 15:11:51,817 [trainer.py] => CNN: {'total': np.float64(82.64), '00-04': np.float64(90.2), '05-09': np.float64(92.4), '10-14': np.float64(77.4), '15-19': np.float64(89.4), '20-24': np.float64(79.6), '25-29': np.float64(78.6), '30-34': np.float64(89.0), '35-39': np.float64(84.4), '40-44': np.float64(90.6), '45-49': np.float64(79.2), '50-54': np.float64(84.2), '55-59': np.float64(65.4), '60-64': np.float64(75.8), '65-69': np.float64(86.6), '70-74': np.float64(57.2), '75-79': np.float64(95.0), '80-84': np.float64(74.0), '85-89': np.float64(90.2), '90-94': np.float64(91.0), 'old': np.float64(82.18), 'new': np.float64(91.0)}
2025-12-10 15:11:51,817 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84), np.float64(83.86), np.float64(82.76), np.float64(82.99), np.float64(82.64)]
2025-12-10 15:11:51,817 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17), np.float64(99.24), np.float64(99.36), np.float64(99.29), np.float64(99.38)]
2025-12-10 15:11:51,817 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333, 0.839125, 0.828, 0.8303333333333334, 0.8272631578947368]
2025-12-10 15:11:53,604 [trainer.py] => All params: 114881051
2025-12-10 15:11:53,607 [trainer.py] => Trainable params: 188165
2025-12-10 15:11:53,607 [inflora.py] => Learning on 95-100
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'classifier_pool.19.bias', 'classifier_pool.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight'}
2025-12-10 15:16:45,822 [inflora.py] => Task 19, Epoch 20/20 => Loss 0.046, Train_accy 98.24
Threshold:  0.9975
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 103/768 type remove
Layer 5 : 143/768 type remove
Layer 6 : 207/768 type remove
Layer 7 : 263/768 type remove
Layer 8 : 358/768 type remove
Layer 9 : 333/768 type retain
Layer 10 : 299/768 type retain
Layer 11 : 372/768 type remove
Layer 12 : 336/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:17:03,802 [trainer.py] => Time:310.19493865966797
10000 10000
10000 10000
2025-12-10 15:17:30,029 [trainer.py] => Time:26.22651195526123
2025-12-10 15:17:30,029 [inflora.py] => Exemplar size: 0
2025-12-10 15:17:30,029 [trainer.py] => CNN: {'total': np.float64(81.16), '00-04': np.float64(91.6), '05-09': np.float64(91.6), '10-14': np.float64(75.6), '15-19': np.float64(88.8), '20-24': np.float64(80.4), '25-29': np.float64(80.8), '30-34': np.float64(88.2), '35-39': np.float64(86.0), '40-44': np.float64(88.4), '45-49': np.float64(74.4), '50-54': np.float64(83.2), '55-59': np.float64(66.0), '60-64': np.float64(77.8), '65-69': np.float64(87.2), '70-74': np.float64(52.8), '75-79': np.float64(93.8), '80-84': np.float64(78.8), '85-89': np.float64(88.0), '90-94': np.float64(90.0), '95-99': np.float64(59.8), 'old': np.float64(82.28), 'new': np.float64(59.8)}
2025-12-10 15:17:30,029 [trainer.py] => CNN top1 curve: [np.float64(99.6), np.float64(99.3), np.float64(95.93), np.float64(95.8), np.float64(94.32), np.float64(92.93), np.float64(92.6), np.float64(90.82), np.float64(90.51), np.float64(89.16), np.float64(87.62), np.float64(86.35), np.float64(85.63), np.float64(85.0), np.float64(82.84), np.float64(83.86), np.float64(82.76), np.float64(82.99), np.float64(82.64), np.float64(81.16)]
2025-12-10 15:17:30,030 [trainer.py] => CNN top1 with task curve: [np.float64(99.6), np.float64(99.6), np.float64(99.6), np.float64(99.5), np.float64(99.6), np.float64(99.5), np.float64(99.43), np.float64(99.58), np.float64(99.38), np.float64(99.4), np.float64(99.45), np.float64(99.38), np.float64(99.37), np.float64(99.36), np.float64(99.17), np.float64(99.24), np.float64(99.36), np.float64(99.29), np.float64(99.38), np.float64(99.37)]
2025-12-10 15:17:30,030 [trainer.py] => CNN top1 task curve: [1.0, 0.997, 0.962, 0.9605, 0.9444, 0.931, 0.9271428571428572, 0.9095, 0.9068888888888889, 0.8928, 0.8774545454545455, 0.8645, 0.8567692307692307, 0.8508571428571429, 0.8289333333333333, 0.839125, 0.828, 0.8303333333333334, 0.8272631578947368, 0.8124]
