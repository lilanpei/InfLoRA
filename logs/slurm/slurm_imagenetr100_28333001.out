logs/ImageNet_R/2_2_sip/InfLoRA/adam/10/0.98_1.0-0.0005/42
2025-12-11 17:06:18,263 [trainer.py] => config: configs/mimg100_inflora_seed42.json
2025-12-11 17:06:18,263 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-11 17:06:18,263 [trainer.py] => prefix: reproduce
2025-12-11 17:06:18,263 [trainer.py] => dataset: ImageNet_R
2025-12-11 17:06:18,263 [trainer.py] => data_path: data/imagenet-r
2025-12-11 17:06:18,263 [trainer.py] => memory_size: 0
2025-12-11 17:06:18,263 [trainer.py] => memory_per_class: 0
2025-12-11 17:06:18,264 [trainer.py] => fixed_memory: True
2025-12-11 17:06:18,264 [trainer.py] => shuffle: False
2025-12-11 17:06:18,264 [trainer.py] => init_cls: 2
2025-12-11 17:06:18,264 [trainer.py] => increment: 2
2025-12-11 17:06:18,264 [trainer.py] => model_name: InfLoRA
2025-12-11 17:06:18,264 [trainer.py] => net_type: sip
2025-12-11 17:06:18,264 [trainer.py] => embd_dim: 768
2025-12-11 17:06:18,264 [trainer.py] => num_heads: 12
2025-12-11 17:06:18,264 [trainer.py] => total_sessions: 100
2025-12-11 17:06:18,264 [trainer.py] => seed: 42
2025-12-11 17:06:18,264 [trainer.py] => EPSILON: 1e-08
2025-12-11 17:06:18,264 [trainer.py] => init_epoch: 50
2025-12-11 17:06:18,264 [trainer.py] => optim: adam
2025-12-11 17:06:18,264 [trainer.py] => init_lr: 0.0005
2025-12-11 17:06:18,264 [trainer.py] => init_lr_decay: 0.1
2025-12-11 17:06:18,264 [trainer.py] => init_weight_decay: 0.0
2025-12-11 17:06:18,264 [trainer.py] => epochs: 50
2025-12-11 17:06:18,264 [trainer.py] => lrate: 0.0005
2025-12-11 17:06:18,264 [trainer.py] => lrate_decay: 0.1
2025-12-11 17:06:18,264 [trainer.py] => batch_size: 128
2025-12-11 17:06:18,264 [trainer.py] => weight_decay: 0.0
2025-12-11 17:06:18,264 [trainer.py] => rank: 10
2025-12-11 17:06:18,264 [trainer.py] => lamb: 0.98
2025-12-11 17:06:18,264 [trainer.py] => lame: 1.0
2025-12-11 17:06:18,265 [trainer.py] => num_workers: 8
2025-12-11 17:06:18,265 [trainer.py] => use_wncm: True
2025-12-11 17:06:18,265 [trainer.py] => wncm_lambda: 0.07
2025-12-11 17:06:18,265 [trainer.py] => save_checkpoints: False
2025-12-11 17:06:18,534 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 4802, unexpected 0
2025-12-11 17:06:21,235 [whitened_ncm_head.py] => WhitenedNCM: Using CPU
2025-12-11 17:06:21,248 [trainer.py] => All params: 144526051
2025-12-11 17:06:21,260 [trainer.py] => Trainable params: 144526051
2025-12-11 17:06:21,261 [inflora.py] => Learning on 0-2
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight'}
2025-12-11 17:08:42,466 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.017, Train_accy 99.42
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 37/768 type remove
Layer 6 : 35/768 type remove
Layer 7 : 37/768 type remove
Layer 8 : 46/768 type remove
Layer 9 : 67/768 type remove
Layer 10 : 49/768 type remove
Layer 11 : 18/768 type remove
Layer 12 : 35/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:08:47,979 [trainer.py] => Time:146.7183403968811
93 93
93 93
2025-12-11 17:08:49,151 [trainer.py] => Time:1.172201156616211
2025-12-11 17:08:49,152 [inflora.py] => Exemplar size: 0
2025-12-11 17:08:49,152 [trainer.py] => CNN: {'total': np.float64(98.92), '00-01': np.float64(98.92), 'old': 0, 'new': np.float64(98.92)}
2025-12-11 17:08:49,152 [trainer.py] => CNN top1 curve: [np.float64(98.92)]
2025-12-11 17:08:49,152 [trainer.py] => CNN top1 with task curve: [np.float64(98.92)]
2025-12-11 17:08:49,152 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-11 17:08:52,248 [trainer.py] => W-NCM: {'00-01': 97.84946236559139}
2025-12-11 17:08:52,248 [trainer.py] => Ave Acc (W-NCM): 97.85%
2025-12-11 17:08:52,248 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 97.85% (best 97.85%)
2025-12-11 17:08:52,248 [trainer.py] => Average forgetting (W-NCM): 0.00% | Max forgetting (W-NCM): 0.00%
2025-12-11 17:08:52,261 [trainer.py] => All params: 144526051
2025-12-11 17:08:52,274 [trainer.py] => Trainable params: 185858
2025-12-11 17:08:52,274 [inflora.py] => Learning on 2-4
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'classifier_pool.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'classifier_pool.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'classifier_pool.18.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight'}
2025-12-11 17:10:42,291 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.053, Train_accy 99.13
Threshold:  0.9802
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 26/768 type remove
Layer 5 : 41/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 41/768 type remove
Layer 8 : 55/768 type remove
Layer 9 : 79/768 type remove
Layer 10 : 61/768 type remove
Layer 11 : 23/768 type remove
Layer 12 : 55/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:10:48,932 [trainer.py] => Time:116.65818476676941
155 155
155 155
2025-12-11 17:10:50,373 [trainer.py] => Time:1.4409737586975098
2025-12-11 17:10:50,374 [inflora.py] => Exemplar size: 0
2025-12-11 17:10:50,374 [trainer.py] => CNN: {'total': np.float64(79.35), '00-01': np.float64(79.57), '02-03': np.float64(79.03), 'old': np.float64(79.57), 'new': np.float64(79.03)}
2025-12-11 17:10:50,374 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35)]
2025-12-11 17:10:50,374 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77)]
2025-12-11 17:10:50,374 [trainer.py] => CNN top1 task curve: [1.0, 0.8]
2025-12-11 17:10:53,829 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 90.32258064516128}
2025-12-11 17:10:53,829 [trainer.py] => Ave Acc (W-NCM): 77.42%
2025-12-11 17:10:53,829 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 90.32% (best 90.32%)
2025-12-11 17:10:53,830 [trainer.py] => Average forgetting (W-NCM): 33.33% | Max forgetting (W-NCM): 33.33%
2025-12-11 17:10:53,842 [trainer.py] => All params: 144526051
2025-12-11 17:10:53,855 [trainer.py] => Trainable params: 2044438
2025-12-11 17:10:53,855 [inflora.py] => Learning on 4-6
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'classifier_pool.22.bias', 'classifier_pool.27.bias', 'classifier_pool.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight'}
2025-12-11 17:13:12,235 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.057, Train_accy 97.90
Threshold:  0.9803999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 23/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 45/768 type remove
Layer 6 : 42/768 type remove
Layer 7 : 45/768 type remove
Layer 8 : 59/768 type remove
Layer 9 : 83/768 type remove
Layer 10 : 65/768 type remove
Layer 11 : 25/768 type remove
Layer 12 : 62/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:13:19,355 [trainer.py] => Time:145.50059580802917
244 244
244 244
2025-12-11 17:13:20,938 [trainer.py] => Time:1.582298755645752
2025-12-11 17:13:20,938 [inflora.py] => Exemplar size: 0
2025-12-11 17:13:20,938 [trainer.py] => CNN: {'total': np.float64(69.26), '00-01': np.float64(77.42), '02-03': np.float64(70.97), '04-05': np.float64(59.55), 'old': np.float64(74.84), 'new': np.float64(59.55)}
2025-12-11 17:13:20,938 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26)]
2025-12-11 17:13:20,938 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08)]
2025-12-11 17:13:20,938 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722]
2025-12-11 17:13:24,640 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 85.48387096774194, '04-05': 91.01123595505618}
2025-12-11 17:13:24,640 [trainer.py] => Ave Acc (W-NCM): 80.70%
2025-12-11 17:13:24,640 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 85.48% (best 90.32%); T3: W-NCM 91.01% (best 91.01%)
2025-12-11 17:13:24,640 [trainer.py] => Average forgetting (W-NCM): 18.55% | Max forgetting (W-NCM): 32.26%
2025-12-11 17:13:24,653 [trainer.py] => All params: 144526051
2025-12-11 17:13:24,665 [trainer.py] => Trainable params: 2044438
2025-12-11 17:13:24,665 [inflora.py] => Learning on 6-8
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'classifier_pool.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight'}
2025-12-11 17:15:02,478 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.018, Train_accy 99.44
Threshold:  0.9806
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 47/768 type remove
Layer 6 : 44/768 type remove
Layer 7 : 48/768 type remove
Layer 8 : 62/768 type remove
Layer 9 : 88/768 type remove
Layer 10 : 73/768 type remove
Layer 11 : 29/768 type remove
Layer 12 : 65/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:15:09,094 [trainer.py] => Time:104.42910361289978
300 300
300 300
2025-12-11 17:15:10,838 [trainer.py] => Time:1.7438957691192627
2025-12-11 17:15:10,839 [inflora.py] => Exemplar size: 0
2025-12-11 17:15:10,839 [trainer.py] => CNN: {'total': np.float64(71.67), '00-01': np.float64(81.72), '02-03': np.float64(67.74), '04-05': np.float64(59.55), '06-07': np.float64(78.57), 'old': np.float64(70.08), 'new': np.float64(78.57)}
2025-12-11 17:15:10,839 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67)]
2025-12-11 17:15:10,839 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0)]
2025-12-11 17:15:10,839 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667]
2025-12-11 17:15:14,437 [trainer.py] => W-NCM: {'00-01': 77.41935483870968, '02-03': 82.25806451612904, '04-05': 88.76404494382022, '06-07': 92.85714285714286}
2025-12-11 17:15:14,438 [trainer.py] => Ave Acc (W-NCM): 85.32%
2025-12-11 17:15:14,438 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 77.42% (best 97.85%); T2: W-NCM 82.26% (best 90.32%); T3: W-NCM 88.76% (best 91.01%); T4: W-NCM 92.86% (best 92.86%)
2025-12-11 17:15:14,438 [trainer.py] => Average forgetting (W-NCM): 10.25% | Max forgetting (W-NCM): 20.43%
2025-12-11 17:15:14,451 [trainer.py] => All params: 144526051
2025-12-11 17:15:14,463 [trainer.py] => Trainable params: 2044438
2025-12-11 17:15:14,463 [inflora.py] => Learning on 8-10
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'classifier_pool.48.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'classifier_pool.43.bias', 'classifier_pool.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'classifier_pool.44.bias', 'classifier_pool.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'classifier_pool.47.weight'}
2025-12-11 17:17:41,183 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.089, Train_accy 96.22
Threshold:  0.9808
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 34/768 type remove
Layer 5 : 51/768 type remove
Layer 6 : 48/768 type remove
Layer 7 : 52/768 type remove
Layer 8 : 67/768 type remove
Layer 9 : 96/768 type remove
Layer 10 : 85/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 67/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:17:48,407 [trainer.py] => Time:153.94430112838745
373 373
373 373
2025-12-11 17:17:50,338 [trainer.py] => Time:1.9305095672607422
2025-12-11 17:17:50,338 [inflora.py] => Exemplar size: 0
2025-12-11 17:17:50,338 [trainer.py] => CNN: {'total': np.float64(66.22), '00-01': np.float64(78.49), '02-03': np.float64(62.9), '04-05': np.float64(59.55), '06-07': np.float64(76.79), '08-09': np.float64(53.42), 'old': np.float64(69.33), 'new': np.float64(53.42)}
2025-12-11 17:17:50,338 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22)]
2025-12-11 17:17:50,338 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98)]
2025-12-11 17:17:50,339 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115]
2025-12-11 17:17:54,423 [trainer.py] => W-NCM: {'00-01': 78.49462365591397, '02-03': 75.80645161290323, '04-05': 78.65168539325843, '06-07': 85.71428571428571, '08-09': 83.56164383561644}
2025-12-11 17:17:54,424 [trainer.py] => Ave Acc (W-NCM): 80.45%
2025-12-11 17:17:54,424 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 78.49% (best 97.85%); T2: W-NCM 75.81% (best 90.32%); T3: W-NCM 78.65% (best 91.01%); T4: W-NCM 85.71% (best 92.86%); T5: W-NCM 83.56% (best 83.56%)
2025-12-11 17:17:54,424 [trainer.py] => Average forgetting (W-NCM): 13.34% | Max forgetting (W-NCM): 19.35%
2025-12-11 17:17:54,437 [trainer.py] => All params: 144526051
2025-12-11 17:17:54,450 [trainer.py] => Trainable params: 2044438
2025-12-11 17:17:54,450 [inflora.py] => Learning on 10-12
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.57.weight', 'image_encoder.blocks.10.attn.lora_B_k.55.weight', 'image_encoder.blocks.5.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.50.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.57.weight', 'image_encoder.blocks.2.attn.lora_B_v.57.weight', 'image_encoder.blocks.5.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.54.weight', 'image_encoder.blocks.7.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_v.57.weight', 'image_encoder.blocks.3.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_v.58.weight', 'image_encoder.blocks.1.attn.lora_B_k.58.weight', 'image_encoder.blocks.1.attn.lora_B_k.55.weight', 'classifier_pool.59.weight', 'image_encoder.blocks.9.attn.lora_B_v.51.weight', 'image_encoder.blocks.1.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_k.57.weight', 'image_encoder.blocks.11.attn.lora_B_k.56.weight', 'image_encoder.blocks.8.attn.lora_B_k.50.weight', 'image_encoder.blocks.3.attn.lora_B_k.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_v.54.weight', 'image_encoder.blocks.3.attn.lora_B_k.57.weight', 'image_encoder.blocks.3.attn.lora_B_v.53.weight', 'image_encoder.blocks.11.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_k.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.57.weight', 'image_encoder.blocks.4.attn.lora_B_k.52.weight', 'image_encoder.blocks.11.attn.lora_B_k.57.weight', 'image_encoder.blocks.10.attn.lora_B_k.59.weight', 'image_encoder.blocks.10.attn.lora_B_k.51.weight', 'image_encoder.blocks.0.attn.lora_B_v.50.weight', 'image_encoder.blocks.3.attn.lora_B_k.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.52.weight', 'image_encoder.blocks.10.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.54.weight', 'classifier_pool.50.weight', 'image_encoder.blocks.5.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.52.weight', 'image_encoder.blocks.3.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_v.59.weight', 'image_encoder.blocks.2.attn.lora_B_v.58.weight', 'image_encoder.blocks.9.attn.lora_B_v.56.weight', 'classifier_pool.56.weight', 'image_encoder.blocks.1.attn.lora_B_k.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.59.weight', 'image_encoder.blocks.6.attn.lora_B_k.51.weight', 'classifier_pool.59.bias', 'image_encoder.blocks.3.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_k.53.weight', 'classifier_pool.52.bias', 'image_encoder.blocks.7.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_v.50.weight', 'image_encoder.blocks.5.attn.lora_B_k.58.weight', 'image_encoder.blocks.2.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_k.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.55.weight', 'image_encoder.blocks.3.attn.lora_B_k.54.weight', 'image_encoder.blocks.6.attn.lora_B_v.52.weight', 'image_encoder.blocks.2.attn.lora_B_k.51.weight', 'image_encoder.blocks.9.attn.lora_B_v.52.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.56.weight', 'image_encoder.blocks.7.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.53.weight', 'image_encoder.blocks.10.attn.lora_B_k.57.weight', 'image_encoder.blocks.9.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_v.53.weight', 'image_encoder.blocks.8.attn.lora_B_v.58.weight', 'image_encoder.blocks.0.attn.lora_B_k.52.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.54.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.59.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.56.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.51.weight', 'image_encoder.blocks.6.attn.lora_B_v.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.52.weight', 'image_encoder.blocks.0.attn.lora_B_v.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.54.weight', 'image_encoder.blocks.5.attn.lora_B_k.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.51.weight', 'classifier_pool.57.bias', 'image_encoder.blocks.9.attn.lora_B_k.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.59.weight', 'image_encoder.blocks.7.attn.lora_B_k.59.weight', 'image_encoder.blocks.9.attn.lora_B_k.56.weight', 'image_encoder.blocks.6.attn.lora_B_v.54.weight', 'image_encoder.blocks.6.attn.lora_B_k.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.57.weight', 'image_encoder.blocks.1.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_v.50.weight', 'image_encoder.blocks.8.attn.lora_B_k.59.weight', 'image_encoder.blocks.11.attn.lora_B_v.53.weight', 'image_encoder.blocks.6.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.53.weight', 'image_encoder.blocks.9.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.59.weight', 'image_encoder.blocks.8.attn.lora_B_k.55.weight', 'classifier_pool.55.weight', 'image_encoder.blocks.8.attn.lora_B_v.51.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.52.weight', 'image_encoder.blocks.7.attn.lora_B_k.56.weight', 'image_encoder.blocks.6.attn.lora_B_v.58.weight', 'image_encoder.blocks.11.attn.lora_B_k.55.weight', 'image_encoder.blocks.11.attn.lora_B_k.50.weight', 'image_encoder.blocks.4.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_k.59.weight', 'image_encoder.blocks.0.attn.lora_B_k.50.weight', 'image_encoder.blocks.1.attn.lora_B_k.53.weight', 'image_encoder.blocks.8.attn.lora_B_v.50.weight', 'image_encoder.blocks.11.attn.lora_B_k.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.53.weight', 'image_encoder.blocks.5.attn.lora_B_v.51.weight', 'image_encoder.blocks.0.attn.lora_B_k.51.weight', 'image_encoder.blocks.5.attn.lora_B_k.59.weight', 'image_encoder.blocks.11.attn.lora_B_v.52.weight', 'classifier_pool.51.bias', 'image_encoder.blocks.6.attn.lora_B_v.50.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.58.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.55.weight', 'image_encoder.blocks.7.attn.lora_B_k.57.weight', 'image_encoder.blocks.4.attn.lora_B_k.59.weight', 'image_encoder.blocks.0.attn.lora_B_k.58.weight', 'image_encoder.blocks.0.attn.lora_B_k.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.53.weight', 'image_encoder.blocks.3.attn.lora_B_k.58.weight', 'image_encoder.blocks.10.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.57.weight', 'image_encoder.blocks.1.attn.lora_B_k.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.52.weight', 'image_encoder.blocks.9.attn.lora_B_k.51.weight', 'image_encoder.blocks.0.attn.lora_B_v.52.weight', 'image_encoder.blocks.8.attn.lora_B_v.55.weight', 'image_encoder.blocks.8.attn.lora_B_v.54.weight', 'classifier_pool.58.weight', 'image_encoder.blocks.1.attn.lora_B_v.54.weight', 'image_encoder.blocks.10.attn.lora_B_v.51.weight', 'image_encoder.blocks.9.attn.lora_B_k.50.weight', 'image_encoder.blocks.10.attn.lora_B_k.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.52.weight', 'image_encoder.blocks.1.attn.lora_B_v.50.weight', 'image_encoder.blocks.8.attn.lora_B_v.52.weight', 'image_encoder.blocks.4.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_v.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.59.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.3.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_k.53.weight', 'image_encoder.blocks.11.attn.lora_B_v.51.weight', 'image_encoder.blocks.1.attn.lora_B_v.55.weight', 'image_encoder.blocks.10.attn.lora_B_v.55.weight', 'image_encoder.blocks.11.attn.lora_B_v.59.weight', 'classifier_pool.53.bias', 'image_encoder.blocks.6.attn.lora_B_v.55.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.55.weight', 'image_encoder.blocks.1.attn.lora_B_k.50.weight', 'image_encoder.blocks.3.attn.lora_B_v.56.weight', 'image_encoder.blocks.4.attn.lora_B_k.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.53.weight', 'image_encoder.blocks.0.attn.lora_B_v.58.weight', 'image_encoder.blocks.2.attn.lora_B_k.59.weight', 'image_encoder.blocks.6.attn.lora_B_k.57.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.56.weight', 'image_encoder.blocks.5.attn.lora_B_k.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.57.weight', 'image_encoder.blocks.10.attn.lora_B_v.50.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'classifier_pool.57.weight', 'image_encoder.blocks.11.attn.lora_B_k.53.weight', 'image_encoder.blocks.3.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_v.53.weight', 'image_encoder.blocks.9.attn.lora_B_v.58.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.51.weight', 'image_encoder.blocks.2.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_k.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_k.52.weight', 'image_encoder.blocks.4.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.57.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'classifier_pool.50.bias', 'image_encoder.blocks.6.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.59.weight', 'classifier_pool.55.bias', 'image_encoder.blocks.8.attn.lora_B_v.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.51.weight', 'image_encoder.blocks.10.attn.lora_B_v.53.weight', 'classifier_pool.54.weight', 'image_encoder.blocks.7.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.57.weight', 'image_encoder.blocks.10.attn.lora_B_k.52.weight', 'classifier_pool.58.bias', 'image_encoder.blocks.6.attn.lora_B_v.56.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.55.weight', 'image_encoder.blocks.7.attn.lora_B_v.57.weight', 'image_encoder.blocks.10.attn.lora_B_v.59.weight', 'image_encoder.blocks.4.attn.lora_B_v.51.weight', 'image_encoder.blocks.0.attn.lora_B_v.54.weight', 'image_encoder.blocks.3.attn.lora_B_v.55.weight', 'classifier_pool.52.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.50.weight', 'image_encoder.blocks.1.attn.lora_B_v.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.51.weight', 'image_encoder.blocks.6.attn.lora_B_k.54.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.58.weight', 'image_encoder.blocks.2.attn.lora_B_k.56.weight', 'image_encoder.blocks.3.attn.lora_B_v.57.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.54.weight', 'image_encoder.blocks.2.attn.lora_B_v.52.weight', 'image_encoder.blocks.2.attn.lora_B_v.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_k.55.weight', 'image_encoder.blocks.10.attn.lora_B_k.58.weight', 'image_encoder.blocks.10.attn.lora_B_k.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.50.weight', 'image_encoder.blocks.0.attn.lora_B_k.55.weight', 'image_encoder.blocks.11.attn.lora_B_v.55.weight', 'image_encoder.blocks.2.attn.lora_B_k.57.weight', 'classifier_pool.56.bias', 'image_encoder.blocks.7.attn.lora_B_k.54.weight', 'image_encoder.blocks.2.attn.lora_B_k.58.weight', 'image_encoder.blocks.4.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.57.weight', 'image_encoder.blocks.7.attn.lora_B_k.58.weight', 'classifier_pool.51.weight', 'image_encoder.blocks.9.attn.lora_B_v.54.weight', 'image_encoder.blocks.7.attn.lora_B_v.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.56.weight', 'classifier_pool.53.weight', 'image_encoder.blocks.1.attn.lora_B_k.54.weight', 'image_encoder.blocks.5.attn.lora_B_k.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.56.weight', 'image_encoder.blocks.4.attn.lora_B_v.50.weight', 'classifier_pool.54.bias', 'image_encoder.blocks.8.attn.lora_B_k.51.weight', 'image_encoder.blocks.8.attn.lora_B_k.58.weight', 'image_encoder.blocks.4.attn.lora_B_k.50.weight', 'image_encoder.blocks.6.attn.lora_B_k.59.weight'}
2025-12-11 17:19:38,344 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.029, Train_accy 99.01
Threshold:  0.981
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 36/768 type remove
Layer 5 : 53/768 type remove
Layer 6 : 50/768 type remove
Layer 7 : 54/768 type remove
Layer 8 : 69/768 type remove
Layer 9 : 98/768 type remove
Layer 10 : 88/768 type remove
Layer 11 : 40/768 type remove
Layer 12 : 71/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:19:44,985 [trainer.py] => Time:110.53560447692871
414 414
414 414
2025-12-11 17:19:47,043 [trainer.py] => Time:2.0574684143066406
2025-12-11 17:19:47,043 [inflora.py] => Exemplar size: 0
2025-12-11 17:19:47,043 [trainer.py] => CNN: {'total': np.float64(63.77), '00-01': np.float64(80.65), '02-03': np.float64(62.9), '04-05': np.float64(62.92), '06-07': np.float64(71.43), '08-09': np.float64(50.68), '10-11': np.float64(41.46), 'old': np.float64(66.22), 'new': np.float64(41.46)}
2025-12-11 17:19:47,043 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77)]
2025-12-11 17:19:47,043 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93)]
2025-12-11 17:19:47,043 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898]
2025-12-11 17:19:50,959 [trainer.py] => W-NCM: {'00-01': 68.81720430107528, '02-03': 69.35483870967742, '04-05': 70.78651685393258, '06-07': 87.5, '08-09': 80.82191780821918, '10-11': 80.48780487804879}
2025-12-11 17:19:50,959 [trainer.py] => Ave Acc (W-NCM): 76.29%
2025-12-11 17:19:50,959 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 68.82% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 70.79% (best 91.01%); T4: W-NCM 87.50% (best 92.86%); T5: W-NCM 80.82% (best 83.56%); T6: W-NCM 80.49% (best 80.49%)
2025-12-11 17:19:50,959 [trainer.py] => Average forgetting (W-NCM): 15.66% | Max forgetting (W-NCM): 29.03%
2025-12-11 17:19:50,972 [trainer.py] => All params: 144526051
2025-12-11 17:19:50,984 [trainer.py] => Trainable params: 2044438
2025-12-11 17:19:50,984 [inflora.py] => Learning on 12-14
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.62.weight', 'image_encoder.blocks.0.attn.lora_B_v.66.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.60.weight', 'image_encoder.blocks.9.attn.lora_B_k.63.weight', 'image_encoder.blocks.11.attn.lora_B_v.63.weight', 'image_encoder.blocks.3.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.60.weight', 'image_encoder.blocks.8.attn.lora_B_k.66.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.62.weight', 'image_encoder.blocks.1.attn.lora_B_k.68.weight', 'image_encoder.blocks.3.attn.lora_B_k.67.weight', 'image_encoder.blocks.3.attn.lora_B_v.67.weight', 'image_encoder.blocks.6.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.61.weight', 'image_encoder.blocks.6.attn.lora_B_k.60.weight', 'image_encoder.blocks.7.attn.lora_B_v.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.69.weight', 'image_encoder.blocks.5.attn.lora_B_k.64.weight', 'image_encoder.blocks.9.attn.lora_B_k.69.weight', 'classifier_pool.62.bias', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'classifier_pool.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.66.weight', 'image_encoder.blocks.0.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.64.weight', 'classifier_pool.67.bias', 'image_encoder.blocks.4.attn.lora_B_v.63.weight', 'image_encoder.blocks.10.attn.lora_B_k.61.weight', 'image_encoder.blocks.8.attn.lora_B_v.63.weight', 'image_encoder.blocks.3.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_k.64.weight', 'image_encoder.blocks.4.attn.lora_B_v.69.weight', 'image_encoder.blocks.1.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.61.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.63.weight', 'image_encoder.blocks.6.attn.lora_B_v.68.weight', 'image_encoder.blocks.1.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_k.69.weight', 'image_encoder.blocks.10.attn.lora_B_v.62.weight', 'image_encoder.blocks.8.attn.lora_B_v.69.weight', 'image_encoder.blocks.9.attn.lora_B_k.61.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.68.weight', 'image_encoder.blocks.5.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_k.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.62.weight', 'image_encoder.blocks.6.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_v.67.weight', 'image_encoder.blocks.3.attn.lora_B_k.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.66.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.60.weight', 'image_encoder.blocks.10.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_v.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.66.weight', 'image_encoder.blocks.8.attn.lora_B_k.67.weight', 'image_encoder.blocks.6.attn.lora_B_k.63.weight', 'image_encoder.blocks.0.attn.lora_B_k.65.weight', 'image_encoder.blocks.11.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.66.weight', 'image_encoder.blocks.11.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_v.62.weight', 'image_encoder.blocks.6.attn.lora_B_k.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.66.weight', 'image_encoder.blocks.0.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.65.weight', 'image_encoder.blocks.11.attn.lora_B_v.62.weight', 'image_encoder.blocks.2.attn.lora_B_k.68.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.68.weight', 'image_encoder.blocks.8.attn.lora_B_k.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.60.weight', 'image_encoder.blocks.8.attn.lora_B_k.68.weight', 'image_encoder.blocks.5.attn.lora_B_v.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_v.68.weight', 'image_encoder.blocks.3.attn.lora_B_k.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.68.weight', 'image_encoder.blocks.3.attn.lora_B_v.65.weight', 'image_encoder.blocks.2.attn.lora_B_v.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.62.weight', 'image_encoder.blocks.5.attn.lora_B_k.60.weight', 'image_encoder.blocks.5.attn.lora_B_v.65.weight', 'image_encoder.blocks.7.attn.lora_B_k.60.weight', 'image_encoder.blocks.3.attn.lora_B_k.61.weight', 'image_encoder.blocks.7.attn.lora_B_v.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.69.weight', 'image_encoder.blocks.9.attn.lora_B_k.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.62.weight', 'image_encoder.blocks.5.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.64.weight', 'image_encoder.blocks.3.attn.lora_B_v.64.weight', 'image_encoder.blocks.10.attn.lora_B_k.62.weight', 'image_encoder.blocks.9.attn.lora_B_v.61.weight', 'image_encoder.blocks.7.attn.lora_B_k.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.60.weight', 'image_encoder.blocks.4.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.66.weight', 'classifier_pool.61.bias', 'image_encoder.blocks.1.attn.lora_B_k.61.weight', 'image_encoder.blocks.7.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.65.weight', 'image_encoder.blocks.11.attn.lora_B_v.60.weight', 'image_encoder.blocks.1.attn.lora_B_k.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.61.weight', 'image_encoder.blocks.4.attn.lora_B_v.65.weight', 'image_encoder.blocks.2.attn.lora_B_v.60.weight', 'classifier_pool.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.68.weight', 'classifier_pool.67.weight', 'image_encoder.blocks.7.attn.lora_B_k.68.weight', 'image_encoder.blocks.10.attn.lora_B_v.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.65.weight', 'image_encoder.blocks.8.attn.lora_B_k.62.weight', 'image_encoder.blocks.3.attn.lora_B_v.62.weight', 'image_encoder.blocks.9.attn.lora_B_k.67.weight', 'image_encoder.blocks.6.attn.lora_B_v.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.69.weight', 'image_encoder.blocks.1.attn.lora_B_v.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.69.weight', 'image_encoder.blocks.4.attn.lora_B_v.67.weight', 'image_encoder.blocks.4.attn.lora_B_k.65.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.60.weight', 'image_encoder.blocks.9.attn.lora_B_k.65.weight', 'image_encoder.blocks.0.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.69.weight', 'image_encoder.blocks.7.attn.lora_B_k.66.weight', 'image_encoder.blocks.5.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_k.61.weight', 'image_encoder.blocks.6.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_v.66.weight', 'image_encoder.blocks.5.attn.lora_B_k.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_k.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.64.weight', 'image_encoder.blocks.8.attn.lora_B_v.65.weight', 'classifier_pool.65.bias', 'image_encoder.blocks.3.attn.lora_B_v.63.weight', 'classifier_pool.64.bias', 'image_encoder.blocks.2.attn.lora_B_k.64.weight', 'image_encoder.blocks.10.attn.lora_B_k.69.weight', 'classifier_pool.66.weight', 'image_encoder.blocks.0.attn.lora_B_k.62.weight', 'image_encoder.blocks.9.attn.lora_B_k.66.weight', 'image_encoder.blocks.10.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_v.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.63.weight', 'image_encoder.blocks.11.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.66.weight', 'image_encoder.blocks.8.attn.lora_B_k.64.weight', 'image_encoder.blocks.9.attn.lora_B_k.60.weight', 'image_encoder.blocks.9.attn.lora_B_v.63.weight', 'image_encoder.blocks.8.attn.lora_B_v.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.61.weight', 'image_encoder.blocks.9.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_k.67.weight', 'image_encoder.blocks.7.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.68.weight', 'image_encoder.blocks.2.attn.lora_B_v.63.weight', 'image_encoder.blocks.5.attn.lora_B_k.68.weight', 'classifier_pool.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.69.weight', 'image_encoder.blocks.11.attn.lora_B_k.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.61.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.65.weight', 'image_encoder.blocks.10.attn.lora_B_v.61.weight', 'image_encoder.blocks.6.attn.lora_B_k.64.weight', 'image_encoder.blocks.2.attn.lora_B_k.67.weight', 'classifier_pool.65.weight', 'classifier_pool.69.bias', 'image_encoder.blocks.9.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_v.69.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.66.weight', 'image_encoder.blocks.11.attn.lora_B_k.61.weight', 'image_encoder.blocks.4.attn.lora_B_v.66.weight', 'image_encoder.blocks.0.attn.lora_B_v.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.62.weight', 'image_encoder.blocks.4.attn.lora_B_k.67.weight', 'image_encoder.blocks.4.attn.lora_B_v.61.weight', 'image_encoder.blocks.5.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_k.63.weight', 'image_encoder.blocks.1.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_k.67.weight', 'image_encoder.blocks.6.attn.lora_B_v.62.weight', 'image_encoder.blocks.3.attn.lora_B_k.68.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.67.weight', 'image_encoder.blocks.10.attn.lora_B_v.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.60.weight', 'classifier_pool.68.weight', 'image_encoder.blocks.2.attn.lora_B_k.63.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.61.weight', 'classifier_pool.68.bias', 'classifier_pool.62.weight', 'image_encoder.blocks.6.attn.lora_B_v.66.weight', 'image_encoder.blocks.7.attn.lora_B_v.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.62.weight', 'image_encoder.blocks.9.attn.lora_B_v.67.weight', 'classifier_pool.63.bias', 'image_encoder.blocks.5.attn.lora_B_v.61.weight', 'image_encoder.blocks.10.attn.lora_B_v.69.weight', 'image_encoder.blocks.1.attn.lora_B_k.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.65.weight', 'classifier_pool.66.bias', 'image_encoder.blocks.10.attn.lora_B_v.65.weight', 'classifier_pool.63.weight', 'image_encoder.blocks.0.attn.lora_B_v.63.weight', 'image_encoder.blocks.0.attn.lora_B_v.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.61.weight', 'classifier_pool.60.bias', 'classifier_pool.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.68.weight', 'image_encoder.blocks.0.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_k.67.weight', 'image_encoder.blocks.3.attn.lora_B_v.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.65.weight', 'image_encoder.blocks.5.attn.lora_B_v.66.weight', 'image_encoder.blocks.4.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.60.weight', 'image_encoder.blocks.4.attn.lora_B_k.69.weight', 'image_encoder.blocks.2.attn.lora_B_k.61.weight', 'image_encoder.blocks.4.attn.lora_B_v.60.weight', 'image_encoder.blocks.7.attn.lora_B_k.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.69.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.69.weight', 'image_encoder.blocks.3.attn.lora_B_k.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.61.weight', 'image_encoder.blocks.9.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_k.66.weight', 'image_encoder.blocks.5.attn.lora_B_k.65.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.9.attn.lora_B_v.64.weight'}
2025-12-11 17:21:39,844 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.013, Train_accy 99.55
Threshold:  0.9812
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 34/768 type remove
Layer 4 : 42/768 type remove
Layer 5 : 59/768 type remove
Layer 6 : 53/768 type remove
Layer 7 : 57/768 type remove
Layer 8 : 71/768 type remove
Layer 9 : 100/768 type remove
Layer 10 : 91/768 type remove
Layer 11 : 42/768 type remove
Layer 12 : 74/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:21:46,581 [trainer.py] => Time:115.596435546875
468 468
468 468
2025-12-11 17:21:48,751 [trainer.py] => Time:2.1699886322021484
2025-12-11 17:21:48,751 [inflora.py] => Exemplar size: 0
2025-12-11 17:21:48,751 [trainer.py] => CNN: {'total': np.float64(63.68), '00-01': np.float64(78.49), '02-03': np.float64(64.52), '04-05': np.float64(62.92), '06-07': np.float64(75.0), '08-09': np.float64(47.95), '10-11': np.float64(39.02), '12-13': np.float64(66.67), 'old': np.float64(63.29), 'new': np.float64(66.67)}
2025-12-11 17:21:48,751 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68)]
2025-12-11 17:21:48,751 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66)]
2025-12-11 17:21:48,751 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367]
2025-12-11 17:21:52,727 [trainer.py] => W-NCM: {'00-01': 75.26881720430107, '02-03': 74.19354838709677, '04-05': 79.7752808988764, '06-07': 87.5, '08-09': 76.71232876712328, '10-11': 70.73170731707317, '12-13': 90.74074074074075}
2025-12-11 17:21:52,727 [trainer.py] => Ave Acc (W-NCM): 79.27%
2025-12-11 17:21:52,727 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 75.27% (best 97.85%); T2: W-NCM 74.19% (best 90.32%); T3: W-NCM 79.78% (best 91.01%); T4: W-NCM 87.50% (best 92.86%); T5: W-NCM 76.71% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 90.74% (best 90.74%)
2025-12-11 17:21:52,727 [trainer.py] => Average forgetting (W-NCM): 11.98% | Max forgetting (W-NCM): 22.58%
2025-12-11 17:21:52,740 [trainer.py] => All params: 144526051
2025-12-11 17:21:52,752 [trainer.py] => Trainable params: 2044438
2025-12-11 17:21:52,752 [inflora.py] => Learning on 14-16
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.75.weight', 'image_encoder.blocks.6.attn.lora_B_k.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.77.weight', 'image_encoder.blocks.0.attn.lora_B_v.77.weight', 'classifier_pool.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.76.weight', 'image_encoder.blocks.8.attn.lora_B_v.70.weight', 'image_encoder.blocks.11.attn.lora_B_k.76.weight', 'image_encoder.blocks.6.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.70.weight', 'image_encoder.blocks.10.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_v.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.79.weight', 'image_encoder.blocks.11.attn.lora_B_k.75.weight', 'image_encoder.blocks.5.attn.lora_B_k.70.weight', 'image_encoder.blocks.8.attn.lora_B_k.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.78.weight', 'image_encoder.blocks.10.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_v.71.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.72.weight', 'image_encoder.blocks.2.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.73.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_k.77.weight', 'classifier_pool.74.weight', 'image_encoder.blocks.7.attn.lora_B_v.71.weight', 'image_encoder.blocks.4.attn.lora_B_k.71.weight', 'image_encoder.blocks.7.attn.lora_B_k.75.weight', 'image_encoder.blocks.9.attn.lora_B_k.75.weight', 'image_encoder.blocks.8.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.72.weight', 'image_encoder.blocks.8.attn.lora_B_v.74.weight', 'image_encoder.blocks.9.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.75.weight', 'image_encoder.blocks.8.attn.lora_B_v.79.weight', 'image_encoder.blocks.3.attn.lora_B_k.78.weight', 'classifier_pool.76.bias', 'image_encoder.blocks.7.attn.lora_B_v.77.weight', 'image_encoder.blocks.10.attn.lora_B_k.72.weight', 'image_encoder.blocks.10.attn.lora_B_v.76.weight', 'image_encoder.blocks.4.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.79.weight', 'classifier_pool.79.bias', 'image_encoder.blocks.9.attn.lora_B_v.76.weight', 'image_encoder.blocks.6.attn.lora_B_k.74.weight', 'image_encoder.blocks.8.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.77.weight', 'classifier_pool.7.bias', 'classifier_pool.77.weight', 'image_encoder.blocks.5.attn.lora_B_k.72.weight', 'image_encoder.blocks.7.attn.lora_B_k.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.71.weight', 'image_encoder.blocks.5.attn.lora_B_k.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.78.weight', 'image_encoder.blocks.8.attn.lora_B_v.71.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.71.weight', 'image_encoder.blocks.5.attn.lora_B_k.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.71.weight', 'image_encoder.blocks.4.attn.lora_B_v.72.weight', 'image_encoder.blocks.4.attn.lora_B_v.75.weight', 'image_encoder.blocks.5.attn.lora_B_v.75.weight', 'image_encoder.blocks.2.attn.lora_B_k.79.weight', 'image_encoder.blocks.7.attn.lora_B_v.79.weight', 'image_encoder.blocks.10.attn.lora_B_k.70.weight', 'image_encoder.blocks.0.attn.lora_B_k.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.70.weight', 'image_encoder.blocks.5.attn.lora_B_v.70.weight', 'image_encoder.blocks.6.attn.lora_B_k.76.weight', 'image_encoder.blocks.3.attn.lora_B_v.77.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.77.weight', 'image_encoder.blocks.6.attn.lora_B_v.74.weight', 'classifier_pool.72.bias', 'image_encoder.blocks.5.attn.lora_B_v.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.72.weight', 'image_encoder.blocks.9.attn.lora_B_v.73.weight', 'image_encoder.blocks.8.attn.lora_B_k.72.weight', 'image_encoder.blocks.5.attn.lora_B_v.77.weight', 'image_encoder.blocks.9.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.74.weight', 'image_encoder.blocks.11.attn.lora_B_k.74.weight', 'image_encoder.blocks.3.attn.lora_B_v.72.weight', 'image_encoder.blocks.2.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_v.78.weight', 'image_encoder.blocks.9.attn.lora_B_k.71.weight', 'image_encoder.blocks.0.attn.lora_B_k.79.weight', 'image_encoder.blocks.9.attn.lora_B_v.71.weight', 'image_encoder.blocks.10.attn.lora_B_k.74.weight', 'image_encoder.blocks.1.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_k.76.weight', 'image_encoder.blocks.8.attn.lora_B_v.72.weight', 'image_encoder.blocks.2.attn.lora_B_k.71.weight', 'image_encoder.blocks.9.attn.lora_B_v.70.weight', 'image_encoder.blocks.7.attn.lora_B_k.79.weight', 'classifier_pool.78.weight', 'image_encoder.blocks.7.attn.lora_B_k.72.weight', 'image_encoder.blocks.9.attn.lora_B_k.72.weight', 'image_encoder.blocks.10.attn.lora_B_v.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.70.weight', 'image_encoder.blocks.2.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.78.weight', 'image_encoder.blocks.9.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_k.73.weight', 'image_encoder.blocks.2.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.71.weight', 'image_encoder.blocks.7.attn.lora_B_k.70.weight', 'image_encoder.blocks.5.attn.lora_B_k.73.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.76.weight', 'image_encoder.blocks.2.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.70.weight', 'image_encoder.blocks.8.attn.lora_B_k.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.79.weight', 'image_encoder.blocks.7.attn.lora_B_k.71.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.79.weight', 'image_encoder.blocks.4.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.76.weight', 'image_encoder.blocks.10.attn.lora_B_k.77.weight', 'image_encoder.blocks.11.attn.lora_B_k.72.weight', 'image_encoder.blocks.2.attn.lora_B_v.79.weight', 'classifier_pool.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.76.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'classifier_pool.73.bias', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'classifier_pool.71.bias', 'classifier_pool.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.75.weight', 'image_encoder.blocks.1.attn.lora_B_k.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_v.79.weight', 'image_encoder.blocks.3.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.76.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.78.weight', 'image_encoder.blocks.4.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_k.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_v.78.weight', 'image_encoder.blocks.4.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_v.78.weight', 'image_encoder.blocks.0.attn.lora_B_v.71.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.77.weight', 'classifier_pool.74.bias', 'image_encoder.blocks.1.attn.lora_B_v.74.weight', 'image_encoder.blocks.7.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_k.75.weight', 'classifier_pool.77.bias', 'image_encoder.blocks.5.attn.lora_B_k.76.weight', 'image_encoder.blocks.9.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.74.weight', 'image_encoder.blocks.3.attn.lora_B_v.75.weight', 'image_encoder.blocks.5.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_k.74.weight', 'image_encoder.blocks.8.attn.lora_B_k.73.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.75.weight', 'image_encoder.blocks.11.attn.lora_B_v.75.weight', 'classifier_pool.70.bias', 'image_encoder.blocks.10.attn.lora_B_v.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.76.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.73.weight', 'image_encoder.blocks.1.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.72.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.79.weight', 'image_encoder.blocks.5.attn.lora_B_k.77.weight', 'image_encoder.blocks.5.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.70.weight', 'image_encoder.blocks.10.attn.lora_B_v.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.72.weight', 'classifier_pool.75.bias', 'image_encoder.blocks.1.attn.lora_B_v.78.weight', 'image_encoder.blocks.3.attn.lora_B_k.77.weight', 'image_encoder.blocks.4.attn.lora_B_v.73.weight', 'image_encoder.blocks.11.attn.lora_B_k.71.weight', 'classifier_pool.75.weight', 'image_encoder.blocks.3.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.70.weight', 'image_encoder.blocks.7.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.74.weight', 'classifier_pool.78.bias', 'image_encoder.blocks.10.attn.lora_B_k.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.70.weight', 'image_encoder.blocks.3.attn.lora_B_k.76.weight', 'image_encoder.blocks.1.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_k.70.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.74.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.72.weight', 'image_encoder.blocks.1.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_k.77.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.70.weight', 'image_encoder.blocks.2.attn.lora_B_k.75.weight', 'image_encoder.blocks.6.attn.lora_B_v.73.weight', 'image_encoder.blocks.0.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.77.weight', 'image_encoder.blocks.6.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.78.weight', 'image_encoder.blocks.7.attn.lora_B_k.76.weight', 'image_encoder.blocks.1.attn.lora_B_k.76.weight', 'image_encoder.blocks.7.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_k.71.weight', 'image_encoder.blocks.8.attn.lora_B_k.75.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.70.weight', 'image_encoder.blocks.8.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.74.weight', 'image_encoder.blocks.3.attn.lora_B_k.70.weight', 'classifier_pool.76.weight', 'image_encoder.blocks.1.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_k.77.weight', 'classifier_pool.73.weight', 'classifier_pool.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.73.weight', 'image_encoder.blocks.10.attn.lora_B_v.78.weight', 'image_encoder.blocks.6.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.73.weight', 'image_encoder.blocks.11.attn.lora_B_v.73.weight', 'image_encoder.blocks.4.attn.lora_B_v.79.weight', 'image_encoder.blocks.4.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_k.78.weight', 'image_encoder.blocks.5.attn.lora_B_v.74.weight', 'image_encoder.blocks.4.attn.lora_B_v.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.76.weight', 'image_encoder.blocks.2.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.74.weight', 'image_encoder.blocks.5.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.77.weight', 'image_encoder.blocks.9.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_v.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.75.weight', 'image_encoder.blocks.5.attn.lora_B_v.78.weight', 'image_encoder.blocks.3.attn.lora_B_v.70.weight', 'image_encoder.blocks.9.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_v.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.76.weight', 'image_encoder.blocks.3.attn.lora_B_k.74.weight', 'image_encoder.blocks.8.attn.lora_B_k.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.73.weight', 'image_encoder.blocks.4.attn.lora_B_k.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.78.weight', 'image_encoder.blocks.7.attn.lora_B_v.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.77.weight', 'image_encoder.blocks.4.attn.lora_B_v.78.weight'}
2025-12-11 17:23:45,924 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.062, Train_accy 97.79
Threshold:  0.9813999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 38/768 type remove
Layer 4 : 46/768 type remove
Layer 5 : 64/768 type remove
Layer 6 : 56/768 type remove
Layer 7 : 60/768 type remove
Layer 8 : 74/768 type remove
Layer 9 : 103/768 type remove
Layer 10 : 96/768 type remove
Layer 11 : 46/768 type remove
Layer 12 : 82/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:23:52,753 [trainer.py] => Time:120.00111985206604
524 524
524 524
2025-12-11 17:23:55,057 [trainer.py] => Time:2.3029181957244873
2025-12-11 17:23:55,057 [inflora.py] => Exemplar size: 0
2025-12-11 17:23:55,057 [trainer.py] => CNN: {'total': np.float64(61.83), '00-01': np.float64(77.42), '02-03': np.float64(67.74), '04-05': np.float64(64.04), '06-07': np.float64(69.64), '08-09': np.float64(41.1), '10-11': np.float64(36.59), '12-13': np.float64(62.96), '14-15': np.float64(62.5), 'old': np.float64(61.75), 'new': np.float64(62.5)}
2025-12-11 17:23:55,057 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83)]
2025-12-11 17:23:55,057 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42)]
2025-12-11 17:23:55,057 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878]
2025-12-11 17:23:59,381 [trainer.py] => W-NCM: {'00-01': 81.72043010752688, '02-03': 70.96774193548387, '04-05': 74.15730337078652, '06-07': 85.71428571428571, '08-09': 73.97260273972603, '10-11': 68.29268292682927, '12-13': 90.74074074074075, '14-15': 94.64285714285714}
2025-12-11 17:23:59,381 [trainer.py] => Ave Acc (W-NCM): 80.03%
2025-12-11 17:23:59,381 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 81.72% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 74.16% (best 91.01%); T4: W-NCM 85.71% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 68.29% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 94.64% (best 94.64%)
2025-12-11 17:23:59,381 [trainer.py] => Average forgetting (W-NCM): 11.61% | Max forgetting (W-NCM): 19.35%
2025-12-11 17:23:59,394 [trainer.py] => All params: 144526051
2025-12-11 17:23:59,406 [trainer.py] => Trainable params: 2044438
2025-12-11 17:23:59,406 [inflora.py] => Learning on 16-18
Parameters to be updated: {'classifier_pool.84.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.81.weight', 'image_encoder.blocks.7.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_v.81.weight', 'image_encoder.blocks.8.attn.lora_B_k.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.82.weight', 'image_encoder.blocks.0.attn.lora_B_k.80.weight', 'image_encoder.blocks.6.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_v.84.weight', 'image_encoder.blocks.7.attn.lora_B_k.81.weight', 'image_encoder.blocks.6.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.88.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_v.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'classifier_pool.87.bias', 'image_encoder.blocks.1.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.86.weight', 'image_encoder.blocks.11.attn.lora_B_k.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.83.weight', 'image_encoder.blocks.2.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_k.80.weight', 'image_encoder.blocks.1.attn.lora_B_v.83.weight', 'image_encoder.blocks.7.attn.lora_B_v.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.80.weight', 'image_encoder.blocks.0.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_k.83.weight', 'image_encoder.blocks.5.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_k.87.weight', 'image_encoder.blocks.7.attn.lora_B_k.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.87.weight', 'image_encoder.blocks.11.attn.lora_B_v.85.weight', 'classifier_pool.85.bias', 'image_encoder.blocks.11.attn.lora_B_k.86.weight', 'image_encoder.blocks.8.attn.lora_B_v.87.weight', 'image_encoder.blocks.9.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_v.89.weight', 'image_encoder.blocks.4.attn.lora_B_k.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.85.weight', 'image_encoder.blocks.3.attn.lora_B_k.84.weight', 'image_encoder.blocks.3.attn.lora_B_v.80.weight', 'image_encoder.blocks.5.attn.lora_B_v.81.weight', 'image_encoder.blocks.4.attn.lora_B_k.84.weight', 'classifier_pool.81.weight', 'image_encoder.blocks.4.attn.lora_B_k.89.weight', 'image_encoder.blocks.9.attn.lora_B_v.88.weight', 'image_encoder.blocks.10.attn.lora_B_v.88.weight', 'image_encoder.blocks.7.attn.lora_B_v.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.86.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.80.weight', 'image_encoder.blocks.2.attn.lora_B_v.89.weight', 'image_encoder.blocks.3.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_k.88.weight', 'image_encoder.blocks.4.attn.lora_B_v.86.weight', 'image_encoder.blocks.1.attn.lora_B_v.86.weight', 'image_encoder.blocks.6.attn.lora_B_k.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.84.weight', 'image_encoder.blocks.9.attn.lora_B_k.87.weight', 'classifier_pool.81.bias', 'image_encoder.blocks.5.attn.lora_B_k.88.weight', 'classifier_pool.83.bias', 'classifier_pool.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.85.weight', 'image_encoder.blocks.2.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.86.weight', 'classifier_pool.80.bias', 'classifier_pool.89.bias', 'image_encoder.blocks.9.attn.lora_B_k.83.weight', 'image_encoder.blocks.11.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_k.84.weight', 'image_encoder.blocks.3.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_k.89.weight', 'image_encoder.blocks.3.attn.lora_B_v.83.weight', 'image_encoder.blocks.4.attn.lora_B_k.83.weight', 'image_encoder.blocks.4.attn.lora_B_k.85.weight', 'image_encoder.blocks.6.attn.lora_B_k.88.weight', 'image_encoder.blocks.2.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_v.81.weight', 'image_encoder.blocks.8.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_v.89.weight', 'image_encoder.blocks.8.attn.lora_B_v.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'classifier_pool.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.82.weight', 'image_encoder.blocks.1.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.88.weight', 'image_encoder.blocks.6.attn.lora_B_k.81.weight', 'image_encoder.blocks.7.attn.lora_B_k.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.87.weight', 'image_encoder.blocks.8.attn.lora_B_v.86.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.88.weight', 'image_encoder.blocks.1.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.83.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.80.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.84.weight', 'classifier_pool.89.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.80.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.84.weight', 'image_encoder.blocks.0.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_k.84.weight', 'image_encoder.blocks.5.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.81.weight', 'classifier_pool.88.bias', 'image_encoder.blocks.6.attn.lora_B_v.83.weight', 'classifier_pool.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.85.weight', 'image_encoder.blocks.5.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_k.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.80.weight', 'image_encoder.blocks.5.attn.lora_B_v.86.weight', 'image_encoder.blocks.6.attn.lora_B_k.83.weight', 'image_encoder.blocks.5.attn.lora_B_k.82.weight', 'image_encoder.blocks.0.attn.lora_B_v.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.87.weight', 'image_encoder.blocks.8.attn.lora_B_v.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.80.weight', 'image_encoder.blocks.11.attn.lora_B_v.88.weight', 'image_encoder.blocks.0.attn.lora_B_v.83.weight', 'image_encoder.blocks.8.attn.lora_B_v.81.weight', 'image_encoder.blocks.5.attn.lora_B_v.88.weight', 'image_encoder.blocks.6.attn.lora_B_k.80.weight', 'image_encoder.blocks.11.attn.lora_B_k.88.weight', 'classifier_pool.82.bias', 'classifier_pool.84.bias', 'image_encoder.blocks.10.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.87.weight', 'image_encoder.blocks.0.attn.lora_B_k.85.weight', 'image_encoder.blocks.3.attn.lora_B_k.86.weight', 'image_encoder.blocks.5.attn.lora_B_k.81.weight', 'classifier_pool.86.bias', 'image_encoder.blocks.9.attn.lora_B_v.85.weight', 'image_encoder.blocks.5.attn.lora_B_v.80.weight', 'image_encoder.blocks.2.attn.lora_B_k.85.weight', 'image_encoder.blocks.7.attn.lora_B_k.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.85.weight', 'image_encoder.blocks.7.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.82.weight', 'classifier_pool.85.weight', 'image_encoder.blocks.2.attn.lora_B_v.85.weight', 'image_encoder.blocks.11.attn.lora_B_k.87.weight', 'image_encoder.blocks.2.attn.lora_B_v.82.weight', 'image_encoder.blocks.1.attn.lora_B_k.84.weight', 'image_encoder.blocks.2.attn.lora_B_k.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_k.81.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.85.weight', 'image_encoder.blocks.4.attn.lora_B_v.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.84.weight', 'image_encoder.blocks.4.attn.lora_B_v.89.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.86.weight', 'image_encoder.blocks.6.attn.lora_B_v.80.weight', 'classifier_pool.87.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_v.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_v.83.weight', 'image_encoder.blocks.2.attn.lora_B_k.81.weight', 'image_encoder.blocks.5.attn.lora_B_k.85.weight', 'image_encoder.blocks.3.attn.lora_B_v.82.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.0.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.88.weight', 'image_encoder.blocks.7.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.83.weight', 'image_encoder.blocks.8.attn.lora_B_v.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.84.weight', 'image_encoder.blocks.2.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.85.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.84.weight', 'image_encoder.blocks.0.attn.lora_B_v.86.weight', 'image_encoder.blocks.0.attn.lora_B_k.88.weight', 'image_encoder.blocks.6.attn.lora_B_k.82.weight', 'image_encoder.blocks.8.attn.lora_B_v.83.weight', 'image_encoder.blocks.10.attn.lora_B_v.83.weight', 'classifier_pool.88.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.88.weight', 'image_encoder.blocks.7.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.85.weight', 'image_encoder.blocks.5.attn.lora_B_v.89.weight', 'image_encoder.blocks.2.attn.lora_B_v.81.weight', 'image_encoder.blocks.5.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_v.87.weight', 'image_encoder.blocks.4.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_v.85.weight', 'image_encoder.blocks.0.attn.lora_B_k.87.weight', 'image_encoder.blocks.10.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_v.80.weight', 'image_encoder.blocks.4.attn.lora_B_v.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.81.weight', 'image_encoder.blocks.7.attn.lora_B_v.83.weight', 'image_encoder.blocks.6.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_k.80.weight', 'image_encoder.blocks.4.attn.lora_B_v.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_v.82.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.85.weight', 'image_encoder.blocks.4.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_k.87.weight', 'image_encoder.blocks.6.attn.lora_B_v.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.89.weight', 'image_encoder.blocks.3.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.80.weight', 'image_encoder.blocks.0.attn.lora_B_v.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.83.weight', 'image_encoder.blocks.8.attn.lora_B_k.83.weight', 'classifier_pool.80.weight', 'image_encoder.blocks.3.attn.lora_B_v.89.weight', 'image_encoder.blocks.7.attn.lora_B_k.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.82.weight', 'image_encoder.blocks.5.attn.lora_B_v.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.88.weight', 'image_encoder.blocks.9.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_v.82.weight', 'image_encoder.blocks.11.attn.lora_B_v.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.87.weight', 'image_encoder.blocks.9.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.85.weight', 'image_encoder.blocks.4.attn.lora_B_v.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.83.weight'}
2025-12-11 17:26:11,429 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.027, Train_accy 98.96
Threshold:  0.9816
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 39/768 type remove
Layer 4 : 48/768 type remove
Layer 5 : 66/768 type remove
Layer 6 : 58/768 type remove
Layer 7 : 63/768 type remove
Layer 8 : 78/768 type remove
Layer 9 : 107/768 type remove
Layer 10 : 101/768 type remove
Layer 11 : 50/768 type remove
Layer 12 : 85/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:26:18,627 [trainer.py] => Time:139.2212052345276
594 594
594 594
2025-12-11 17:26:21,193 [trainer.py] => Time:2.5652666091918945
2025-12-11 17:26:21,193 [inflora.py] => Exemplar size: 0
2025-12-11 17:26:21,193 [trainer.py] => CNN: {'total': np.float64(61.11), '00-01': np.float64(72.04), '02-03': np.float64(75.81), '04-05': np.float64(67.42), '06-07': np.float64(69.64), '08-09': np.float64(46.58), '10-11': np.float64(41.46), '12-13': np.float64(62.96), '14-15': np.float64(64.29), '16-17': np.float64(41.43), 'old': np.float64(63.74), 'new': np.float64(41.43)}
2025-12-11 17:26:21,193 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11)]
2025-12-11 17:26:21,193 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13)]
2025-12-11 17:26:21,193 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161]
2025-12-11 17:26:25,902 [trainer.py] => W-NCM: {'00-01': 74.19354838709677, '02-03': 72.58064516129032, '04-05': 75.28089887640449, '06-07': 85.71428571428571, '08-09': 72.6027397260274, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 85.71428571428571, '16-17': 98.57142857142858}
2025-12-11 17:26:25,903 [trainer.py] => Ave Acc (W-NCM): 80.14%
2025-12-11 17:26:25,903 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 74.19% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 75.28% (best 91.01%); T4: W-NCM 85.71% (best 92.86%); T5: W-NCM 72.60% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 85.71% (best 94.64%); T9: W-NCM 98.57% (best 98.57%)
2025-12-11 17:26:25,903 [trainer.py] => Average forgetting (W-NCM): 12.35% | Max forgetting (W-NCM): 23.66%
2025-12-11 17:26:25,916 [trainer.py] => All params: 144526051
2025-12-11 17:26:25,928 [trainer.py] => Trainable params: 2044438
2025-12-11 17:26:25,928 [inflora.py] => Learning on 18-20
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.94.weight', 'image_encoder.blocks.10.attn.lora_B_v.97.weight', 'image_encoder.blocks.7.attn.lora_B_v.94.weight', 'image_encoder.blocks.11.attn.lora_B_v.98.weight', 'image_encoder.blocks.8.attn.lora_B_k.93.weight', 'image_encoder.blocks.3.attn.lora_B_v.97.weight', 'image_encoder.blocks.11.attn.lora_B_v.95.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.90.weight', 'image_encoder.blocks.4.attn.lora_B_v.93.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.90.weight', 'image_encoder.blocks.5.attn.lora_B_v.92.weight', 'image_encoder.blocks.9.attn.lora_B_k.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.99.weight', 'image_encoder.blocks.2.attn.lora_B_v.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.98.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_v.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.97.weight', 'classifier_pool.9.weight', 'classifier_pool.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.90.weight', 'image_encoder.blocks.2.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.96.weight', 'image_encoder.blocks.10.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.98.weight', 'classifier_pool.98.bias', 'image_encoder.blocks.4.attn.lora_B_v.97.weight', 'classifier_pool.93.bias', 'image_encoder.blocks.7.attn.lora_B_v.93.weight', 'classifier_pool.94.weight', 'image_encoder.blocks.8.attn.lora_B_v.92.weight', 'image_encoder.blocks.6.attn.lora_B_k.92.weight', 'classifier_pool.94.bias', 'image_encoder.blocks.9.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_v.93.weight', 'image_encoder.blocks.2.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.91.weight', 'image_encoder.blocks.6.attn.lora_B_v.92.weight', 'image_encoder.blocks.10.attn.lora_B_k.99.weight', 'image_encoder.blocks.8.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.92.weight', 'classifier_pool.99.weight', 'image_encoder.blocks.1.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_k.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.94.weight', 'image_encoder.blocks.6.attn.lora_B_k.99.weight', 'classifier_pool.97.bias', 'image_encoder.blocks.2.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_k.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.93.weight', 'image_encoder.blocks.8.attn.lora_B_k.94.weight', 'classifier_pool.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.92.weight', 'image_encoder.blocks.6.attn.lora_B_v.93.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.92.weight', 'image_encoder.blocks.2.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_k.93.weight', 'image_encoder.blocks.11.attn.lora_B_v.96.weight', 'image_encoder.blocks.2.attn.lora_B_k.95.weight', 'image_encoder.blocks.7.attn.lora_B_k.92.weight', 'image_encoder.blocks.6.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.92.weight', 'image_encoder.blocks.10.attn.lora_B_k.96.weight', 'image_encoder.blocks.2.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_v.99.weight', 'image_encoder.blocks.7.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_k.90.weight', 'classifier_pool.92.bias', 'image_encoder.blocks.10.attn.lora_B_v.90.weight', 'image_encoder.blocks.1.attn.lora_B_v.96.weight', 'image_encoder.blocks.4.attn.lora_B_v.99.weight', 'image_encoder.blocks.1.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.99.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.94.weight', 'image_encoder.blocks.6.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.99.weight', 'image_encoder.blocks.6.attn.lora_B_v.96.weight', 'image_encoder.blocks.8.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_k.93.weight', 'image_encoder.blocks.0.attn.lora_B_k.93.weight', 'image_encoder.blocks.5.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_k.98.weight', 'image_encoder.blocks.2.attn.lora_B_v.96.weight', 'image_encoder.blocks.7.attn.lora_B_k.99.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_k.97.weight', 'image_encoder.blocks.7.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.95.weight', 'image_encoder.blocks.10.attn.lora_B_v.91.weight', 'classifier_pool.95.weight', 'image_encoder.blocks.3.attn.lora_B_k.93.weight', 'image_encoder.blocks.7.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_k.91.weight', 'image_encoder.blocks.9.attn.lora_B_k.91.weight', 'image_encoder.blocks.5.attn.lora_B_k.94.weight', 'classifier_pool.90.bias', 'image_encoder.blocks.4.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_k.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.91.weight', 'image_encoder.blocks.0.attn.lora_B_v.96.weight', 'image_encoder.blocks.4.attn.lora_B_k.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.90.weight', 'image_encoder.blocks.0.attn.lora_B_v.94.weight', 'image_encoder.blocks.7.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.97.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.96.weight', 'image_encoder.blocks.7.attn.lora_B_v.99.weight', 'image_encoder.blocks.3.attn.lora_B_v.90.weight', 'image_encoder.blocks.0.attn.lora_B_v.95.weight', 'image_encoder.blocks.5.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_v.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.98.weight', 'image_encoder.blocks.11.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_v.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.90.weight', 'image_encoder.blocks.9.attn.lora_B_k.90.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'classifier_pool.99.bias', 'image_encoder.blocks.2.attn.lora_B_v.92.weight', 'image_encoder.blocks.8.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.99.weight', 'image_encoder.blocks.0.attn.lora_B_k.95.weight', 'image_encoder.blocks.9.attn.lora_B_v.90.weight', 'image_encoder.blocks.2.attn.lora_B_k.90.weight', 'image_encoder.blocks.9.attn.lora_B_v.99.weight', 'image_encoder.blocks.2.attn.lora_B_v.93.weight', 'image_encoder.blocks.0.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.91.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'classifier_pool.93.weight', 'image_encoder.blocks.3.attn.lora_B_v.96.weight', 'image_encoder.blocks.5.attn.lora_B_v.93.weight', 'image_encoder.blocks.6.attn.lora_B_k.96.weight', 'image_encoder.blocks.8.attn.lora_B_k.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.97.weight', 'image_encoder.blocks.10.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_k.95.weight', 'image_encoder.blocks.4.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_k.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.99.weight', 'image_encoder.blocks.8.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_v.93.weight', 'image_encoder.blocks.11.attn.lora_B_v.97.weight', 'image_encoder.blocks.1.attn.lora_B_k.94.weight', 'image_encoder.blocks.6.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_v.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.90.weight', 'image_encoder.blocks.10.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_k.93.weight', 'classifier_pool.96.weight', 'classifier_pool.91.bias', 'image_encoder.blocks.6.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.95.weight', 'image_encoder.blocks.11.attn.lora_B_v.92.weight', 'image_encoder.blocks.5.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.97.weight', 'image_encoder.blocks.7.attn.lora_B_v.91.weight', 'image_encoder.blocks.10.attn.lora_B_v.92.weight', 'image_encoder.blocks.11.attn.lora_B_k.97.weight', 'image_encoder.blocks.5.attn.lora_B_k.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.92.weight', 'image_encoder.blocks.7.attn.lora_B_v.97.weight', 'image_encoder.blocks.10.attn.lora_B_k.98.weight', 'classifier_pool.96.bias', 'image_encoder.blocks.5.attn.lora_B_v.91.weight', 'image_encoder.blocks.1.attn.lora_B_v.93.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_k.97.weight', 'image_encoder.blocks.11.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.94.weight', 'image_encoder.blocks.7.attn.lora_B_v.95.weight', 'image_encoder.blocks.6.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'classifier_pool.91.weight', 'image_encoder.blocks.11.attn.lora_B_v.94.weight', 'image_encoder.blocks.2.attn.lora_B_k.92.weight', 'image_encoder.blocks.10.attn.lora_B_v.95.weight', 'image_encoder.blocks.7.attn.lora_B_v.92.weight', 'image_encoder.blocks.2.attn.lora_B_k.97.weight', 'image_encoder.blocks.4.attn.lora_B_v.95.weight', 'image_encoder.blocks.7.attn.lora_B_k.97.weight', 'image_encoder.blocks.9.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_k.91.weight', 'image_encoder.blocks.3.attn.lora_B_k.99.weight', 'image_encoder.blocks.10.attn.lora_B_k.97.weight', 'image_encoder.blocks.0.attn.lora_B_v.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.96.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.94.weight', 'image_encoder.blocks.10.attn.lora_B_k.95.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.93.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.0.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_k.91.weight', 'image_encoder.blocks.8.attn.lora_B_v.95.weight', 'image_encoder.blocks.0.attn.lora_B_k.92.weight', 'image_encoder.blocks.7.attn.lora_B_v.96.weight', 'classifier_pool.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.98.weight', 'image_encoder.blocks.5.attn.lora_B_k.92.weight', 'image_encoder.blocks.0.attn.lora_B_k.99.weight', 'classifier_pool.95.bias', 'image_encoder.blocks.5.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_k.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.99.weight', 'image_encoder.blocks.5.attn.lora_B_k.90.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_v.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.92.weight', 'image_encoder.blocks.8.attn.lora_B_v.96.weight', 'image_encoder.blocks.9.attn.lora_B_k.99.weight', 'image_encoder.blocks.4.attn.lora_B_v.98.weight', 'image_encoder.blocks.7.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_k.95.weight', 'classifier_pool.97.weight', 'image_encoder.blocks.2.attn.lora_B_v.99.weight', 'image_encoder.blocks.10.attn.lora_B_v.93.weight', 'image_encoder.blocks.3.attn.lora_B_k.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.97.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.96.weight', 'image_encoder.blocks.9.attn.lora_B_k.97.weight', 'image_encoder.blocks.1.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_k.92.weight', 'image_encoder.blocks.5.attn.lora_B_k.95.weight', 'image_encoder.blocks.3.attn.lora_B_k.92.weight', 'image_encoder.blocks.0.attn.lora_B_k.97.weight', 'image_encoder.blocks.6.attn.lora_B_v.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.95.weight', 'image_encoder.blocks.5.attn.lora_B_k.97.weight', 'image_encoder.blocks.5.attn.lora_B_v.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.90.weight'}
2025-12-11 17:28:16,963 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.009, Train_accy 100.00
Threshold:  0.9818
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 41/768 type remove
Layer 4 : 52/768 type remove
Layer 5 : 68/768 type remove
Layer 6 : 60/768 type remove
Layer 7 : 66/768 type remove
Layer 8 : 83/768 type remove
Layer 9 : 114/768 type remove
Layer 10 : 111/768 type remove
Layer 11 : 56/768 type remove
Layer 12 : 94/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:28:23,575 [trainer.py] => Time:117.6474072933197
650 650
650 650
2025-12-11 17:28:26,250 [trainer.py] => Time:2.6743671894073486
2025-12-11 17:28:26,250 [inflora.py] => Exemplar size: 0
2025-12-11 17:28:26,250 [trainer.py] => CNN: {'total': np.float64(58.62), '00-01': np.float64(76.34), '02-03': np.float64(69.35), '04-05': np.float64(59.55), '06-07': np.float64(67.86), '08-09': np.float64(47.95), '10-11': np.float64(39.02), '12-13': np.float64(57.41), '14-15': np.float64(64.29), '16-17': np.float64(27.14), '18-19': np.float64(69.64), 'old': np.float64(57.58), 'new': np.float64(69.64)}
2025-12-11 17:28:26,250 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62)]
2025-12-11 17:28:26,250 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0)]
2025-12-11 17:28:26,250 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692]
2025-12-11 17:28:30,831 [trainer.py] => W-NCM: {'00-01': 68.81720430107528, '02-03': 79.03225806451613, '04-05': 66.29213483146067, '06-07': 82.14285714285714, '08-09': 69.86301369863014, '10-11': 63.41463414634146, '12-13': 90.74074074074075, '14-15': 80.35714285714286, '16-17': 97.14285714285714, '18-19': 94.64285714285714}
2025-12-11 17:28:30,831 [trainer.py] => Ave Acc (W-NCM): 79.24%
2025-12-11 17:28:30,831 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 68.82% (best 97.85%); T2: W-NCM 79.03% (best 90.32%); T3: W-NCM 66.29% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 80.36% (best 94.64%); T9: W-NCM 97.14% (best 98.57%); T10: W-NCM 94.64% (best 94.64%)
2025-12-11 17:28:30,831 [trainer.py] => Average forgetting (W-NCM): 13.58% | Max forgetting (W-NCM): 29.03%
2025-12-11 17:28:30,844 [trainer.py] => All params: 144526051
2025-12-11 17:28:30,856 [trainer.py] => Trainable params: 2044438
2025-12-11 17:28:30,856 [inflora.py] => Learning on 20-22
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'classifier_pool.10.bias', 'classifier_pool.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight'}
2025-12-11 17:31:04,514 [inflora.py] => Task 10, Epoch 50/50 => Loss 0.017, Train_accy 99.74
Threshold:  0.982
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 53/768 type remove
Layer 5 : 69/768 type remove
Layer 6 : 61/768 type remove
Layer 7 : 68/768 type remove
Layer 8 : 86/768 type remove
Layer 9 : 117/768 type remove
Layer 10 : 114/768 type remove
Layer 11 : 58/768 type remove
Layer 12 : 97/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:31:12,225 [trainer.py] => Time:161.3683135509491
741 741
741 741
2025-12-11 17:31:15,147 [trainer.py] => Time:2.9227027893066406
2025-12-11 17:31:15,148 [inflora.py] => Exemplar size: 0
2025-12-11 17:31:15,148 [trainer.py] => CNN: {'total': np.float64(58.03), '00-01': np.float64(69.89), '02-03': np.float64(58.06), '04-05': np.float64(60.67), '06-07': np.float64(75.0), '08-09': np.float64(47.95), '10-11': np.float64(31.71), '12-13': np.float64(51.85), '14-15': np.float64(64.29), '16-17': np.float64(28.57), '18-19': np.float64(71.43), '20-21': np.float64(67.03), 'old': np.float64(56.77), 'new': np.float64(67.03)}
2025-12-11 17:31:15,148 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03)]
2025-12-11 17:31:15,148 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49)]
2025-12-11 17:31:15,148 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687]
2025-12-11 17:31:20,194 [trainer.py] => W-NCM: {'00-01': 70.96774193548387, '02-03': 77.41935483870968, '04-05': 69.66292134831461, '06-07': 78.57142857142857, '08-09': 75.34246575342466, '10-11': 65.85365853658537, '12-13': 88.88888888888889, '14-15': 82.14285714285714, '16-17': 94.28571428571428, '18-19': 87.5, '20-21': 95.6043956043956}
2025-12-11 17:31:20,194 [trainer.py] => Ave Acc (W-NCM): 80.57%
2025-12-11 17:31:20,194 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 70.97% (best 97.85%); T2: W-NCM 77.42% (best 90.32%); T3: W-NCM 69.66% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 88.89% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 94.29% (best 98.57%); T10: W-NCM 87.50% (best 94.64%); T11: W-NCM 95.60% (best 95.60%)
2025-12-11 17:31:20,194 [trainer.py] => Average forgetting (W-NCM): 12.41% | Max forgetting (W-NCM): 26.88%
2025-12-11 17:31:20,207 [trainer.py] => All params: 144526051
2025-12-11 17:31:20,219 [trainer.py] => Trainable params: 185858
2025-12-11 17:31:20,220 [inflora.py] => Learning on 22-24
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight'}
2025-12-11 17:34:01,343 [inflora.py] => Task 11, Epoch 50/50 => Loss 0.052, Train_accy 97.56
Threshold:  0.9822
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 54/768 type remove
Layer 5 : 70/768 type remove
Layer 6 : 63/768 type remove
Layer 7 : 71/768 type remove
Layer 8 : 88/768 type remove
Layer 9 : 120/768 type remove
Layer 10 : 117/768 type remove
Layer 11 : 61/768 type remove
Layer 12 : 99/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:34:08,943 [trainer.py] => Time:168.72368264198303
869 869
869 869
2025-12-11 17:34:12,198 [trainer.py] => Time:3.2545735836029053
2025-12-11 17:34:12,198 [inflora.py] => Exemplar size: 0
2025-12-11 17:34:12,198 [trainer.py] => CNN: {'total': np.float64(58.11), '00-01': np.float64(70.97), '02-03': np.float64(59.68), '04-05': np.float64(64.04), '06-07': np.float64(69.64), '08-09': np.float64(43.84), '10-11': np.float64(29.27), '12-13': np.float64(48.15), '14-15': np.float64(60.71), '16-17': np.float64(30.0), '18-19': np.float64(64.29), '20-21': np.float64(63.74), '22-23': np.float64(67.97), 'old': np.float64(56.41), 'new': np.float64(67.97)}
2025-12-11 17:34:12,199 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11)]
2025-12-11 17:34:12,199 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2)]
2025-12-11 17:34:12,199 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582]
2025-12-11 17:34:17,740 [trainer.py] => W-NCM: {'00-01': 58.06451612903226, '02-03': 69.35483870967742, '04-05': 56.17977528089888, '06-07': 75.0, '08-09': 71.23287671232876, '10-11': 56.09756097560976, '12-13': 88.88888888888889, '14-15': 78.57142857142857, '16-17': 87.14285714285714, '18-19': 78.57142857142857, '20-21': 86.81318681318682, '22-23': 95.3125}
2025-12-11 17:34:17,740 [trainer.py] => Ave Acc (W-NCM): 75.10%
2025-12-11 17:34:17,740 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 58.06% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 88.89% (best 90.74%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 87.14% (best 98.57%); T10: W-NCM 78.57% (best 94.64%); T11: W-NCM 86.81% (best 95.60%); T12: W-NCM 95.31% (best 95.31%)
2025-12-11 17:34:17,740 [trainer.py] => Average forgetting (W-NCM): 18.58% | Max forgetting (W-NCM): 39.78%
2025-12-11 17:34:17,753 [trainer.py] => All params: 144526051
2025-12-11 17:34:17,765 [trainer.py] => Trainable params: 185858
2025-12-11 17:34:17,765 [inflora.py] => Learning on 24-26
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight'}
2025-12-11 17:35:51,812 [inflora.py] => Task 12, Epoch 50/50 => Loss 0.060, Train_accy 98.06
Threshold:  0.9823999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 44/768 type remove
Layer 4 : 55/768 type remove
Layer 5 : 71/768 type remove
Layer 6 : 65/768 type remove
Layer 7 : 73/768 type remove
Layer 8 : 90/768 type remove
Layer 9 : 122/768 type remove
Layer 10 : 120/768 type remove
Layer 11 : 63/768 type remove
Layer 12 : 104/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:35:58,022 [trainer.py] => Time:100.25751876831055
918 918
918 918
2025-12-11 17:36:01,435 [trainer.py] => Time:3.4130172729492188
2025-12-11 17:36:01,436 [inflora.py] => Exemplar size: 0
2025-12-11 17:36:01,436 [trainer.py] => CNN: {'total': np.float64(56.75), '00-01': np.float64(68.82), '02-03': np.float64(62.9), '04-05': np.float64(59.55), '06-07': np.float64(67.86), '08-09': np.float64(43.84), '10-11': np.float64(36.59), '12-13': np.float64(51.85), '14-15': np.float64(62.5), '16-17': np.float64(24.29), '18-19': np.float64(62.5), '20-21': np.float64(68.13), '22-23': np.float64(64.06), '24-25': np.float64(42.86), 'old': np.float64(57.54), 'new': np.float64(42.86)}
2025-12-11 17:36:01,436 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75)]
2025-12-11 17:36:01,436 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08)]
2025-12-11 17:36:01,436 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921]
2025-12-11 17:36:06,486 [trainer.py] => W-NCM: {'00-01': 61.29032258064516, '02-03': 67.74193548387096, '04-05': 50.56179775280899, '06-07': 73.21428571428571, '08-09': 65.75342465753424, '10-11': 58.536585365853654, '12-13': 85.18518518518519, '14-15': 78.57142857142857, '16-17': 87.14285714285714, '18-19': 69.64285714285714, '20-21': 79.12087912087912, '22-23': 75.78125, '24-25': 87.75510204081633}
2025-12-11 17:36:06,487 [trainer.py] => Ave Acc (W-NCM): 72.33%
2025-12-11 17:36:06,487 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 61.29% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 73.21% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 85.19% (best 90.74%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 87.14% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 75.78% (best 95.31%); T13: W-NCM 87.76% (best 87.76%)
2025-12-11 17:36:06,487 [trainer.py] => Average forgetting (W-NCM): 21.09% | Max forgetting (W-NCM): 40.45%
2025-12-11 17:36:06,500 [trainer.py] => All params: 144526051
2025-12-11 17:36:06,512 [trainer.py] => Trainable params: 185858
2025-12-11 17:36:06,512 [inflora.py] => Learning on 26-28
Parameters to be updated: {'classifier_pool.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight'}
2025-12-11 17:39:14,528 [inflora.py] => Task 13, Epoch 50/50 => Loss 0.020, Train_accy 99.02
Threshold:  0.9826
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 45/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 72/768 type remove
Layer 6 : 66/768 type remove
Layer 7 : 75/768 type remove
Layer 8 : 92/768 type remove
Layer 9 : 125/768 type remove
Layer 10 : 124/768 type remove
Layer 11 : 66/768 type remove
Layer 12 : 108/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:39:22,744 [trainer.py] => Time:196.23248100280762
1033 1033
1033 1033
2025-12-11 17:39:26,461 [trainer.py] => Time:3.717054605484009
2025-12-11 17:39:26,462 [inflora.py] => Exemplar size: 0
2025-12-11 17:39:26,462 [trainer.py] => CNN: {'total': np.float64(59.54), '00-01': np.float64(76.34), '02-03': np.float64(53.23), '04-05': np.float64(60.67), '06-07': np.float64(66.07), '08-09': np.float64(41.1), '10-11': np.float64(24.39), '12-13': np.float64(59.26), '14-15': np.float64(64.29), '16-17': np.float64(31.43), '18-19': np.float64(60.71), '20-21': np.float64(70.33), '22-23': np.float64(63.28), '24-25': np.float64(44.9), '26-27': np.float64(77.39), 'old': np.float64(57.3), 'new': np.float64(77.39)}
2025-12-11 17:39:26,462 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54)]
2025-12-11 17:39:26,462 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03)]
2025-12-11 17:39:26,462 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281]
2025-12-11 17:39:32,611 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 74.19354838709677, '04-05': 62.92134831460674, '06-07': 80.35714285714286, '08-09': 72.6027397260274, '10-11': 60.97560975609756, '12-13': 90.74074074074075, '14-15': 83.92857142857143, '16-17': 85.71428571428571, '18-19': 76.78571428571429, '20-21': 82.41758241758241, '22-23': 71.09375, '24-25': 75.51020408163265, '26-27': 94.78260869565217}
2025-12-11 17:39:32,612 [trainer.py] => Ave Acc (W-NCM): 77.05%
2025-12-11 17:39:32,612 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 74.19% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 72.60% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 83.93% (best 94.64%); T9: W-NCM 85.71% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 82.42% (best 95.60%); T12: W-NCM 71.09% (best 95.31%); T13: W-NCM 75.51% (best 87.76%); T14: W-NCM 94.78% (best 94.78%)
2025-12-11 17:39:32,612 [trainer.py] => Average forgetting (W-NCM): 16.11% | Max forgetting (W-NCM): 31.18%
2025-12-11 17:39:32,624 [trainer.py] => All params: 144526051
2025-12-11 17:39:32,636 [trainer.py] => Trainable params: 185858
2025-12-11 17:39:32,636 [inflora.py] => Learning on 28-30
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight'}
2025-12-11 17:41:35,373 [inflora.py] => Task 14, Epoch 50/50 => Loss 0.027, Train_accy 98.86
Threshold:  0.9828
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 46/768 type remove
Layer 4 : 57/768 type remove
Layer 5 : 74/768 type remove
Layer 6 : 67/768 type remove
Layer 7 : 76/768 type remove
Layer 8 : 93/768 type remove
Layer 9 : 126/768 type remove
Layer 10 : 125/768 type remove
Layer 11 : 67/768 type remove
Layer 12 : 112/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:41:42,522 [trainer.py] => Time:129.88569140434265
1114 1114
1114 1114
2025-12-11 17:41:46,447 [trainer.py] => Time:3.9247817993164062
2025-12-11 17:41:46,447 [inflora.py] => Exemplar size: 0
2025-12-11 17:41:46,447 [trainer.py] => CNN: {'total': np.float64(61.85), '00-01': np.float64(74.19), '02-03': np.float64(54.84), '04-05': np.float64(61.8), '06-07': np.float64(64.29), '08-09': np.float64(42.47), '10-11': np.float64(21.95), '12-13': np.float64(62.96), '14-15': np.float64(51.79), '16-17': np.float64(40.0), '18-19': np.float64(66.07), '20-21': np.float64(75.82), '22-23': np.float64(64.84), '24-25': np.float64(40.82), '26-27': np.float64(78.26), '28-29': np.float64(80.25), 'old': np.float64(60.41), 'new': np.float64(80.25)}
2025-12-11 17:41:46,447 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85)]
2025-12-11 17:41:46,447 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05)]
2025-12-11 17:41:46,447 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386]
2025-12-11 17:41:52,228 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 77.41935483870968, '04-05': 59.55056179775281, '06-07': 75.0, '08-09': 73.97260273972603, '10-11': 63.41463414634146, '12-13': 90.74074074074075, '14-15': 82.14285714285714, '16-17': 77.14285714285715, '18-19': 73.21428571428571, '20-21': 78.02197802197803, '22-23': 71.875, '24-25': 73.46938775510205, '26-27': 89.56521739130436, '28-29': 96.29629629629629}
2025-12-11 17:41:52,228 [trainer.py] => Ave Acc (W-NCM): 76.42%
2025-12-11 17:41:52,228 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 77.42% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 71.88% (best 95.31%); T13: W-NCM 73.47% (best 87.76%); T14: W-NCM 89.57% (best 94.78%); T15: W-NCM 96.30% (best 96.30%)
2025-12-11 17:41:52,228 [trainer.py] => Average forgetting (W-NCM): 17.01% | Max forgetting (W-NCM): 33.33%
2025-12-11 17:41:52,241 [trainer.py] => All params: 144526051
2025-12-11 17:41:52,253 [trainer.py] => Trainable params: 185858
2025-12-11 17:41:52,253 [inflora.py] => Learning on 30-32
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'classifier_pool.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight'}
2025-12-11 17:44:42,704 [inflora.py] => Task 15, Epoch 50/50 => Loss 0.028, Train_accy 99.52
Threshold:  0.983
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 47/768 type remove
Layer 4 : 58/768 type remove
Layer 5 : 75/768 type remove
Layer 6 : 68/768 type remove
Layer 7 : 78/768 type remove
Layer 8 : 94/768 type remove
Layer 9 : 128/768 type remove
Layer 10 : 126/768 type remove
Layer 11 : 68/768 type remove
Layer 12 : 115/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:44:51,265 [trainer.py] => Time:179.0125651359558
1224 1224
1224 1224
2025-12-11 17:44:55,459 [trainer.py] => Time:4.193566083908081
2025-12-11 17:44:55,459 [inflora.py] => Exemplar size: 0
2025-12-11 17:44:55,460 [trainer.py] => CNN: {'total': np.float64(59.07), '00-01': np.float64(70.97), '02-03': np.float64(54.84), '04-05': np.float64(60.67), '06-07': np.float64(60.71), '08-09': np.float64(38.36), '10-11': np.float64(21.95), '12-13': np.float64(51.85), '14-15': np.float64(55.36), '16-17': np.float64(34.29), '18-19': np.float64(69.64), '20-21': np.float64(73.63), '22-23': np.float64(67.19), '24-25': np.float64(26.53), '26-27': np.float64(76.52), '28-29': np.float64(80.25), '30-31': np.float64(51.82), 'old': np.float64(59.78), 'new': np.float64(51.82)}
2025-12-11 17:44:55,460 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07)]
2025-12-11 17:44:55,460 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65)]
2025-12-11 17:44:55,460 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039]
2025-12-11 17:45:02,058 [trainer.py] => W-NCM: {'00-01': 63.44086021505376, '02-03': 77.41935483870968, '04-05': 55.0561797752809, '06-07': 80.35714285714286, '08-09': 68.4931506849315, '10-11': 63.41463414634146, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 78.57142857142857, '18-19': 75.0, '20-21': 78.02197802197803, '22-23': 70.3125, '24-25': 73.46938775510205, '26-27': 86.08695652173914, '28-29': 74.07407407407408, '30-31': 93.63636363636364}
2025-12-11 17:45:02,058 [trainer.py] => Ave Acc (W-NCM): 75.42%
2025-12-11 17:45:02,058 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 63.44% (best 97.85%); T2: W-NCM 77.42% (best 90.32%); T3: W-NCM 55.06% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 73.47% (best 87.76%); T14: W-NCM 86.09% (best 94.78%); T15: W-NCM 74.07% (best 96.30%); T16: W-NCM 93.64% (best 93.64%)
2025-12-11 17:45:02,058 [trainer.py] => Average forgetting (W-NCM): 18.09% | Max forgetting (W-NCM): 35.96%
2025-12-11 17:45:02,071 [trainer.py] => All params: 144526051
2025-12-11 17:45:02,083 [trainer.py] => Trainable params: 185858
2025-12-11 17:45:02,083 [inflora.py] => Learning on 32-34
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight'}
2025-12-11 17:46:45,958 [inflora.py] => Task 16, Epoch 50/50 => Loss 0.049, Train_accy 98.31
Threshold:  0.9832
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 59/768 type remove
Layer 5 : 76/768 type remove
Layer 6 : 69/768 type remove
Layer 7 : 80/768 type remove
Layer 8 : 95/768 type remove
Layer 9 : 129/768 type remove
Layer 10 : 128/768 type remove
Layer 11 : 69/768 type remove
Layer 12 : 117/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:46:52,532 [trainer.py] => Time:110.44874548912048
1268 1268
1268 1268
2025-12-11 17:46:56,934 [trainer.py] => Time:4.4022417068481445
2025-12-11 17:46:56,935 [inflora.py] => Exemplar size: 0
2025-12-11 17:46:56,935 [trainer.py] => CNN: {'total': np.float64(58.04), '00-01': np.float64(75.27), '02-03': np.float64(58.06), '04-05': np.float64(60.67), '06-07': np.float64(62.5), '08-09': np.float64(42.47), '10-11': np.float64(24.39), '12-13': np.float64(59.26), '14-15': np.float64(55.36), '16-17': np.float64(22.86), '18-19': np.float64(67.86), '20-21': np.float64(75.82), '22-23': np.float64(62.5), '24-25': np.float64(30.61), '26-27': np.float64(73.91), '28-29': np.float64(76.54), '30-31': np.float64(56.36), '32-33': np.float64(22.73), 'old': np.float64(59.31), 'new': np.float64(22.73)}
2025-12-11 17:46:56,935 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04)]
2025-12-11 17:46:56,935 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53)]
2025-12-11 17:46:56,935 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549]
2025-12-11 17:47:02,899 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 80.64516129032258, '04-05': 56.17977528089888, '06-07': 82.14285714285714, '08-09': 68.4931506849315, '10-11': 60.97560975609756, '12-13': 90.74074074074075, '14-15': 85.71428571428571, '16-17': 77.14285714285715, '18-19': 76.78571428571429, '20-21': 78.02197802197803, '22-23': 68.75, '24-25': 67.3469387755102, '26-27': 82.6086956521739, '28-29': 76.5432098765432, '30-31': 81.81818181818183, '32-33': 97.72727272727273}
2025-12-11 17:47:02,899 [trainer.py] => Ave Acc (W-NCM): 76.31%
2025-12-11 17:47:02,899 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 80.65% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 85.71% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 68.75% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 82.61% (best 94.78%); T15: W-NCM 76.54% (best 96.30%); T16: W-NCM 81.82% (best 93.64%); T17: W-NCM 97.73% (best 97.73%)
2025-12-11 17:47:02,899 [trainer.py] => Average forgetting (W-NCM): 17.41% | Max forgetting (W-NCM): 34.83%
2025-12-11 17:47:02,912 [trainer.py] => All params: 144526051
2025-12-11 17:47:02,924 [trainer.py] => Trainable params: 185858
2025-12-11 17:47:02,924 [inflora.py] => Learning on 34-36
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight'}
2025-12-11 17:48:34,938 [inflora.py] => Task 17, Epoch 50/50 => Loss 0.079, Train_accy 98.48
Threshold:  0.9833999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 49/768 type remove
Layer 4 : 60/768 type remove
Layer 5 : 77/768 type remove
Layer 6 : 70/768 type remove
Layer 7 : 81/768 type remove
Layer 8 : 97/768 type remove
Layer 9 : 130/768 type remove
Layer 10 : 129/768 type remove
Layer 11 : 70/768 type remove
Layer 12 : 120/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:48:41,587 [trainer.py] => Time:98.66283106803894
1299 1299
1299 1299
2025-12-11 17:48:46,033 [trainer.py] => Time:4.445627212524414
2025-12-11 17:48:46,033 [inflora.py] => Exemplar size: 0
2025-12-11 17:48:46,033 [trainer.py] => CNN: {'total': np.float64(57.97), '00-01': np.float64(75.27), '02-03': np.float64(58.06), '04-05': np.float64(59.55), '06-07': np.float64(64.29), '08-09': np.float64(41.1), '10-11': np.float64(29.27), '12-13': np.float64(61.11), '14-15': np.float64(55.36), '16-17': np.float64(28.57), '18-19': np.float64(71.43), '20-21': np.float64(76.92), '22-23': np.float64(63.28), '24-25': np.float64(24.49), '26-27': np.float64(73.04), '28-29': np.float64(74.07), '30-31': np.float64(55.45), '32-33': np.float64(18.18), '34-35': np.float64(51.61), 'old': np.float64(58.12), 'new': np.float64(51.61)}
2025-12-11 17:48:46,033 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97)]
2025-12-11 17:48:46,033 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15)]
2025-12-11 17:48:46,033 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197]
2025-12-11 17:48:52,236 [trainer.py] => W-NCM: {'00-01': 52.68817204301075, '02-03': 56.451612903225815, '04-05': 58.42696629213483, '06-07': 76.78571428571429, '08-09': 69.86301369863014, '10-11': 51.21951219512195, '12-13': 88.88888888888889, '14-15': 83.92857142857143, '16-17': 77.14285714285715, '18-19': 75.0, '20-21': 76.92307692307693, '22-23': 66.40625, '24-25': 63.26530612244898, '26-27': 75.65217391304347, '28-29': 70.37037037037037, '30-31': 76.36363636363637, '32-33': 90.9090909090909, '34-35': 96.7741935483871}
2025-12-11 17:48:52,237 [trainer.py] => Ave Acc (W-NCM): 72.61%
2025-12-11 17:48:52,237 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 52.69% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 58.43% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 88.89% (best 90.74%); T8: W-NCM 83.93% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 75.65% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 76.36% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 96.77% (best 96.77%)
2025-12-11 17:48:52,237 [trainer.py] => Average forgetting (W-NCM): 21.50% | Max forgetting (W-NCM): 45.16%
2025-12-11 17:48:52,250 [trainer.py] => All params: 144526051
2025-12-11 17:48:52,261 [trainer.py] => Trainable params: 185858
2025-12-11 17:48:52,261 [inflora.py] => Learning on 36-38
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight'}
2025-12-11 17:50:45,589 [inflora.py] => Task 18, Epoch 50/50 => Loss 0.050, Train_accy 98.24
Threshold:  0.9836
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 50/768 type remove
Layer 4 : 61/768 type remove
Layer 5 : 78/768 type remove
Layer 6 : 71/768 type remove
Layer 7 : 83/768 type remove
Layer 8 : 99/768 type remove
Layer 9 : 133/768 type remove
Layer 10 : 131/768 type remove
Layer 11 : 72/768 type remove
Layer 12 : 122/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:50:52,256 [trainer.py] => Time:119.9944794178009
1359 1359
1359 1359
2025-12-11 17:50:56,841 [trainer.py] => Time:4.584638357162476
2025-12-11 17:50:56,841 [inflora.py] => Exemplar size: 0
2025-12-11 17:50:56,841 [trainer.py] => CNN: {'total': np.float64(58.2), '00-01': np.float64(69.89), '02-03': np.float64(50.0), '04-05': np.float64(58.43), '06-07': np.float64(69.64), '08-09': np.float64(42.47), '10-11': np.float64(26.83), '12-13': np.float64(62.96), '14-15': np.float64(55.36), '16-17': np.float64(34.29), '18-19': np.float64(71.43), '20-21': np.float64(75.82), '22-23': np.float64(64.06), '24-25': np.float64(36.73), '26-27': np.float64(72.17), '28-29': np.float64(72.84), '30-31': np.float64(55.45), '32-33': np.float64(15.91), '34-35': np.float64(51.61), '36-37': np.float64(63.33), 'old': np.float64(57.97), 'new': np.float64(63.33)}
2025-12-11 17:50:56,841 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2)]
2025-12-11 17:50:56,842 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54)]
2025-12-11 17:50:56,842 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212]
2025-12-11 17:51:03,250 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 54.83870967741935, '04-05': 61.79775280898876, '06-07': 78.57142857142857, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 82.14285714285714, '16-17': 77.14285714285715, '18-19': 75.0, '20-21': 78.02197802197803, '22-23': 69.53125, '24-25': 65.3061224489796, '26-27': 79.13043478260869, '28-29': 71.60493827160494, '30-31': 76.36363636363637, '32-33': 88.63636363636364, '34-35': 90.32258064516128, '36-37': 86.66666666666667}
2025-12-11 17:51:03,250 [trainer.py] => Ave Acc (W-NCM): 74.47%
2025-12-11 17:51:03,250 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 54.84% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 79.13% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 76.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 90.32% (best 96.77%); T19: W-NCM 86.67% (best 86.67%)
2025-12-11 17:51:03,250 [trainer.py] => Average forgetting (W-NCM): 19.12% | Max forgetting (W-NCM): 37.63%
2025-12-11 17:51:03,263 [trainer.py] => All params: 144526051
2025-12-11 17:51:03,275 [trainer.py] => Trainable params: 185858
2025-12-11 17:51:03,275 [inflora.py] => Learning on 38-40
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'classifier_pool.19.bias'}
2025-12-11 17:52:50,634 [inflora.py] => Task 19, Epoch 50/50 => Loss 0.033, Train_accy 98.92
Threshold:  0.9838
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 51/768 type remove
Layer 4 : 62/768 type remove
Layer 5 : 80/768 type remove
Layer 6 : 72/768 type remove
Layer 7 : 85/768 type remove
Layer 8 : 102/768 type remove
Layer 9 : 137/768 type remove
Layer 10 : 136/768 type remove
Layer 11 : 77/768 type remove
Layer 12 : 127/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:52:57,443 [trainer.py] => Time:114.16825199127197
1400 1400
1400 1400
2025-12-11 17:53:02,133 [trainer.py] => Time:4.689612627029419
2025-12-11 17:53:02,133 [inflora.py] => Exemplar size: 0
2025-12-11 17:53:02,133 [trainer.py] => CNN: {'total': np.float64(59.07), '00-01': np.float64(74.19), '02-03': np.float64(53.23), '04-05': np.float64(57.3), '06-07': np.float64(67.86), '08-09': np.float64(43.84), '10-11': np.float64(19.51), '12-13': np.float64(61.11), '14-15': np.float64(53.57), '16-17': np.float64(38.57), '18-19': np.float64(69.64), '20-21': np.float64(80.22), '22-23': np.float64(60.94), '24-25': np.float64(32.65), '26-27': np.float64(72.17), '28-29': np.float64(77.78), '30-31': np.float64(57.27), '32-33': np.float64(13.64), '34-35': np.float64(51.61), '36-37': np.float64(63.33), '38-39': np.float64(75.61), 'old': np.float64(58.57), 'new': np.float64(75.61)}
2025-12-11 17:53:02,134 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07)]
2025-12-11 17:53:02,134 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43)]
2025-12-11 17:53:02,134 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857]
2025-12-11 17:53:08,459 [trainer.py] => W-NCM: {'00-01': 59.13978494623656, '02-03': 67.74193548387096, '04-05': 62.92134831460674, '06-07': 83.92857142857143, '08-09': 71.23287671232876, '10-11': 63.41463414634146, '12-13': 87.03703703703704, '14-15': 82.14285714285714, '16-17': 78.57142857142857, '18-19': 75.0, '20-21': 81.31868131868131, '22-23': 71.875, '24-25': 63.26530612244898, '26-27': 75.65217391304347, '28-29': 77.77777777777779, '30-31': 75.45454545454545, '32-33': 88.63636363636364, '34-35': 90.32258064516128, '36-37': 86.66666666666667, '38-39': 97.5609756097561}
2025-12-11 17:53:08,459 [trainer.py] => Ave Acc (W-NCM): 76.98%
2025-12-11 17:53:08,459 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 59.14% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 87.04% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 71.88% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 75.65% (best 94.78%); T15: W-NCM 77.78% (best 96.30%); T16: W-NCM 75.45% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 90.32% (best 96.77%); T19: W-NCM 86.67% (best 86.67%); T20: W-NCM 97.56% (best 97.56%)
2025-12-11 17:53:08,459 [trainer.py] => Average forgetting (W-NCM): 16.69% | Max forgetting (W-NCM): 38.71%
2025-12-11 17:53:08,472 [trainer.py] => All params: 144526051
2025-12-11 17:53:08,484 [trainer.py] => Trainable params: 185858
2025-12-11 17:53:08,484 [inflora.py] => Learning on 40-42
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight'}
2025-12-11 17:54:47,014 [inflora.py] => Task 20, Epoch 50/50 => Loss 0.024, Train_accy 99.43
Threshold:  0.984
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 63/768 type remove
Layer 5 : 81/768 type remove
Layer 6 : 74/768 type remove
Layer 7 : 88/768 type remove
Layer 8 : 104/768 type remove
Layer 9 : 140/768 type remove
Layer 10 : 139/768 type remove
Layer 11 : 80/768 type remove
Layer 12 : 130/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:54:53,698 [trainer.py] => Time:105.21419835090637
1454 1454
1454 1454
2025-12-11 17:54:58,549 [trainer.py] => Time:4.850126028060913
2025-12-11 17:54:58,549 [inflora.py] => Exemplar size: 0
2025-12-11 17:54:58,549 [trainer.py] => CNN: {'total': np.float64(59.22), '00-01': np.float64(68.82), '02-03': np.float64(56.45), '04-05': np.float64(55.06), '06-07': np.float64(66.07), '08-09': np.float64(46.58), '10-11': np.float64(17.07), '12-13': np.float64(62.96), '14-15': np.float64(51.79), '16-17': np.float64(38.57), '18-19': np.float64(66.07), '20-21': np.float64(79.12), '22-23': np.float64(64.84), '24-25': np.float64(32.65), '26-27': np.float64(75.65), '28-29': np.float64(75.31), '30-31': np.float64(58.18), '32-33': np.float64(18.18), '34-35': np.float64(58.06), '36-37': np.float64(63.33), '38-39': np.float64(82.93), '40-41': np.float64(50.0), 'old': np.float64(59.57), 'new': np.float64(50.0)}
2025-12-11 17:54:58,549 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22)]
2025-12-11 17:54:58,549 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7)]
2025-12-11 17:54:58,549 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381]
2025-12-11 17:55:05,061 [trainer.py] => W-NCM: {'00-01': 55.91397849462365, '02-03': 67.74193548387096, '04-05': 64.04494382022472, '06-07': 82.14285714285714, '08-09': 75.34246575342466, '10-11': 68.29268292682927, '12-13': 90.74074074074075, '14-15': 82.14285714285714, '16-17': 78.57142857142857, '18-19': 75.0, '20-21': 83.51648351648352, '22-23': 73.4375, '24-25': 67.3469387755102, '26-27': 72.17391304347827, '28-29': 75.30864197530865, '30-31': 70.0, '32-33': 90.9090909090909, '34-35': 90.32258064516128, '36-37': 81.66666666666667, '38-39': 95.1219512195122, '40-41': 94.44444444444444}
2025-12-11 17:55:05,061 [trainer.py] => Ave Acc (W-NCM): 77.82%
2025-12-11 17:55:05,061 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 55.91% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 64.04% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 68.29% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 83.52% (best 95.60%); T12: W-NCM 73.44% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 72.17% (best 94.78%); T15: W-NCM 75.31% (best 96.30%); T16: W-NCM 70.00% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 90.32% (best 96.77%); T19: W-NCM 81.67% (best 86.67%); T20: W-NCM 95.12% (best 97.56%); T21: W-NCM 94.44% (best 94.44%)
2025-12-11 17:55:05,061 [trainer.py] => Average forgetting (W-NCM): 15.85% | Max forgetting (W-NCM): 41.94%
2025-12-11 17:55:05,075 [trainer.py] => All params: 144526051
2025-12-11 17:55:05,087 [trainer.py] => Trainable params: 185858
2025-12-11 17:55:05,087 [inflora.py] => Learning on 42-44
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'classifier_pool.21.weight'}
2025-12-11 17:56:34,826 [inflora.py] => Task 21, Epoch 50/50 => Loss 0.059, Train_accy 96.21
Threshold:  0.9842
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 53/768 type remove
Layer 4 : 64/768 type remove
Layer 5 : 82/768 type remove
Layer 6 : 75/768 type remove
Layer 7 : 91/768 type remove
Layer 8 : 107/768 type remove
Layer 9 : 144/768 type remove
Layer 10 : 143/768 type remove
Layer 11 : 84/768 type remove
Layer 12 : 134/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:56:41,284 [trainer.py] => Time:96.19711589813232
1486 1486
1486 1486
2025-12-11 17:56:46,214 [trainer.py] => Time:4.9297096729278564
2025-12-11 17:56:46,214 [inflora.py] => Exemplar size: 0
2025-12-11 17:56:46,214 [trainer.py] => CNN: {'total': np.float64(57.94), '00-01': np.float64(70.97), '02-03': np.float64(53.23), '04-05': np.float64(58.43), '06-07': np.float64(62.5), '08-09': np.float64(49.32), '10-11': np.float64(19.51), '12-13': np.float64(64.81), '14-15': np.float64(51.79), '16-17': np.float64(41.43), '18-19': np.float64(58.93), '20-21': np.float64(71.43), '22-23': np.float64(63.28), '24-25': np.float64(34.69), '26-27': np.float64(75.65), '28-29': np.float64(72.84), '30-31': np.float64(56.36), '32-33': np.float64(25.0), '34-35': np.float64(58.06), '36-37': np.float64(60.0), '38-39': np.float64(82.93), '40-41': np.float64(42.59), '42-43': np.float64(37.5), 'old': np.float64(58.39), 'new': np.float64(37.5)}
2025-12-11 17:56:46,214 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94)]
2025-12-11 17:56:46,215 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43)]
2025-12-11 17:56:46,215 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113]
2025-12-11 17:56:52,683 [trainer.py] => W-NCM: {'00-01': 54.83870967741935, '02-03': 64.51612903225806, '04-05': 61.79775280898876, '06-07': 80.35714285714286, '08-09': 73.97260273972603, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 82.14285714285714, '16-17': 78.57142857142857, '18-19': 71.42857142857143, '20-21': 79.12087912087912, '22-23': 71.875, '24-25': 61.224489795918366, '26-27': 63.47826086956522, '28-29': 69.1358024691358, '30-31': 64.54545454545455, '32-33': 88.63636363636364, '34-35': 90.32258064516128, '36-37': 81.66666666666667, '38-39': 87.8048780487805, '40-41': 59.25925925925925, '42-43': 96.875}
2025-12-11 17:56:52,683 [trainer.py] => Ave Acc (W-NCM): 74.46%
2025-12-11 17:56:52,683 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 54.84% (best 97.85%); T2: W-NCM 64.52% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 71.88% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 63.48% (best 94.78%); T15: W-NCM 69.14% (best 96.30%); T16: W-NCM 64.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 90.32% (best 96.77%); T19: W-NCM 81.67% (best 86.67%); T20: W-NCM 87.80% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 96.88% (best 96.88%)
2025-12-11 17:56:52,683 [trainer.py] => Average forgetting (W-NCM): 19.52% | Max forgetting (W-NCM): 43.01%
2025-12-11 17:56:52,696 [trainer.py] => All params: 144526051
2025-12-11 17:56:52,708 [trainer.py] => Trainable params: 185858
2025-12-11 17:56:52,708 [inflora.py] => Learning on 44-46
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'classifier_pool.22.bias', 'classifier_pool.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight'}
2025-12-11 17:58:21,962 [inflora.py] => Task 22, Epoch 50/50 => Loss 0.153, Train_accy 91.03
Threshold:  0.9843999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 54/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 83/768 type remove
Layer 6 : 76/768 type remove
Layer 7 : 92/768 type remove
Layer 8 : 108/768 type remove
Layer 9 : 146/768 type remove
Layer 10 : 145/768 type remove
Layer 11 : 86/768 type remove
Layer 12 : 136/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 17:58:28,033 [trainer.py] => Time:95.3249990940094
1513 1513
1513 1513
2025-12-11 17:58:33,019 [trainer.py] => Time:4.985880613327026
2025-12-11 17:58:33,020 [inflora.py] => Exemplar size: 0
2025-12-11 17:58:33,020 [trainer.py] => CNN: {'total': np.float64(55.19), '00-01': np.float64(76.34), '02-03': np.float64(48.39), '04-05': np.float64(55.06), '06-07': np.float64(58.93), '08-09': np.float64(43.84), '10-11': np.float64(19.51), '12-13': np.float64(62.96), '14-15': np.float64(42.86), '16-17': np.float64(32.86), '18-19': np.float64(53.57), '20-21': np.float64(71.43), '22-23': np.float64(61.72), '24-25': np.float64(36.73), '26-27': np.float64(69.57), '28-29': np.float64(71.6), '30-31': np.float64(56.36), '32-33': np.float64(22.73), '34-35': np.float64(58.06), '36-37': np.float64(60.0), '38-39': np.float64(85.37), '40-41': np.float64(42.59), '42-43': np.float64(53.12), '44-45': np.float64(0.0), 'old': np.float64(56.19), 'new': np.float64(0.0)}
2025-12-11 17:58:33,020 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19)]
2025-12-11 17:58:33,020 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36)]
2025-12-11 17:58:33,020 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584]
2025-12-11 17:58:39,459 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 66.12903225806451, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 72.6027397260274, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 81.42857142857143, '18-19': 73.21428571428571, '20-21': 84.61538461538461, '22-23': 70.3125, '24-25': 59.183673469387756, '26-27': 70.43478260869566, '28-29': 67.90123456790124, '30-31': 68.18181818181817, '32-33': 88.63636363636364, '34-35': 87.09677419354838, '36-37': 80.0, '38-39': 80.48780487804879, '40-41': 55.55555555555556, '42-43': 90.625, '44-45': 55.55555555555556}
2025-12-11 17:58:39,459 [trainer.py] => Ave Acc (W-NCM): 73.13%
2025-12-11 17:58:39,459 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 66.13% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 72.60% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 84.62% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 70.43% (best 94.78%); T15: W-NCM 67.90% (best 96.30%); T16: W-NCM 68.18% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 87.10% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 80.49% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 90.62% (best 96.88%); T23: W-NCM 55.56% (best 55.56%)
2025-12-11 17:58:39,459 [trainer.py] => Average forgetting (W-NCM): 19.17% | Max forgetting (W-NCM): 38.89%
2025-12-11 17:58:39,472 [trainer.py] => All params: 144526051
2025-12-11 17:58:39,485 [trainer.py] => Trainable params: 185858
2025-12-11 17:58:39,485 [inflora.py] => Learning on 46-48
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight'}
2025-12-11 18:00:09,929 [inflora.py] => Task 23, Epoch 50/50 => Loss 0.028, Train_accy 98.55
Threshold:  0.9846
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 23/768 type remove
Layer 3 : 55/768 type remove
Layer 4 : 66/768 type remove
Layer 5 : 85/768 type remove
Layer 6 : 78/768 type remove
Layer 7 : 95/768 type remove
Layer 8 : 111/768 type remove
Layer 9 : 149/768 type remove
Layer 10 : 148/768 type remove
Layer 11 : 89/768 type remove
Layer 12 : 139/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:00:16,223 [trainer.py] => Time:96.73854851722717
1540 1540
1540 1540
2025-12-11 18:00:21,305 [trainer.py] => Time:5.081019401550293
2025-12-11 18:00:21,305 [inflora.py] => Exemplar size: 0
2025-12-11 18:00:21,305 [trainer.py] => CNN: {'total': np.float64(55.58), '00-01': np.float64(77.42), '02-03': np.float64(50.0), '04-05': np.float64(52.81), '06-07': np.float64(55.36), '08-09': np.float64(46.58), '10-11': np.float64(24.39), '12-13': np.float64(62.96), '14-15': np.float64(50.0), '16-17': np.float64(34.29), '18-19': np.float64(51.79), '20-21': np.float64(68.13), '22-23': np.float64(61.72), '24-25': np.float64(42.86), '26-27': np.float64(71.3), '28-29': np.float64(69.14), '30-31': np.float64(57.27), '32-33': np.float64(25.0), '34-35': np.float64(51.61), '36-37': np.float64(61.67), '38-39': np.float64(82.93), '40-41': np.float64(46.3), '42-43': np.float64(46.88), '44-45': np.float64(0.0), '46-47': np.float64(55.56), 'old': np.float64(55.58), 'new': np.float64(55.56)}
2025-12-11 18:00:21,305 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58)]
2025-12-11 18:00:21,305 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43)]
2025-12-11 18:00:21,305 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065]
2025-12-11 18:00:28,019 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 62.903225806451616, '04-05': 56.17977528089888, '06-07': 78.57142857142857, '08-09': 75.34246575342466, '10-11': 63.41463414634146, '12-13': 90.74074074074075, '14-15': 80.35714285714286, '16-17': 80.0, '18-19': 73.21428571428571, '20-21': 81.31868131868131, '22-23': 70.3125, '24-25': 67.3469387755102, '26-27': 72.17391304347827, '28-29': 70.37037037037037, '30-31': 71.81818181818181, '32-33': 90.9090909090909, '34-35': 90.32258064516128, '36-37': 80.0, '38-39': 75.60975609756098, '40-41': 57.407407407407405, '42-43': 87.5, '44-45': 55.55555555555556, '46-47': 92.5925925925926}
2025-12-11 18:00:28,020 [trainer.py] => Ave Acc (W-NCM): 74.56%
2025-12-11 18:00:28,020 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 80.36% (best 94.64%); T9: W-NCM 80.00% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 72.17% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 71.82% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 90.32% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 75.61% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 87.50% (best 96.88%); T23: W-NCM 55.56% (best 55.56%); T24: W-NCM 92.59% (best 92.59%)
2025-12-11 18:00:28,020 [trainer.py] => Average forgetting (W-NCM): 17.68% | Max forgetting (W-NCM): 37.04%
2025-12-11 18:00:28,033 [trainer.py] => All params: 144526051
2025-12-11 18:00:28,044 [trainer.py] => Trainable params: 185858
2025-12-11 18:00:28,045 [inflora.py] => Learning on 48-50
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight'}
2025-12-11 18:02:34,626 [inflora.py] => Task 24, Epoch 50/50 => Loss 0.010, Train_accy 99.64
Threshold:  0.9848
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 56/768 type remove
Layer 4 : 68/768 type remove
Layer 5 : 87/768 type remove
Layer 6 : 80/768 type remove
Layer 7 : 97/768 type remove
Layer 8 : 113/768 type remove
Layer 9 : 151/768 type remove
Layer 10 : 150/768 type remove
Layer 11 : 92/768 type remove
Layer 12 : 142/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:02:41,589 [trainer.py] => Time:133.54462790489197
1634 1634
1634 1634
2025-12-11 18:02:46,958 [trainer.py] => Time:5.368161678314209
2025-12-11 18:02:46,958 [inflora.py] => Exemplar size: 0
2025-12-11 18:02:46,958 [trainer.py] => CNN: {'total': np.float64(56.06), '00-01': np.float64(68.82), '02-03': np.float64(50.0), '04-05': np.float64(53.93), '06-07': np.float64(58.93), '08-09': np.float64(45.21), '10-11': np.float64(21.95), '12-13': np.float64(55.56), '14-15': np.float64(48.21), '16-17': np.float64(31.43), '18-19': np.float64(57.14), '20-21': np.float64(68.13), '22-23': np.float64(62.5), '24-25': np.float64(34.69), '26-27': np.float64(71.3), '28-29': np.float64(70.37), '30-31': np.float64(55.45), '32-33': np.float64(25.0), '34-35': np.float64(54.84), '36-37': np.float64(63.33), '38-39': np.float64(85.37), '40-41': np.float64(44.44), '42-43': np.float64(50.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(75.53), 'old': np.float64(54.87), 'new': np.float64(75.53)}
2025-12-11 18:02:46,958 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06)]
2025-12-11 18:02:46,958 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14)]
2025-12-11 18:02:46,958 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559]
2025-12-11 18:02:54,116 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 61.29032258064516, '04-05': 61.79775280898876, '06-07': 83.92857142857143, '08-09': 78.08219178082192, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 80.0, '18-19': 76.78571428571429, '20-21': 84.61538461538461, '22-23': 71.09375, '24-25': 67.3469387755102, '26-27': 75.65217391304347, '28-29': 69.1358024691358, '30-31': 72.72727272727273, '32-33': 88.63636363636364, '34-35': 87.09677419354838, '36-37': 80.0, '38-39': 73.17073170731707, '40-41': 64.81481481481481, '42-43': 90.625, '44-45': 51.85185185185185, '46-47': 88.88888888888889, '48-49': 94.68085106382979}
2025-12-11 18:02:54,116 [trainer.py] => Ave Acc (W-NCM): 76.08%
2025-12-11 18:02:54,116 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 61.29% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 78.08% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 90.74%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 80.00% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 84.62% (best 95.60%); T12: W-NCM 71.09% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 75.65% (best 94.78%); T15: W-NCM 69.14% (best 96.30%); T16: W-NCM 72.73% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 87.10% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 64.81% (best 94.44%); T22: W-NCM 90.62% (best 96.88%); T23: W-NCM 51.85% (best 55.56%); T24: W-NCM 88.89% (best 92.59%); T25: W-NCM 94.68% (best 94.68%)
2025-12-11 18:02:54,116 [trainer.py] => Average forgetting (W-NCM): 16.21% | Max forgetting (W-NCM): 33.33%
2025-12-11 18:02:54,129 [trainer.py] => All params: 144526051
2025-12-11 18:02:54,141 [trainer.py] => Trainable params: 185858
2025-12-11 18:02:54,141 [inflora.py] => Learning on 50-52
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight'}
2025-12-11 18:04:33,494 [inflora.py] => Task 25, Epoch 50/50 => Loss 0.016, Train_accy 99.43
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 57/768 type remove
Layer 4 : 69/768 type remove
Layer 5 : 88/768 type remove
Layer 6 : 81/768 type remove
Layer 7 : 99/768 type remove
Layer 8 : 116/768 type remove
Layer 9 : 155/768 type remove
Layer 10 : 153/768 type remove
Layer 11 : 95/768 type remove
Layer 12 : 144/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:04:40,198 [trainer.py] => Time:106.0569007396698
1671 1671
1671 1671
2025-12-11 18:04:45,628 [trainer.py] => Time:5.4294209480285645
2025-12-11 18:04:45,628 [inflora.py] => Exemplar size: 0
2025-12-11 18:04:45,628 [trainer.py] => CNN: {'total': np.float64(54.82), '00-01': np.float64(73.12), '02-03': np.float64(48.39), '04-05': np.float64(48.31), '06-07': np.float64(60.71), '08-09': np.float64(45.21), '10-11': np.float64(24.39), '12-13': np.float64(55.56), '14-15': np.float64(44.64), '16-17': np.float64(32.86), '18-19': np.float64(57.14), '20-21': np.float64(61.54), '22-23': np.float64(63.28), '24-25': np.float64(30.61), '26-27': np.float64(74.78), '28-29': np.float64(60.49), '30-31': np.float64(60.0), '32-33': np.float64(25.0), '34-35': np.float64(54.84), '36-37': np.float64(66.67), '38-39': np.float64(85.37), '40-41': np.float64(40.74), '42-43': np.float64(37.5), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(73.4), '50-51': np.float64(35.14), 'old': np.float64(55.26), 'new': np.float64(35.14)}
2025-12-11 18:04:45,628 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82)]
2025-12-11 18:04:45,628 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11)]
2025-12-11 18:04:45,629 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624]
2025-12-11 18:04:52,660 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 62.903225806451616, '04-05': 65.1685393258427, '06-07': 87.5, '08-09': 76.71232876712328, '10-11': 70.73170731707317, '12-13': 92.5925925925926, '14-15': 75.0, '16-17': 80.0, '18-19': 75.0, '20-21': 82.41758241758241, '22-23': 72.65625, '24-25': 63.26530612244898, '26-27': 67.82608695652173, '28-29': 71.60493827160494, '30-31': 76.36363636363637, '32-33': 88.63636363636364, '34-35': 87.09677419354838, '36-37': 80.0, '38-39': 75.60975609756098, '40-41': 62.96296296296296, '42-43': 87.5, '44-45': 62.96296296296296, '46-47': 81.48148148148148, '48-49': 91.48936170212765, '50-51': 100.0}
2025-12-11 18:04:52,660 [trainer.py] => Ave Acc (W-NCM): 77.12%
2025-12-11 18:04:52,660 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 65.17% (best 91.01%); T4: W-NCM 87.50% (best 92.86%); T5: W-NCM 76.71% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 92.59% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 80.00% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 82.42% (best 95.60%); T12: W-NCM 72.66% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 67.83% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 76.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 87.10% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 75.61% (best 97.56%); T21: W-NCM 62.96% (best 94.44%); T22: W-NCM 87.50% (best 96.88%); T23: W-NCM 62.96% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 91.49% (best 94.68%); T26: W-NCM 100.00% (best 100.00%)
2025-12-11 18:04:52,661 [trainer.py] => Average forgetting (W-NCM): 15.80% | Max forgetting (W-NCM): 31.48%
2025-12-11 18:04:52,673 [trainer.py] => All params: 144526051
2025-12-11 18:04:52,685 [trainer.py] => Trainable params: 185858
2025-12-11 18:04:52,685 [inflora.py] => Learning on 52-54
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight'}
2025-12-11 18:06:27,073 [inflora.py] => Task 26, Epoch 50/50 => Loss 0.107, Train_accy 97.48
Threshold:  0.9852
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 58/768 type remove
Layer 4 : 71/768 type remove
Layer 5 : 90/768 type remove
Layer 6 : 84/768 type remove
Layer 7 : 103/768 type remove
Layer 8 : 119/768 type remove
Layer 9 : 159/768 type remove
Layer 10 : 156/768 type remove
Layer 11 : 98/768 type remove
Layer 12 : 146/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:06:33,277 [trainer.py] => Time:100.59174370765686
1707 1707
1707 1707
2025-12-11 18:06:38,844 [trainer.py] => Time:5.56688380241394
2025-12-11 18:06:38,844 [inflora.py] => Exemplar size: 0
2025-12-11 18:06:38,845 [trainer.py] => CNN: {'total': np.float64(54.77), '00-01': np.float64(75.27), '02-03': np.float64(50.0), '04-05': np.float64(46.07), '06-07': np.float64(62.5), '08-09': np.float64(47.95), '10-11': np.float64(24.39), '12-13': np.float64(55.56), '14-15': np.float64(48.21), '16-17': np.float64(30.0), '18-19': np.float64(58.93), '20-21': np.float64(63.74), '22-23': np.float64(64.06), '24-25': np.float64(28.57), '26-27': np.float64(71.3), '28-29': np.float64(64.2), '30-31': np.float64(56.36), '32-33': np.float64(22.73), '34-35': np.float64(48.39), '36-37': np.float64(63.33), '38-39': np.float64(80.49), '40-41': np.float64(42.59), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(62.96), '48-49': np.float64(75.53), '50-51': np.float64(32.43), '52-53': np.float64(63.89), 'old': np.float64(54.58), 'new': np.float64(63.89)}
2025-12-11 18:06:38,845 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77)]
2025-12-11 18:06:38,845 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9)]
2025-12-11 18:06:38,845 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262]
2025-12-11 18:06:46,006 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 64.51612903225806, '04-05': 65.1685393258427, '06-07': 85.71428571428571, '08-09': 75.34246575342466, '10-11': 68.29268292682927, '12-13': 90.74074074074075, '14-15': 75.0, '16-17': 77.14285714285715, '18-19': 75.0, '20-21': 80.21978021978022, '22-23': 71.09375, '24-25': 65.3061224489796, '26-27': 68.69565217391305, '28-29': 69.1358024691358, '30-31': 69.0909090909091, '32-33': 88.63636363636364, '34-35': 87.09677419354838, '36-37': 80.0, '38-39': 80.48780487804879, '40-41': 59.25925925925925, '42-43': 81.25, '44-45': 59.25925925925925, '46-47': 74.07407407407408, '48-49': 90.42553191489363, '50-51': 89.1891891891892, '52-53': 94.44444444444444}
2025-12-11 18:06:46,007 [trainer.py] => Ave Acc (W-NCM): 75.81%
2025-12-11 18:06:46,007 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 64.52% (best 90.32%); T3: W-NCM 65.17% (best 91.01%); T4: W-NCM 85.71% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 68.29% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 80.22% (best 95.60%); T12: W-NCM 71.09% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 68.70% (best 94.78%); T15: W-NCM 69.14% (best 96.30%); T16: W-NCM 69.09% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 87.10% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 80.49% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 59.26% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 90.43% (best 94.68%); T26: W-NCM 89.19% (best 100.00%); T27: W-NCM 94.44% (best 94.44%)
2025-12-11 18:06:46,007 [trainer.py] => Average forgetting (W-NCM): 17.22% | Max forgetting (W-NCM): 35.48%
2025-12-11 18:06:46,020 [trainer.py] => All params: 144526051
2025-12-11 18:06:46,031 [trainer.py] => Trainable params: 185858
2025-12-11 18:06:46,032 [inflora.py] => Learning on 54-56
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight'}
2025-12-11 18:08:20,998 [inflora.py] => Task 27, Epoch 50/50 => Loss 0.122, Train_accy 96.64
Threshold:  0.9853999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
Skip Updating DualGPM for layer: 3
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 58/768 type remove
Layer 4 : 72/768 type remove
Layer 5 : 92/768 type remove
Layer 6 : 86/768 type remove
Layer 7 : 106/768 type remove
Layer 8 : 123/768 type remove
Layer 9 : 164/768 type remove
Layer 10 : 160/768 type remove
Layer 11 : 102/768 type remove
Layer 12 : 148/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:08:27,420 [trainer.py] => Time:101.3881151676178
1732 1732
1732 1732
2025-12-11 18:08:33,033 [trainer.py] => Time:5.613330841064453
2025-12-11 18:08:33,033 [inflora.py] => Exemplar size: 0
2025-12-11 18:08:33,034 [trainer.py] => CNN: {'total': np.float64(47.69), '00-01': np.float64(75.27), '02-03': np.float64(46.77), '04-05': np.float64(47.19), '06-07': np.float64(46.43), '08-09': np.float64(47.95), '10-11': np.float64(24.39), '12-13': np.float64(35.19), '14-15': np.float64(30.36), '16-17': np.float64(22.86), '18-19': np.float64(48.21), '20-21': np.float64(50.55), '22-23': np.float64(60.94), '24-25': np.float64(30.61), '26-27': np.float64(64.35), '28-29': np.float64(54.32), '30-31': np.float64(43.64), '32-33': np.float64(15.91), '34-35': np.float64(48.39), '36-37': np.float64(50.0), '38-39': np.float64(75.61), '40-41': np.float64(42.59), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(76.6), '50-51': np.float64(45.95), '52-53': np.float64(47.22), '54-55': np.float64(0.0), 'old': np.float64(48.39), 'new': np.float64(0.0)}
2025-12-11 18:08:33,034 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69)]
2025-12-11 18:08:33,034 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32)]
2025-12-11 18:08:33,034 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291]
2025-12-11 18:08:40,226 [trainer.py] => W-NCM: {'00-01': 37.634408602150536, '02-03': 56.451612903225815, '04-05': 46.06741573033708, '06-07': 67.85714285714286, '08-09': 68.4931506849315, '10-11': 58.536585365853654, '12-13': 74.07407407407408, '14-15': 60.71428571428571, '16-17': 70.0, '18-19': 60.71428571428571, '20-21': 72.52747252747253, '22-23': 59.375, '24-25': 59.183673469387756, '26-27': 51.30434782608696, '28-29': 55.55555555555556, '30-31': 32.72727272727273, '32-33': 88.63636363636364, '34-35': 74.19354838709677, '36-37': 73.33333333333333, '38-39': 60.97560975609756, '40-41': 29.629629629629626, '42-43': 78.125, '44-45': 40.74074074074074, '46-47': 70.37037037037037, '48-49': 67.02127659574468, '50-51': 72.97297297297297, '52-53': 88.88888888888889, '54-55': 80.0}
2025-12-11 18:08:40,226 [trainer.py] => Ave Acc (W-NCM): 62.72%
2025-12-11 18:08:40,226 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 37.63% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 46.07% (best 91.01%); T4: W-NCM 67.86% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 74.07% (best 92.59%); T8: W-NCM 60.71% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 60.71% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 59.38% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 51.30% (best 94.78%); T15: W-NCM 55.56% (best 96.30%); T16: W-NCM 32.73% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 29.63% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 67.02% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 80.00% (best 80.00%)
2025-12-11 18:08:40,226 [trainer.py] => Average forgetting (W-NCM): 30.32% | Max forgetting (W-NCM): 64.81%
2025-12-11 18:08:40,239 [trainer.py] => All params: 144526051
2025-12-11 18:08:40,251 [trainer.py] => Trainable params: 185858
2025-12-11 18:08:40,251 [inflora.py] => Learning on 56-58
Parameters to be updated: {'classifier_pool.28.bias', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight'}
2025-12-11 18:10:29,144 [inflora.py] => Task 28, Epoch 50/50 => Loss 0.019, Train_accy 98.99
Threshold:  0.9856
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 59/768 type remove
Layer 4 : 73/768 type remove
Layer 5 : 94/768 type remove
Layer 6 : 88/768 type remove
Layer 7 : 108/768 type remove
Layer 8 : 126/768 type remove
Layer 9 : 167/768 type remove
Layer 10 : 164/768 type remove
Layer 11 : 106/768 type remove
Layer 12 : 151/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:10:35,836 [trainer.py] => Time:115.58497905731201
1772 1772
1772 1772
2025-12-11 18:10:41,533 [trainer.py] => Time:5.696230173110962
2025-12-11 18:10:41,533 [inflora.py] => Exemplar size: 0
2025-12-11 18:10:41,533 [trainer.py] => CNN: {'total': np.float64(50.9), '00-01': np.float64(72.04), '02-03': np.float64(48.39), '04-05': np.float64(46.07), '06-07': np.float64(66.07), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(48.15), '14-15': np.float64(39.29), '16-17': np.float64(21.43), '18-19': np.float64(57.14), '20-21': np.float64(56.04), '22-23': np.float64(61.72), '24-25': np.float64(30.61), '26-27': np.float64(67.83), '28-29': np.float64(54.32), '30-31': np.float64(51.82), '32-33': np.float64(20.45), '34-35': np.float64(51.61), '36-37': np.float64(61.67), '38-39': np.float64(75.61), '40-41': np.float64(44.44), '42-43': np.float64(18.75), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(70.21), '50-51': np.float64(59.46), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(55.0), 'old': np.float64(50.81), 'new': np.float64(55.0)}
2025-12-11 18:10:41,533 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9)]
2025-12-11 18:10:41,533 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49)]
2025-12-11 18:10:41,533 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605]
2025-12-11 18:10:49,096 [trainer.py] => W-NCM: {'00-01': 59.13978494623656, '02-03': 62.903225806451616, '04-05': 61.79775280898876, '06-07': 87.5, '08-09': 75.34246575342466, '10-11': 68.29268292682927, '12-13': 88.88888888888889, '14-15': 73.21428571428571, '16-17': 74.28571428571429, '18-19': 76.78571428571429, '20-21': 78.02197802197803, '22-23': 67.1875, '24-25': 61.224489795918366, '26-27': 68.69565217391305, '28-29': 66.66666666666666, '30-31': 64.54545454545455, '32-33': 88.63636363636364, '34-35': 80.64516129032258, '36-37': 80.0, '38-39': 60.97560975609756, '40-41': 57.407407407407405, '42-43': 78.125, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 85.1063829787234, '50-51': 81.08108108108108, '52-53': 91.66666666666666, '54-55': 72.0, '56-57': 92.5}
2025-12-11 18:10:49,096 [trainer.py] => Ave Acc (W-NCM): 73.65%
2025-12-11 18:10:49,096 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 59.14% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 87.50% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 68.29% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 74.29% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 67.19% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 68.70% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 64.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 80.65% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 85.11% (best 94.68%); T26: W-NCM 81.08% (best 100.00%); T27: W-NCM 91.67% (best 94.44%); T28: W-NCM 72.00% (best 80.00%); T29: W-NCM 92.50% (best 92.50%)
2025-12-11 18:10:49,096 [trainer.py] => Average forgetting (W-NCM): 18.97% | Max forgetting (W-NCM): 38.71%
2025-12-11 18:10:49,109 [trainer.py] => All params: 144526051
2025-12-11 18:10:49,121 [trainer.py] => Trainable params: 185858
2025-12-11 18:10:49,121 [inflora.py] => Learning on 58-60
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight'}
2025-12-11 18:12:39,239 [inflora.py] => Task 29, Epoch 50/50 => Loss 0.061, Train_accy 98.00
Threshold:  0.9858
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 29/768 type remove
Layer 3 : 60/768 type remove
Layer 4 : 74/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 89/768 type remove
Layer 7 : 110/768 type remove
Layer 8 : 127/768 type remove
Layer 9 : 169/768 type remove
Layer 10 : 166/768 type remove
Layer 11 : 108/768 type remove
Layer 12 : 153/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:12:46,308 [trainer.py] => Time:117.18686771392822
1827 1827
1827 1827
2025-12-11 18:12:52,131 [trainer.py] => Time:5.822671175003052
2025-12-11 18:12:52,131 [inflora.py] => Exemplar size: 0
2025-12-11 18:12:52,131 [trainer.py] => CNN: {'total': np.float64(48.55), '00-01': np.float64(66.67), '02-03': np.float64(45.16), '04-05': np.float64(46.07), '06-07': np.float64(66.07), '08-09': np.float64(38.36), '10-11': np.float64(24.39), '12-13': np.float64(38.89), '14-15': np.float64(32.14), '16-17': np.float64(22.86), '18-19': np.float64(55.36), '20-21': np.float64(56.04), '22-23': np.float64(60.94), '24-25': np.float64(26.53), '26-27': np.float64(63.48), '28-29': np.float64(58.02), '30-31': np.float64(50.0), '32-33': np.float64(15.91), '34-35': np.float64(48.39), '36-37': np.float64(61.67), '38-39': np.float64(80.49), '40-41': np.float64(50.0), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(72.34), '50-51': np.float64(45.95), '52-53': np.float64(61.11), '54-55': np.float64(4.0), '56-57': np.float64(60.0), '58-59': np.float64(12.73), 'old': np.float64(49.66), 'new': np.float64(12.73)}
2025-12-11 18:12:52,131 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55)]
2025-12-11 18:12:52,131 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18)]
2025-12-11 18:12:52,131 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306]
2025-12-11 18:12:59,665 [trainer.py] => W-NCM: {'00-01': 56.98924731182796, '02-03': 54.83870967741935, '04-05': 56.17977528089888, '06-07': 80.35714285714286, '08-09': 69.86301369863014, '10-11': 65.85365853658537, '12-13': 87.03703703703704, '14-15': 67.85714285714286, '16-17': 71.42857142857143, '18-19': 73.21428571428571, '20-21': 74.72527472527473, '22-23': 65.625, '24-25': 59.183673469387756, '26-27': 57.391304347826086, '28-29': 59.25925925925925, '30-31': 60.0, '32-33': 86.36363636363636, '34-35': 64.51612903225806, '36-37': 73.33333333333333, '38-39': 43.90243902439025, '40-41': 57.407407407407405, '42-43': 53.125, '44-45': 40.74074074074074, '46-47': 81.48148148148148, '48-49': 39.361702127659576, '50-51': 81.08108108108108, '52-53': 88.88888888888889, '54-55': 60.0, '56-57': 92.5, '58-59': 90.9090909090909}
2025-12-11 18:12:59,665 [trainer.py] => Ave Acc (W-NCM): 67.11%
2025-12-11 18:12:59,665 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 56.99% (best 97.85%); T2: W-NCM 54.84% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 74.73% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 57.39% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 60.00% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 43.90% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 53.12% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 39.36% (best 94.68%); T26: W-NCM 81.08% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 60.00% (best 80.00%); T29: W-NCM 92.50% (best 92.50%); T30: W-NCM 90.91% (best 90.91%)
2025-12-11 18:12:59,665 [trainer.py] => Average forgetting (W-NCM): 25.68% | Max forgetting (W-NCM): 55.32%
2025-12-11 18:12:59,678 [trainer.py] => All params: 144526051
2025-12-11 18:12:59,690 [trainer.py] => Trainable params: 185858
2025-12-11 18:12:59,690 [inflora.py] => Learning on 60-62
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight'}
2025-12-11 18:14:34,153 [inflora.py] => Task 30, Epoch 50/50 => Loss 0.048, Train_accy 97.50
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 30/768 type remove
Layer 3 : 61/768 type remove
Layer 4 : 76/768 type remove
Layer 5 : 97/768 type remove
Layer 6 : 91/768 type remove
Layer 7 : 112/768 type remove
Layer 8 : 130/768 type remove
Layer 9 : 172/768 type remove
Layer 10 : 169/768 type remove
Layer 11 : 111/768 type remove
Layer 12 : 155/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:14:40,769 [trainer.py] => Time:101.07951545715332
1868 1868
1868 1868
2025-12-11 18:14:46,716 [trainer.py] => Time:5.946605682373047
2025-12-11 18:14:46,716 [inflora.py] => Exemplar size: 0
2025-12-11 18:14:46,717 [trainer.py] => CNN: {'total': np.float64(49.95), '00-01': np.float64(65.59), '02-03': np.float64(53.23), '04-05': np.float64(48.31), '06-07': np.float64(66.07), '08-09': np.float64(38.36), '10-11': np.float64(21.95), '12-13': np.float64(44.44), '14-15': np.float64(30.36), '16-17': np.float64(24.29), '18-19': np.float64(53.57), '20-21': np.float64(61.54), '22-23': np.float64(64.84), '24-25': np.float64(26.53), '26-27': np.float64(64.35), '28-29': np.float64(58.02), '30-31': np.float64(50.91), '32-33': np.float64(22.73), '34-35': np.float64(41.94), '36-37': np.float64(60.0), '38-39': np.float64(78.05), '40-41': np.float64(50.0), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(70.21), '50-51': np.float64(48.65), '52-53': np.float64(58.33), '54-55': np.float64(4.0), '56-57': np.float64(60.0), '58-59': np.float64(18.18), '60-61': np.float64(65.85), 'old': np.float64(49.59), 'new': np.float64(65.85)}
2025-12-11 18:14:46,717 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95)]
2025-12-11 18:14:46,717 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61)]
2025-12-11 18:14:46,717 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5]
2025-12-11 18:14:54,313 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 61.29032258064516, '04-05': 61.79775280898876, '06-07': 82.14285714285714, '08-09': 73.97260273972603, '10-11': 70.73170731707317, '12-13': 90.74074074074075, '14-15': 75.0, '16-17': 72.85714285714285, '18-19': 76.78571428571429, '20-21': 79.12087912087912, '22-23': 70.3125, '24-25': 61.224489795918366, '26-27': 68.69565217391305, '28-29': 66.66666666666666, '30-31': 66.36363636363637, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 60.97560975609756, '40-41': 57.407407407407405, '42-43': 65.625, '44-45': 48.148148148148145, '46-47': 81.48148148148148, '48-49': 62.76595744680851, '50-51': 78.37837837837837, '52-53': 91.66666666666666, '54-55': 52.0, '56-57': 87.5, '58-59': 87.27272727272727, '60-61': 87.8048780487805}
2025-12-11 18:14:54,313 [trainer.py] => Ave Acc (W-NCM): 72.05%
2025-12-11 18:14:54,314 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 61.29% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 68.70% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 66.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 62.77% (best 94.68%); T26: W-NCM 78.38% (best 100.00%); T27: W-NCM 91.67% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 87.27% (best 90.91%); T31: W-NCM 87.80% (best 87.80%)
2025-12-11 18:14:54,314 [trainer.py] => Average forgetting (W-NCM): 20.41% | Max forgetting (W-NCM): 37.63%
2025-12-11 18:14:54,327 [trainer.py] => All params: 144526051
2025-12-11 18:14:54,339 [trainer.py] => Trainable params: 185858
2025-12-11 18:14:54,339 [inflora.py] => Learning on 62-64
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight'}
2025-12-11 18:17:42,158 [inflora.py] => Task 31, Epoch 50/50 => Loss 0.019, Train_accy 99.05
Threshold:  0.9862
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 31/768 type remove
Layer 3 : 62/768 type remove
Layer 4 : 77/768 type remove
Layer 5 : 98/768 type remove
Layer 6 : 92/768 type remove
Layer 7 : 114/768 type remove
Layer 8 : 131/768 type remove
Layer 9 : 174/768 type remove
Layer 10 : 171/768 type remove
Layer 11 : 113/768 type remove
Layer 12 : 157/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:17:49,925 [trainer.py] => Time:175.58644890785217
1974 1974
1974 1974
2025-12-11 18:17:56,184 [trainer.py] => Time:6.258166074752808
2025-12-11 18:17:56,184 [inflora.py] => Exemplar size: 0
2025-12-11 18:17:56,184 [trainer.py] => CNN: {'total': np.float64(51.72), '00-01': np.float64(67.74), '02-03': np.float64(59.68), '04-05': np.float64(52.81), '06-07': np.float64(73.21), '08-09': np.float64(41.1), '10-11': np.float64(21.95), '12-13': np.float64(50.0), '14-15': np.float64(37.5), '16-17': np.float64(27.14), '18-19': np.float64(55.36), '20-21': np.float64(61.54), '22-23': np.float64(64.06), '24-25': np.float64(30.61), '26-27': np.float64(66.09), '28-29': np.float64(56.79), '30-31': np.float64(50.0), '32-33': np.float64(22.73), '34-35': np.float64(45.16), '36-37': np.float64(60.0), '38-39': np.float64(78.05), '40-41': np.float64(48.15), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(74.47), '50-51': np.float64(51.35), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(60.0), '58-59': np.float64(16.36), '60-61': np.float64(65.85), '62-63': np.float64(55.66), 'old': np.float64(51.5), 'new': np.float64(55.66)}
2025-12-11 18:17:56,184 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72)]
2025-12-11 18:17:56,184 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8)]
2025-12-11 18:17:56,184 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007]
2025-12-11 18:18:04,521 [trainer.py] => W-NCM: {'00-01': 63.44086021505376, '02-03': 59.67741935483871, '04-05': 66.29213483146067, '06-07': 83.92857142857143, '08-09': 73.97260273972603, '10-11': 70.73170731707317, '12-13': 90.74074074074075, '14-15': 73.21428571428571, '16-17': 77.14285714285715, '18-19': 80.35714285714286, '20-21': 75.82417582417582, '22-23': 67.96875, '24-25': 65.3061224489796, '26-27': 74.78260869565217, '28-29': 70.37037037037037, '30-31': 66.36363636363637, '32-33': 88.63636363636364, '34-35': 80.64516129032258, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 53.70370370370371, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 74.46808510638297, '50-51': 72.97297297297297, '52-53': 91.66666666666666, '54-55': 56.00000000000001, '56-57': 90.0, '58-59': 78.18181818181819, '60-61': 90.2439024390244, '62-63': 90.56603773584906}
2025-12-11 18:18:04,522 [trainer.py] => Ave Acc (W-NCM): 74.23%
2025-12-11 18:18:04,522 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 63.44% (best 97.85%); T2: W-NCM 59.68% (best 90.32%); T3: W-NCM 66.29% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 80.36% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 74.78% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 66.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 80.65% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 74.47% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 91.67% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 90.00% (best 92.50%); T30: W-NCM 78.18% (best 90.91%); T31: W-NCM 90.24% (best 90.24%); T32: W-NCM 90.57% (best 90.57%)
2025-12-11 18:18:04,522 [trainer.py] => Average forgetting (W-NCM): 18.18% | Max forgetting (W-NCM): 40.74%
2025-12-11 18:18:04,534 [trainer.py] => All params: 144526051
2025-12-11 18:18:04,546 [trainer.py] => Trainable params: 185858
2025-12-11 18:18:04,546 [inflora.py] => Learning on 64-66
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight'}
2025-12-11 18:19:30,323 [inflora.py] => Task 32, Epoch 50/50 => Loss 0.102, Train_accy 94.02
Threshold:  0.9863999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 32/768 type remove
Layer 3 : 63/768 type remove
Layer 4 : 78/768 type remove
Layer 5 : 100/768 type remove
Layer 6 : 95/768 type remove
Layer 7 : 118/768 type remove
Layer 8 : 135/768 type remove
Layer 9 : 179/768 type remove
Layer 10 : 174/768 type remove
Layer 11 : 115/768 type remove
Layer 12 : 160/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:19:36,478 [trainer.py] => Time:91.93138122558594
2004 2004
2004 2004
2025-12-11 18:19:42,809 [trainer.py] => Time:6.331256866455078
2025-12-11 18:19:42,809 [inflora.py] => Exemplar size: 0
2025-12-11 18:19:42,810 [trainer.py] => CNN: {'total': np.float64(49.35), '00-01': np.float64(62.37), '02-03': np.float64(59.68), '04-05': np.float64(48.31), '06-07': np.float64(58.93), '08-09': np.float64(46.58), '10-11': np.float64(24.39), '12-13': np.float64(53.7), '14-15': np.float64(35.71), '16-17': np.float64(28.57), '18-19': np.float64(51.79), '20-21': np.float64(56.04), '22-23': np.float64(61.72), '24-25': np.float64(30.61), '26-27': np.float64(61.74), '28-29': np.float64(56.79), '30-31': np.float64(48.18), '32-33': np.float64(25.0), '34-35': np.float64(51.61), '36-37': np.float64(56.67), '38-39': np.float64(78.05), '40-41': np.float64(38.89), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(70.21), '50-51': np.float64(40.54), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(62.5), '58-59': np.float64(18.18), '60-61': np.float64(63.41), '62-63': np.float64(63.21), '64-65': np.float64(16.67), 'old': np.float64(49.85), 'new': np.float64(16.67)}
2025-12-11 18:19:42,810 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35)]
2025-12-11 18:19:42,810 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06)]
2025-12-11 18:19:42,810 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962]
2025-12-11 18:19:50,704 [trainer.py] => W-NCM: {'00-01': 47.31182795698925, '02-03': 58.06451612903226, '04-05': 50.56179775280899, '06-07': 82.14285714285714, '08-09': 69.86301369863014, '10-11': 58.536585365853654, '12-13': 81.48148148148148, '14-15': 67.85714285714286, '16-17': 70.0, '18-19': 73.21428571428571, '20-21': 70.32967032967034, '22-23': 63.28125, '24-25': 59.183673469387756, '26-27': 61.73913043478261, '28-29': 64.19753086419753, '30-31': 54.54545454545454, '32-33': 88.63636363636364, '34-35': 74.19354838709677, '36-37': 76.66666666666667, '38-39': 48.78048780487805, '40-41': 51.85185185185185, '42-43': 71.875, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 79.7872340425532, '50-51': 67.56756756756756, '52-53': 80.55555555555556, '54-55': 56.00000000000001, '56-57': 85.0, '58-59': 61.81818181818181, '60-61': 85.36585365853658, '62-63': 85.84905660377359, '64-65': 93.33333333333333}
2025-12-11 18:19:50,704 [trainer.py] => Ave Acc (W-NCM): 68.88%
2025-12-11 18:19:50,704 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 47.31% (best 97.85%); T2: W-NCM 58.06% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 81.48% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 63.28% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 61.74% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 54.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 48.78% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 79.79% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 85.00% (best 92.50%); T30: W-NCM 61.82% (best 90.91%); T31: W-NCM 85.37% (best 90.24%); T32: W-NCM 85.85% (best 90.57%); T33: W-NCM 93.33% (best 93.33%)
2025-12-11 18:19:50,704 [trainer.py] => Average forgetting (W-NCM): 23.73% | Max forgetting (W-NCM): 50.54%
2025-12-11 18:19:50,717 [trainer.py] => All params: 144526051
2025-12-11 18:19:50,729 [trainer.py] => Trainable params: 185858
2025-12-11 18:19:50,730 [inflora.py] => Learning on 66-68
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight'}
2025-12-11 18:21:08,124 [inflora.py] => Task 33, Epoch 50/50 => Loss 0.043, Train_accy 97.12
Threshold:  0.9866
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 79/768 type remove
Layer 5 : 101/768 type remove
Layer 6 : 96/768 type remove
Layer 7 : 120/768 type remove
Layer 8 : 137/768 type remove
Layer 9 : 182/768 type remove
Layer 10 : 177/768 type remove
Layer 11 : 117/768 type remove
Layer 12 : 164/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:21:14,251 [trainer.py] => Time:83.5211775302887
2023 2023
2023 2023
2025-12-11 18:21:20,603 [trainer.py] => Time:6.351713418960571
2025-12-11 18:21:20,603 [inflora.py] => Exemplar size: 0
2025-12-11 18:21:20,603 [trainer.py] => CNN: {'total': np.float64(49.13), '00-01': np.float64(64.52), '02-03': np.float64(64.52), '04-05': np.float64(49.44), '06-07': np.float64(60.71), '08-09': np.float64(42.47), '10-11': np.float64(21.95), '12-13': np.float64(50.0), '14-15': np.float64(35.71), '16-17': np.float64(30.0), '18-19': np.float64(53.57), '20-21': np.float64(48.35), '22-23': np.float64(62.5), '24-25': np.float64(30.61), '26-27': np.float64(64.35), '28-29': np.float64(54.32), '30-31': np.float64(46.36), '32-33': np.float64(20.45), '34-35': np.float64(54.84), '36-37': np.float64(56.67), '38-39': np.float64(78.05), '40-41': np.float64(44.44), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(67.02), '50-51': np.float64(43.24), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(55.0), '58-59': np.float64(18.18), '60-61': np.float64(60.98), '62-63': np.float64(65.09), '64-65': np.float64(20.0), '66-67': np.float64(42.11), 'old': np.float64(49.2), 'new': np.float64(42.11)}
2025-12-11 18:21:20,603 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13)]
2025-12-11 18:21:20,603 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44)]
2025-12-11 18:21:20,603 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581]
2025-12-11 18:21:28,341 [trainer.py] => W-NCM: {'00-01': 49.46236559139785, '02-03': 51.61290322580645, '04-05': 49.43820224719101, '06-07': 83.92857142857143, '08-09': 65.75342465753424, '10-11': 53.65853658536586, '12-13': 85.18518518518519, '14-15': 66.07142857142857, '16-17': 68.57142857142857, '18-19': 67.85714285714286, '20-21': 68.13186813186813, '22-23': 59.375, '24-25': 61.224489795918366, '26-27': 49.56521739130435, '28-29': 58.0246913580247, '30-31': 43.63636363636363, '32-33': 88.63636363636364, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 48.78048780487805, '40-41': 50.0, '42-43': 71.875, '44-45': 40.74074074074074, '46-47': 74.07407407407408, '48-49': 77.6595744680851, '50-51': 64.86486486486487, '52-53': 80.55555555555556, '54-55': 56.00000000000001, '56-57': 82.5, '58-59': 60.0, '60-61': 80.48780487804879, '62-63': 90.56603773584906, '64-65': 83.33333333333334, '66-67': 100.0}
2025-12-11 18:21:28,342 [trainer.py] => Ave Acc (W-NCM): 67.08%
2025-12-11 18:21:28,342 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 49.46% (best 97.85%); T2: W-NCM 51.61% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 66.07% (best 94.64%); T9: W-NCM 68.57% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 68.13% (best 95.60%); T12: W-NCM 59.38% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 49.57% (best 94.78%); T15: W-NCM 58.02% (best 96.30%); T16: W-NCM 43.64% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 48.78% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 77.66% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 82.50% (best 92.50%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 80.49% (best 90.24%); T32: W-NCM 90.57% (best 90.57%); T33: W-NCM 83.33% (best 93.33%); T34: W-NCM 100.00% (best 100.00%)
2025-12-11 18:21:28,342 [trainer.py] => Average forgetting (W-NCM): 25.80% | Max forgetting (W-NCM): 50.00%
2025-12-11 18:21:28,355 [trainer.py] => All params: 144526051
2025-12-11 18:21:28,367 [trainer.py] => Trainable params: 185858
2025-12-11 18:21:28,367 [inflora.py] => Learning on 68-70
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'classifier_pool.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight'}
2025-12-11 18:22:53,854 [inflora.py] => Task 34, Epoch 50/50 => Loss 0.029, Train_accy 100.00
Threshold:  0.9868
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 65/768 type remove
Layer 4 : 80/768 type remove
Layer 5 : 103/768 type remove
Layer 6 : 99/768 type remove
Layer 7 : 125/768 type remove
Layer 8 : 141/768 type remove
Layer 9 : 188/768 type remove
Layer 10 : 183/768 type remove
Layer 11 : 121/768 type remove
Layer 12 : 168/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:22:59,977 [trainer.py] => Time:91.61005711555481
2047 2047
2047 2047
2025-12-11 18:23:06,419 [trainer.py] => Time:6.4418065547943115
2025-12-11 18:23:06,419 [inflora.py] => Exemplar size: 0
2025-12-11 18:23:06,419 [trainer.py] => CNN: {'total': np.float64(48.95), '00-01': np.float64(65.59), '02-03': np.float64(69.35), '04-05': np.float64(52.81), '06-07': np.float64(55.36), '08-09': np.float64(46.58), '10-11': np.float64(24.39), '12-13': np.float64(57.41), '14-15': np.float64(33.93), '16-17': np.float64(28.57), '18-19': np.float64(51.79), '20-21': np.float64(41.76), '22-23': np.float64(63.28), '24-25': np.float64(36.73), '26-27': np.float64(62.61), '28-29': np.float64(61.73), '30-31': np.float64(44.55), '32-33': np.float64(27.27), '34-35': np.float64(58.06), '36-37': np.float64(61.67), '38-39': np.float64(78.05), '40-41': np.float64(44.44), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(70.21), '50-51': np.float64(37.84), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(20.0), '60-61': np.float64(63.41), '62-63': np.float64(53.77), '64-65': np.float64(20.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), 'old': np.float64(49.43), 'new': np.float64(8.33)}
2025-12-11 18:23:06,419 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95)]
2025-12-11 18:23:06,419 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04)]
2025-12-11 18:23:06,419 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715]
2025-12-11 18:23:14,375 [trainer.py] => W-NCM: {'00-01': 52.68817204301075, '02-03': 56.451612903225815, '04-05': 46.06741573033708, '06-07': 83.92857142857143, '08-09': 65.75342465753424, '10-11': 53.65853658536586, '12-13': 85.18518518518519, '14-15': 67.85714285714286, '16-17': 71.42857142857143, '18-19': 69.64285714285714, '20-21': 69.23076923076923, '22-23': 63.28125, '24-25': 59.183673469387756, '26-27': 46.08695652173913, '28-29': 59.25925925925925, '30-31': 40.0, '32-33': 84.0909090909091, '34-35': 70.96774193548387, '36-37': 76.66666666666667, '38-39': 48.78048780487805, '40-41': 55.55555555555556, '42-43': 71.875, '44-45': 44.44444444444444, '46-47': 70.37037037037037, '48-49': 75.53191489361703, '50-51': 56.75675675675676, '52-53': 77.77777777777779, '54-55': 40.0, '56-57': 77.5, '58-59': 49.09090909090909, '60-61': 58.536585365853654, '62-63': 85.84905660377359, '64-65': 76.66666666666667, '66-67': 73.68421052631578, '68-69': 91.66666666666666}
2025-12-11 18:23:14,376 [trainer.py] => Ave Acc (W-NCM): 65.01%
2025-12-11 18:23:14,376 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 52.69% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 46.07% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 69.23% (best 95.60%); T12: W-NCM 63.28% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 46.09% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 40.00% (best 93.64%); T17: W-NCM 84.09% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 48.78% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 75.53% (best 94.68%); T26: W-NCM 56.76% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 40.00% (best 80.00%); T29: W-NCM 77.50% (best 92.50%); T30: W-NCM 49.09% (best 90.91%); T31: W-NCM 58.54% (best 90.24%); T32: W-NCM 85.85% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 73.68% (best 100.00%); T35: W-NCM 91.67% (best 91.67%)
2025-12-11 18:23:14,376 [trainer.py] => Average forgetting (W-NCM): 27.89% | Max forgetting (W-NCM): 53.64%
2025-12-11 18:23:14,388 [trainer.py] => All params: 144526051
2025-12-11 18:23:14,400 [trainer.py] => Trainable params: 185858
2025-12-11 18:23:14,400 [inflora.py] => Learning on 70-72
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight'}
2025-12-11 18:25:12,347 [inflora.py] => Task 35, Epoch 50/50 => Loss 0.048, Train_accy 97.53
Threshold:  0.987
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 38/768 type remove
Layer 3 : 66/768 type remove
Layer 4 : 82/768 type remove
Layer 5 : 104/768 type remove
Layer 6 : 102/768 type remove
Layer 7 : 128/768 type remove
Layer 8 : 145/768 type remove
Layer 9 : 196/768 type remove
Layer 10 : 192/768 type remove
Layer 11 : 125/768 type remove
Layer 12 : 175/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:25:19,054 [trainer.py] => Time:124.65375518798828
2120 2120
2120 2120
2025-12-11 18:25:25,671 [trainer.py] => Time:6.617077350616455
2025-12-11 18:25:25,672 [inflora.py] => Exemplar size: 0
2025-12-11 18:25:25,672 [trainer.py] => CNN: {'total': np.float64(48.07), '00-01': np.float64(69.89), '02-03': np.float64(66.13), '04-05': np.float64(51.69), '06-07': np.float64(53.57), '08-09': np.float64(43.84), '10-11': np.float64(21.95), '12-13': np.float64(51.85), '14-15': np.float64(30.36), '16-17': np.float64(27.14), '18-19': np.float64(55.36), '20-21': np.float64(46.15), '22-23': np.float64(64.84), '24-25': np.float64(34.69), '26-27': np.float64(58.26), '28-29': np.float64(62.96), '30-31': np.float64(47.27), '32-33': np.float64(20.45), '34-35': np.float64(51.61), '36-37': np.float64(63.33), '38-39': np.float64(75.61), '40-41': np.float64(42.59), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(65.96), '50-51': np.float64(45.95), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(18.18), '60-61': np.float64(53.66), '62-63': np.float64(60.38), '64-65': np.float64(20.0), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(35.62), 'old': np.float64(48.51), 'new': np.float64(35.62)}
2025-12-11 18:25:25,672 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07)]
2025-12-11 18:25:25,672 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47)]
2025-12-11 18:25:25,672 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054]
2025-12-11 18:25:34,154 [trainer.py] => W-NCM: {'00-01': 50.53763440860215, '02-03': 67.74193548387096, '04-05': 47.19101123595505, '06-07': 82.14285714285714, '08-09': 69.86301369863014, '10-11': 56.09756097560976, '12-13': 90.74074074074075, '14-15': 71.42857142857143, '16-17': 70.0, '18-19': 67.85714285714286, '20-21': 72.52747252747253, '22-23': 65.625, '24-25': 61.224489795918366, '26-27': 47.82608695652174, '28-29': 62.96296296296296, '30-31': 40.0, '32-33': 86.36363636363636, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 46.34146341463415, '40-41': 55.55555555555556, '42-43': 81.25, '44-45': 40.74074074074074, '46-47': 74.07407407407408, '48-49': 75.53191489361703, '50-51': 62.16216216216216, '52-53': 77.77777777777779, '54-55': 52.0, '56-57': 82.5, '58-59': 52.72727272727272, '60-61': 65.85365853658537, '62-63': 83.01886792452831, '64-65': 73.33333333333333, '66-67': 68.42105263157895, '68-69': 75.0, '70-71': 91.78082191780823}
2025-12-11 18:25:34,155 [trainer.py] => Ave Acc (W-NCM): 67.15%
2025-12-11 18:25:34,155 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 50.54% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 47.19% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 47.83% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 40.00% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 46.34% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 75.53% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 82.50% (best 92.50%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 83.02% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 68.42% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 91.78% (best 91.78%)
2025-12-11 18:25:34,155 [trainer.py] => Average forgetting (W-NCM): 25.66% | Max forgetting (W-NCM): 53.64%
2025-12-11 18:25:34,168 [trainer.py] => All params: 144526051
2025-12-11 18:25:34,180 [trainer.py] => Trainable params: 185858
2025-12-11 18:25:34,180 [inflora.py] => Learning on 72-74
Parameters to be updated: {'classifier_pool.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight'}
2025-12-11 18:27:13,661 [inflora.py] => Task 36, Epoch 50/50 => Loss 0.016, Train_accy 98.83
Threshold:  0.9872
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 40/768 type remove
Layer 3 : 68/768 type remove
Layer 4 : 86/768 type remove
Layer 5 : 107/768 type remove
Layer 6 : 105/768 type remove
Layer 7 : 133/768 type remove
Layer 8 : 152/768 type remove
Layer 9 : 210/768 type remove
Layer 10 : 205/768 type remove
Layer 11 : 134/768 type remove
Layer 12 : 181/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:27:20,378 [trainer.py] => Time:106.19834661483765
2160 2160
2160 2160
2025-12-11 18:27:27,103 [trainer.py] => Time:6.724104166030884
2025-12-11 18:27:27,103 [inflora.py] => Exemplar size: 0
2025-12-11 18:27:27,103 [trainer.py] => CNN: {'total': np.float64(49.58), '00-01': np.float64(68.82), '02-03': np.float64(66.13), '04-05': np.float64(52.81), '06-07': np.float64(53.57), '08-09': np.float64(45.21), '10-11': np.float64(29.27), '12-13': np.float64(55.56), '14-15': np.float64(35.71), '16-17': np.float64(30.0), '18-19': np.float64(51.79), '20-21': np.float64(43.96), '22-23': np.float64(65.62), '24-25': np.float64(34.69), '26-27': np.float64(61.74), '28-29': np.float64(64.2), '30-31': np.float64(46.36), '32-33': np.float64(22.73), '34-35': np.float64(45.16), '36-37': np.float64(56.67), '38-39': np.float64(78.05), '40-41': np.float64(42.59), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(74.47), '50-51': np.float64(45.95), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(14.55), '60-61': np.float64(68.29), '62-63': np.float64(62.26), '64-65': np.float64(16.67), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(38.36), '72-73': np.float64(62.5), 'old': np.float64(49.34), 'new': np.float64(62.5)}
2025-12-11 18:27:27,103 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58)]
2025-12-11 18:27:27,103 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88)]
2025-12-11 18:27:27,103 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335]
2025-12-11 18:27:35,458 [trainer.py] => W-NCM: {'00-01': 56.98924731182796, '02-03': 69.35483870967742, '04-05': 53.93258426966292, '06-07': 85.71428571428571, '08-09': 75.34246575342466, '10-11': 60.97560975609756, '12-13': 90.74074074074075, '14-15': 73.21428571428571, '16-17': 74.28571428571429, '18-19': 71.42857142857143, '20-21': 76.92307692307693, '22-23': 67.96875, '24-25': 65.3061224489796, '26-27': 61.73913043478261, '28-29': 65.4320987654321, '30-31': 47.27272727272727, '32-33': 88.63636363636364, '34-35': 77.41935483870968, '36-37': 78.33333333333333, '38-39': 56.09756097560976, '40-41': 55.55555555555556, '42-43': 81.25, '44-45': 44.44444444444444, '46-47': 70.37037037037037, '48-49': 78.72340425531915, '50-51': 64.86486486486487, '52-53': 86.11111111111111, '54-55': 56.00000000000001, '56-57': 87.5, '58-59': 61.81818181818181, '60-61': 70.73170731707317, '62-63': 87.73584905660378, '64-65': 76.66666666666667, '66-67': 63.1578947368421, '68-69': 83.33333333333334, '70-71': 89.04109589041096, '72-73': 92.5}
2025-12-11 18:27:35,458 [trainer.py] => Ave Acc (W-NCM): 71.54%
2025-12-11 18:27:35,458 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 56.99% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 85.71% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 74.29% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 61.74% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 47.27% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 77.42% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 56.10% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 61.82% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 87.74% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 83.33% (best 91.67%); T36: W-NCM 89.04% (best 91.78%); T37: W-NCM 92.50% (best 92.50%)
2025-12-11 18:27:35,458 [trainer.py] => Average forgetting (W-NCM): 21.15% | Max forgetting (W-NCM): 46.36%
2025-12-11 18:27:35,471 [trainer.py] => All params: 144526051
2025-12-11 18:27:35,483 [trainer.py] => Trainable params: 185858
2025-12-11 18:27:35,483 [inflora.py] => Learning on 74-76
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight'}
2025-12-11 18:30:12,751 [inflora.py] => Task 37, Epoch 50/50 => Loss 0.025, Train_accy 99.74
Threshold:  0.9873999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 41/768 type remove
Layer 3 : 69/768 type remove
Layer 4 : 87/768 type remove
Layer 5 : 110/768 type remove
Layer 6 : 109/768 type remove
Layer 7 : 138/768 type remove
Layer 8 : 158/768 type remove
Layer 9 : 220/768 type remove
Layer 10 : 215/768 type remove
Layer 11 : 141/768 type remove
Layer 12 : 193/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:30:20,334 [trainer.py] => Time:164.85166239738464
2238 2238
2238 2238
2025-12-11 18:30:27,284 [trainer.py] => Time:6.949539661407471
2025-12-11 18:30:27,285 [inflora.py] => Exemplar size: 0
2025-12-11 18:30:27,285 [trainer.py] => CNN: {'total': np.float64(45.98), '00-01': np.float64(66.67), '02-03': np.float64(59.68), '04-05': np.float64(47.19), '06-07': np.float64(58.93), '08-09': np.float64(39.73), '10-11': np.float64(19.51), '12-13': np.float64(51.85), '14-15': np.float64(30.36), '16-17': np.float64(18.57), '18-19': np.float64(50.0), '20-21': np.float64(53.85), '22-23': np.float64(64.06), '24-25': np.float64(28.57), '26-27': np.float64(53.04), '28-29': np.float64(64.2), '30-31': np.float64(50.91), '32-33': np.float64(18.18), '34-35': np.float64(51.61), '36-37': np.float64(66.67), '38-39': np.float64(65.85), '40-41': np.float64(24.07), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(25.93), '48-49': np.float64(63.83), '50-51': np.float64(8.11), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(16.36), '60-61': np.float64(51.22), '62-63': np.float64(53.77), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(35.62), '72-73': np.float64(45.0), '74-75': np.float64(79.49), 'old': np.float64(44.77), 'new': np.float64(79.49)}
2025-12-11 18:30:27,285 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98)]
2025-12-11 18:30:27,285 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62)]
2025-12-11 18:30:27,285 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037]
2025-12-11 18:30:36,379 [trainer.py] => W-NCM: {'00-01': 61.29032258064516, '02-03': 70.96774193548387, '04-05': 61.79775280898876, '06-07': 83.92857142857143, '08-09': 65.75342465753424, '10-11': 60.97560975609756, '12-13': 92.5925925925926, '14-15': 73.21428571428571, '16-17': 72.85714285714285, '18-19': 71.42857142857143, '20-21': 79.12087912087912, '22-23': 71.09375, '24-25': 65.3061224489796, '26-27': 54.78260869565217, '28-29': 66.66666666666666, '30-31': 55.45454545454545, '32-33': 86.36363636363636, '34-35': 80.64516129032258, '36-37': 76.66666666666667, '38-39': 60.97560975609756, '40-41': 62.96296296296296, '42-43': 84.375, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 81.91489361702128, '50-51': 64.86486486486487, '52-53': 77.77777777777779, '54-55': 44.0, '56-57': 87.5, '58-59': 63.63636363636363, '60-61': 68.29268292682927, '62-63': 82.0754716981132, '64-65': 66.66666666666666, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 82.1917808219178, '72-73': 82.5, '74-75': 87.17948717948718}
2025-12-11 18:30:36,379 [trainer.py] => Ave Acc (W-NCM): 71.31%
2025-12-11 18:30:36,380 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 61.29% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 92.59% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 71.09% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 55.45% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 80.65% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 62.96% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 63.64% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 82.08% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 82.19% (best 91.78%); T37: W-NCM 82.50% (best 92.50%); T38: W-NCM 87.18% (best 87.18%)
2025-12-11 18:30:36,380 [trainer.py] => Average forgetting (W-NCM): 21.23% | Max forgetting (W-NCM): 42.11%
2025-12-11 18:30:36,392 [trainer.py] => All params: 144526051
2025-12-11 18:30:36,404 [trainer.py] => Trainable params: 185858
2025-12-11 18:30:36,404 [inflora.py] => Learning on 76-78
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'classifier_pool.38.bias'}
2025-12-11 18:32:52,665 [inflora.py] => Task 38, Epoch 50/50 => Loss 0.034, Train_accy 99.03
Threshold:  0.9876
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 42/768 type remove
Layer 3 : 71/768 type remove
Layer 4 : 90/768 type remove
Layer 5 : 114/768 type remove
Layer 6 : 113/768 type remove
Layer 7 : 141/768 type remove
Layer 8 : 162/768 type remove
Layer 9 : 223/768 type remove
Layer 10 : 218/768 type remove
Layer 11 : 144/768 type remove
Layer 12 : 195/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:32:59,845 [trainer.py] => Time:143.440984249115
2321 2321
2321 2321
2025-12-11 18:33:07,068 [trainer.py] => Time:7.222053527832031
2025-12-11 18:33:07,068 [inflora.py] => Exemplar size: 0
2025-12-11 18:33:07,068 [trainer.py] => CNN: {'total': np.float64(46.57), '00-01': np.float64(65.59), '02-03': np.float64(58.06), '04-05': np.float64(52.81), '06-07': np.float64(58.93), '08-09': np.float64(39.73), '10-11': np.float64(19.51), '12-13': np.float64(35.19), '14-15': np.float64(32.14), '16-17': np.float64(24.29), '18-19': np.float64(55.36), '20-21': np.float64(53.85), '22-23': np.float64(65.62), '24-25': np.float64(26.53), '26-27': np.float64(58.26), '28-29': np.float64(66.67), '30-31': np.float64(53.64), '32-33': np.float64(6.82), '34-35': np.float64(48.39), '36-37': np.float64(65.0), '38-39': np.float64(68.29), '40-41': np.float64(27.78), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(33.33), '48-49': np.float64(60.64), '50-51': np.float64(8.11), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(16.36), '60-61': np.float64(60.98), '62-63': np.float64(49.06), '64-65': np.float64(3.33), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(34.25), '72-73': np.float64(50.0), '74-75': np.float64(71.79), '76-77': np.float64(50.6), 'old': np.float64(46.43), 'new': np.float64(50.6)}
2025-12-11 18:33:07,068 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57)]
2025-12-11 18:33:07,068 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78)]
2025-12-11 18:33:07,068 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054]
2025-12-11 18:33:16,673 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 74.19354838709677, '04-05': 69.66292134831461, '06-07': 80.35714285714286, '08-09': 73.97260273972603, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 75.0, '16-17': 77.14285714285715, '18-19': 69.64285714285714, '20-21': 79.12087912087912, '22-23': 71.09375, '24-25': 65.3061224489796, '26-27': 64.34782608695652, '28-29': 72.8395061728395, '30-31': 53.63636363636364, '32-33': 90.9090909090909, '34-35': 77.41935483870968, '36-37': 78.33333333333333, '38-39': 60.97560975609756, '40-41': 61.111111111111114, '42-43': 78.125, '44-45': 55.55555555555556, '46-47': 81.48148148148148, '48-49': 81.91489361702128, '50-51': 62.16216216216216, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 85.0, '58-59': 61.81818181818181, '60-61': 70.73170731707317, '62-63': 83.9622641509434, '64-65': 76.66666666666667, '66-67': 63.1578947368421, '68-69': 75.0, '70-71': 82.1917808219178, '72-73': 75.0, '74-75': 75.64102564102564, '76-77': 78.3132530120482}
2025-12-11 18:33:16,673 [trainer.py] => Ave Acc (W-NCM): 72.59%
2025-12-11 18:33:16,673 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 74.19% (best 90.32%); T3: W-NCM 69.66% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 71.09% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 64.35% (best 94.78%); T15: W-NCM 72.84% (best 96.30%); T16: W-NCM 53.64% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 77.42% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 61.11% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 92.50%); T30: W-NCM 61.82% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 83.96% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 82.19% (best 91.78%); T37: W-NCM 75.00% (best 92.50%); T38: W-NCM 75.64% (best 87.18%); T39: W-NCM 78.31% (best 78.31%)
2025-12-11 18:33:16,673 [trainer.py] => Average forgetting (W-NCM): 19.54% | Max forgetting (W-NCM): 40.00%
2025-12-11 18:33:16,686 [trainer.py] => All params: 144526051
2025-12-11 18:33:16,698 [trainer.py] => Trainable params: 185858
2025-12-11 18:33:16,698 [inflora.py] => Learning on 78-80
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'classifier_pool.39.weight'}
2025-12-11 18:35:48,677 [inflora.py] => Task 39, Epoch 50/50 => Loss 0.035, Train_accy 98.44
Threshold:  0.9878
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 42/768 type remove
Layer 3 : 72/768 type remove
Layer 4 : 91/768 type remove
Layer 5 : 115/768 type remove
Layer 6 : 114/768 type remove
Layer 7 : 143/768 type remove
Layer 8 : 165/768 type remove
Layer 9 : 227/768 type remove
Layer 10 : 221/768 type remove
Layer 11 : 146/768 type remove
Layer 12 : 200/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:35:56,175 [trainer.py] => Time:159.47716975212097
2403 2403
2403 2403
2025-12-11 18:36:03,629 [trainer.py] => Time:7.452963829040527
2025-12-11 18:36:03,629 [inflora.py] => Exemplar size: 0
2025-12-11 18:36:03,629 [trainer.py] => CNN: {'total': np.float64(45.19), '00-01': np.float64(62.37), '02-03': np.float64(54.84), '04-05': np.float64(48.31), '06-07': np.float64(58.93), '08-09': np.float64(41.1), '10-11': np.float64(19.51), '12-13': np.float64(37.04), '14-15': np.float64(32.14), '16-17': np.float64(20.0), '18-19': np.float64(55.36), '20-21': np.float64(56.04), '22-23': np.float64(67.19), '24-25': np.float64(24.49), '26-27': np.float64(57.39), '28-29': np.float64(62.96), '30-31': np.float64(52.73), '32-33': np.float64(9.09), '34-35': np.float64(38.71), '36-37': np.float64(53.33), '38-39': np.float64(68.29), '40-41': np.float64(22.22), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(57.45), '50-51': np.float64(5.41), '52-53': np.float64(61.11), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(9.09), '60-61': np.float64(56.1), '62-63': np.float64(47.17), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(31.51), '72-73': np.float64(50.0), '74-75': np.float64(69.23), '76-77': np.float64(49.4), '78-79': np.float64(54.88), 'old': np.float64(44.85), 'new': np.float64(54.88)}
2025-12-11 18:36:03,629 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19)]
2025-12-11 18:36:03,629 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92)]
2025-12-11 18:36:03,629 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643]
2025-12-11 18:36:13,044 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 67.74193548387096, '04-05': 64.04494382022472, '06-07': 82.14285714285714, '08-09': 75.34246575342466, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 74.28571428571429, '18-19': 73.21428571428571, '20-21': 76.92307692307693, '22-23': 69.53125, '24-25': 63.26530612244898, '26-27': 55.65217391304348, '28-29': 71.60493827160494, '30-31': 57.27272727272727, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 73.33333333333333, '38-39': 68.29268292682927, '40-41': 59.25925925925925, '42-43': 75.0, '44-45': 44.44444444444444, '46-47': 81.48148148148148, '48-49': 81.91489361702128, '50-51': 70.27027027027027, '52-53': 86.11111111111111, '54-55': 52.0, '56-57': 82.5, '58-59': 65.45454545454545, '60-61': 70.73170731707317, '62-63': 79.24528301886792, '64-65': 80.0, '66-67': 63.1578947368421, '68-69': 75.0, '70-71': 73.97260273972603, '72-73': 72.5, '74-75': 61.53846153846154, '76-77': 77.10843373493977, '78-79': 89.02439024390245}
2025-12-11 18:36:13,045 [trainer.py] => Ave Acc (W-NCM): 71.98%
2025-12-11 18:36:13,045 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 64.04% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 75.34% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 74.29% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 55.65% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 57.27% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 82.50% (best 92.50%); T30: W-NCM 65.45% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 79.25% (best 90.57%); T33: W-NCM 80.00% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 73.97% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 61.54% (best 87.18%); T39: W-NCM 77.11% (best 78.31%); T40: W-NCM 89.02% (best 89.02%)
2025-12-11 18:36:13,045 [trainer.py] => Average forgetting (W-NCM): 20.09% | Max forgetting (W-NCM): 39.13%
2025-12-11 18:36:13,058 [trainer.py] => All params: 144526051
2025-12-11 18:36:13,070 [trainer.py] => Trainable params: 185858
2025-12-11 18:36:13,070 [inflora.py] => Learning on 80-82
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight'}
2025-12-11 18:38:23,546 [inflora.py] => Task 40, Epoch 50/50 => Loss 0.026, Train_accy 99.28
Threshold:  0.988
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 42/768 type remove
Layer 3 : 73/768 type remove
Layer 4 : 94/768 type remove
Layer 5 : 117/768 type remove
Layer 6 : 116/768 type remove
Layer 7 : 147/768 type remove
Layer 8 : 170/768 type remove
Layer 9 : 231/768 type remove
Layer 10 : 227/768 type remove
Layer 11 : 151/768 type remove
Layer 12 : 206/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:38:31,045 [trainer.py] => Time:137.9750542640686
2475 2475
2475 2475
2025-12-11 18:38:38,643 [trainer.py] => Time:7.598058462142944
2025-12-11 18:38:38,643 [inflora.py] => Exemplar size: 0
2025-12-11 18:38:38,643 [trainer.py] => CNN: {'total': np.float64(47.92), '00-01': np.float64(67.74), '02-03': np.float64(53.23), '04-05': np.float64(56.18), '06-07': np.float64(60.71), '08-09': np.float64(42.47), '10-11': np.float64(17.07), '12-13': np.float64(37.04), '14-15': np.float64(39.29), '16-17': np.float64(24.29), '18-19': np.float64(57.14), '20-21': np.float64(57.14), '22-23': np.float64(67.19), '24-25': np.float64(28.57), '26-27': np.float64(62.61), '28-29': np.float64(61.73), '30-31': np.float64(51.82), '32-33': np.float64(15.91), '34-35': np.float64(41.94), '36-37': np.float64(61.67), '38-39': np.float64(65.85), '40-41': np.float64(27.78), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(61.7), '50-51': np.float64(10.81), '52-53': np.float64(61.11), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(10.91), '60-61': np.float64(56.1), '62-63': np.float64(51.89), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(35.62), '72-73': np.float64(52.5), '74-75': np.float64(66.67), '76-77': np.float64(46.99), '78-79': np.float64(58.54), '80-81': np.float64(63.89), 'old': np.float64(47.44), 'new': np.float64(63.89)}
2025-12-11 18:38:38,644 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92)]
2025-12-11 18:38:38,644 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2)]
2025-12-11 18:38:38,644 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917]
2025-12-11 18:38:48,219 [trainer.py] => W-NCM: {'00-01': 73.11827956989248, '02-03': 74.19354838709677, '04-05': 62.92134831460674, '06-07': 82.14285714285714, '08-09': 72.6027397260274, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 75.71428571428571, '18-19': 75.0, '20-21': 79.12087912087912, '22-23': 69.53125, '24-25': 67.3469387755102, '26-27': 60.86956521739131, '28-29': 70.37037037037037, '30-31': 58.18181818181818, '32-33': 90.9090909090909, '34-35': 77.41935483870968, '36-37': 76.66666666666667, '38-39': 65.85365853658537, '40-41': 62.96296296296296, '42-43': 68.75, '44-45': 48.148148148148145, '46-47': 81.48148148148148, '48-49': 85.1063829787234, '50-51': 70.27027027027027, '52-53': 91.66666666666666, '54-55': 52.0, '56-57': 85.0, '58-59': 61.81818181818181, '60-61': 70.73170731707317, '62-63': 80.18867924528303, '64-65': 83.33333333333334, '66-67': 63.1578947368421, '68-69': 75.0, '70-71': 73.97260273972603, '72-73': 77.5, '74-75': 58.97435897435898, '76-77': 75.90361445783132, '78-79': 85.36585365853658, '80-81': 97.22222222222221}
2025-12-11 18:38:48,219 [trainer.py] => Ave Acc (W-NCM): 73.55%
2025-12-11 18:38:48,219 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 73.12% (best 97.85%); T2: W-NCM 74.19% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 72.60% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 75.71% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 60.87% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 58.18% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 77.42% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 62.96% (best 94.44%); T22: W-NCM 68.75% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 85.11% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 91.67% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 92.50%); T30: W-NCM 61.82% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 80.19% (best 90.57%); T33: W-NCM 83.33% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 73.97% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 58.97% (best 87.18%); T39: W-NCM 75.90% (best 78.31%); T40: W-NCM 85.37% (best 89.02%); T41: W-NCM 97.22% (best 97.22%)
2025-12-11 18:38:48,219 [trainer.py] => Average forgetting (W-NCM): 18.61% | Max forgetting (W-NCM): 36.84%
2025-12-11 18:38:48,232 [trainer.py] => All params: 144526051
2025-12-11 18:38:48,244 [trainer.py] => Trainable params: 185858
2025-12-11 18:38:48,244 [inflora.py] => Learning on 82-84
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight'}
2025-12-11 18:41:38,161 [inflora.py] => Task 41, Epoch 50/50 => Loss 0.050, Train_accy 97.67
Threshold:  0.9882
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 74/768 type remove
Layer 4 : 98/768 type remove
Layer 5 : 121/768 type remove
Layer 6 : 118/768 type remove
Layer 7 : 149/768 type remove
Layer 8 : 173/768 type remove
Layer 9 : 235/768 type remove
Layer 10 : 232/768 type remove
Layer 11 : 153/768 type remove
Layer 12 : 213/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:41:45,838 [trainer.py] => Time:177.59379267692566
2593 2593
2593 2593
2025-12-11 18:41:53,735 [trainer.py] => Time:7.896505117416382
2025-12-11 18:41:53,735 [inflora.py] => Exemplar size: 0
2025-12-11 18:41:53,735 [trainer.py] => CNN: {'total': np.float64(49.06), '00-01': np.float64(70.97), '02-03': np.float64(62.9), '04-05': np.float64(53.93), '06-07': np.float64(60.71), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(50.0), '14-15': np.float64(37.5), '16-17': np.float64(20.0), '18-19': np.float64(57.14), '20-21': np.float64(56.04), '22-23': np.float64(66.41), '24-25': np.float64(30.61), '26-27': np.float64(66.09), '28-29': np.float64(64.2), '30-31': np.float64(58.18), '32-33': np.float64(20.45), '34-35': np.float64(48.39), '36-37': np.float64(65.0), '38-39': np.float64(65.85), '40-41': np.float64(31.48), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(62.77), '50-51': np.float64(13.51), '52-53': np.float64(61.11), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(14.55), '60-61': np.float64(60.98), '62-63': np.float64(50.94), '64-65': np.float64(3.33), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(36.99), '72-73': np.float64(52.5), '74-75': np.float64(60.26), '76-77': np.float64(49.4), '78-79': np.float64(60.98), '80-81': np.float64(69.44), '82-83': np.float64(35.59), 'old': np.float64(49.7), 'new': np.float64(35.59)}
2025-12-11 18:41:53,735 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06)]
2025-12-11 18:41:53,735 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22)]
2025-12-11 18:41:53,735 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649]
2025-12-11 18:42:03,839 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 77.41935483870968, '04-05': 62.92134831460674, '06-07': 76.78571428571429, '08-09': 73.97260273972603, '10-11': 70.73170731707317, '12-13': 88.88888888888889, '14-15': 80.35714285714286, '16-17': 74.28571428571429, '18-19': 75.0, '20-21': 82.41758241758241, '22-23': 67.1875, '24-25': 67.3469387755102, '26-27': 67.82608695652173, '28-29': 71.60493827160494, '30-31': 61.81818181818181, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 81.66666666666667, '38-39': 68.29268292682927, '40-41': 64.81481481481481, '42-43': 68.75, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 86.17021276595744, '50-51': 64.86486486486487, '52-53': 88.88888888888889, '54-55': 56.00000000000001, '56-57': 87.5, '58-59': 60.0, '60-61': 70.73170731707317, '62-63': 83.01886792452831, '64-65': 86.66666666666667, '66-67': 63.1578947368421, '68-69': 79.16666666666666, '70-71': 78.08219178082192, '72-73': 75.0, '74-75': 50.0, '76-77': 73.49397590361446, '78-79': 86.58536585365853, '80-81': 90.27777777777779, '82-83': 89.83050847457628}
2025-12-11 18:42:03,839 [trainer.py] => Ave Acc (W-NCM): 74.21%
2025-12-11 18:42:03,839 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 77.42% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 80.36% (best 94.64%); T9: W-NCM 74.29% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 82.42% (best 95.60%); T12: W-NCM 67.19% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 67.83% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 61.82% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 81.67% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 64.81% (best 94.44%); T22: W-NCM 68.75% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 86.17% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 83.02% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 79.17% (best 91.67%); T36: W-NCM 78.08% (best 91.78%); T37: W-NCM 75.00% (best 92.50%); T38: W-NCM 50.00% (best 87.18%); T39: W-NCM 73.49% (best 78.31%); T40: W-NCM 86.59% (best 89.02%); T41: W-NCM 90.28% (best 97.22%); T42: W-NCM 89.83% (best 89.83%)
2025-12-11 18:42:03,839 [trainer.py] => Average forgetting (W-NCM): 17.88% | Max forgetting (W-NCM): 37.18%
2025-12-11 18:42:03,852 [trainer.py] => All params: 144526051
2025-12-11 18:42:03,864 [trainer.py] => Trainable params: 185858
2025-12-11 18:42:03,864 [inflora.py] => Learning on 84-86
Parameters to be updated: {'classifier_pool.42.bias', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight'}
2025-12-11 18:43:57,489 [inflora.py] => Task 42, Epoch 50/50 => Loss 0.058, Train_accy 98.16
Threshold:  0.9884
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 75/768 type remove
Layer 4 : 99/768 type remove
Layer 5 : 123/768 type remove
Layer 6 : 120/768 type remove
Layer 7 : 153/768 type remove
Layer 8 : 177/768 type remove
Layer 9 : 238/768 type remove
Layer 10 : 237/768 type remove
Layer 11 : 155/768 type remove
Layer 12 : 216/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:44:04,518 [trainer.py] => Time:120.65425419807434
2639 2639
2639 2639
2025-12-11 18:44:12,543 [trainer.py] => Time:8.024895906448364
2025-12-11 18:44:12,544 [inflora.py] => Exemplar size: 0
2025-12-11 18:44:12,544 [trainer.py] => CNN: {'total': np.float64(48.58), '00-01': np.float64(66.67), '02-03': np.float64(62.9), '04-05': np.float64(55.06), '06-07': np.float64(62.5), '08-09': np.float64(42.47), '10-11': np.float64(21.95), '12-13': np.float64(38.89), '14-15': np.float64(32.14), '16-17': np.float64(20.0), '18-19': np.float64(60.71), '20-21': np.float64(56.04), '22-23': np.float64(68.75), '24-25': np.float64(26.53), '26-27': np.float64(64.35), '28-29': np.float64(67.9), '30-31': np.float64(58.18), '32-33': np.float64(15.91), '34-35': np.float64(45.16), '36-37': np.float64(60.0), '38-39': np.float64(70.73), '40-41': np.float64(29.63), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(58.51), '50-51': np.float64(8.11), '52-53': np.float64(61.11), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(12.73), '60-61': np.float64(56.1), '62-63': np.float64(52.83), '64-65': np.float64(6.67), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(35.62), '72-73': np.float64(55.0), '74-75': np.float64(64.1), '76-77': np.float64(48.19), '78-79': np.float64(59.76), '80-81': np.float64(70.83), '82-83': np.float64(38.14), '84-85': np.float64(45.65), 'old': np.float64(48.63), 'new': np.float64(45.65)}
2025-12-11 18:44:12,544 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58)]
2025-12-11 18:44:12,544 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51)]
2025-12-11 18:44:12,544 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827]
2025-12-11 18:44:22,252 [trainer.py] => W-NCM: {'00-01': 63.44086021505376, '02-03': 79.03225806451613, '04-05': 70.78651685393258, '06-07': 75.0, '08-09': 71.23287671232876, '10-11': 68.29268292682927, '12-13': 88.88888888888889, '14-15': 80.35714285714286, '16-17': 78.57142857142857, '18-19': 82.14285714285714, '20-21': 83.51648351648352, '22-23': 70.3125, '24-25': 69.38775510204081, '26-27': 66.95652173913044, '28-29': 70.37037037037037, '30-31': 60.909090909090914, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 81.66666666666667, '38-39': 65.85365853658537, '40-41': 59.25925925925925, '42-43': 71.875, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 86.17021276595744, '50-51': 67.56756756756756, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 87.5, '58-59': 52.72727272727272, '60-61': 70.73170731707317, '62-63': 83.9622641509434, '64-65': 80.0, '66-67': 63.1578947368421, '68-69': 79.16666666666666, '70-71': 75.34246575342466, '72-73': 77.5, '74-75': 55.12820512820513, '76-77': 74.69879518072288, '78-79': 84.14634146341463, '80-81': 87.5, '82-83': 84.7457627118644, '84-85': 89.13043478260869}
2025-12-11 18:44:22,252 [trainer.py] => Ave Acc (W-NCM): 74.20%
2025-12-11 18:44:22,253 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 63.44% (best 97.85%); T2: W-NCM 79.03% (best 90.32%); T3: W-NCM 70.79% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 68.29% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 80.36% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 82.14% (best 94.64%); T11: W-NCM 83.52% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 66.96% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 60.91% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 81.67% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 86.17% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 83.96% (best 90.57%); T33: W-NCM 80.00% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 79.17% (best 91.67%); T36: W-NCM 75.34% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 55.13% (best 87.18%); T39: W-NCM 74.70% (best 78.31%); T40: W-NCM 84.15% (best 89.02%); T41: W-NCM 87.50% (best 97.22%); T42: W-NCM 84.75% (best 89.83%); T43: W-NCM 89.13% (best 89.13%)
2025-12-11 18:44:22,253 [trainer.py] => Average forgetting (W-NCM): 17.81% | Max forgetting (W-NCM): 38.18%
2025-12-11 18:44:22,265 [trainer.py] => All params: 144526051
2025-12-11 18:44:22,277 [trainer.py] => Trainable params: 185858
2025-12-11 18:44:22,278 [inflora.py] => Learning on 86-88
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'classifier_pool.43.bias', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight'}
2025-12-11 18:47:01,697 [inflora.py] => Task 43, Epoch 50/50 => Loss 0.074, Train_accy 97.42
Threshold:  0.9886
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 77/768 type remove
Layer 4 : 104/768 type remove
Layer 5 : 128/768 type remove
Layer 6 : 123/768 type remove
Layer 7 : 158/768 type remove
Layer 8 : 184/768 type remove
Layer 9 : 246/768 type remove
Layer 10 : 248/768 type remove
Layer 11 : 158/768 type remove
Layer 12 : 219/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:47:09,457 [trainer.py] => Time:167.1791229248047
2726 2726
2726 2726
2025-12-11 18:47:17,772 [trainer.py] => Time:8.315495491027832
2025-12-11 18:47:17,773 [inflora.py] => Exemplar size: 0
2025-12-11 18:47:17,773 [trainer.py] => CNN: {'total': np.float64(47.4), '00-01': np.float64(67.74), '02-03': np.float64(56.45), '04-05': np.float64(52.81), '06-07': np.float64(62.5), '08-09': np.float64(41.1), '10-11': np.float64(19.51), '12-13': np.float64(37.04), '14-15': np.float64(33.93), '16-17': np.float64(18.57), '18-19': np.float64(60.71), '20-21': np.float64(58.24), '22-23': np.float64(67.19), '24-25': np.float64(26.53), '26-27': np.float64(63.48), '28-29': np.float64(66.67), '30-31': np.float64(57.27), '32-33': np.float64(13.64), '34-35': np.float64(38.71), '36-37': np.float64(56.67), '38-39': np.float64(73.17), '40-41': np.float64(27.78), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(51.06), '50-51': np.float64(13.51), '52-53': np.float64(58.33), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(9.09), '60-61': np.float64(53.66), '62-63': np.float64(51.89), '64-65': np.float64(3.33), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(57.5), '74-75': np.float64(60.26), '76-77': np.float64(46.99), '78-79': np.float64(56.1), '80-81': np.float64(72.22), '82-83': np.float64(34.75), '84-85': np.float64(52.17), '86-87': np.float64(54.02), 'old': np.float64(47.18), 'new': np.float64(54.02)}
2025-12-11 18:47:17,773 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4)]
2025-12-11 18:47:17,773 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82)]
2025-12-11 18:47:17,773 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831]
2025-12-11 18:47:28,190 [trainer.py] => W-NCM: {'00-01': 70.96774193548387, '02-03': 75.80645161290323, '04-05': 74.15730337078652, '06-07': 76.78571428571429, '08-09': 73.97260273972603, '10-11': 70.73170731707317, '12-13': 88.88888888888889, '14-15': 82.14285714285714, '16-17': 80.0, '18-19': 80.35714285714286, '20-21': 83.51648351648352, '22-23': 71.875, '24-25': 71.42857142857143, '26-27': 67.82608695652173, '28-29': 71.60493827160494, '30-31': 63.63636363636363, '32-33': 90.9090909090909, '34-35': 70.96774193548387, '36-37': 76.66666666666667, '38-39': 70.73170731707317, '40-41': 59.25925925925925, '42-43': 71.875, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 82.97872340425532, '50-51': 75.67567567567568, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 87.5, '58-59': 58.18181818181818, '60-61': 70.73170731707317, '62-63': 84.90566037735849, '64-65': 86.66666666666667, '66-67': 63.1578947368421, '68-69': 75.0, '70-71': 75.34246575342466, '72-73': 80.0, '74-75': 50.0, '76-77': 74.69879518072288, '78-79': 84.14634146341463, '80-81': 88.88888888888889, '82-83': 80.50847457627118, '84-85': 80.43478260869566, '86-87': 91.95402298850574}
2025-12-11 18:47:28,190 [trainer.py] => Ave Acc (W-NCM): 75.08%
2025-12-11 18:47:28,190 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 70.97% (best 97.85%); T2: W-NCM 75.81% (best 90.32%); T3: W-NCM 74.16% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 73.97% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 82.14% (best 94.64%); T9: W-NCM 80.00% (best 98.57%); T10: W-NCM 80.36% (best 94.64%); T11: W-NCM 83.52% (best 95.60%); T12: W-NCM 71.88% (best 95.31%); T13: W-NCM 71.43% (best 87.76%); T14: W-NCM 67.83% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 63.64% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 58.18% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 84.91% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 75.34% (best 91.78%); T37: W-NCM 80.00% (best 92.50%); T38: W-NCM 50.00% (best 87.18%); T39: W-NCM 74.70% (best 78.31%); T40: W-NCM 84.15% (best 89.02%); T41: W-NCM 88.89% (best 97.22%); T42: W-NCM 80.51% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 91.95% (best 91.95%)
2025-12-11 18:47:28,190 [trainer.py] => Average forgetting (W-NCM): 16.91% | Max forgetting (W-NCM): 37.18%
2025-12-11 18:47:28,203 [trainer.py] => All params: 144526051
2025-12-11 18:47:28,215 [trainer.py] => Trainable params: 185858
2025-12-11 18:47:28,216 [inflora.py] => Learning on 88-90
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight'}
2025-12-11 18:49:35,382 [inflora.py] => Task 44, Epoch 50/50 => Loss 0.018, Train_accy 99.61
Threshold:  0.9888
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 44/768 type remove
Layer 3 : 79/768 type remove
Layer 4 : 108/768 type remove
Layer 5 : 133/768 type remove
Layer 6 : 127/768 type remove
Layer 7 : 164/768 type remove
Layer 8 : 194/768 type remove
Layer 9 : 254/768 type remove
Layer 10 : 252/768 type remove
Layer 11 : 161/768 type remove
Layer 12 : 222/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:49:42,462 [trainer.py] => Time:134.24612545967102
2786 2786
2786 2786
2025-12-11 18:49:50,860 [trainer.py] => Time:8.397984027862549
2025-12-11 18:49:50,860 [inflora.py] => Exemplar size: 0
2025-12-11 18:49:50,860 [trainer.py] => CNN: {'total': np.float64(48.06), '00-01': np.float64(66.67), '02-03': np.float64(54.84), '04-05': np.float64(56.18), '06-07': np.float64(58.93), '08-09': np.float64(41.1), '10-11': np.float64(19.51), '12-13': np.float64(37.04), '14-15': np.float64(33.93), '16-17': np.float64(17.14), '18-19': np.float64(60.71), '20-21': np.float64(57.14), '22-23': np.float64(64.84), '24-25': np.float64(22.45), '26-27': np.float64(65.22), '28-29': np.float64(65.43), '30-31': np.float64(60.0), '32-33': np.float64(6.82), '34-35': np.float64(45.16), '36-37': np.float64(61.67), '38-39': np.float64(75.61), '40-41': np.float64(31.48), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(52.13), '50-51': np.float64(8.11), '52-53': np.float64(58.33), '54-55': np.float64(0.0), '56-57': np.float64(50.0), '58-59': np.float64(9.09), '60-61': np.float64(53.66), '62-63': np.float64(51.89), '64-65': np.float64(3.33), '66-67': np.float64(42.11), '68-69': np.float64(12.5), '70-71': np.float64(34.25), '72-73': np.float64(60.0), '74-75': np.float64(57.69), '76-77': np.float64(43.37), '78-79': np.float64(57.32), '80-81': np.float64(68.06), '82-83': np.float64(34.75), '84-85': np.float64(52.17), '86-87': np.float64(50.57), '88-89': np.float64(81.67), 'old': np.float64(47.32), 'new': np.float64(81.67)}
2025-12-11 18:49:50,860 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06)]
2025-12-11 18:49:50,860 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02)]
2025-12-11 18:49:50,861 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388]
2025-12-11 18:50:01,240 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 79.03225806451613, '04-05': 71.91011235955057, '06-07': 75.0, '08-09': 67.12328767123287, '10-11': 70.73170731707317, '12-13': 88.88888888888889, '14-15': 83.92857142857143, '16-17': 77.14285714285715, '18-19': 78.57142857142857, '20-21': 83.51648351648352, '22-23': 70.3125, '24-25': 71.42857142857143, '26-27': 73.91304347826086, '28-29': 71.60493827160494, '30-31': 63.63636363636363, '32-33': 90.9090909090909, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 68.29268292682927, '40-41': 53.70370370370371, '42-43': 78.125, '44-45': 55.55555555555556, '46-47': 81.48148148148148, '48-49': 85.1063829787234, '50-51': 70.27027027027027, '52-53': 77.77777777777779, '54-55': 48.0, '56-57': 87.5, '58-59': 52.72727272727272, '60-61': 70.73170731707317, '62-63': 83.01886792452831, '64-65': 86.66666666666667, '66-67': 68.42105263157895, '68-69': 79.16666666666666, '70-71': 75.34246575342466, '72-73': 80.0, '74-75': 51.28205128205128, '76-77': 74.69879518072288, '78-79': 79.26829268292683, '80-81': 84.72222222222221, '82-83': 78.8135593220339, '84-85': 78.26086956521739, '86-87': 85.0574712643678, '88-89': 96.66666666666667}
2025-12-11 18:50:01,241 [trainer.py] => Ave Acc (W-NCM): 74.74%
2025-12-11 18:50:01,241 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 79.03% (best 90.32%); T3: W-NCM 71.91% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 70.73% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 83.93% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 78.57% (best 94.64%); T11: W-NCM 83.52% (best 95.60%); T12: W-NCM 70.31% (best 95.31%); T13: W-NCM 71.43% (best 87.76%); T14: W-NCM 73.91% (best 94.78%); T15: W-NCM 71.60% (best 96.30%); T16: W-NCM 63.64% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 85.11% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 87.50% (best 92.50%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 83.02% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 68.42% (best 100.00%); T35: W-NCM 79.17% (best 91.67%); T36: W-NCM 75.34% (best 91.78%); T37: W-NCM 80.00% (best 92.50%); T38: W-NCM 51.28% (best 87.18%); T39: W-NCM 74.70% (best 78.31%); T40: W-NCM 79.27% (best 89.02%); T41: W-NCM 84.72% (best 97.22%); T42: W-NCM 78.81% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 85.06% (best 91.95%); T45: W-NCM 96.67% (best 96.67%)
2025-12-11 18:50:01,241 [trainer.py] => Average forgetting (W-NCM): 17.37% | Max forgetting (W-NCM): 40.74%
2025-12-11 18:50:01,254 [trainer.py] => All params: 144526051
2025-12-11 18:50:01,266 [trainer.py] => Trainable params: 185858
2025-12-11 18:50:01,266 [inflora.py] => Learning on 90-92
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight'}
2025-12-11 18:51:33,779 [inflora.py] => Task 45, Epoch 50/50 => Loss 0.045, Train_accy 97.81
Threshold:  0.989
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 80/768 type remove
Layer 4 : 109/768 type remove
Layer 5 : 135/768 type remove
Layer 6 : 129/768 type remove
Layer 7 : 166/768 type remove
Layer 8 : 196/768 type remove
Layer 9 : 257/768 type remove
Layer 10 : 255/768 type remove
Layer 11 : 164/768 type remove
Layer 12 : 228/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:51:40,497 [trainer.py] => Time:99.23101115226746
2826 2826
2826 2826
2025-12-11 18:51:49,137 [trainer.py] => Time:8.63944411277771
2025-12-11 18:51:49,137 [inflora.py] => Exemplar size: 0
2025-12-11 18:51:49,137 [trainer.py] => CNN: {'total': np.float64(47.24), '00-01': np.float64(61.29), '02-03': np.float64(48.39), '04-05': np.float64(58.43), '06-07': np.float64(60.71), '08-09': np.float64(39.73), '10-11': np.float64(14.63), '12-13': np.float64(40.74), '14-15': np.float64(39.29), '16-17': np.float64(24.29), '18-19': np.float64(57.14), '20-21': np.float64(58.24), '22-23': np.float64(65.62), '24-25': np.float64(24.49), '26-27': np.float64(63.48), '28-29': np.float64(64.2), '30-31': np.float64(60.0), '32-33': np.float64(9.09), '34-35': np.float64(41.94), '36-37': np.float64(60.0), '38-39': np.float64(70.73), '40-41': np.float64(35.19), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(62.96), '48-49': np.float64(39.36), '50-51': np.float64(13.51), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(5.45), '60-61': np.float64(51.22), '62-63': np.float64(51.89), '64-65': np.float64(6.67), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(26.03), '72-73': np.float64(55.0), '74-75': np.float64(64.1), '76-77': np.float64(43.37), '78-79': np.float64(54.88), '80-81': np.float64(70.83), '82-83': np.float64(33.9), '84-85': np.float64(54.35), '86-87': np.float64(47.13), '88-89': np.float64(83.33), '90-91': np.float64(45.0), 'old': np.float64(47.27), 'new': np.float64(45.0)}
2025-12-11 18:51:49,137 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24)]
2025-12-11 18:51:49,137 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25)]
2025-12-11 18:51:49,138 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998]
2025-12-11 18:51:59,245 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 72.58064516129032, '04-05': 62.92134831460674, '06-07': 76.78571428571429, '08-09': 64.38356164383562, '10-11': 58.536585365853654, '12-13': 88.88888888888889, '14-15': 80.35714285714286, '16-17': 72.85714285714285, '18-19': 69.64285714285714, '20-21': 76.92307692307693, '22-23': 67.1875, '24-25': 69.38775510204081, '26-27': 66.95652173913044, '28-29': 64.19753086419753, '30-31': 60.0, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 75.0, '38-39': 65.85365853658537, '40-41': 53.70370370370371, '42-43': 75.0, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 82.97872340425532, '50-51': 67.56756756756756, '52-53': 80.55555555555556, '54-55': 48.0, '56-57': 90.0, '58-59': 43.63636363636363, '60-61': 73.17073170731707, '62-63': 80.18867924528303, '64-65': 83.33333333333334, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 72.6027397260274, '72-73': 80.0, '74-75': 42.30769230769231, '76-77': 71.08433734939759, '78-79': 75.60975609756098, '80-81': 79.16666666666666, '82-83': 72.88135593220339, '84-85': 78.26086956521739, '86-87': 85.0574712643678, '88-89': 96.66666666666667, '90-91': 92.5}
2025-12-11 18:51:59,245 [trainer.py] => Ave Acc (W-NCM): 71.64%
2025-12-11 18:51:59,245 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 80.36% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 67.19% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 66.96% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 60.00% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 90.00% (best 92.50%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 73.17% (best 90.24%); T32: W-NCM 80.19% (best 90.57%); T33: W-NCM 83.33% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 72.60% (best 91.78%); T37: W-NCM 80.00% (best 92.50%); T38: W-NCM 42.31% (best 87.18%); T39: W-NCM 71.08% (best 78.31%); T40: W-NCM 75.61% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 72.88% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 85.06% (best 91.95%); T45: W-NCM 96.67% (best 96.67%); T46: W-NCM 92.50% (best 92.50%)
2025-12-11 18:51:59,245 [trainer.py] => Average forgetting (W-NCM): 20.55% | Max forgetting (W-NCM): 47.27%
2025-12-11 18:51:59,258 [trainer.py] => All params: 144526051
2025-12-11 18:51:59,270 [trainer.py] => Trainable params: 185858
2025-12-11 18:51:59,270 [inflora.py] => Learning on 92-94
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight'}
2025-12-11 18:53:44,328 [inflora.py] => Task 46, Epoch 50/50 => Loss 0.048, Train_accy 98.40
Threshold:  0.9892
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 81/768 type remove
Layer 4 : 110/768 type remove
Layer 5 : 136/768 type remove
Layer 6 : 131/768 type remove
Layer 7 : 168/768 type remove
Layer 8 : 198/768 type remove
Layer 9 : 260/768 type remove
Layer 10 : 258/768 type remove
Layer 11 : 168/768 type remove
Layer 12 : 243/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:53:51,209 [trainer.py] => Time:111.93923711776733
2875 2875
2875 2875
2025-12-11 18:53:59,878 [trainer.py] => Time:8.668315172195435
2025-12-11 18:53:59,878 [inflora.py] => Exemplar size: 0
2025-12-11 18:53:59,878 [trainer.py] => CNN: {'total': np.float64(45.98), '00-01': np.float64(60.22), '02-03': np.float64(56.45), '04-05': np.float64(59.55), '06-07': np.float64(60.71), '08-09': np.float64(36.99), '10-11': np.float64(19.51), '12-13': np.float64(38.89), '14-15': np.float64(37.5), '16-17': np.float64(17.14), '18-19': np.float64(58.93), '20-21': np.float64(57.14), '22-23': np.float64(60.94), '24-25': np.float64(28.57), '26-27': np.float64(63.48), '28-29': np.float64(65.43), '30-31': np.float64(55.45), '32-33': np.float64(9.09), '34-35': np.float64(41.94), '36-37': np.float64(58.33), '38-39': np.float64(68.29), '40-41': np.float64(35.19), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(37.23), '50-51': np.float64(10.81), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(47.5), '58-59': np.float64(5.45), '60-61': np.float64(51.22), '62-63': np.float64(51.89), '64-65': np.float64(13.33), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(57.5), '74-75': np.float64(62.82), '76-77': np.float64(44.58), '78-79': np.float64(53.66), '80-81': np.float64(70.83), '82-83': np.float64(33.9), '84-85': np.float64(56.52), '86-87': np.float64(49.43), '88-89': np.float64(80.0), '90-91': np.float64(37.5), '92-93': np.float64(14.29), 'old': np.float64(46.53), 'new': np.float64(14.29)}
2025-12-11 18:53:59,878 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98)]
2025-12-11 18:53:59,879 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03)]
2025-12-11 18:53:59,879 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827]
2025-12-11 18:54:10,163 [trainer.py] => W-NCM: {'00-01': 54.83870967741935, '02-03': 54.83870967741935, '04-05': 61.79775280898876, '06-07': 75.0, '08-09': 58.9041095890411, '10-11': 56.09756097560976, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 74.28571428571429, '18-19': 69.64285714285714, '20-21': 71.42857142857143, '22-23': 62.5, '24-25': 63.26530612244898, '26-27': 56.52173913043478, '28-29': 56.79012345679012, '30-31': 51.81818181818182, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 70.0, '38-39': 58.536585365853654, '40-41': 48.148148148148145, '42-43': 65.625, '44-45': 48.148148148148145, '46-47': 77.77777777777779, '48-49': 79.7872340425532, '50-51': 67.56756756756756, '52-53': 77.77777777777779, '54-55': 48.0, '56-57': 85.0, '58-59': 36.36363636363637, '60-61': 65.85365853658537, '62-63': 77.35849056603774, '64-65': 70.0, '66-67': 63.1578947368421, '68-69': 70.83333333333334, '70-71': 60.273972602739725, '72-73': 67.5, '74-75': 29.48717948717949, '76-77': 68.67469879518072, '78-79': 59.756097560975604, '80-81': 70.83333333333334, '82-83': 61.86440677966102, '84-85': 76.08695652173914, '86-87': 80.45977011494253, '88-89': 93.33333333333333, '90-91': 80.0, '92-93': 85.71428571428571}
2025-12-11 18:54:10,163 [trainer.py] => Ave Acc (W-NCM): 66.49%
2025-12-11 18:54:10,163 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 54.84% (best 97.85%); T2: W-NCM 54.84% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 58.90% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 74.29% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 62.50% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 56.52% (best 94.78%); T15: W-NCM 56.79% (best 96.30%); T16: W-NCM 51.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 70.00% (best 86.67%); T20: W-NCM 58.54% (best 97.56%); T21: W-NCM 48.15% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 79.79% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 85.00% (best 92.50%); T30: W-NCM 36.36% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 77.36% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 29.49% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 70.83% (best 97.22%); T42: W-NCM 61.86% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 80.46% (best 91.95%); T45: W-NCM 93.33% (best 96.67%); T46: W-NCM 80.00% (best 92.50%); T47: W-NCM 85.71% (best 85.71%)
2025-12-11 18:54:10,163 [trainer.py] => Average forgetting (W-NCM): 25.67% | Max forgetting (W-NCM): 57.69%
2025-12-11 18:54:10,176 [trainer.py] => All params: 144526051
2025-12-11 18:54:10,188 [trainer.py] => Trainable params: 185858
2025-12-11 18:54:10,188 [inflora.py] => Learning on 94-96
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'classifier_pool.47.weight'}
2025-12-11 18:56:17,747 [inflora.py] => Task 47, Epoch 50/50 => Loss 0.035, Train_accy 98.45
Threshold:  0.9894
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 46/768 type remove
Layer 3 : 82/768 type remove
Layer 4 : 112/768 type remove
Layer 5 : 139/768 type remove
Layer 6 : 133/768 type remove
Layer 7 : 170/768 type remove
Layer 8 : 200/768 type remove
Layer 9 : 262/768 type remove
Layer 10 : 260/768 type remove
Layer 11 : 170/768 type remove
Layer 12 : 245/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:56:25,168 [trainer.py] => Time:134.97983074188232
2946 2946
2946 2946
2025-12-11 18:56:34,051 [trainer.py] => Time:8.8827223777771
2025-12-11 18:56:34,052 [inflora.py] => Exemplar size: 0
2025-12-11 18:56:34,052 [trainer.py] => CNN: {'total': np.float64(47.15), '00-01': np.float64(64.52), '02-03': np.float64(61.29), '04-05': np.float64(64.04), '06-07': np.float64(57.14), '08-09': np.float64(39.73), '10-11': np.float64(21.95), '12-13': np.float64(46.3), '14-15': np.float64(41.07), '16-17': np.float64(17.14), '18-19': np.float64(60.71), '20-21': np.float64(59.34), '22-23': np.float64(63.28), '24-25': np.float64(24.49), '26-27': np.float64(66.09), '28-29': np.float64(62.96), '30-31': np.float64(53.64), '32-33': np.float64(15.91), '34-35': np.float64(41.94), '36-37': np.float64(56.67), '38-39': np.float64(68.29), '40-41': np.float64(37.04), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(45.74), '50-51': np.float64(8.11), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(9.09), '60-61': np.float64(53.66), '62-63': np.float64(54.72), '64-65': np.float64(3.33), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(57.5), '74-75': np.float64(67.95), '76-77': np.float64(46.99), '78-79': np.float64(57.32), '80-81': np.float64(72.22), '82-83': np.float64(35.59), '84-85': np.float64(50.0), '86-87': np.float64(49.43), '88-89': np.float64(76.67), '90-91': np.float64(37.5), '92-93': np.float64(10.2), '94-95': np.float64(32.39), 'old': np.float64(47.51), 'new': np.float64(32.39)}
2025-12-11 18:56:34,052 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15)]
2025-12-11 18:56:34,052 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23)]
2025-12-11 18:56:34,052 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105]
2025-12-11 18:56:45,010 [trainer.py] => W-NCM: {'00-01': 61.29032258064516, '02-03': 67.74193548387096, '04-05': 67.41573033707866, '06-07': 80.35714285714286, '08-09': 65.75342465753424, '10-11': 58.536585365853654, '12-13': 87.03703703703704, '14-15': 76.78571428571429, '16-17': 75.71428571428571, '18-19': 76.78571428571429, '20-21': 75.82417582417582, '22-23': 64.84375, '24-25': 69.38775510204081, '26-27': 60.0, '28-29': 65.4320987654321, '30-31': 60.909090909090914, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 73.33333333333333, '38-39': 63.41463414634146, '40-41': 57.407407407407405, '42-43': 78.125, '44-45': 48.148148148148145, '46-47': 74.07407407407408, '48-49': 82.97872340425532, '50-51': 67.56756756756756, '52-53': 83.33333333333334, '54-55': 56.00000000000001, '56-57': 90.0, '58-59': 50.90909090909091, '60-61': 68.29268292682927, '62-63': 80.18867924528303, '64-65': 80.0, '66-67': 52.63157894736842, '68-69': 75.0, '70-71': 61.64383561643836, '72-73': 77.5, '74-75': 37.17948717948718, '76-77': 73.49397590361446, '78-79': 68.29268292682927, '80-81': 79.16666666666666, '82-83': 70.33898305084746, '84-85': 76.08695652173914, '86-87': 82.75862068965517, '88-89': 95.0, '90-91': 77.5, '92-93': 79.59183673469387, '94-95': 95.77464788732394}
2025-12-11 18:56:45,011 [trainer.py] => Ave Acc (W-NCM): 71.56%
2025-12-11 18:56:45,011 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 61.29% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 67.42% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 76.79% (best 94.64%); T9: W-NCM 75.71% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 60.00% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 60.91% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 90.00% (best 92.50%); T30: W-NCM 50.91% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 80.19% (best 90.57%); T33: W-NCM 80.00% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 61.64% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 73.49% (best 78.31%); T40: W-NCM 68.29% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 70.34% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 82.76% (best 91.95%); T45: W-NCM 95.00% (best 96.67%); T46: W-NCM 77.50% (best 92.50%); T47: W-NCM 79.59% (best 85.71%); T48: W-NCM 95.77% (best 95.77%)
2025-12-11 18:56:45,011 [trainer.py] => Average forgetting (W-NCM): 20.57% | Max forgetting (W-NCM): 50.00%
2025-12-11 18:56:45,023 [trainer.py] => All params: 144526051
2025-12-11 18:56:45,035 [trainer.py] => Trainable params: 185858
2025-12-11 18:56:45,035 [inflora.py] => Learning on 96-98
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.5.attn.lora_B_v.48.weight'}
2025-12-11 18:59:11,122 [inflora.py] => Task 48, Epoch 50/50 => Loss 0.115, Train_accy 95.87
Threshold:  0.9896
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 47/768 type remove
Layer 3 : 83/768 type remove
Layer 4 : 115/768 type remove
Layer 5 : 143/768 type remove
Layer 6 : 137/768 type remove
Layer 7 : 173/768 type remove
Layer 8 : 203/768 type remove
Layer 9 : 267/768 type remove
Layer 10 : 270/768 type remove
Layer 11 : 178/768 type remove
Layer 12 : 254/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 18:59:18,471 [trainer.py] => Time:153.43538808822632
3045 3045
3045 3045
2025-12-11 18:59:27,640 [trainer.py] => Time:9.168822526931763
2025-12-11 18:59:27,640 [inflora.py] => Exemplar size: 0
2025-12-11 18:59:27,641 [trainer.py] => CNN: {'total': np.float64(45.29), '00-01': np.float64(65.59), '02-03': np.float64(50.0), '04-05': np.float64(62.92), '06-07': np.float64(62.5), '08-09': np.float64(42.47), '10-11': np.float64(14.63), '12-13': np.float64(44.44), '14-15': np.float64(42.86), '16-17': np.float64(20.0), '18-19': np.float64(62.5), '20-21': np.float64(57.14), '22-23': np.float64(61.72), '24-25': np.float64(20.41), '26-27': np.float64(60.87), '28-29': np.float64(61.73), '30-31': np.float64(52.73), '32-33': np.float64(13.64), '34-35': np.float64(41.94), '36-37': np.float64(56.67), '38-39': np.float64(73.17), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(43.62), '50-51': np.float64(8.11), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(9.09), '60-61': np.float64(58.54), '62-63': np.float64(55.66), '64-65': np.float64(6.67), '66-67': np.float64(52.63), '68-69': np.float64(4.17), '70-71': np.float64(20.55), '72-73': np.float64(45.0), '74-75': np.float64(62.82), '76-77': np.float64(46.99), '78-79': np.float64(53.66), '80-81': np.float64(68.06), '82-83': np.float64(32.2), '84-85': np.float64(56.52), '86-87': np.float64(47.13), '88-89': np.float64(75.0), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(32.39), '96-97': np.float64(24.24), 'old': np.float64(45.99), 'new': np.float64(24.24)}
2025-12-11 18:59:27,641 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29)]
2025-12-11 18:59:27,641 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67)]
2025-12-11 18:59:27,641 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976]
2025-12-11 18:59:38,761 [trainer.py] => W-NCM: {'00-01': 54.83870967741935, '02-03': 61.29032258064516, '04-05': 62.92134831460674, '06-07': 78.57142857142857, '08-09': 64.38356164383562, '10-11': 56.09756097560976, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 77.14285714285715, '18-19': 73.21428571428571, '20-21': 75.82417582417582, '22-23': 64.0625, '24-25': 67.3469387755102, '26-27': 61.73913043478261, '28-29': 65.4320987654321, '30-31': 52.72727272727272, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 71.66666666666667, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 75.0, '44-45': 40.74074074074074, '46-47': 77.77777777777779, '48-49': 82.97872340425532, '50-51': 72.97297297297297, '52-53': 80.55555555555556, '54-55': 56.00000000000001, '56-57': 90.0, '58-59': 43.63636363636363, '60-61': 68.29268292682927, '62-63': 78.30188679245283, '64-65': 76.66666666666667, '66-67': 57.89473684210527, '68-69': 83.33333333333334, '70-71': 63.013698630136986, '72-73': 72.5, '74-75': 34.61538461538461, '76-77': 72.28915662650603, '78-79': 60.97560975609756, '80-81': 73.61111111111111, '82-83': 64.40677966101694, '84-85': 78.26086956521739, '86-87': 83.9080459770115, '88-89': 90.0, '90-91': 72.5, '92-93': 57.14285714285714, '94-95': 90.14084507042254, '96-97': 84.84848484848484}
2025-12-11 18:59:38,761 [trainer.py] => Ave Acc (W-NCM): 69.54%
2025-12-11 18:59:38,761 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 54.84% (best 97.85%); T2: W-NCM 61.29% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 64.06% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 61.74% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 52.73% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 71.67% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 90.00% (best 92.50%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 78.30% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 83.33% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 34.62% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 60.98% (best 89.02%); T41: W-NCM 73.61% (best 97.22%); T42: W-NCM 64.41% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 83.91% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 72.50% (best 92.50%); T47: W-NCM 57.14% (best 85.71%); T48: W-NCM 90.14% (best 95.77%); T49: W-NCM 84.85% (best 84.85%)
2025-12-11 18:59:38,761 [trainer.py] => Average forgetting (W-NCM): 22.47% | Max forgetting (W-NCM): 52.56%
2025-12-11 18:59:38,774 [trainer.py] => All params: 144526051
2025-12-11 18:59:38,786 [trainer.py] => Trainable params: 185858
2025-12-11 18:59:38,786 [inflora.py] => Learning on 98-100
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight'}
2025-12-11 19:02:00,386 [inflora.py] => Task 49, Epoch 50/50 => Loss 0.043, Train_accy 98.71
Threshold:  0.9898
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 48/768 type remove
Layer 3 : 84/768 type remove
Layer 4 : 116/768 type remove
Layer 5 : 145/768 type remove
Layer 6 : 138/768 type remove
Layer 7 : 175/768 type remove
Layer 8 : 205/768 type remove
Layer 9 : 270/768 type remove
Layer 10 : 274/768 type remove
Layer 11 : 182/768 type remove
Layer 12 : 261/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:02:07,623 [trainer.py] => Time:148.83626675605774
3119 3119
3119 3119
2025-12-11 19:02:17,005 [trainer.py] => Time:9.382484436035156
2025-12-11 19:02:17,006 [inflora.py] => Exemplar size: 0
2025-12-11 19:02:17,006 [trainer.py] => CNN: {'total': np.float64(45.21), '00-01': np.float64(63.44), '02-03': np.float64(48.39), '04-05': np.float64(55.06), '06-07': np.float64(62.5), '08-09': np.float64(41.1), '10-11': np.float64(24.39), '12-13': np.float64(48.15), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(60.71), '20-21': np.float64(56.04), '22-23': np.float64(58.59), '24-25': np.float64(26.53), '26-27': np.float64(64.35), '28-29': np.float64(60.49), '30-31': np.float64(53.64), '32-33': np.float64(13.64), '34-35': np.float64(41.94), '36-37': np.float64(58.33), '38-39': np.float64(63.41), '40-41': np.float64(42.59), '42-43': np.float64(37.5), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(44.68), '50-51': np.float64(10.81), '52-53': np.float64(58.33), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(10.91), '60-61': np.float64(60.98), '62-63': np.float64(51.89), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(52.5), '74-75': np.float64(62.82), '76-77': np.float64(44.58), '78-79': np.float64(52.44), '80-81': np.float64(70.83), '82-83': np.float64(33.05), '84-85': np.float64(50.0), '86-87': np.float64(44.83), '88-89': np.float64(73.33), '90-91': np.float64(45.0), '92-93': np.float64(16.33), '94-95': np.float64(30.99), '96-97': np.float64(25.25), '98-99': np.float64(47.3), 'old': np.float64(45.16), 'new': np.float64(47.3)}
2025-12-11 19:02:17,006 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21)]
2025-12-11 19:02:17,006 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09)]
2025-12-11 19:02:17,006 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376]
2025-12-11 19:02:28,298 [trainer.py] => W-NCM: {'00-01': 56.98924731182796, '02-03': 58.06451612903226, '04-05': 66.29213483146067, '06-07': 78.57142857142857, '08-09': 68.4931506849315, '10-11': 56.09756097560976, '12-13': 90.74074074074075, '14-15': 78.57142857142857, '16-17': 77.14285714285715, '18-19': 76.78571428571429, '20-21': 79.12087912087912, '22-23': 65.625, '24-25': 61.224489795918366, '26-27': 71.30434782608695, '28-29': 70.37037037037037, '30-31': 56.36363636363636, '32-33': 90.9090909090909, '34-35': 70.96774193548387, '36-37': 75.0, '38-39': 63.41463414634146, '40-41': 57.407407407407405, '42-43': 84.375, '44-45': 48.148148148148145, '46-47': 85.18518518518519, '48-49': 81.91489361702128, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 56.00000000000001, '56-57': 92.5, '58-59': 60.0, '60-61': 75.60975609756098, '62-63': 80.18867924528303, '64-65': 76.66666666666667, '66-67': 63.1578947368421, '68-69': 70.83333333333334, '70-71': 64.38356164383562, '72-73': 75.0, '74-75': 35.8974358974359, '76-77': 72.28915662650603, '78-79': 65.85365853658537, '80-81': 79.16666666666666, '82-83': 66.10169491525424, '84-85': 78.26086956521739, '86-87': 81.60919540229885, '88-89': 88.33333333333333, '90-91': 75.0, '92-93': 53.06122448979592, '94-95': 90.14084507042254, '96-97': 75.75757575757575, '98-99': 91.8918918918919}
2025-12-11 19:02:28,299 [trainer.py] => Ave Acc (W-NCM): 71.92%
2025-12-11 19:02:28,299 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 56.99% (best 97.85%); T2: W-NCM 58.06% (best 90.32%); T3: W-NCM 66.29% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 71.30% (best 94.78%); T15: W-NCM 70.37% (best 96.30%); T16: W-NCM 56.36% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 92.50% (best 92.50%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 75.61% (best 90.24%); T32: W-NCM 80.19% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 64.38% (best 91.78%); T37: W-NCM 75.00% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 65.85% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 66.10% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 81.61% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 75.00% (best 92.50%); T47: W-NCM 53.06% (best 85.71%); T48: W-NCM 90.14% (best 95.77%); T49: W-NCM 75.76% (best 84.85%); T50: W-NCM 91.89% (best 91.89%)
2025-12-11 19:02:28,299 [trainer.py] => Average forgetting (W-NCM): 20.05% | Max forgetting (W-NCM): 51.28%
2025-12-11 19:02:28,311 [trainer.py] => All params: 144526051
2025-12-11 19:02:28,323 [trainer.py] => Trainable params: 185858
2025-12-11 19:02:28,323 [inflora.py] => Learning on 100-102
Parameters to be updated: {'classifier_pool.50.bias', 'image_encoder.blocks.5.attn.lora_B_k.50.weight', 'image_encoder.blocks.2.attn.lora_B_k.50.weight', 'image_encoder.blocks.11.attn.lora_B_k.50.weight', 'image_encoder.blocks.0.attn.lora_B_k.50.weight', 'image_encoder.blocks.8.attn.lora_B_v.50.weight', 'image_encoder.blocks.2.attn.lora_B_v.50.weight', 'image_encoder.blocks.6.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_k.50.weight', 'image_encoder.blocks.8.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_v.50.weight', 'image_encoder.blocks.3.attn.lora_B_k.50.weight', 'image_encoder.blocks.6.attn.lora_B_k.50.weight', 'image_encoder.blocks.9.attn.lora_B_k.50.weight', 'image_encoder.blocks.1.attn.lora_B_v.50.weight', 'image_encoder.blocks.0.attn.lora_B_v.50.weight', 'image_encoder.blocks.11.attn.lora_B_v.50.weight', 'image_encoder.blocks.3.attn.lora_B_v.50.weight', 'image_encoder.blocks.10.attn.lora_B_k.50.weight', 'image_encoder.blocks.1.attn.lora_B_k.50.weight', 'classifier_pool.50.weight', 'image_encoder.blocks.5.attn.lora_B_v.50.weight', 'image_encoder.blocks.10.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_v.50.weight', 'image_encoder.blocks.7.attn.lora_B_v.50.weight', 'image_encoder.blocks.4.attn.lora_B_k.50.weight'}
2025-12-11 19:04:33,423 [inflora.py] => Task 50, Epoch 50/50 => Loss 0.064, Train_accy 96.92
Threshold:  0.99
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 48/768 type remove
Layer 3 : 85/768 type remove
Layer 4 : 117/768 type remove
Layer 5 : 146/768 type remove
Layer 6 : 139/768 type remove
Layer 7 : 176/768 type remove
Layer 8 : 206/768 type remove
Layer 9 : 271/768 type remove
Layer 10 : 275/768 type remove
Layer 11 : 183/768 type remove
Layer 12 : 264/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:04:40,558 [trainer.py] => Time:132.23436331748962
3179 3179
3179 3179
2025-12-11 19:04:50,073 [trainer.py] => Time:9.515583038330078
2025-12-11 19:04:50,074 [inflora.py] => Exemplar size: 0
2025-12-11 19:04:50,074 [trainer.py] => CNN: {'total': np.float64(43.98), '00-01': np.float64(62.37), '02-03': np.float64(43.55), '04-05': np.float64(49.44), '06-07': np.float64(55.36), '08-09': np.float64(39.73), '10-11': np.float64(19.51), '12-13': np.float64(42.59), '14-15': np.float64(33.93), '16-17': np.float64(18.57), '18-19': np.float64(60.71), '20-21': np.float64(48.35), '22-23': np.float64(65.62), '24-25': np.float64(22.45), '26-27': np.float64(59.13), '28-29': np.float64(56.79), '30-31': np.float64(57.27), '32-33': np.float64(20.45), '34-35': np.float64(41.94), '36-37': np.float64(55.0), '38-39': np.float64(65.85), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(37.04), '48-49': np.float64(54.26), '50-51': np.float64(5.41), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(12.73), '60-61': np.float64(65.85), '62-63': np.float64(47.17), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(20.55), '72-73': np.float64(47.5), '74-75': np.float64(74.36), '76-77': np.float64(38.55), '78-79': np.float64(50.0), '80-81': np.float64(66.67), '82-83': np.float64(29.66), '84-85': np.float64(50.0), '86-87': np.float64(41.38), '88-89': np.float64(78.33), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(29.58), '96-97': np.float64(21.21), '98-99': np.float64(40.54), '100-101': np.float64(60.0), 'old': np.float64(43.67), 'new': np.float64(60.0)}
2025-12-11 19:04:50,074 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98)]
2025-12-11 19:04:50,074 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38)]
2025-12-11 19:04:50,075 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724]
2025-12-11 19:05:01,438 [trainer.py] => W-NCM: {'00-01': 59.13978494623656, '02-03': 56.451612903225815, '04-05': 61.79775280898876, '06-07': 80.35714285714286, '08-09': 71.23287671232876, '10-11': 58.536585365853654, '12-13': 88.88888888888889, '14-15': 78.57142857142857, '16-17': 72.85714285714285, '18-19': 76.78571428571429, '20-21': 81.31868131868131, '22-23': 66.40625, '24-25': 57.14285714285714, '26-27': 66.08695652173913, '28-29': 66.66666666666666, '30-31': 53.63636363636364, '32-33': 90.9090909090909, '34-35': 67.74193548387096, '36-37': 71.66666666666667, '38-39': 65.85365853658537, '40-41': 59.25925925925925, '42-43': 84.375, '44-45': 44.44444444444444, '46-47': 77.77777777777779, '48-49': 78.72340425531915, '50-51': 72.97297297297297, '52-53': 88.88888888888889, '54-55': 52.0, '56-57': 95.0, '58-59': 63.63636363636363, '60-61': 73.17073170731707, '62-63': 79.24528301886792, '64-65': 73.33333333333333, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 63.013698630136986, '72-73': 77.5, '74-75': 38.46153846153847, '76-77': 73.49397590361446, '78-79': 74.39024390243902, '80-81': 76.38888888888889, '82-83': 66.10169491525424, '84-85': 73.91304347826086, '86-87': 79.3103448275862, '88-89': 86.66666666666667, '90-91': 72.5, '92-93': 44.89795918367347, '94-95': 88.73239436619718, '96-97': 67.67676767676768, '98-99': 85.13513513513513, '100-101': 91.66666666666666}
2025-12-11 19:05:01,439 [trainer.py] => Ave Acc (W-NCM): 71.05%
2025-12-11 19:05:01,439 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 59.14% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 61.80% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 78.57% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 66.09% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 53.64% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 71.67% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 59.26% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 95.00% (best 95.00%); T30: W-NCM 63.64% (best 90.91%); T31: W-NCM 73.17% (best 90.24%); T32: W-NCM 79.25% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 73.49% (best 78.31%); T40: W-NCM 74.39% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 66.10% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 79.31% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 72.50% (best 92.50%); T47: W-NCM 44.90% (best 85.71%); T48: W-NCM 88.73% (best 95.77%); T49: W-NCM 67.68% (best 84.85%); T50: W-NCM 85.14% (best 91.89%); T51: W-NCM 91.67% (best 91.67%)
2025-12-11 19:05:01,439 [trainer.py] => Average forgetting (W-NCM): 20.98% | Max forgetting (W-NCM): 48.72%
2025-12-11 19:05:01,452 [trainer.py] => All params: 144526051
2025-12-11 19:05:01,464 [trainer.py] => Trainable params: 185858
2025-12-11 19:05:01,464 [inflora.py] => Learning on 102-104
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.51.weight', 'image_encoder.blocks.11.attn.lora_B_k.51.weight', 'image_encoder.blocks.3.attn.lora_B_v.51.weight', 'image_encoder.blocks.5.attn.lora_B_v.51.weight', 'classifier_pool.51.bias', 'image_encoder.blocks.0.attn.lora_B_k.51.weight', 'image_encoder.blocks.9.attn.lora_B_v.51.weight', 'image_encoder.blocks.2.attn.lora_B_k.51.weight', 'image_encoder.blocks.4.attn.lora_B_v.51.weight', 'image_encoder.blocks.9.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_v.51.weight', 'image_encoder.blocks.10.attn.lora_B_v.51.weight', 'image_encoder.blocks.0.attn.lora_B_v.51.weight', 'image_encoder.blocks.10.attn.lora_B_k.51.weight', 'image_encoder.blocks.3.attn.lora_B_k.51.weight', 'image_encoder.blocks.6.attn.lora_B_v.51.weight', 'image_encoder.blocks.11.attn.lora_B_v.51.weight', 'image_encoder.blocks.7.attn.lora_B_k.51.weight', 'classifier_pool.51.weight', 'image_encoder.blocks.7.attn.lora_B_v.51.weight', 'image_encoder.blocks.5.attn.lora_B_k.51.weight', 'image_encoder.blocks.1.attn.lora_B_k.51.weight', 'image_encoder.blocks.2.attn.lora_B_v.51.weight', 'image_encoder.blocks.8.attn.lora_B_k.51.weight', 'image_encoder.blocks.8.attn.lora_B_v.51.weight', 'image_encoder.blocks.4.attn.lora_B_k.51.weight'}
2025-12-11 19:06:48,739 [inflora.py] => Task 51, Epoch 50/50 => Loss 0.032, Train_accy 98.51
Threshold:  0.9902
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 49/768 type remove
Layer 3 : 86/768 type remove
Layer 4 : 119/768 type remove
Layer 5 : 148/768 type remove
Layer 6 : 140/768 type remove
Layer 7 : 179/768 type remove
Layer 8 : 209/768 type remove
Layer 9 : 275/768 type remove
Layer 10 : 280/768 type remove
Layer 11 : 190/768 type remove
Layer 12 : 266/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:06:55,498 [trainer.py] => Time:114.0345401763916
3237 3237
3237 3237
2025-12-11 19:07:05,160 [trainer.py] => Time:9.661420345306396
2025-12-11 19:07:05,160 [inflora.py] => Exemplar size: 0
2025-12-11 19:07:05,161 [trainer.py] => CNN: {'total': np.float64(45.07), '00-01': np.float64(64.52), '02-03': np.float64(50.0), '04-05': np.float64(52.81), '06-07': np.float64(57.14), '08-09': np.float64(42.47), '10-11': np.float64(17.07), '12-13': np.float64(51.85), '14-15': np.float64(33.93), '16-17': np.float64(17.14), '18-19': np.float64(58.93), '20-21': np.float64(47.25), '22-23': np.float64(67.19), '24-25': np.float64(28.57), '26-27': np.float64(60.87), '28-29': np.float64(58.02), '30-31': np.float64(53.64), '32-33': np.float64(13.64), '34-35': np.float64(51.61), '36-37': np.float64(50.0), '38-39': np.float64(63.41), '40-41': np.float64(38.89), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(40.74), '48-49': np.float64(54.26), '50-51': np.float64(8.11), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(10.91), '60-61': np.float64(63.41), '62-63': np.float64(48.11), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(47.5), '74-75': np.float64(70.51), '76-77': np.float64(39.76), '78-79': np.float64(54.88), '80-81': np.float64(63.89), '82-83': np.float64(31.36), '84-85': np.float64(45.65), '86-87': np.float64(47.13), '88-89': np.float64(78.33), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(30.99), '96-97': np.float64(21.21), '98-99': np.float64(40.54), '100-101': np.float64(58.33), '102-103': np.float64(74.14), 'old': np.float64(44.54), 'new': np.float64(74.14)}
2025-12-11 19:07:05,161 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07)]
2025-12-11 19:07:05,161 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11)]
2025-12-11 19:07:05,161 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276]
2025-12-11 19:07:16,457 [trainer.py] => W-NCM: {'00-01': 56.98924731182796, '02-03': 64.51612903225806, '04-05': 64.04494382022472, '06-07': 80.35714285714286, '08-09': 71.23287671232876, '10-11': 65.85365853658537, '12-13': 90.74074074074075, '14-15': 75.0, '16-17': 77.14285714285715, '18-19': 75.0, '20-21': 81.31868131868131, '22-23': 67.96875, '24-25': 57.14285714285714, '26-27': 65.21739130434783, '28-29': 66.66666666666666, '30-31': 57.27272727272727, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 71.66666666666667, '38-39': 73.17073170731707, '40-41': 55.55555555555556, '42-43': 78.125, '44-45': 44.44444444444444, '46-47': 81.48148148148148, '48-49': 78.72340425531915, '50-51': 70.27027027027027, '52-53': 91.66666666666666, '54-55': 52.0, '56-57': 95.0, '58-59': 60.0, '60-61': 68.29268292682927, '62-63': 78.30188679245283, '64-65': 76.66666666666667, '66-67': 57.89473684210527, '68-69': 75.0, '70-71': 64.38356164383562, '72-73': 72.5, '74-75': 38.46153846153847, '76-77': 73.49397590361446, '78-79': 69.51219512195121, '80-81': 83.33333333333334, '82-83': 68.64406779661016, '84-85': 76.08695652173914, '86-87': 81.60919540229885, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 34.69387755102041, '94-95': 88.73239436619718, '96-97': 67.67676767676768, '98-99': 81.08108108108108, '100-101': 85.0, '102-103': 87.93103448275862}
2025-12-11 19:07:16,457 [trainer.py] => Ave Acc (W-NCM): 71.52%
2025-12-11 19:07:16,457 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 56.99% (best 97.85%); T2: W-NCM 64.52% (best 90.32%); T3: W-NCM 64.04% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 65.85% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 77.14% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 65.22% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 57.27% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 71.67% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 91.67% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 95.00% (best 95.00%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 78.30% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 64.38% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 73.49% (best 78.31%); T40: W-NCM 69.51% (best 89.02%); T41: W-NCM 83.33% (best 97.22%); T42: W-NCM 68.64% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 81.61% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 34.69% (best 85.71%); T48: W-NCM 88.73% (best 95.77%); T49: W-NCM 67.68% (best 84.85%); T50: W-NCM 81.08% (best 91.89%); T51: W-NCM 85.00% (best 91.67%); T52: W-NCM 87.93% (best 87.93%)
2025-12-11 19:07:16,457 [trainer.py] => Average forgetting (W-NCM): 20.42% | Max forgetting (W-NCM): 51.02%
2025-12-11 19:07:16,470 [trainer.py] => All params: 144526051
2025-12-11 19:07:16,482 [trainer.py] => Trainable params: 185858
2025-12-11 19:07:16,482 [inflora.py] => Learning on 104-106
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_k.52.weight', 'image_encoder.blocks.8.attn.lora_B_k.52.weight', 'image_encoder.blocks.3.attn.lora_B_k.52.weight', 'classifier_pool.52.bias', 'image_encoder.blocks.11.attn.lora_B_v.52.weight', 'image_encoder.blocks.10.attn.lora_B_k.52.weight', 'image_encoder.blocks.6.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_v.52.weight', 'image_encoder.blocks.7.attn.lora_B_v.52.weight', 'image_encoder.blocks.5.attn.lora_B_k.52.weight', 'classifier_pool.52.weight', 'image_encoder.blocks.0.attn.lora_B_v.52.weight', 'image_encoder.blocks.0.attn.lora_B_k.52.weight', 'image_encoder.blocks.4.attn.lora_B_k.52.weight', 'image_encoder.blocks.1.attn.lora_B_v.52.weight', 'image_encoder.blocks.2.attn.lora_B_v.52.weight', 'image_encoder.blocks.8.attn.lora_B_v.52.weight', 'image_encoder.blocks.1.attn.lora_B_k.52.weight', 'image_encoder.blocks.3.attn.lora_B_v.52.weight', 'image_encoder.blocks.11.attn.lora_B_k.52.weight', 'image_encoder.blocks.7.attn.lora_B_k.52.weight', 'image_encoder.blocks.5.attn.lora_B_v.52.weight', 'image_encoder.blocks.9.attn.lora_B_k.52.weight', 'image_encoder.blocks.10.attn.lora_B_v.52.weight', 'image_encoder.blocks.4.attn.lora_B_v.52.weight'}
2025-12-11 19:09:46,649 [inflora.py] => Task 52, Epoch 50/50 => Loss 0.119, Train_accy 95.51
Threshold:  0.9904
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 50/768 type remove
Layer 3 : 89/768 type remove
Layer 4 : 125/768 type remove
Layer 5 : 153/768 type remove
Layer 6 : 143/768 type remove
Layer 7 : 185/768 type remove
Layer 8 : 213/768 type remove
Layer 9 : 283/768 type remove
Layer 10 : 293/768 type remove
Layer 11 : 201/768 type remove
Layer 12 : 269/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:09:54,329 [trainer.py] => Time:157.84652185440063
3331 3331
3331 3331
2025-12-11 19:10:04,236 [trainer.py] => Time:9.907395362854004
2025-12-11 19:10:04,237 [inflora.py] => Exemplar size: 0
2025-12-11 19:10:04,237 [trainer.py] => CNN: {'total': np.float64(42.39), '00-01': np.float64(68.82), '02-03': np.float64(35.48), '04-05': np.float64(59.55), '06-07': np.float64(46.43), '08-09': np.float64(41.1), '10-11': np.float64(17.07), '12-13': np.float64(46.3), '14-15': np.float64(39.29), '16-17': np.float64(18.57), '18-19': np.float64(60.71), '20-21': np.float64(48.35), '22-23': np.float64(64.06), '24-25': np.float64(20.41), '26-27': np.float64(54.78), '28-29': np.float64(54.32), '30-31': np.float64(50.0), '32-33': np.float64(9.09), '34-35': np.float64(45.16), '36-37': np.float64(51.67), '38-39': np.float64(60.98), '40-41': np.float64(37.04), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(43.62), '50-51': np.float64(8.11), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(14.55), '60-61': np.float64(58.54), '62-63': np.float64(48.11), '64-65': np.float64(3.33), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(26.03), '72-73': np.float64(40.0), '74-75': np.float64(73.08), '76-77': np.float64(43.37), '78-79': np.float64(51.22), '80-81': np.float64(58.33), '82-83': np.float64(26.27), '84-85': np.float64(32.61), '86-87': np.float64(41.38), '88-89': np.float64(76.67), '90-91': np.float64(55.0), '92-93': np.float64(12.24), '94-95': np.float64(29.58), '96-97': np.float64(22.22), '98-99': np.float64(40.54), '100-101': np.float64(56.67), '102-103': np.float64(74.14), '104-105': np.float64(11.7), 'old': np.float64(43.28), 'new': np.float64(11.7)}
2025-12-11 19:10:04,237 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39)]
2025-12-11 19:10:04,237 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86)]
2025-12-11 19:10:04,237 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953]
2025-12-11 19:10:16,326 [trainer.py] => W-NCM: {'00-01': 51.61290322580645, '02-03': 56.451612903225815, '04-05': 66.29213483146067, '06-07': 82.14285714285714, '08-09': 68.4931506849315, '10-11': 63.41463414634146, '12-13': 90.74074074074075, '14-15': 69.64285714285714, '16-17': 71.42857142857143, '18-19': 73.21428571428571, '20-21': 75.82417582417582, '22-23': 64.84375, '24-25': 57.14285714285714, '26-27': 61.73913043478261, '28-29': 64.19753086419753, '30-31': 50.0, '32-33': 86.36363636363636, '34-35': 74.19354838709677, '36-37': 76.66666666666667, '38-39': 68.29268292682927, '40-41': 55.55555555555556, '42-43': 81.25, '44-45': 40.74074074074074, '46-47': 81.48148148148148, '48-49': 81.91489361702128, '50-51': 70.27027027027027, '52-53': 88.88888888888889, '54-55': 56.00000000000001, '56-57': 95.0, '58-59': 56.36363636363636, '60-61': 73.17073170731707, '62-63': 76.41509433962264, '64-65': 73.33333333333333, '66-67': 57.89473684210527, '68-69': 75.0, '70-71': 64.38356164383562, '72-73': 67.5, '74-75': 34.61538461538461, '76-77': 74.69879518072288, '78-79': 68.29268292682927, '80-81': 77.77777777777779, '82-83': 69.49152542372882, '84-85': 71.73913043478261, '86-87': 81.60919540229885, '88-89': 85.0, '90-91': 72.5, '92-93': 34.69387755102041, '94-95': 87.32394366197182, '96-97': 58.58585858585859, '98-99': 78.37837837837837, '100-101': 80.0, '102-103': 79.3103448275862, '104-105': 79.7872340425532}
2025-12-11 19:10:16,327 [trainer.py] => Ave Acc (W-NCM): 69.84%
2025-12-11 19:10:16,327 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 51.61% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 66.29% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 63.41% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 61.74% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 50.00% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 95.00% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 73.17% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 64.38% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 34.62% (best 87.18%); T39: W-NCM 74.70% (best 78.31%); T40: W-NCM 68.29% (best 89.02%); T41: W-NCM 77.78% (best 97.22%); T42: W-NCM 69.49% (best 89.83%); T43: W-NCM 71.74% (best 89.13%); T44: W-NCM 81.61% (best 91.95%); T45: W-NCM 85.00% (best 96.67%); T46: W-NCM 72.50% (best 92.50%); T47: W-NCM 34.69% (best 85.71%); T48: W-NCM 87.32% (best 95.77%); T49: W-NCM 58.59% (best 84.85%); T50: W-NCM 78.38% (best 91.89%); T51: W-NCM 80.00% (best 91.67%); T52: W-NCM 79.31% (best 87.93%); T53: W-NCM 79.79% (best 79.79%)
2025-12-11 19:10:16,327 [trainer.py] => Average forgetting (W-NCM): 21.89% | Max forgetting (W-NCM): 52.56%
2025-12-11 19:10:16,340 [trainer.py] => All params: 144526051
2025-12-11 19:10:16,351 [trainer.py] => Trainable params: 185858
2025-12-11 19:10:16,352 [inflora.py] => Learning on 106-108
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.53.weight', 'image_encoder.blocks.5.attn.lora_B_k.53.weight', 'image_encoder.blocks.6.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_k.53.weight', 'image_encoder.blocks.6.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_k.53.weight', 'image_encoder.blocks.8.attn.lora_B_k.53.weight', 'image_encoder.blocks.10.attn.lora_B_v.53.weight', 'image_encoder.blocks.7.attn.lora_B_v.53.weight', 'image_encoder.blocks.3.attn.lora_B_k.53.weight', 'image_encoder.blocks.2.attn.lora_B_v.53.weight', 'image_encoder.blocks.1.attn.lora_B_v.53.weight', 'image_encoder.blocks.8.attn.lora_B_v.53.weight', 'image_encoder.blocks.2.attn.lora_B_k.53.weight', 'image_encoder.blocks.4.attn.lora_B_v.53.weight', 'image_encoder.blocks.9.attn.lora_B_v.53.weight', 'image_encoder.blocks.0.attn.lora_B_v.53.weight', 'image_encoder.blocks.3.attn.lora_B_v.53.weight', 'image_encoder.blocks.10.attn.lora_B_k.53.weight', 'image_encoder.blocks.7.attn.lora_B_k.53.weight', 'classifier_pool.53.bias', 'image_encoder.blocks.0.attn.lora_B_k.53.weight', 'classifier_pool.53.weight', 'image_encoder.blocks.11.attn.lora_B_v.53.weight', 'image_encoder.blocks.11.attn.lora_B_k.53.weight', 'image_encoder.blocks.5.attn.lora_B_v.53.weight'}
2025-12-11 19:11:53,611 [inflora.py] => Task 53, Epoch 50/50 => Loss 0.139, Train_accy 93.04
Threshold:  0.9906
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 51/768 type remove
Layer 3 : 90/768 type remove
Layer 4 : 126/768 type remove
Layer 5 : 154/768 type remove
Layer 6 : 144/768 type remove
Layer 7 : 186/768 type remove
Layer 8 : 214/768 type remove
Layer 9 : 284/768 type remove
Layer 10 : 294/768 type remove
Layer 11 : 203/768 type remove
Layer 12 : 272/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:12:00,382 [trainer.py] => Time:104.03041410446167
3370 3370
3370 3370
2025-12-11 19:12:10,428 [trainer.py] => Time:10.045801162719727
2025-12-11 19:12:10,429 [inflora.py] => Exemplar size: 0
2025-12-11 19:12:10,429 [trainer.py] => CNN: {'total': np.float64(41.96), '00-01': np.float64(69.89), '02-03': np.float64(40.32), '04-05': np.float64(58.43), '06-07': np.float64(46.43), '08-09': np.float64(42.47), '10-11': np.float64(17.07), '12-13': np.float64(42.59), '14-15': np.float64(32.14), '16-17': np.float64(27.14), '18-19': np.float64(58.93), '20-21': np.float64(45.05), '22-23': np.float64(66.41), '24-25': np.float64(18.37), '26-27': np.float64(50.43), '28-29': np.float64(55.56), '30-31': np.float64(57.27), '32-33': np.float64(6.82), '34-35': np.float64(38.71), '36-37': np.float64(53.33), '38-39': np.float64(63.41), '40-41': np.float64(37.04), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(40.74), '48-49': np.float64(40.43), '50-51': np.float64(5.41), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(7.27), '60-61': np.float64(63.41), '62-63': np.float64(43.4), '64-65': np.float64(3.33), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(37.5), '74-75': np.float64(73.08), '76-77': np.float64(42.17), '78-79': np.float64(50.0), '80-81': np.float64(55.56), '82-83': np.float64(27.12), '84-85': np.float64(41.3), '86-87': np.float64(33.33), '88-89': np.float64(80.0), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(38.03), '96-97': np.float64(23.23), '98-99': np.float64(33.78), '100-101': np.float64(56.67), '102-103': np.float64(68.97), '104-105': np.float64(11.7), '106-107': np.float64(46.15), 'old': np.float64(41.91), 'new': np.float64(46.15)}
2025-12-11 19:12:10,429 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96)]
2025-12-11 19:12:10,429 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67)]
2025-12-11 19:12:10,429 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822]
2025-12-11 19:12:21,927 [trainer.py] => W-NCM: {'00-01': 52.68817204301075, '02-03': 54.83870967741935, '04-05': 53.93258426966292, '06-07': 82.14285714285714, '08-09': 64.38356164383562, '10-11': 51.21951219512195, '12-13': 87.03703703703704, '14-15': 67.85714285714286, '16-17': 68.57142857142857, '18-19': 67.85714285714286, '20-21': 71.42857142857143, '22-23': 60.9375, '24-25': 55.10204081632652, '26-27': 53.91304347826087, '28-29': 58.0246913580247, '30-31': 50.90909090909091, '32-33': 86.36363636363636, '34-35': 61.29032258064516, '36-37': 70.0, '38-39': 65.85365853658537, '40-41': 51.85185185185185, '42-43': 75.0, '44-45': 37.03703703703704, '46-47': 74.07407407407408, '48-49': 77.6595744680851, '50-51': 70.27027027027027, '52-53': 88.88888888888889, '54-55': 56.00000000000001, '56-57': 95.0, '58-59': 49.09090909090909, '60-61': 70.73170731707317, '62-63': 77.35849056603774, '64-65': 70.0, '66-67': 52.63157894736842, '68-69': 70.83333333333334, '70-71': 61.64383561643836, '72-73': 72.5, '74-75': 26.923076923076923, '76-77': 72.28915662650603, '78-79': 59.756097560975604, '80-81': 75.0, '82-83': 61.016949152542374, '84-85': 73.91304347826086, '86-87': 74.71264367816092, '88-89': 85.0, '90-91': 65.0, '92-93': 26.53061224489796, '94-95': 83.09859154929578, '96-97': 54.54545454545454, '98-99': 63.51351351351351, '100-101': 73.33333333333333, '102-103': 82.75862068965517, '104-105': 68.08510638297872, '106-107': 89.74358974358975}
2025-12-11 19:12:21,927 [trainer.py] => Ave Acc (W-NCM): 66.11%
2025-12-11 19:12:21,927 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 52.69% (best 97.85%); T2: W-NCM 54.84% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 68.57% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 53.91% (best 94.78%); T15: W-NCM 58.02% (best 96.30%); T16: W-NCM 50.91% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 70.00% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 37.04% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 77.66% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 88.89% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 95.00% (best 95.00%); T30: W-NCM 49.09% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 77.36% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 61.64% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 26.92% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 61.02% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 85.00% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 83.10% (best 95.77%); T49: W-NCM 54.55% (best 84.85%); T50: W-NCM 63.51% (best 91.89%); T51: W-NCM 73.33% (best 91.67%); T52: W-NCM 82.76% (best 87.93%); T53: W-NCM 68.09% (best 79.79%); T54: W-NCM 89.74% (best 89.74%)
2025-12-11 19:12:21,927 [trainer.py] => Average forgetting (W-NCM): 25.65% | Max forgetting (W-NCM): 60.26%
2025-12-11 19:12:21,940 [trainer.py] => All params: 144526051
2025-12-11 19:12:21,952 [trainer.py] => Trainable params: 185858
2025-12-11 19:12:21,952 [inflora.py] => Learning on 108-110
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.54.weight', 'image_encoder.blocks.11.attn.lora_B_k.54.weight', 'classifier_pool.54.weight', 'image_encoder.blocks.9.attn.lora_B_k.54.weight', 'image_encoder.blocks.3.attn.lora_B_k.54.weight', 'image_encoder.blocks.0.attn.lora_B_k.54.weight', 'image_encoder.blocks.10.attn.lora_B_k.54.weight', 'image_encoder.blocks.0.attn.lora_B_v.54.weight', 'image_encoder.blocks.7.attn.lora_B_v.54.weight', 'image_encoder.blocks.8.attn.lora_B_v.54.weight', 'image_encoder.blocks.6.attn.lora_B_k.54.weight', 'image_encoder.blocks.1.attn.lora_B_v.54.weight', 'image_encoder.blocks.10.attn.lora_B_v.54.weight', 'image_encoder.blocks.8.attn.lora_B_k.54.weight', 'image_encoder.blocks.11.attn.lora_B_v.54.weight', 'image_encoder.blocks.2.attn.lora_B_k.54.weight', 'image_encoder.blocks.2.attn.lora_B_v.54.weight', 'image_encoder.blocks.7.attn.lora_B_k.54.weight', 'image_encoder.blocks.5.attn.lora_B_v.54.weight', 'image_encoder.blocks.6.attn.lora_B_v.54.weight', 'image_encoder.blocks.9.attn.lora_B_v.54.weight', 'image_encoder.blocks.4.attn.lora_B_k.54.weight', 'image_encoder.blocks.1.attn.lora_B_k.54.weight', 'image_encoder.blocks.5.attn.lora_B_k.54.weight', 'image_encoder.blocks.3.attn.lora_B_v.54.weight', 'classifier_pool.54.bias'}
2025-12-11 19:14:21,690 [inflora.py] => Task 54, Epoch 50/50 => Loss 0.055, Train_accy 98.79
Threshold:  0.9908
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 51/768 type remove
Layer 3 : 91/768 type remove
Layer 4 : 127/768 type remove
Layer 5 : 155/768 type remove
Layer 6 : 145/768 type remove
Layer 7 : 188/768 type remove
Layer 8 : 216/768 type remove
Layer 9 : 286/768 type remove
Layer 10 : 298/768 type remove
Layer 11 : 208/768 type remove
Layer 12 : 275/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:14:28,909 [trainer.py] => Time:126.95641994476318
3437 3437
3437 3437
2025-12-11 19:14:39,155 [trainer.py] => Time:10.246222972869873
2025-12-11 19:14:39,156 [inflora.py] => Exemplar size: 0
2025-12-11 19:14:39,156 [trainer.py] => CNN: {'total': np.float64(43.38), '00-01': np.float64(76.34), '02-03': np.float64(38.71), '04-05': np.float64(58.43), '06-07': np.float64(44.64), '08-09': np.float64(43.84), '10-11': np.float64(24.39), '12-13': np.float64(51.85), '14-15': np.float64(39.29), '16-17': np.float64(25.71), '18-19': np.float64(57.14), '20-21': np.float64(60.44), '22-23': np.float64(65.62), '24-25': np.float64(22.45), '26-27': np.float64(55.65), '28-29': np.float64(53.09), '30-31': np.float64(46.36), '32-33': np.float64(6.82), '34-35': np.float64(45.16), '36-37': np.float64(58.33), '38-39': np.float64(60.98), '40-41': np.float64(38.89), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(41.49), '50-51': np.float64(10.81), '52-53': np.float64(66.67), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(5.45), '60-61': np.float64(63.41), '62-63': np.float64(45.28), '64-65': np.float64(3.33), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(27.5), '74-75': np.float64(67.95), '76-77': np.float64(40.96), '78-79': np.float64(52.44), '80-81': np.float64(63.89), '82-83': np.float64(29.66), '84-85': np.float64(36.96), '86-87': np.float64(36.78), '88-89': np.float64(76.67), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(36.62), '96-97': np.float64(17.17), '98-99': np.float64(28.38), '100-101': np.float64(60.0), '102-103': np.float64(67.24), '104-105': np.float64(13.83), '106-107': np.float64(48.72), '108-109': np.float64(71.64), 'old': np.float64(42.82), 'new': np.float64(71.64)}
2025-12-11 19:14:39,156 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38)]
2025-12-11 19:14:39,156 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66)]
2025-12-11 19:14:39,156 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011]
2025-12-11 19:14:50,991 [trainer.py] => W-NCM: {'00-01': 33.33333333333333, '02-03': 41.935483870967744, '04-05': 35.95505617977528, '06-07': 73.21428571428571, '08-09': 56.16438356164384, '10-11': 34.146341463414636, '12-13': 72.22222222222221, '14-15': 58.92857142857143, '16-17': 64.28571428571429, '18-19': 41.07142857142857, '20-21': 60.43956043956044, '22-23': 55.46875, '24-25': 42.857142857142854, '26-27': 39.130434782608695, '28-29': 48.148148148148145, '30-31': 25.454545454545453, '32-33': 84.0909090909091, '34-35': 48.38709677419355, '36-37': 63.33333333333333, '38-39': 36.58536585365854, '40-41': 51.85185185185185, '42-43': 59.375, '44-45': 33.33333333333333, '46-47': 70.37037037037037, '48-49': 71.27659574468085, '50-51': 70.27027027027027, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 82.5, '58-59': 23.636363636363637, '60-61': 65.85365853658537, '62-63': 66.0377358490566, '64-65': 63.33333333333333, '66-67': 52.63157894736842, '68-69': 62.5, '70-71': 49.31506849315068, '72-73': 57.49999999999999, '74-75': 19.230769230769234, '76-77': 66.26506024096386, '78-79': 52.4390243902439, '80-81': 65.27777777777779, '82-83': 34.74576271186441, '84-85': 67.3913043478261, '86-87': 56.32183908045977, '88-89': 83.33333333333334, '90-91': 47.5, '92-93': 20.408163265306122, '94-95': 78.87323943661971, '96-97': 48.484848484848484, '98-99': 50.0, '100-101': 61.66666666666667, '102-103': 75.86206896551724, '104-105': 57.446808510638306, '106-107': 87.17948717948718, '108-109': 91.04477611940298}
2025-12-11 19:14:50,992 [trainer.py] => Ave Acc (W-NCM): 56.20%
2025-12-11 19:14:50,992 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 33.33% (best 97.85%); T2: W-NCM 41.94% (best 90.32%); T3: W-NCM 35.96% (best 91.01%); T4: W-NCM 73.21% (best 92.86%); T5: W-NCM 56.16% (best 83.56%); T6: W-NCM 34.15% (best 80.49%); T7: W-NCM 72.22% (best 92.59%); T8: W-NCM 58.93% (best 94.64%); T9: W-NCM 64.29% (best 98.57%); T10: W-NCM 41.07% (best 94.64%); T11: W-NCM 60.44% (best 95.60%); T12: W-NCM 55.47% (best 95.31%); T13: W-NCM 42.86% (best 87.76%); T14: W-NCM 39.13% (best 94.78%); T15: W-NCM 48.15% (best 96.30%); T16: W-NCM 25.45% (best 93.64%); T17: W-NCM 84.09% (best 97.73%); T18: W-NCM 48.39% (best 96.77%); T19: W-NCM 63.33% (best 86.67%); T20: W-NCM 36.59% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 33.33% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 71.28% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 23.64% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 66.04% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 49.32% (best 91.78%); T37: W-NCM 57.50% (best 92.50%); T38: W-NCM 19.23% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 52.44% (best 89.02%); T41: W-NCM 65.28% (best 97.22%); T42: W-NCM 34.75% (best 89.83%); T43: W-NCM 67.39% (best 89.13%); T44: W-NCM 56.32% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 47.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 50.00% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 57.45% (best 79.79%); T54: W-NCM 87.18% (best 89.74%); T55: W-NCM 91.04% (best 91.04%)
2025-12-11 19:14:50,992 [trainer.py] => Average forgetting (W-NCM): 35.74% | Max forgetting (W-NCM): 68.18%
2025-12-11 19:14:51,005 [trainer.py] => All params: 144526051
2025-12-11 19:14:51,017 [trainer.py] => Trainable params: 185858
2025-12-11 19:14:51,017 [inflora.py] => Learning on 110-112
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.55.weight', 'image_encoder.blocks.2.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_v.55.weight', 'image_encoder.blocks.11.attn.lora_B_k.55.weight', 'classifier_pool.55.bias', 'image_encoder.blocks.1.attn.lora_B_k.55.weight', 'image_encoder.blocks.5.attn.lora_B_v.55.weight', 'image_encoder.blocks.7.attn.lora_B_v.55.weight', 'image_encoder.blocks.9.attn.lora_B_k.55.weight', 'image_encoder.blocks.2.attn.lora_B_k.55.weight', 'image_encoder.blocks.9.attn.lora_B_v.55.weight', 'image_encoder.blocks.3.attn.lora_B_v.55.weight', 'image_encoder.blocks.8.attn.lora_B_v.55.weight', 'image_encoder.blocks.3.attn.lora_B_k.55.weight', 'image_encoder.blocks.7.attn.lora_B_k.55.weight', 'image_encoder.blocks.0.attn.lora_B_k.55.weight', 'image_encoder.blocks.5.attn.lora_B_k.55.weight', 'image_encoder.blocks.1.attn.lora_B_v.55.weight', 'image_encoder.blocks.10.attn.lora_B_v.55.weight', 'image_encoder.blocks.11.attn.lora_B_v.55.weight', 'image_encoder.blocks.6.attn.lora_B_v.55.weight', 'image_encoder.blocks.0.attn.lora_B_v.55.weight', 'image_encoder.blocks.4.attn.lora_B_k.55.weight', 'image_encoder.blocks.6.attn.lora_B_k.55.weight', 'classifier_pool.55.weight', 'image_encoder.blocks.8.attn.lora_B_k.55.weight'}
2025-12-11 19:17:06,009 [inflora.py] => Task 55, Epoch 50/50 => Loss 0.055, Train_accy 97.93
Threshold:  0.991
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 52/768 type remove
Layer 3 : 92/768 type remove
Layer 4 : 128/768 type remove
Layer 5 : 156/768 type remove
Layer 6 : 146/768 type remove
Layer 7 : 191/768 type remove
Layer 8 : 220/768 type remove
Layer 9 : 294/768 type remove
Layer 10 : 308/768 type remove
Layer 11 : 216/768 type remove
Layer 12 : 280/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:17:13,170 [trainer.py] => Time:142.15304470062256
3497 3497
3497 3497
2025-12-11 19:17:23,628 [trainer.py] => Time:10.457611799240112
2025-12-11 19:17:23,629 [inflora.py] => Exemplar size: 0
2025-12-11 19:17:23,629 [trainer.py] => CNN: {'total': np.float64(42.26), '00-01': np.float64(72.04), '02-03': np.float64(30.65), '04-05': np.float64(48.31), '06-07': np.float64(48.21), '08-09': np.float64(41.1), '10-11': np.float64(17.07), '12-13': np.float64(48.15), '14-15': np.float64(35.71), '16-17': np.float64(25.71), '18-19': np.float64(60.71), '20-21': np.float64(52.75), '22-23': np.float64(67.19), '24-25': np.float64(28.57), '26-27': np.float64(54.78), '28-29': np.float64(49.38), '30-31': np.float64(42.73), '32-33': np.float64(9.09), '34-35': np.float64(38.71), '36-37': np.float64(66.67), '38-39': np.float64(65.85), '40-41': np.float64(40.74), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(42.55), '50-51': np.float64(8.11), '52-53': np.float64(66.67), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(5.45), '60-61': np.float64(60.98), '62-63': np.float64(40.57), '64-65': np.float64(3.33), '66-67': np.float64(47.37), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(25.0), '74-75': np.float64(62.82), '76-77': np.float64(43.37), '78-79': np.float64(56.1), '80-81': np.float64(65.28), '82-83': np.float64(26.27), '84-85': np.float64(34.78), '86-87': np.float64(31.03), '88-89': np.float64(78.33), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(29.58), '96-97': np.float64(20.2), '98-99': np.float64(35.14), '100-101': np.float64(58.33), '102-103': np.float64(68.97), '104-105': np.float64(10.64), '106-107': np.float64(46.15), '108-109': np.float64(61.19), '110-111': np.float64(66.67), 'old': np.float64(41.84), 'new': np.float64(66.67)}
2025-12-11 19:17:23,629 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26)]
2025-12-11 19:17:23,629 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62)]
2025-12-11 19:17:23,629 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061]
2025-12-11 19:17:35,892 [trainer.py] => W-NCM: {'00-01': 40.86021505376344, '02-03': 50.0, '04-05': 44.9438202247191, '06-07': 78.57142857142857, '08-09': 63.013698630136986, '10-11': 46.34146341463415, '12-13': 79.62962962962963, '14-15': 64.28571428571429, '16-17': 70.0, '18-19': 51.78571428571429, '20-21': 70.32967032967034, '22-23': 59.375, '24-25': 57.14285714285714, '26-27': 53.04347826086957, '28-29': 58.0246913580247, '30-31': 41.81818181818181, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 75.0, '38-39': 60.97560975609756, '40-41': 50.0, '42-43': 84.375, '44-45': 37.03703703703704, '46-47': 74.07407407407408, '48-49': 81.91489361702128, '50-51': 75.67567567567568, '52-53': 86.11111111111111, '54-55': 52.0, '56-57': 92.5, '58-59': 43.63636363636363, '60-61': 70.73170731707317, '62-63': 72.64150943396226, '64-65': 73.33333333333333, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 56.16438356164384, '72-73': 67.5, '74-75': 32.05128205128205, '76-77': 63.85542168674698, '78-79': 56.09756097560976, '80-81': 70.83333333333334, '82-83': 41.52542372881356, '84-85': 67.3913043478261, '86-87': 64.36781609195403, '88-89': 86.66666666666667, '90-91': 52.5, '92-93': 28.57142857142857, '94-95': 77.46478873239437, '96-97': 45.45454545454545, '98-99': 58.108108108108105, '100-101': 63.33333333333333, '102-103': 75.86206896551724, '104-105': 60.63829787234043, '106-107': 84.61538461538461, '108-109': 83.5820895522388, '110-111': 96.66666666666667}
2025-12-11 19:17:35,893 [trainer.py] => Ave Acc (W-NCM): 63.83%
2025-12-11 19:17:35,893 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 40.86% (best 97.85%); T2: W-NCM 50.00% (best 90.32%); T3: W-NCM 44.94% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 46.34% (best 80.49%); T7: W-NCM 79.63% (best 92.59%); T8: W-NCM 64.29% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 51.79% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 59.38% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 53.04% (best 94.78%); T15: W-NCM 58.02% (best 96.30%); T16: W-NCM 41.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 37.04% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 92.50% (best 95.00%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 72.64% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 32.05% (best 87.18%); T39: W-NCM 63.86% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 70.83% (best 97.22%); T42: W-NCM 41.53% (best 89.83%); T43: W-NCM 67.39% (best 89.13%); T44: W-NCM 64.37% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 52.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 45.45% (best 84.85%); T50: W-NCM 58.11% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 60.64% (best 79.79%); T54: W-NCM 84.62% (best 89.74%); T55: W-NCM 83.58% (best 91.04%); T56: W-NCM 96.67% (best 96.67%)
2025-12-11 19:17:35,893 [trainer.py] => Average forgetting (W-NCM): 28.06% | Max forgetting (W-NCM): 57.14%
2025-12-11 19:17:35,906 [trainer.py] => All params: 144526051
2025-12-11 19:17:35,918 [trainer.py] => Trainable params: 185858
2025-12-11 19:17:35,918 [inflora.py] => Learning on 112-114
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.56.weight', 'classifier_pool.56.weight', 'image_encoder.blocks.5.attn.lora_B_k.56.weight', 'image_encoder.blocks.7.attn.lora_B_k.56.weight', 'image_encoder.blocks.10.attn.lora_B_v.56.weight', 'image_encoder.blocks.7.attn.lora_B_v.56.weight', 'image_encoder.blocks.6.attn.lora_B_v.56.weight', 'image_encoder.blocks.5.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_k.56.weight', 'image_encoder.blocks.1.attn.lora_B_v.56.weight', 'image_encoder.blocks.2.attn.lora_B_k.56.weight', 'image_encoder.blocks.10.attn.lora_B_k.56.weight', 'image_encoder.blocks.2.attn.lora_B_v.56.weight', 'image_encoder.blocks.11.attn.lora_B_v.56.weight', 'image_encoder.blocks.4.attn.lora_B_v.56.weight', 'image_encoder.blocks.3.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_v.56.weight', 'classifier_pool.56.bias', 'image_encoder.blocks.3.attn.lora_B_v.56.weight', 'image_encoder.blocks.9.attn.lora_B_k.56.weight', 'image_encoder.blocks.8.attn.lora_B_k.56.weight', 'image_encoder.blocks.0.attn.lora_B_k.56.weight', 'image_encoder.blocks.8.attn.lora_B_v.56.weight', 'image_encoder.blocks.4.attn.lora_B_k.56.weight', 'image_encoder.blocks.1.attn.lora_B_k.56.weight', 'image_encoder.blocks.6.attn.lora_B_k.56.weight'}
2025-12-11 19:19:16,703 [inflora.py] => Task 56, Epoch 50/50 => Loss 0.019, Train_accy 99.44
Threshold:  0.9912
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 93/768 type remove
Layer 4 : 130/768 type remove
Layer 5 : 159/768 type remove
Layer 6 : 150/768 type remove
Layer 7 : 197/768 type remove
Layer 8 : 228/768 type remove
Layer 9 : 307/768 type remove
Layer 10 : 328/768 type remove
Layer 11 : 231/768 type remove
Layer 12 : 294/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:19:23,466 [trainer.py] => Time:107.54800415039062
3549 3549
3549 3549
2025-12-11 19:19:34,052 [trainer.py] => Time:10.585044622421265
2025-12-11 19:19:34,052 [inflora.py] => Exemplar size: 0
2025-12-11 19:19:34,052 [trainer.py] => CNN: {'total': np.float64(42.32), '00-01': np.float64(67.74), '02-03': np.float64(30.65), '04-05': np.float64(53.93), '06-07': np.float64(39.29), '08-09': np.float64(38.36), '10-11': np.float64(14.63), '12-13': np.float64(48.15), '14-15': np.float64(37.5), '16-17': np.float64(30.0), '18-19': np.float64(58.93), '20-21': np.float64(58.24), '22-23': np.float64(64.06), '24-25': np.float64(24.49), '26-27': np.float64(54.78), '28-29': np.float64(45.68), '30-31': np.float64(41.82), '32-33': np.float64(9.09), '34-35': np.float64(38.71), '36-37': np.float64(65.0), '38-39': np.float64(60.98), '40-41': np.float64(42.59), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(38.3), '50-51': np.float64(8.11), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(58.54), '62-63': np.float64(31.13), '64-65': np.float64(6.67), '66-67': np.float64(52.63), '68-69': np.float64(8.33), '70-71': np.float64(19.18), '72-73': np.float64(25.0), '74-75': np.float64(61.54), '76-77': np.float64(43.37), '78-79': np.float64(53.66), '80-81': np.float64(69.44), '82-83': np.float64(31.36), '84-85': np.float64(34.78), '86-87': np.float64(32.18), '88-89': np.float64(75.0), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(32.39), '96-97': np.float64(19.19), '98-99': np.float64(32.43), '100-101': np.float64(61.67), '102-103': np.float64(72.41), '104-105': np.float64(9.57), '106-107': np.float64(43.59), '108-109': np.float64(62.69), '110-111': np.float64(65.0), '112-113': np.float64(82.69), 'old': np.float64(41.72), 'new': np.float64(82.69)}
2025-12-11 19:19:34,052 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32)]
2025-12-11 19:19:34,052 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49)]
2025-12-11 19:19:34,053 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996]
2025-12-11 19:19:46,237 [trainer.py] => W-NCM: {'00-01': 47.31182795698925, '02-03': 58.06451612903226, '04-05': 47.19101123595505, '06-07': 83.92857142857143, '08-09': 69.86301369863014, '10-11': 48.78048780487805, '12-13': 85.18518518518519, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 58.92857142857143, '20-21': 73.62637362637363, '22-23': 63.28125, '24-25': 57.14285714285714, '26-27': 58.26086956521739, '28-29': 61.72839506172839, '30-31': 47.27272727272727, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 63.41463414634146, '40-41': 51.85185185185185, '42-43': 81.25, '44-45': 40.74074074074074, '46-47': 81.48148148148148, '48-49': 86.17021276595744, '50-51': 78.37837837837837, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 92.5, '58-59': 54.54545454545454, '60-61': 70.73170731707317, '62-63': 76.41509433962264, '64-65': 73.33333333333333, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 63.013698630136986, '72-73': 67.5, '74-75': 38.46153846153847, '76-77': 67.46987951807229, '78-79': 58.536585365853654, '80-81': 79.16666666666666, '82-83': 51.69491525423729, '84-85': 71.73913043478261, '86-87': 71.26436781609196, '88-89': 88.33333333333333, '90-91': 62.5, '92-93': 32.6530612244898, '94-95': 80.28169014084507, '96-97': 55.55555555555556, '98-99': 67.56756756756756, '100-101': 65.0, '102-103': 77.58620689655173, '104-105': 62.76595744680851, '106-107': 87.17948717948718, '108-109': 86.56716417910447, '110-111': 95.0, '112-113': 94.23076923076923}
2025-12-11 19:19:46,238 [trainer.py] => Ave Acc (W-NCM): 67.87%
2025-12-11 19:19:46,238 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 47.31% (best 97.85%); T2: W-NCM 58.06% (best 90.32%); T3: W-NCM 47.19% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 48.78% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 58.93% (best 94.64%); T11: W-NCM 73.63% (best 95.60%); T12: W-NCM 63.28% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 47.27% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 86.17% (best 94.68%); T26: W-NCM 78.38% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 92.50% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 51.69% (best 89.83%); T43: W-NCM 71.74% (best 89.13%); T44: W-NCM 71.26% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 32.65% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 55.56% (best 84.85%); T50: W-NCM 67.57% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 77.59% (best 87.93%); T53: W-NCM 62.77% (best 79.79%); T54: W-NCM 87.18% (best 89.74%); T55: W-NCM 86.57% (best 91.04%); T56: W-NCM 95.00% (best 96.67%); T57: W-NCM 94.23% (best 94.23%)
2025-12-11 19:19:46,238 [trainer.py] => Average forgetting (W-NCM): 23.99% | Max forgetting (W-NCM): 53.06%
2025-12-11 19:19:46,250 [trainer.py] => All params: 144526051
2025-12-11 19:19:46,262 [trainer.py] => Trainable params: 185858
2025-12-11 19:19:46,262 [inflora.py] => Learning on 114-116
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.57.weight', 'image_encoder.blocks.11.attn.lora_B_k.57.weight', 'image_encoder.blocks.6.attn.lora_B_v.57.weight', 'image_encoder.blocks.1.attn.lora_B_k.57.weight', 'image_encoder.blocks.1.attn.lora_B_v.57.weight', 'image_encoder.blocks.2.attn.lora_B_v.57.weight', 'image_encoder.blocks.10.attn.lora_B_v.57.weight', 'image_encoder.blocks.8.attn.lora_B_k.57.weight', 'image_encoder.blocks.8.attn.lora_B_v.57.weight', 'image_encoder.blocks.7.attn.lora_B_k.57.weight', 'image_encoder.blocks.4.attn.lora_B_k.57.weight', 'image_encoder.blocks.10.attn.lora_B_k.57.weight', 'image_encoder.blocks.7.attn.lora_B_v.57.weight', 'image_encoder.blocks.4.attn.lora_B_v.57.weight', 'image_encoder.blocks.5.attn.lora_B_v.57.weight', 'image_encoder.blocks.3.attn.lora_B_k.57.weight', 'image_encoder.blocks.3.attn.lora_B_v.57.weight', 'image_encoder.blocks.0.attn.lora_B_k.57.weight', 'image_encoder.blocks.2.attn.lora_B_k.57.weight', 'classifier_pool.57.bias', 'image_encoder.blocks.5.attn.lora_B_k.57.weight', 'image_encoder.blocks.6.attn.lora_B_k.57.weight', 'image_encoder.blocks.9.attn.lora_B_k.57.weight', 'classifier_pool.57.weight', 'image_encoder.blocks.0.attn.lora_B_v.57.weight', 'image_encoder.blocks.11.attn.lora_B_v.57.weight'}
2025-12-11 19:21:23,716 [inflora.py] => Task 57, Epoch 50/50 => Loss 0.014, Train_accy 99.33
Threshold:  0.9914
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
Skip Updating DualGPM for layer: 3
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 93/768 type remove
Layer 4 : 131/768 type remove
Layer 5 : 160/768 type remove
Layer 6 : 151/768 type remove
Layer 7 : 199/768 type remove
Layer 8 : 231/768 type remove
Layer 9 : 310/768 type remove
Layer 10 : 331/768 type remove
Layer 11 : 234/768 type remove
Layer 12 : 299/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:21:30,616 [trainer.py] => Time:104.35394072532654
3589 3589
3589 3589
2025-12-11 19:21:41,337 [trainer.py] => Time:10.720742225646973
2025-12-11 19:21:41,338 [inflora.py] => Exemplar size: 0
2025-12-11 19:21:41,338 [trainer.py] => CNN: {'total': np.float64(42.8), '00-01': np.float64(68.82), '02-03': np.float64(30.65), '04-05': np.float64(58.43), '06-07': np.float64(42.86), '08-09': np.float64(38.36), '10-11': np.float64(14.63), '12-13': np.float64(44.44), '14-15': np.float64(41.07), '16-17': np.float64(21.43), '18-19': np.float64(58.93), '20-21': np.float64(53.85), '22-23': np.float64(63.28), '24-25': np.float64(22.45), '26-27': np.float64(56.52), '28-29': np.float64(48.15), '30-31': np.float64(40.0), '32-33': np.float64(9.09), '34-35': np.float64(41.94), '36-37': np.float64(63.33), '38-39': np.float64(63.41), '40-41': np.float64(42.59), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(42.55), '50-51': np.float64(5.41), '52-53': np.float64(66.67), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(9.09), '60-61': np.float64(53.66), '62-63': np.float64(35.85), '64-65': np.float64(10.0), '66-67': np.float64(42.11), '68-69': np.float64(8.33), '70-71': np.float64(19.18), '72-73': np.float64(32.5), '74-75': np.float64(62.82), '76-77': np.float64(44.58), '78-79': np.float64(52.44), '80-81': np.float64(66.67), '82-83': np.float64(30.51), '84-85': np.float64(41.3), '86-87': np.float64(35.63), '88-89': np.float64(73.33), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(33.8), '96-97': np.float64(23.23), '98-99': np.float64(33.78), '100-101': np.float64(60.0), '102-103': np.float64(74.14), '104-105': np.float64(6.38), '106-107': np.float64(51.28), '108-109': np.float64(64.18), '110-111': np.float64(61.67), '112-113': np.float64(80.77), '114-115': np.float64(55.0), 'old': np.float64(42.66), 'new': np.float64(55.0)}
2025-12-11 19:21:41,338 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8)]
2025-12-11 19:21:41,338 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76)]
2025-12-11 19:21:41,338 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216]
2025-12-11 19:21:53,614 [trainer.py] => W-NCM: {'00-01': 44.086021505376344, '02-03': 62.903225806451616, '04-05': 50.56179775280899, '06-07': 80.35714285714286, '08-09': 67.12328767123287, '10-11': 56.09756097560976, '12-13': 83.33333333333334, '14-15': 71.42857142857143, '16-17': 71.42857142857143, '18-19': 66.07142857142857, '20-21': 72.52747252747253, '22-23': 66.40625, '24-25': 59.183673469387756, '26-27': 58.26086956521739, '28-29': 60.49382716049383, '30-31': 43.63636363636363, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 75.0, '38-39': 65.85365853658537, '40-41': 46.2962962962963, '42-43': 78.125, '44-45': 33.33333333333333, '46-47': 81.48148148148148, '48-49': 82.97872340425532, '50-51': 78.37837837837837, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 92.5, '58-59': 50.90909090909091, '60-61': 68.29268292682927, '62-63': 74.52830188679245, '64-65': 86.66666666666667, '66-67': 63.1578947368421, '68-69': 66.66666666666666, '70-71': 64.38356164383562, '72-73': 62.5, '74-75': 35.8974358974359, '76-77': 67.46987951807229, '78-79': 58.536585365853654, '80-81': 81.94444444444444, '82-83': 55.08474576271186, '84-85': 71.73913043478261, '86-87': 72.41379310344827, '88-89': 88.33333333333333, '90-91': 65.0, '92-93': 26.53061224489796, '94-95': 78.87323943661971, '96-97': 51.515151515151516, '98-99': 64.86486486486487, '100-101': 65.0, '102-103': 74.13793103448276, '104-105': 60.63829787234043, '106-107': 76.92307692307693, '108-109': 80.59701492537313, '110-111': 91.66666666666666, '112-113': 92.3076923076923, '114-115': 92.5}
2025-12-11 19:21:53,614 [trainer.py] => Ave Acc (W-NCM): 67.69%
2025-12-11 19:21:53,615 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 44.09% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 66.07% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 60.49% (best 96.30%); T16: W-NCM 43.64% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 33.33% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 78.38% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 92.50% (best 95.00%); T30: W-NCM 50.91% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 74.53% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 64.38% (best 91.78%); T37: W-NCM 62.50% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 55.08% (best 89.83%); T43: W-NCM 71.74% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 51.52% (best 84.85%); T50: W-NCM 64.86% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 74.14% (best 87.93%); T53: W-NCM 60.64% (best 79.79%); T54: W-NCM 76.92% (best 89.74%); T55: W-NCM 80.60% (best 91.04%); T56: W-NCM 91.67% (best 96.67%); T57: W-NCM 92.31% (best 94.23%); T58: W-NCM 92.50% (best 92.50%)
2025-12-11 19:21:53,615 [trainer.py] => Average forgetting (W-NCM): 24.18% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:21:53,627 [trainer.py] => All params: 144526051
2025-12-11 19:21:53,639 [trainer.py] => Trainable params: 185858
2025-12-11 19:21:53,639 [inflora.py] => Learning on 116-118
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.58.weight', 'image_encoder.blocks.11.attn.lora_B_v.58.weight', 'image_encoder.blocks.6.attn.lora_B_v.58.weight', 'image_encoder.blocks.4.attn.lora_B_k.58.weight', 'image_encoder.blocks.7.attn.lora_B_v.58.weight', 'image_encoder.blocks.10.attn.lora_B_v.58.weight', 'image_encoder.blocks.6.attn.lora_B_k.58.weight', 'image_encoder.blocks.1.attn.lora_B_k.58.weight', 'image_encoder.blocks.5.attn.lora_B_k.58.weight', 'classifier_pool.58.bias', 'image_encoder.blocks.0.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_k.58.weight', 'classifier_pool.58.weight', 'image_encoder.blocks.8.attn.lora_B_v.58.weight', 'image_encoder.blocks.9.attn.lora_B_k.58.weight', 'image_encoder.blocks.11.attn.lora_B_k.58.weight', 'image_encoder.blocks.10.attn.lora_B_k.58.weight', 'image_encoder.blocks.4.attn.lora_B_v.58.weight', 'image_encoder.blocks.7.attn.lora_B_k.58.weight', 'image_encoder.blocks.2.attn.lora_B_k.58.weight', 'image_encoder.blocks.0.attn.lora_B_v.58.weight', 'image_encoder.blocks.5.attn.lora_B_v.58.weight', 'image_encoder.blocks.8.attn.lora_B_k.58.weight', 'image_encoder.blocks.3.attn.lora_B_v.58.weight', 'image_encoder.blocks.9.attn.lora_B_v.58.weight', 'image_encoder.blocks.2.attn.lora_B_v.58.weight'}
2025-12-11 19:23:28,685 [inflora.py] => Task 58, Epoch 50/50 => Loss 0.053, Train_accy 99.31
Threshold:  0.9916
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 94/768 type remove
Layer 4 : 132/768 type remove
Layer 5 : 163/768 type remove
Layer 6 : 155/768 type remove
Layer 7 : 203/768 type remove
Layer 8 : 235/768 type remove
Layer 9 : 317/768 type remove
Layer 10 : 344/768 type remove
Layer 11 : 242/768 type remove
Layer 12 : 321/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:23:35,083 [trainer.py] => Time:101.44423699378967
3625 3625
3625 3625
2025-12-11 19:23:45,860 [trainer.py] => Time:10.777029514312744
2025-12-11 19:23:45,861 [inflora.py] => Exemplar size: 0
2025-12-11 19:23:45,861 [trainer.py] => CNN: {'total': np.float64(42.1), '00-01': np.float64(69.89), '02-03': np.float64(33.87), '04-05': np.float64(51.69), '06-07': np.float64(48.21), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(40.74), '14-15': np.float64(32.14), '16-17': np.float64(27.14), '18-19': np.float64(64.29), '20-21': np.float64(53.85), '22-23': np.float64(62.5), '24-25': np.float64(22.45), '26-27': np.float64(56.52), '28-29': np.float64(45.68), '30-31': np.float64(41.82), '32-33': np.float64(9.09), '34-35': np.float64(38.71), '36-37': np.float64(61.67), '38-39': np.float64(63.41), '40-41': np.float64(40.74), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(41.49), '50-51': np.float64(8.11), '52-53': np.float64(63.89), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(7.27), '60-61': np.float64(48.78), '62-63': np.float64(33.02), '64-65': np.float64(6.67), '66-67': np.float64(42.11), '68-69': np.float64(4.17), '70-71': np.float64(23.29), '72-73': np.float64(27.5), '74-75': np.float64(66.67), '76-77': np.float64(46.99), '78-79': np.float64(46.34), '80-81': np.float64(59.72), '82-83': np.float64(25.42), '84-85': np.float64(41.3), '86-87': np.float64(31.03), '88-89': np.float64(78.33), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(28.17), '96-97': np.float64(22.22), '98-99': np.float64(39.19), '100-101': np.float64(55.0), '102-103': np.float64(77.59), '104-105': np.float64(7.45), '106-107': np.float64(38.46), '108-109': np.float64(61.19), '110-111': np.float64(58.33), '112-113': np.float64(78.85), '114-115': np.float64(55.0), '116-117': np.float64(63.89), 'old': np.float64(41.88), 'new': np.float64(63.89)}
2025-12-11 19:23:45,861 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1)]
2025-12-11 19:23:45,861 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97)]
2025-12-11 19:23:45,861 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483]
2025-12-11 19:23:58,128 [trainer.py] => W-NCM: {'00-01': 52.68817204301075, '02-03': 67.74193548387096, '04-05': 53.93258426966292, '06-07': 83.92857142857143, '08-09': 67.12328767123287, '10-11': 58.536585365853654, '12-13': 87.03703703703704, '14-15': 71.42857142857143, '16-17': 72.85714285714285, '18-19': 82.14285714285714, '20-21': 79.12087912087912, '22-23': 69.53125, '24-25': 63.26530612244898, '26-27': 62.60869565217392, '28-29': 64.19753086419753, '30-31': 57.27272727272727, '32-33': 90.9090909090909, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 48.148148148148145, '42-43': 84.375, '44-45': 37.03703703703704, '46-47': 85.18518518518519, '48-49': 82.97872340425532, '50-51': 81.08108108108108, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 92.5, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 83.33333333333334, '66-67': 63.1578947368421, '68-69': 66.66666666666666, '70-71': 68.4931506849315, '72-73': 70.0, '74-75': 41.02564102564102, '76-77': 68.67469879518072, '78-79': 64.63414634146342, '80-81': 86.11111111111111, '82-83': 57.6271186440678, '84-85': 71.73913043478261, '86-87': 77.01149425287356, '88-89': 90.0, '90-91': 70.0, '92-93': 32.6530612244898, '94-95': 80.28169014084507, '96-97': 54.54545454545454, '98-99': 64.86486486486487, '100-101': 68.33333333333333, '102-103': 75.86206896551724, '104-105': 67.02127659574468, '106-107': 84.61538461538461, '108-109': 80.59701492537313, '110-111': 91.66666666666666, '112-113': 92.3076923076923, '114-115': 87.5, '116-117': 94.44444444444444}
2025-12-11 19:23:58,129 [trainer.py] => Ave Acc (W-NCM): 71.16%
2025-12-11 19:23:58,129 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 52.69% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 82.14% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 62.61% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 57.27% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 48.15% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 37.04% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 81.08% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 92.50% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 83.33% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 68.49% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 41.03% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 64.63% (best 89.02%); T41: W-NCM 86.11% (best 97.22%); T42: W-NCM 57.63% (best 89.83%); T43: W-NCM 71.74% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 70.00% (best 92.50%); T47: W-NCM 32.65% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 54.55% (best 84.85%); T50: W-NCM 64.86% (best 91.89%); T51: W-NCM 68.33% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 67.02% (best 79.79%); T54: W-NCM 84.62% (best 89.74%); T55: W-NCM 80.60% (best 91.04%); T56: W-NCM 91.67% (best 96.67%); T57: W-NCM 92.31% (best 94.23%); T58: W-NCM 87.50% (best 92.50%); T59: W-NCM 94.44% (best 94.44%)
2025-12-11 19:23:58,129 [trainer.py] => Average forgetting (W-NCM): 20.69% | Max forgetting (W-NCM): 53.06%
2025-12-11 19:23:58,142 [trainer.py] => All params: 144526051
2025-12-11 19:23:58,154 [trainer.py] => Trainable params: 185858
2025-12-11 19:23:58,154 [inflora.py] => Learning on 118-120
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.59.weight', 'image_encoder.blocks.4.attn.lora_B_v.59.weight', 'image_encoder.blocks.2.attn.lora_B_v.59.weight', 'image_encoder.blocks.3.attn.lora_B_k.59.weight', 'image_encoder.blocks.8.attn.lora_B_v.59.weight', 'classifier_pool.59.bias', 'image_encoder.blocks.5.attn.lora_B_k.59.weight', 'image_encoder.blocks.7.attn.lora_B_v.59.weight', 'classifier_pool.59.weight', 'image_encoder.blocks.4.attn.lora_B_k.59.weight', 'image_encoder.blocks.10.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_k.59.weight', 'image_encoder.blocks.6.attn.lora_B_v.59.weight', 'image_encoder.blocks.10.attn.lora_B_k.59.weight', 'image_encoder.blocks.0.attn.lora_B_k.59.weight', 'image_encoder.blocks.11.attn.lora_B_k.59.weight', 'image_encoder.blocks.11.attn.lora_B_v.59.weight', 'image_encoder.blocks.9.attn.lora_B_k.59.weight', 'image_encoder.blocks.3.attn.lora_B_v.59.weight', 'image_encoder.blocks.7.attn.lora_B_k.59.weight', 'image_encoder.blocks.2.attn.lora_B_k.59.weight', 'image_encoder.blocks.0.attn.lora_B_v.59.weight', 'image_encoder.blocks.8.attn.lora_B_k.59.weight', 'image_encoder.blocks.6.attn.lora_B_k.59.weight', 'image_encoder.blocks.9.attn.lora_B_v.59.weight', 'image_encoder.blocks.1.attn.lora_B_v.59.weight'}
2025-12-11 19:25:52,516 [inflora.py] => Task 59, Epoch 50/50 => Loss 0.058, Train_accy 98.24
Threshold:  0.9918
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 95/768 type remove
Layer 4 : 134/768 type remove
Layer 5 : 166/768 type remove
Layer 6 : 161/768 type remove
Layer 7 : 209/768 type remove
Layer 8 : 241/768 type remove
Layer 9 : 325/768 type remove
Layer 10 : 357/768 type remove
Layer 11 : 256/768 type remove
Layer 12 : 363/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:25:59,761 [trainer.py] => Time:121.60662245750427
3677 3677
3677 3677
2025-12-11 19:26:10,755 [trainer.py] => Time:10.994516134262085
2025-12-11 19:26:10,756 [inflora.py] => Exemplar size: 0
2025-12-11 19:26:10,756 [trainer.py] => CNN: {'total': np.float64(41.66), '00-01': np.float64(72.04), '02-03': np.float64(35.48), '04-05': np.float64(47.19), '06-07': np.float64(60.71), '08-09': np.float64(36.99), '10-11': np.float64(19.51), '12-13': np.float64(40.74), '14-15': np.float64(35.71), '16-17': np.float64(12.86), '18-19': np.float64(62.5), '20-21': np.float64(46.15), '22-23': np.float64(62.5), '24-25': np.float64(22.45), '26-27': np.float64(60.87), '28-29': np.float64(46.91), '30-31': np.float64(40.91), '32-33': np.float64(6.82), '34-35': np.float64(38.71), '36-37': np.float64(61.67), '38-39': np.float64(63.41), '40-41': np.float64(37.04), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(44.68), '50-51': np.float64(8.11), '52-53': np.float64(61.11), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(3.64), '60-61': np.float64(48.78), '62-63': np.float64(33.96), '64-65': np.float64(6.67), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(23.29), '72-73': np.float64(35.0), '74-75': np.float64(64.1), '76-77': np.float64(48.19), '78-79': np.float64(43.9), '80-81': np.float64(58.33), '82-83': np.float64(27.12), '84-85': np.float64(39.13), '86-87': np.float64(35.63), '88-89': np.float64(78.33), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(21.21), '98-99': np.float64(32.43), '100-101': np.float64(51.67), '102-103': np.float64(72.41), '104-105': np.float64(10.64), '106-107': np.float64(33.33), '108-109': np.float64(59.7), '110-111': np.float64(50.0), '112-113': np.float64(78.85), '114-115': np.float64(57.5), '116-117': np.float64(66.67), '118-119': np.float64(44.23), 'old': np.float64(41.63), 'new': np.float64(44.23)}
2025-12-11 19:26:10,756 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66)]
2025-12-11 19:26:10,756 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92)]
2025-12-11 19:26:10,756 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943]
2025-12-11 19:26:23,303 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 72.58064516129032, '04-05': 59.55056179775281, '06-07': 83.92857142857143, '08-09': 68.4931506849315, '10-11': 60.97560975609756, '12-13': 90.74074074074075, '14-15': 75.0, '16-17': 72.85714285714285, '18-19': 78.57142857142857, '20-21': 83.51648351648352, '22-23': 69.53125, '24-25': 61.224489795918366, '26-27': 64.34782608695652, '28-29': 65.4320987654321, '30-31': 60.0, '32-33': 90.9090909090909, '34-35': 74.19354838709677, '36-37': 78.33333333333333, '38-39': 68.29268292682927, '40-41': 50.0, '42-43': 93.75, '44-45': 40.74074074074074, '46-47': 88.88888888888889, '48-49': 81.91489361702128, '50-51': 81.08108108108108, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 92.5, '58-59': 52.72727272727272, '60-61': 75.60975609756098, '62-63': 76.41509433962264, '64-65': 86.66666666666667, '66-67': 57.89473684210527, '68-69': 70.83333333333334, '70-71': 71.23287671232876, '72-73': 77.5, '74-75': 43.58974358974359, '76-77': 68.67469879518072, '78-79': 65.85365853658537, '80-81': 80.55555555555556, '82-83': 59.32203389830508, '84-85': 73.91304347826086, '86-87': 77.01149425287356, '88-89': 90.0, '90-91': 70.0, '92-93': 30.612244897959183, '94-95': 84.50704225352112, '96-97': 56.56565656565656, '98-99': 67.56756756756756, '100-101': 75.0, '102-103': 82.75862068965517, '104-105': 64.8936170212766, '106-107': 84.61538461538461, '108-109': 83.5820895522388, '110-111': 88.33333333333333, '112-113': 92.3076923076923, '114-115': 82.5, '116-117': 94.44444444444444, '118-119': 100.0}
2025-12-11 19:26:23,304 [trainer.py] => Ave Acc (W-NCM): 73.17%
2025-12-11 19:26:23,304 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 68.49% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 78.57% (best 94.64%); T11: W-NCM 83.52% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 64.35% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 60.00% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 74.19% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 93.75% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 88.89% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 81.08% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 92.50% (best 95.00%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 75.61% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 71.23% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 43.59% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 65.85% (best 89.02%); T41: W-NCM 80.56% (best 97.22%); T42: W-NCM 59.32% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 70.00% (best 92.50%); T47: W-NCM 30.61% (best 85.71%); T48: W-NCM 84.51% (best 95.77%); T49: W-NCM 56.57% (best 84.85%); T50: W-NCM 67.57% (best 91.89%); T51: W-NCM 75.00% (best 91.67%); T52: W-NCM 82.76% (best 87.93%); T53: W-NCM 64.89% (best 79.79%); T54: W-NCM 84.62% (best 89.74%); T55: W-NCM 83.58% (best 91.04%); T56: W-NCM 88.33% (best 96.67%); T57: W-NCM 92.31% (best 94.23%); T58: W-NCM 82.50% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 100.00% (best 100.00%)
2025-12-11 19:26:23,304 [trainer.py] => Average forgetting (W-NCM): 18.79% | Max forgetting (W-NCM): 55.10%
2025-12-11 19:26:23,317 [trainer.py] => All params: 144526051
2025-12-11 19:26:23,329 [trainer.py] => Trainable params: 185858
2025-12-11 19:26:23,329 [inflora.py] => Learning on 120-122
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.60.weight', 'image_encoder.blocks.5.attn.lora_B_v.60.weight', 'image_encoder.blocks.1.attn.lora_B_v.60.weight', 'image_encoder.blocks.2.attn.lora_B_k.60.weight', 'image_encoder.blocks.9.attn.lora_B_v.60.weight', 'image_encoder.blocks.8.attn.lora_B_v.60.weight', 'image_encoder.blocks.6.attn.lora_B_k.60.weight', 'image_encoder.blocks.0.attn.lora_B_k.60.weight', 'image_encoder.blocks.8.attn.lora_B_k.60.weight', 'image_encoder.blocks.10.attn.lora_B_v.60.weight', 'image_encoder.blocks.3.attn.lora_B_k.60.weight', 'classifier_pool.60.bias', 'image_encoder.blocks.1.attn.lora_B_k.60.weight', 'image_encoder.blocks.9.attn.lora_B_k.60.weight', 'image_encoder.blocks.3.attn.lora_B_v.60.weight', 'image_encoder.blocks.7.attn.lora_B_k.60.weight', 'image_encoder.blocks.4.attn.lora_B_k.60.weight', 'image_encoder.blocks.4.attn.lora_B_v.60.weight', 'image_encoder.blocks.11.attn.lora_B_k.60.weight', 'image_encoder.blocks.7.attn.lora_B_v.60.weight', 'classifier_pool.60.weight', 'image_encoder.blocks.11.attn.lora_B_v.60.weight', 'image_encoder.blocks.6.attn.lora_B_v.60.weight', 'image_encoder.blocks.2.attn.lora_B_v.60.weight', 'image_encoder.blocks.5.attn.lora_B_k.60.weight', 'image_encoder.blocks.10.attn.lora_B_k.60.weight'}
2025-12-11 19:28:20,193 [inflora.py] => Task 60, Epoch 50/50 => Loss 0.028, Train_accy 99.16
Threshold:  0.992
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 96/768 type remove
Layer 4 : 137/768 type remove
Layer 5 : 171/768 type remove
Layer 6 : 165/768 type remove
Layer 7 : 215/768 type remove
Layer 8 : 249/768 type remove
Layer 9 : 334/768 type remove
Layer 10 : 367/768 type remove
Layer 11 : 262/768 type remove
Layer 12 : 374/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:28:26,944 [trainer.py] => Time:123.61520671844482
3738 3738
3738 3738
2025-12-11 19:28:38,034 [trainer.py] => Time:11.089699983596802
2025-12-11 19:28:38,035 [inflora.py] => Exemplar size: 0
2025-12-11 19:28:38,035 [trainer.py] => CNN: {'total': np.float64(41.95), '00-01': np.float64(73.12), '02-03': np.float64(35.48), '04-05': np.float64(48.31), '06-07': np.float64(64.29), '08-09': np.float64(38.36), '10-11': np.float64(19.51), '12-13': np.float64(37.04), '14-15': np.float64(50.0), '16-17': np.float64(17.14), '18-19': np.float64(60.71), '20-21': np.float64(48.35), '22-23': np.float64(61.72), '24-25': np.float64(22.45), '26-27': np.float64(60.87), '28-29': np.float64(46.91), '30-31': np.float64(46.36), '32-33': np.float64(9.09), '34-35': np.float64(41.94), '36-37': np.float64(60.0), '38-39': np.float64(65.85), '40-41': np.float64(35.19), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(40.74), '48-49': np.float64(41.49), '50-51': np.float64(8.11), '52-53': np.float64(55.56), '54-55': np.float64(0.0), '56-57': np.float64(45.0), '58-59': np.float64(1.82), '60-61': np.float64(48.78), '62-63': np.float64(36.79), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(52.5), '74-75': np.float64(64.1), '76-77': np.float64(49.4), '78-79': np.float64(37.8), '80-81': np.float64(56.94), '82-83': np.float64(21.19), '84-85': np.float64(39.13), '86-87': np.float64(36.78), '88-89': np.float64(80.0), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(18.31), '96-97': np.float64(21.21), '98-99': np.float64(31.08), '100-101': np.float64(50.0), '102-103': np.float64(72.41), '104-105': np.float64(10.64), '106-107': np.float64(46.15), '108-109': np.float64(58.21), '110-111': np.float64(40.0), '112-113': np.float64(76.92), '114-115': np.float64(55.0), '116-117': np.float64(69.44), '118-119': np.float64(36.54), '120-121': np.float64(59.02), 'old': np.float64(41.66), 'new': np.float64(59.02)}
2025-12-11 19:28:38,035 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95)]
2025-12-11 19:28:38,035 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04)]
2025-12-11 19:28:38,035 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445]
2025-12-11 19:28:50,713 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 75.80645161290323, '04-05': 62.92134831460674, '06-07': 82.14285714285714, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 75.0, '16-17': 71.42857142857143, '18-19': 80.35714285714286, '20-21': 82.41758241758241, '22-23': 68.75, '24-25': 63.26530612244898, '26-27': 61.73913043478261, '28-29': 66.66666666666666, '30-31': 64.54545454545455, '32-33': 90.9090909090909, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 68.29268292682927, '40-41': 48.148148148148145, '42-43': 93.75, '44-45': 44.44444444444444, '46-47': 88.88888888888889, '48-49': 80.85106382978722, '50-51': 81.08108108108108, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 90.0, '58-59': 54.54545454545454, '60-61': 80.48780487804879, '62-63': 76.41509433962264, '64-65': 86.66666666666667, '66-67': 57.89473684210527, '68-69': 66.66666666666666, '70-71': 69.86301369863014, '72-73': 77.5, '74-75': 42.30769230769231, '76-77': 68.67469879518072, '78-79': 67.07317073170732, '80-81': 84.72222222222221, '82-83': 63.559322033898304, '84-85': 73.91304347826086, '86-87': 79.3103448275862, '88-89': 90.0, '90-91': 75.0, '92-93': 30.612244897959183, '94-95': 84.50704225352112, '96-97': 57.57575757575758, '98-99': 70.27027027027027, '100-101': 71.66666666666667, '102-103': 81.03448275862068, '104-105': 65.95744680851064, '106-107': 84.61538461538461, '108-109': 79.1044776119403, '110-111': 86.66666666666667, '112-113': 92.3076923076923, '114-115': 82.5, '116-117': 94.44444444444444, '118-119': 94.23076923076923, '120-121': 95.08196721311475}
2025-12-11 19:28:50,714 [trainer.py] => Ave Acc (W-NCM): 73.60%
2025-12-11 19:28:50,714 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 75.81% (best 90.32%); T3: W-NCM 62.92% (best 91.01%); T4: W-NCM 82.14% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 80.36% (best 94.64%); T11: W-NCM 82.42% (best 95.60%); T12: W-NCM 68.75% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 61.74% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 64.55% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 48.15% (best 94.44%); T22: W-NCM 93.75% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 88.89% (best 92.59%); T25: W-NCM 80.85% (best 94.68%); T26: W-NCM 81.08% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 80.49% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 86.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 69.86% (best 91.78%); T37: W-NCM 77.50% (best 92.50%); T38: W-NCM 42.31% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 67.07% (best 89.02%); T41: W-NCM 84.72% (best 97.22%); T42: W-NCM 63.56% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 79.31% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 75.00% (best 92.50%); T47: W-NCM 30.61% (best 85.71%); T48: W-NCM 84.51% (best 95.77%); T49: W-NCM 57.58% (best 84.85%); T50: W-NCM 70.27% (best 91.89%); T51: W-NCM 71.67% (best 91.67%); T52: W-NCM 81.03% (best 87.93%); T53: W-NCM 65.96% (best 79.79%); T54: W-NCM 84.62% (best 89.74%); T55: W-NCM 79.10% (best 91.04%); T56: W-NCM 86.67% (best 96.67%); T57: W-NCM 92.31% (best 94.23%); T58: W-NCM 82.50% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 94.23% (best 100.00%); T61: W-NCM 95.08% (best 95.08%)
2025-12-11 19:28:50,714 [trainer.py] => Average forgetting (W-NCM): 18.41% | Max forgetting (W-NCM): 55.10%
2025-12-11 19:28:50,726 [trainer.py] => All params: 144526051
2025-12-11 19:28:50,739 [trainer.py] => Trainable params: 185858
2025-12-11 19:28:50,739 [inflora.py] => Learning on 122-124
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.61.weight', 'image_encoder.blocks.8.attn.lora_B_v.61.weight', 'image_encoder.blocks.7.attn.lora_B_k.61.weight', 'image_encoder.blocks.0.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_v.61.weight', 'image_encoder.blocks.6.attn.lora_B_v.61.weight', 'image_encoder.blocks.5.attn.lora_B_v.61.weight', 'image_encoder.blocks.10.attn.lora_B_k.61.weight', 'image_encoder.blocks.2.attn.lora_B_v.61.weight', 'image_encoder.blocks.0.attn.lora_B_v.61.weight', 'image_encoder.blocks.5.attn.lora_B_k.61.weight', 'image_encoder.blocks.3.attn.lora_B_k.61.weight', 'image_encoder.blocks.1.attn.lora_B_v.61.weight', 'classifier_pool.61.weight', 'image_encoder.blocks.2.attn.lora_B_k.61.weight', 'image_encoder.blocks.11.attn.lora_B_v.61.weight', 'image_encoder.blocks.9.attn.lora_B_v.61.weight', 'image_encoder.blocks.10.attn.lora_B_v.61.weight', 'image_encoder.blocks.9.attn.lora_B_k.61.weight', 'classifier_pool.61.bias', 'image_encoder.blocks.1.attn.lora_B_k.61.weight', 'image_encoder.blocks.4.attn.lora_B_k.61.weight', 'image_encoder.blocks.7.attn.lora_B_v.61.weight', 'image_encoder.blocks.6.attn.lora_B_k.61.weight', 'image_encoder.blocks.8.attn.lora_B_k.61.weight', 'image_encoder.blocks.11.attn.lora_B_k.61.weight'}
2025-12-11 19:30:48,729 [inflora.py] => Task 61, Epoch 50/50 => Loss 0.044, Train_accy 98.51
Threshold:  0.9922
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 97/768 type remove
Layer 4 : 139/768 type remove
Layer 5 : 174/768 type remove
Layer 6 : 170/768 type remove
Layer 7 : 220/768 type remove
Layer 8 : 257/768 type remove
Layer 9 : 343/768 type remove
Layer 10 : 378/768 type remove
Layer 11 : 272/768 type remove
Layer 12 : 363/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:30:56,152 [trainer.py] => Time:125.41323661804199
3794 3794
3794 3794
2025-12-11 19:31:07,440 [trainer.py] => Time:11.287397146224976
2025-12-11 19:31:07,440 [inflora.py] => Exemplar size: 0
2025-12-11 19:31:07,440 [trainer.py] => CNN: {'total': np.float64(40.35), '00-01': np.float64(76.34), '02-03': np.float64(38.71), '04-05': np.float64(50.56), '06-07': np.float64(58.93), '08-09': np.float64(34.25), '10-11': np.float64(19.51), '12-13': np.float64(40.74), '14-15': np.float64(42.86), '16-17': np.float64(14.29), '18-19': np.float64(57.14), '20-21': np.float64(45.05), '22-23': np.float64(61.72), '24-25': np.float64(24.49), '26-27': np.float64(63.48), '28-29': np.float64(48.15), '30-31': np.float64(38.18), '32-33': np.float64(6.82), '34-35': np.float64(38.71), '36-37': np.float64(55.0), '38-39': np.float64(63.41), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(45.74), '50-51': np.float64(10.81), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(7.27), '60-61': np.float64(48.78), '62-63': np.float64(35.85), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(20.55), '72-73': np.float64(47.5), '74-75': np.float64(60.26), '76-77': np.float64(42.17), '78-79': np.float64(39.02), '80-81': np.float64(61.11), '82-83': np.float64(19.49), '84-85': np.float64(32.61), '86-87': np.float64(32.18), '88-89': np.float64(75.0), '90-91': np.float64(27.5), '92-93': np.float64(12.24), '94-95': np.float64(18.31), '96-97': np.float64(23.23), '98-99': np.float64(35.14), '100-101': np.float64(41.67), '102-103': np.float64(67.24), '104-105': np.float64(9.57), '106-107': np.float64(35.9), '108-109': np.float64(56.72), '110-111': np.float64(35.0), '112-113': np.float64(75.0), '114-115': np.float64(55.0), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(17.86), 'old': np.float64(40.69), 'new': np.float64(17.86)}
2025-12-11 19:31:07,440 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35)]
2025-12-11 19:31:07,441 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97)]
2025-12-11 19:31:07,441 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797]
2025-12-11 19:31:20,379 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 72.58064516129032, '04-05': 56.17977528089888, '06-07': 78.57142857142857, '08-09': 67.12328767123287, '10-11': 60.97560975609756, '12-13': 90.74074074074075, '14-15': 73.21428571428571, '16-17': 72.85714285714285, '18-19': 75.0, '20-21': 74.72527472527473, '22-23': 65.625, '24-25': 61.224489795918366, '26-27': 56.52173913043478, '28-29': 66.66666666666666, '30-31': 53.63636363636364, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 73.33333333333333, '38-39': 73.17073170731707, '40-41': 50.0, '42-43': 87.5, '44-45': 44.44444444444444, '46-47': 85.18518518518519, '48-49': 79.7872340425532, '50-51': 78.37837837837837, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 87.5, '58-59': 45.45454545454545, '60-61': 70.73170731707317, '62-63': 78.30188679245283, '64-65': 80.0, '66-67': 63.1578947368421, '68-69': 62.5, '70-71': 63.013698630136986, '72-73': 75.0, '74-75': 32.05128205128205, '76-77': 66.26506024096386, '78-79': 59.756097560975604, '80-81': 75.0, '82-83': 60.16949152542372, '84-85': 73.91304347826086, '86-87': 80.45977011494253, '88-89': 88.33333333333333, '90-91': 70.0, '92-93': 26.53061224489796, '94-95': 83.09859154929578, '96-97': 53.535353535353536, '98-99': 67.56756756756756, '100-101': 68.33333333333333, '102-103': 74.13793103448276, '104-105': 62.76595744680851, '106-107': 82.05128205128204, '108-109': 76.11940298507463, '110-111': 75.0, '112-113': 90.38461538461539, '114-115': 77.5, '116-117': 94.44444444444444, '118-119': 90.38461538461539, '120-121': 86.88524590163934, '122-123': 94.64285714285714}
2025-12-11 19:31:20,380 [trainer.py] => Ave Acc (W-NCM): 70.72%
2025-12-11 19:31:20,380 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 90.74% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 74.73% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 56.52% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 53.64% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 87.50% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 79.79% (best 94.68%); T26: W-NCM 78.38% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 45.45% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 78.30% (best 90.57%); T33: W-NCM 80.00% (best 93.33%); T34: W-NCM 63.16% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 75.00% (best 92.50%); T38: W-NCM 32.05% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 60.17% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 80.46% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 70.00% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 83.10% (best 95.77%); T49: W-NCM 53.54% (best 84.85%); T50: W-NCM 67.57% (best 91.89%); T51: W-NCM 68.33% (best 91.67%); T52: W-NCM 74.14% (best 87.93%); T53: W-NCM 62.77% (best 79.79%); T54: W-NCM 82.05% (best 89.74%); T55: W-NCM 76.12% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 90.38% (best 94.23%); T58: W-NCM 77.50% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 90.38% (best 100.00%); T61: W-NCM 86.89% (best 95.08%); T62: W-NCM 94.64% (best 94.64%)
2025-12-11 19:31:20,380 [trainer.py] => Average forgetting (W-NCM): 21.38% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:31:20,393 [trainer.py] => All params: 144526051
2025-12-11 19:31:20,405 [trainer.py] => Trainable params: 185858
2025-12-11 19:31:20,405 [inflora.py] => Learning on 124-126
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.62.weight', 'image_encoder.blocks.9.attn.lora_B_v.62.weight', 'image_encoder.blocks.3.attn.lora_B_v.62.weight', 'image_encoder.blocks.6.attn.lora_B_v.62.weight', 'image_encoder.blocks.1.attn.lora_B_k.62.weight', 'image_encoder.blocks.2.attn.lora_B_v.62.weight', 'classifier_pool.62.bias', 'image_encoder.blocks.3.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_v.62.weight', 'classifier_pool.62.weight', 'image_encoder.blocks.0.attn.lora_B_v.62.weight', 'image_encoder.blocks.5.attn.lora_B_k.62.weight', 'image_encoder.blocks.11.attn.lora_B_k.62.weight', 'image_encoder.blocks.7.attn.lora_B_v.62.weight', 'image_encoder.blocks.9.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_k.62.weight', 'image_encoder.blocks.0.attn.lora_B_k.62.weight', 'image_encoder.blocks.8.attn.lora_B_v.62.weight', 'image_encoder.blocks.6.attn.lora_B_k.62.weight', 'image_encoder.blocks.7.attn.lora_B_k.62.weight', 'image_encoder.blocks.4.attn.lora_B_v.62.weight', 'image_encoder.blocks.10.attn.lora_B_k.62.weight', 'image_encoder.blocks.10.attn.lora_B_v.62.weight', 'image_encoder.blocks.2.attn.lora_B_k.62.weight', 'image_encoder.blocks.1.attn.lora_B_v.62.weight', 'image_encoder.blocks.8.attn.lora_B_k.62.weight'}
2025-12-11 19:33:27,907 [inflora.py] => Task 62, Epoch 50/50 => Loss 0.088, Train_accy 96.62
Threshold:  0.9924
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 53/768 type remove
Layer 3 : 98/768 type remove
Layer 4 : 140/768 type remove
Layer 5 : 176/768 type remove
Layer 6 : 172/768 type remove
Layer 7 : 222/768 type remove
Layer 8 : 259/768 type remove
Layer 9 : 345/768 type remove
Layer 10 : 380/768 type remove
Layer 11 : 275/768 type remove
Layer 12 : 349/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:33:34,816 [trainer.py] => Time:134.4112024307251
3843 3843
3843 3843
2025-12-11 19:33:46,212 [trainer.py] => Time:11.39535140991211
2025-12-11 19:33:46,212 [inflora.py] => Exemplar size: 0
2025-12-11 19:33:46,212 [trainer.py] => CNN: {'total': np.float64(39.89), '00-01': np.float64(76.34), '02-03': np.float64(29.03), '04-05': np.float64(52.81), '06-07': np.float64(55.36), '08-09': np.float64(35.62), '10-11': np.float64(17.07), '12-13': np.float64(42.59), '14-15': np.float64(39.29), '16-17': np.float64(12.86), '18-19': np.float64(46.43), '20-21': np.float64(58.24), '22-23': np.float64(58.59), '24-25': np.float64(20.41), '26-27': np.float64(65.22), '28-29': np.float64(53.09), '30-31': np.float64(28.18), '32-33': np.float64(6.82), '34-35': np.float64(41.94), '36-37': np.float64(55.0), '38-39': np.float64(58.54), '40-41': np.float64(31.48), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(45.74), '50-51': np.float64(8.11), '52-53': np.float64(50.0), '54-55': np.float64(0.0), '56-57': np.float64(42.5), '58-59': np.float64(9.09), '60-61': np.float64(46.34), '62-63': np.float64(36.79), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(40.0), '74-75': np.float64(52.56), '76-77': np.float64(43.37), '78-79': np.float64(45.12), '80-81': np.float64(59.72), '82-83': np.float64(22.03), '84-85': np.float64(26.09), '86-87': np.float64(36.78), '88-89': np.float64(63.33), '90-91': np.float64(25.0), '92-93': np.float64(16.33), '94-95': np.float64(18.31), '96-97': np.float64(24.24), '98-99': np.float64(33.78), '100-101': np.float64(36.67), '102-103': np.float64(65.52), '104-105': np.float64(10.64), '106-107': np.float64(30.77), '108-109': np.float64(59.7), '110-111': np.float64(40.0), '112-113': np.float64(75.0), '114-115': np.float64(55.0), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(14.29), '124-125': np.float64(42.86), 'old': np.float64(39.85), 'new': np.float64(42.86)}
2025-12-11 19:33:46,213 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89)]
2025-12-11 19:33:46,213 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86)]
2025-12-11 19:33:46,213 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716]
2025-12-11 19:33:59,472 [trainer.py] => W-NCM: {'00-01': 55.91397849462365, '02-03': 62.903225806451616, '04-05': 49.43820224719101, '06-07': 78.57142857142857, '08-09': 61.64383561643836, '10-11': 51.21951219512195, '12-13': 79.62962962962963, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 69.64285714285714, '20-21': 70.32967032967034, '22-23': 59.375, '24-25': 57.14285714285714, '26-27': 48.69565217391305, '28-29': 60.49382716049383, '30-31': 35.45454545454545, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 65.85365853658537, '40-41': 44.44444444444444, '42-43': 75.0, '44-45': 40.74074074074074, '46-47': 81.48148148148148, '48-49': 76.59574468085107, '50-51': 83.78378378378379, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 82.5, '58-59': 32.72727272727273, '60-61': 68.29268292682927, '62-63': 69.81132075471697, '64-65': 70.0, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 56.16438356164384, '72-73': 67.5, '74-75': 24.358974358974358, '76-77': 61.44578313253012, '78-79': 56.09756097560976, '80-81': 66.66666666666666, '82-83': 39.83050847457627, '84-85': 73.91304347826086, '86-87': 65.51724137931035, '88-89': 88.33333333333333, '90-91': 60.0, '92-93': 24.489795918367346, '94-95': 81.69014084507043, '96-97': 49.494949494949495, '98-99': 54.054054054054056, '100-101': 60.0, '102-103': 68.96551724137932, '104-105': 50.0, '106-107': 66.66666666666666, '108-109': 67.16417910447761, '110-111': 68.33333333333333, '112-113': 90.38461538461539, '114-115': 72.5, '116-117': 91.66666666666666, '118-119': 78.84615384615384, '120-121': 85.24590163934425, '122-123': 76.78571428571429, '124-125': 93.87755102040816}
2025-12-11 19:33:59,473 [trainer.py] => Ave Acc (W-NCM): 64.91%
2025-12-11 19:33:59,473 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 55.91% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 79.63% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 59.38% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 48.70% (best 94.78%); T15: W-NCM 60.49% (best 96.30%); T16: W-NCM 35.45% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 44.44% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 83.78% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 32.73% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 69.81% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 24.36% (best 87.18%); T39: W-NCM 61.45% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 66.67% (best 97.22%); T42: W-NCM 39.83% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 65.52% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 60.00% (best 92.50%); T47: W-NCM 24.49% (best 85.71%); T48: W-NCM 81.69% (best 95.77%); T49: W-NCM 49.49% (best 84.85%); T50: W-NCM 54.05% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 68.97% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 67.16% (best 91.04%); T56: W-NCM 68.33% (best 96.67%); T57: W-NCM 90.38% (best 94.23%); T58: W-NCM 72.50% (best 92.50%); T59: W-NCM 91.67% (best 94.44%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 85.25% (best 95.08%); T62: W-NCM 76.79% (best 94.64%); T63: W-NCM 93.88% (best 93.88%)
2025-12-11 19:33:59,473 [trainer.py] => Average forgetting (W-NCM): 27.31% | Max forgetting (W-NCM): 62.82%
2025-12-11 19:33:59,485 [trainer.py] => All params: 144526051
2025-12-11 19:33:59,498 [trainer.py] => Trainable params: 185858
2025-12-11 19:33:59,498 [inflora.py] => Learning on 126-128
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.63.weight', 'image_encoder.blocks.9.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_k.63.weight', 'image_encoder.blocks.11.attn.lora_B_v.63.weight', 'image_encoder.blocks.6.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_v.63.weight', 'image_encoder.blocks.2.attn.lora_B_k.63.weight', 'image_encoder.blocks.1.attn.lora_B_k.63.weight', 'classifier_pool.63.bias', 'image_encoder.blocks.4.attn.lora_B_v.63.weight', 'image_encoder.blocks.8.attn.lora_B_v.63.weight', 'image_encoder.blocks.3.attn.lora_B_v.63.weight', 'classifier_pool.63.weight', 'image_encoder.blocks.0.attn.lora_B_v.63.weight', 'image_encoder.blocks.10.attn.lora_B_k.63.weight', 'image_encoder.blocks.10.attn.lora_B_v.63.weight', 'image_encoder.blocks.0.attn.lora_B_k.63.weight', 'image_encoder.blocks.9.attn.lora_B_v.63.weight', 'image_encoder.blocks.1.attn.lora_B_v.63.weight', 'image_encoder.blocks.4.attn.lora_B_k.63.weight', 'image_encoder.blocks.2.attn.lora_B_v.63.weight', 'image_encoder.blocks.6.attn.lora_B_v.63.weight', 'image_encoder.blocks.3.attn.lora_B_k.63.weight', 'image_encoder.blocks.7.attn.lora_B_k.63.weight', 'image_encoder.blocks.8.attn.lora_B_k.63.weight', 'image_encoder.blocks.5.attn.lora_B_v.63.weight'}
2025-12-11 19:35:50,395 [inflora.py] => Task 63, Epoch 50/50 => Loss 0.109, Train_accy 96.76
Threshold:  0.9926
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 99/768 type remove
Layer 4 : 142/768 type remove
Layer 5 : 180/768 type remove
Layer 6 : 178/768 type remove
Layer 7 : 229/768 type remove
Layer 8 : 265/768 type remove
Layer 9 : 352/768 type remove
Layer 10 : 379/768 type retain
Layer 11 : 285/768 type remove
Layer 12 : 329/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:35:57,113 [trainer.py] => Time:117.61474370956421
3896 3896
3896 3896
2025-12-11 19:36:08,703 [trainer.py] => Time:11.590511560440063
2025-12-11 19:36:08,704 [inflora.py] => Exemplar size: 0
2025-12-11 19:36:08,704 [trainer.py] => CNN: {'total': np.float64(39.01), '00-01': np.float64(76.34), '02-03': np.float64(37.1), '04-05': np.float64(55.06), '06-07': np.float64(55.36), '08-09': np.float64(35.62), '10-11': np.float64(17.07), '12-13': np.float64(44.44), '14-15': np.float64(39.29), '16-17': np.float64(14.29), '18-19': np.float64(48.21), '20-21': np.float64(54.95), '22-23': np.float64(60.94), '24-25': np.float64(14.29), '26-27': np.float64(66.09), '28-29': np.float64(59.26), '30-31': np.float64(32.73), '32-33': np.float64(2.27), '34-35': np.float64(45.16), '36-37': np.float64(51.67), '38-39': np.float64(51.22), '40-41': np.float64(29.63), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(52.13), '50-51': np.float64(5.41), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(25.0), '58-59': np.float64(9.09), '60-61': np.float64(46.34), '62-63': np.float64(35.85), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(37.5), '74-75': np.float64(52.56), '76-77': np.float64(40.96), '78-79': np.float64(40.24), '80-81': np.float64(56.94), '82-83': np.float64(20.34), '84-85': np.float64(26.09), '86-87': np.float64(39.08), '88-89': np.float64(60.0), '90-91': np.float64(27.5), '92-93': np.float64(14.29), '94-95': np.float64(18.31), '96-97': np.float64(21.21), '98-99': np.float64(35.14), '100-101': np.float64(35.0), '102-103': np.float64(62.07), '104-105': np.float64(9.57), '106-107': np.float64(35.9), '108-109': np.float64(55.22), '110-111': np.float64(28.33), '112-113': np.float64(75.0), '114-115': np.float64(47.5), '116-117': np.float64(75.0), '118-119': np.float64(40.38), '120-121': np.float64(60.66), '122-123': np.float64(14.29), '124-125': np.float64(40.82), '126-127': np.float64(7.55), 'old': np.float64(39.45), 'new': np.float64(7.55)}
2025-12-11 19:36:08,704 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01)]
2025-12-11 19:36:08,704 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79)]
2025-12-11 19:36:08,705 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117]
2025-12-11 19:36:21,962 [trainer.py] => W-NCM: {'00-01': 60.215053763440864, '02-03': 70.96774193548387, '04-05': 51.68539325842697, '06-07': 83.92857142857143, '08-09': 67.12328767123287, '10-11': 53.65853658536586, '12-13': 85.18518518518519, '14-15': 69.64285714285714, '16-17': 71.42857142857143, '18-19': 71.42857142857143, '20-21': 73.62637362637363, '22-23': 64.84375, '24-25': 63.26530612244898, '26-27': 57.391304347826086, '28-29': 67.90123456790124, '30-31': 47.27272727272727, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 73.17073170731707, '40-41': 46.2962962962963, '42-43': 81.25, '44-45': 48.148148148148145, '46-47': 85.18518518518519, '48-49': 77.6595744680851, '50-51': 75.67567567567568, '52-53': 80.55555555555556, '54-55': 48.0, '56-57': 90.0, '58-59': 47.27272727272727, '60-61': 70.73170731707317, '62-63': 72.64150943396226, '64-65': 76.66666666666667, '66-67': 57.89473684210527, '68-69': 75.0, '70-71': 60.273972602739725, '72-73': 70.0, '74-75': 35.8974358974359, '76-77': 69.87951807228916, '78-79': 57.3170731707317, '80-81': 75.0, '82-83': 49.152542372881356, '84-85': 73.91304347826086, '86-87': 72.41379310344827, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 78.87323943661971, '96-97': 52.52525252525253, '98-99': 56.75675675675676, '100-101': 58.333333333333336, '102-103': 74.13793103448276, '104-105': 53.191489361702125, '106-107': 66.66666666666666, '108-109': 71.64179104477611, '110-111': 68.33333333333333, '112-113': 90.38461538461539, '114-115': 67.5, '116-117': 91.66666666666666, '118-119': 84.61538461538461, '120-121': 83.60655737704919, '122-123': 76.78571428571429, '124-125': 79.59183673469387, '126-127': 84.90566037735849}
2025-12-11 19:36:21,962 [trainer.py] => Ave Acc (W-NCM): 68.46%
2025-12-11 19:36:21,962 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 60.22% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 51.69% (best 91.01%); T4: W-NCM 83.93% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 73.63% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 57.39% (best 94.78%); T15: W-NCM 67.90% (best 96.30%); T16: W-NCM 47.27% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 77.66% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 47.27% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 72.64% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 69.88% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 49.15% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 52.53% (best 84.85%); T50: W-NCM 56.76% (best 91.89%); T51: W-NCM 58.33% (best 91.67%); T52: W-NCM 74.14% (best 87.93%); T53: W-NCM 53.19% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 71.64% (best 91.04%); T56: W-NCM 68.33% (best 96.67%); T57: W-NCM 90.38% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 91.67% (best 94.44%); T60: W-NCM 84.62% (best 100.00%); T61: W-NCM 83.61% (best 95.08%); T62: W-NCM 76.79% (best 94.64%); T63: W-NCM 79.59% (best 93.88%); T64: W-NCM 84.91% (best 84.91%)
2025-12-11 19:36:21,963 [trainer.py] => Average forgetting (W-NCM): 23.59% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:36:21,975 [trainer.py] => All params: 144526051
2025-12-11 19:36:21,987 [trainer.py] => Trainable params: 185858
2025-12-11 19:36:21,987 [inflora.py] => Learning on 128-130
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.64.weight', 'image_encoder.blocks.1.attn.lora_B_v.64.weight', 'image_encoder.blocks.11.attn.lora_B_v.64.weight', 'image_encoder.blocks.5.attn.lora_B_k.64.weight', 'image_encoder.blocks.0.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_v.64.weight', 'classifier_pool.64.weight', 'image_encoder.blocks.10.attn.lora_B_k.64.weight', 'image_encoder.blocks.4.attn.lora_B_k.64.weight', 'image_encoder.blocks.0.attn.lora_B_k.64.weight', 'image_encoder.blocks.5.attn.lora_B_v.64.weight', 'image_encoder.blocks.1.attn.lora_B_k.64.weight', 'classifier_pool.64.bias', 'image_encoder.blocks.2.attn.lora_B_k.64.weight', 'image_encoder.blocks.11.attn.lora_B_k.64.weight', 'image_encoder.blocks.2.attn.lora_B_v.64.weight', 'image_encoder.blocks.8.attn.lora_B_k.64.weight', 'image_encoder.blocks.7.attn.lora_B_v.64.weight', 'image_encoder.blocks.9.attn.lora_B_k.64.weight', 'image_encoder.blocks.4.attn.lora_B_v.64.weight', 'image_encoder.blocks.3.attn.lora_B_v.64.weight', 'image_encoder.blocks.7.attn.lora_B_k.64.weight', 'image_encoder.blocks.6.attn.lora_B_k.64.weight', 'image_encoder.blocks.6.attn.lora_B_v.64.weight', 'image_encoder.blocks.10.attn.lora_B_v.64.weight', 'image_encoder.blocks.9.attn.lora_B_v.64.weight'}
2025-12-11 19:38:26,212 [inflora.py] => Task 64, Epoch 50/50 => Loss 0.032, Train_accy 97.70
Threshold:  0.9928
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 100/768 type remove
Layer 4 : 145/768 type remove
Layer 5 : 183/768 type remove
Layer 6 : 182/768 type remove
Layer 7 : 232/768 type remove
Layer 8 : 268/768 type remove
Layer 9 : 355/768 type remove
Layer 10 : 377/768 type retain
Layer 11 : 287/768 type remove
Layer 12 : 316/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:38:33,207 [trainer.py] => Time:131.21956253051758
3975 3975
3975 3975
2025-12-11 19:38:44,986 [trainer.py] => Time:11.778620958328247
2025-12-11 19:38:44,986 [inflora.py] => Exemplar size: 0
2025-12-11 19:38:44,986 [trainer.py] => CNN: {'total': np.float64(38.39), '00-01': np.float64(75.27), '02-03': np.float64(41.94), '04-05': np.float64(53.93), '06-07': np.float64(62.5), '08-09': np.float64(34.25), '10-11': np.float64(12.2), '12-13': np.float64(42.59), '14-15': np.float64(32.14), '16-17': np.float64(12.86), '18-19': np.float64(42.86), '20-21': np.float64(51.65), '22-23': np.float64(57.03), '24-25': np.float64(12.24), '26-27': np.float64(65.22), '28-29': np.float64(55.56), '30-31': np.float64(29.09), '32-33': np.float64(2.27), '34-35': np.float64(41.94), '36-37': np.float64(53.33), '38-39': np.float64(60.98), '40-41': np.float64(31.48), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(62.96), '48-49': np.float64(54.26), '50-51': np.float64(16.22), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(22.5), '58-59': np.float64(10.91), '60-61': np.float64(46.34), '62-63': np.float64(33.96), '64-65': np.float64(0.0), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(37.5), '74-75': np.float64(53.85), '76-77': np.float64(43.37), '78-79': np.float64(43.9), '80-81': np.float64(59.72), '82-83': np.float64(20.34), '84-85': np.float64(39.13), '86-87': np.float64(35.63), '88-89': np.float64(63.33), '90-91': np.float64(27.5), '92-93': np.float64(16.33), '94-95': np.float64(16.9), '96-97': np.float64(21.21), '98-99': np.float64(32.43), '100-101': np.float64(38.33), '102-103': np.float64(58.62), '104-105': np.float64(11.7), '106-107': np.float64(28.21), '108-109': np.float64(46.27), '110-111': np.float64(26.67), '112-113': np.float64(75.0), '114-115': np.float64(47.5), '116-117': np.float64(72.22), '118-119': np.float64(40.38), '120-121': np.float64(54.1), '122-123': np.float64(14.29), '124-125': np.float64(42.86), '126-127': np.float64(9.43), '128-129': np.float64(29.11), 'old': np.float64(38.58), 'new': np.float64(29.11)}
2025-12-11 19:38:44,986 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39)]
2025-12-11 19:38:44,986 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6)]
2025-12-11 19:38:44,987 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145]
2025-12-11 19:38:58,605 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 72.58064516129032, '04-05': 49.43820224719101, '06-07': 78.57142857142857, '08-09': 61.64383561643836, '10-11': 53.65853658536586, '12-13': 81.48148148148148, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 67.85714285714286, '20-21': 70.32967032967034, '22-23': 64.84375, '24-25': 59.183673469387756, '26-27': 47.82608695652174, '28-29': 59.25925925925925, '30-31': 37.27272727272727, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 75.0, '38-39': 68.29268292682927, '40-41': 46.2962962962963, '42-43': 81.25, '44-45': 48.148148148148145, '46-47': 85.18518518518519, '48-49': 76.59574468085107, '50-51': 78.37837837837837, '52-53': 77.77777777777779, '54-55': 44.0, '56-57': 85.0, '58-59': 43.63636363636363, '60-61': 68.29268292682927, '62-63': 69.81132075471697, '64-65': 70.0, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 58.9041095890411, '72-73': 67.5, '74-75': 28.205128205128204, '76-77': 65.06024096385542, '78-79': 54.87804878048781, '80-81': 75.0, '82-83': 46.61016949152542, '84-85': 76.08695652173914, '86-87': 71.26436781609196, '88-89': 88.33333333333333, '90-91': 65.0, '92-93': 20.408163265306122, '94-95': 78.87323943661971, '96-97': 49.494949494949495, '98-99': 55.4054054054054, '100-101': 61.66666666666667, '102-103': 72.41379310344827, '104-105': 53.191489361702125, '106-107': 61.53846153846154, '108-109': 70.1492537313433, '110-111': 68.33333333333333, '112-113': 88.46153846153845, '114-115': 67.5, '116-117': 91.66666666666666, '118-119': 75.0, '120-121': 78.68852459016394, '122-123': 71.42857142857143, '124-125': 65.3061224489796, '126-127': 64.15094339622641, '128-129': 92.40506329113924}
2025-12-11 19:38:58,605 [trainer.py] => Ave Acc (W-NCM): 65.93%
2025-12-11 19:38:58,606 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 81.48% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 47.83% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 37.27% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 81.25% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 78.38% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 69.81% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 58.90% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 28.21% (best 87.18%); T39: W-NCM 65.06% (best 78.31%); T40: W-NCM 54.88% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 46.61% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 71.26% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 49.49% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 53.19% (best 79.79%); T54: W-NCM 61.54% (best 89.74%); T55: W-NCM 70.15% (best 91.04%); T56: W-NCM 68.33% (best 96.67%); T57: W-NCM 88.46% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 91.67% (best 94.44%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 71.43% (best 94.64%); T63: W-NCM 65.31% (best 93.88%); T64: W-NCM 64.15% (best 84.91%); T65: W-NCM 92.41% (best 92.41%)
2025-12-11 19:38:58,606 [trainer.py] => Average forgetting (W-NCM): 26.16% | Max forgetting (W-NCM): 65.31%
2025-12-11 19:38:58,618 [trainer.py] => All params: 144526051
2025-12-11 19:38:58,630 [trainer.py] => Trainable params: 185858
2025-12-11 19:38:58,630 [inflora.py] => Learning on 130-132
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.65.weight', 'image_encoder.blocks.3.attn.lora_B_k.65.weight', 'image_encoder.blocks.4.attn.lora_B_k.65.weight', 'image_encoder.blocks.2.attn.lora_B_v.65.weight', 'image_encoder.blocks.0.attn.lora_B_k.65.weight', 'image_encoder.blocks.9.attn.lora_B_k.65.weight', 'image_encoder.blocks.11.attn.lora_B_k.65.weight', 'image_encoder.blocks.2.attn.lora_B_k.65.weight', 'image_encoder.blocks.8.attn.lora_B_k.65.weight', 'image_encoder.blocks.8.attn.lora_B_v.65.weight', 'classifier_pool.65.bias', 'image_encoder.blocks.11.attn.lora_B_v.65.weight', 'image_encoder.blocks.6.attn.lora_B_k.65.weight', 'image_encoder.blocks.10.attn.lora_B_v.65.weight', 'image_encoder.blocks.3.attn.lora_B_v.65.weight', 'image_encoder.blocks.0.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_v.65.weight', 'image_encoder.blocks.10.attn.lora_B_k.65.weight', 'image_encoder.blocks.1.attn.lora_B_v.65.weight', 'image_encoder.blocks.9.attn.lora_B_v.65.weight', 'image_encoder.blocks.1.attn.lora_B_k.65.weight', 'image_encoder.blocks.7.attn.lora_B_k.65.weight', 'classifier_pool.65.weight', 'image_encoder.blocks.7.attn.lora_B_v.65.weight', 'image_encoder.blocks.5.attn.lora_B_k.65.weight', 'image_encoder.blocks.4.attn.lora_B_v.65.weight'}
2025-12-11 19:40:40,862 [inflora.py] => Task 65, Epoch 50/50 => Loss 0.025, Train_accy 98.68
Threshold:  0.993
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 102/768 type remove
Layer 4 : 148/768 type remove
Layer 5 : 187/768 type remove
Layer 6 : 186/768 type remove
Layer 7 : 236/768 type remove
Layer 8 : 271/768 type remove
Layer 9 : 359/768 type remove
Layer 10 : 370/768 type retain
Layer 11 : 291/768 type remove
Layer 12 : 312/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:40:47,823 [trainer.py] => Time:109.19293522834778
4020 4020
4020 4020
2025-12-11 19:40:59,754 [trainer.py] => Time:11.930898666381836
2025-12-11 19:40:59,754 [inflora.py] => Exemplar size: 0
2025-12-11 19:40:59,755 [trainer.py] => CNN: {'total': np.float64(38.63), '00-01': np.float64(77.42), '02-03': np.float64(32.26), '04-05': np.float64(51.69), '06-07': np.float64(62.5), '08-09': np.float64(35.62), '10-11': np.float64(12.2), '12-13': np.float64(44.44), '14-15': np.float64(33.93), '16-17': np.float64(17.14), '18-19': np.float64(44.64), '20-21': np.float64(51.65), '22-23': np.float64(57.03), '24-25': np.float64(12.24), '26-27': np.float64(66.09), '28-29': np.float64(54.32), '30-31': np.float64(32.73), '32-33': np.float64(2.27), '34-35': np.float64(38.71), '36-37': np.float64(55.0), '38-39': np.float64(63.41), '40-41': np.float64(31.48), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(47.87), '50-51': np.float64(18.92), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(5.45), '60-61': np.float64(43.9), '62-63': np.float64(34.91), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(20.55), '72-73': np.float64(42.5), '74-75': np.float64(53.85), '76-77': np.float64(44.58), '78-79': np.float64(36.59), '80-81': np.float64(52.78), '82-83': np.float64(17.8), '84-85': np.float64(41.3), '86-87': np.float64(33.33), '88-89': np.float64(66.67), '90-91': np.float64(32.5), '92-93': np.float64(14.29), '94-95': np.float64(15.49), '96-97': np.float64(21.21), '98-99': np.float64(31.08), '100-101': np.float64(45.0), '102-103': np.float64(65.52), '104-105': np.float64(11.7), '106-107': np.float64(35.9), '108-109': np.float64(52.24), '110-111': np.float64(30.0), '112-113': np.float64(73.08), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(55.74), '122-123': np.float64(14.29), '124-125': np.float64(42.86), '126-127': np.float64(7.55), '128-129': np.float64(31.65), '130-131': np.float64(51.11), 'old': np.float64(38.49), 'new': np.float64(51.11)}
2025-12-11 19:40:59,755 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63)]
2025-12-11 19:40:59,755 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65)]
2025-12-11 19:40:59,755 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154]
2025-12-11 19:41:13,267 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 69.35483870967742, '04-05': 50.56179775280899, '06-07': 78.57142857142857, '08-09': 63.013698630136986, '10-11': 58.536585365853654, '12-13': 85.18518518518519, '14-15': 69.64285714285714, '16-17': 72.85714285714285, '18-19': 69.64285714285714, '20-21': 74.72527472527473, '22-23': 69.53125, '24-25': 63.26530612244898, '26-27': 57.391304347826086, '28-29': 62.96296296296296, '30-31': 46.36363636363636, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 70.73170731707317, '40-41': 50.0, '42-43': 87.5, '44-45': 44.44444444444444, '46-47': 85.18518518518519, '48-49': 78.72340425531915, '50-51': 75.67567567567568, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 87.5, '58-59': 50.90909090909091, '60-61': 68.29268292682927, '62-63': 74.52830188679245, '64-65': 76.66666666666667, '66-67': 57.89473684210527, '68-69': 75.0, '70-71': 61.64383561643836, '72-73': 67.5, '74-75': 37.17948717948718, '76-77': 68.67469879518072, '78-79': 59.756097560975604, '80-81': 76.38888888888889, '82-83': 51.69491525423729, '84-85': 73.91304347826086, '86-87': 74.71264367816092, '88-89': 90.0, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 77.46478873239437, '96-97': 52.52525252525253, '98-99': 56.75675675675676, '100-101': 61.66666666666667, '102-103': 72.41379310344827, '104-105': 58.51063829787234, '106-107': 71.7948717948718, '108-109': 70.1492537313433, '110-111': 78.33333333333333, '112-113': 88.46153846153845, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 75.0, '120-121': 80.32786885245902, '122-123': 71.42857142857143, '124-125': 61.224489795918366, '126-127': 69.81132075471697, '128-129': 81.0126582278481, '130-131': 95.55555555555556}
2025-12-11 19:41:13,268 [trainer.py] => Ave Acc (W-NCM): 68.74%
2025-12-11 19:41:13,268 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 74.73% (best 95.60%); T12: W-NCM 69.53% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 57.39% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 46.36% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 87.50% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 85.19% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 50.91% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 74.53% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 61.64% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 51.69% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 52.53% (best 84.85%); T50: W-NCM 56.76% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 58.51% (best 79.79%); T54: W-NCM 71.79% (best 89.74%); T55: W-NCM 70.15% (best 91.04%); T56: W-NCM 78.33% (best 96.67%); T57: W-NCM 88.46% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 94.44%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 71.43% (best 94.64%); T63: W-NCM 61.22% (best 93.88%); T64: W-NCM 69.81% (best 84.91%); T65: W-NCM 81.01% (best 92.41%); T66: W-NCM 95.56% (best 95.56%)
2025-12-11 19:41:13,268 [trainer.py] => Average forgetting (W-NCM): 23.36% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:41:13,281 [trainer.py] => All params: 144526051
2025-12-11 19:41:13,293 [trainer.py] => Trainable params: 185858
2025-12-11 19:41:13,293 [inflora.py] => Learning on 132-134
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.66.weight', 'image_encoder.blocks.2.attn.lora_B_k.66.weight', 'image_encoder.blocks.4.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_v.66.weight', 'image_encoder.blocks.11.attn.lora_B_k.66.weight', 'image_encoder.blocks.8.attn.lora_B_k.66.weight', 'image_encoder.blocks.1.attn.lora_B_v.66.weight', 'image_encoder.blocks.7.attn.lora_B_k.66.weight', 'image_encoder.blocks.5.attn.lora_B_k.66.weight', 'image_encoder.blocks.8.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_v.66.weight', 'image_encoder.blocks.2.attn.lora_B_v.66.weight', 'image_encoder.blocks.10.attn.lora_B_k.66.weight', 'image_encoder.blocks.3.attn.lora_B_v.66.weight', 'classifier_pool.66.bias', 'classifier_pool.66.weight', 'image_encoder.blocks.9.attn.lora_B_k.66.weight', 'image_encoder.blocks.11.attn.lora_B_v.66.weight', 'image_encoder.blocks.6.attn.lora_B_k.66.weight', 'image_encoder.blocks.9.attn.lora_B_v.66.weight', 'image_encoder.blocks.5.attn.lora_B_v.66.weight', 'image_encoder.blocks.4.attn.lora_B_k.66.weight', 'image_encoder.blocks.1.attn.lora_B_k.66.weight', 'image_encoder.blocks.0.attn.lora_B_k.66.weight', 'image_encoder.blocks.7.attn.lora_B_v.66.weight', 'image_encoder.blocks.3.attn.lora_B_k.66.weight'}
2025-12-11 19:42:54,822 [inflora.py] => Task 66, Epoch 50/50 => Loss 0.037, Train_accy 98.83
Threshold:  0.9932
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 103/768 type remove
Layer 4 : 150/768 type remove
Layer 5 : 191/768 type remove
Layer 6 : 192/768 type remove
Layer 7 : 242/768 type remove
Layer 8 : 280/768 type remove
Layer 9 : 371/768 type remove
Layer 10 : 359/768 type retain
Layer 11 : 297/768 type remove
Layer 12 : 301/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:43:01,841 [trainer.py] => Time:108.54765224456787
4054 4054
4054 4054
2025-12-11 19:43:13,947 [trainer.py] => Time:12.106515169143677
2025-12-11 19:43:13,948 [inflora.py] => Exemplar size: 0
2025-12-11 19:43:13,948 [trainer.py] => CNN: {'total': np.float64(38.9), '00-01': np.float64(74.19), '02-03': np.float64(40.32), '04-05': np.float64(43.82), '06-07': np.float64(62.5), '08-09': np.float64(32.88), '10-11': np.float64(14.63), '12-13': np.float64(40.74), '14-15': np.float64(33.93), '16-17': np.float64(12.86), '18-19': np.float64(55.36), '20-21': np.float64(52.75), '22-23': np.float64(60.16), '24-25': np.float64(16.33), '26-27': np.float64(60.87), '28-29': np.float64(51.85), '30-31': np.float64(36.36), '32-33': np.float64(4.55), '34-35': np.float64(32.26), '36-37': np.float64(55.0), '38-39': np.float64(65.85), '40-41': np.float64(25.93), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(54.26), '50-51': np.float64(13.51), '52-53': np.float64(52.78), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(3.64), '60-61': np.float64(36.59), '62-63': np.float64(36.79), '64-65': np.float64(0.0), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(52.5), '74-75': np.float64(51.28), '76-77': np.float64(43.37), '78-79': np.float64(36.59), '80-81': np.float64(50.0), '82-83': np.float64(19.49), '84-85': np.float64(34.78), '86-87': np.float64(35.63), '88-89': np.float64(66.67), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(11.27), '96-97': np.float64(22.22), '98-99': np.float64(32.43), '100-101': np.float64(46.67), '102-103': np.float64(68.97), '104-105': np.float64(12.77), '106-107': np.float64(38.46), '108-109': np.float64(50.75), '110-111': np.float64(31.67), '112-113': np.float64(76.92), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(38.46), '120-121': np.float64(59.02), '122-123': np.float64(16.07), '124-125': np.float64(40.82), '126-127': np.float64(13.21), '128-129': np.float64(29.11), '130-131': np.float64(44.44), '132-133': np.float64(47.06), 'old': np.float64(38.83), 'new': np.float64(47.06)}
2025-12-11 19:43:13,948 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9)]
2025-12-11 19:43:13,948 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68)]
2025-12-11 19:43:13,948 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464]
2025-12-11 19:43:27,522 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 67.74193548387096, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 64.38356164383562, '10-11': 53.65853658536586, '12-13': 88.88888888888889, '14-15': 71.42857142857143, '16-17': 72.85714285714285, '18-19': 75.0, '20-21': 76.92307692307693, '22-23': 67.96875, '24-25': 63.26530612244898, '26-27': 54.78260869565217, '28-29': 65.4320987654321, '30-31': 50.0, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 46.2962962962963, '42-43': 84.375, '44-45': 48.148148148148145, '46-47': 81.48148148148148, '48-49': 74.46808510638297, '50-51': 75.67567567567568, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 90.0, '58-59': 60.0, '60-61': 68.29268292682927, '62-63': 75.47169811320755, '64-65': 76.66666666666667, '66-67': 57.89473684210527, '68-69': 75.0, '70-71': 60.273972602739725, '72-73': 72.5, '74-75': 37.17948717948718, '76-77': 73.49397590361446, '78-79': 62.19512195121951, '80-81': 76.38888888888889, '82-83': 55.932203389830505, '84-85': 73.91304347826086, '86-87': 77.01149425287356, '88-89': 88.33333333333333, '90-91': 65.0, '92-93': 28.57142857142857, '94-95': 78.87323943661971, '96-97': 51.515151515151516, '98-99': 55.4054054054054, '100-101': 66.66666666666666, '102-103': 72.41379310344827, '104-105': 59.57446808510638, '106-107': 69.23076923076923, '108-109': 67.16417910447761, '110-111': 76.66666666666667, '112-113': 88.46153846153845, '114-115': 70.0, '116-117': 88.88888888888889, '118-119': 76.92307692307693, '120-121': 77.04918032786885, '122-123': 71.42857142857143, '124-125': 57.14285714285714, '126-127': 64.15094339622641, '128-129': 70.88607594936708, '130-131': 88.88888888888889, '132-133': 94.11764705882352}
2025-12-11 19:43:27,523 [trainer.py] => Ave Acc (W-NCM): 69.25%
2025-12-11 19:43:27,523 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 50.00% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 84.38% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 74.47% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 75.47% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 57.89% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 73.49% (best 78.31%); T40: W-NCM 62.20% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 55.93% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 51.52% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 59.57% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 67.16% (best 91.04%); T56: W-NCM 76.67% (best 96.67%); T57: W-NCM 88.46% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 88.89% (best 94.44%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 71.43% (best 94.64%); T63: W-NCM 57.14% (best 93.88%); T64: W-NCM 64.15% (best 84.91%); T65: W-NCM 70.89% (best 92.41%); T66: W-NCM 88.89% (best 95.56%); T67: W-NCM 94.12% (best 94.12%)
2025-12-11 19:43:27,523 [trainer.py] => Average forgetting (W-NCM): 22.87% | Max forgetting (W-NCM): 57.14%
2025-12-11 19:43:27,535 [trainer.py] => All params: 144526051
2025-12-11 19:43:27,547 [trainer.py] => Trainable params: 185858
2025-12-11 19:43:27,547 [inflora.py] => Learning on 134-136
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.67.weight', 'image_encoder.blocks.4.attn.lora_B_k.67.weight', 'image_encoder.blocks.4.attn.lora_B_v.67.weight', 'image_encoder.blocks.8.attn.lora_B_k.67.weight', 'image_encoder.blocks.1.attn.lora_B_v.67.weight', 'image_encoder.blocks.5.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_k.67.weight', 'image_encoder.blocks.2.attn.lora_B_v.67.weight', 'image_encoder.blocks.10.attn.lora_B_v.67.weight', 'image_encoder.blocks.0.attn.lora_B_k.67.weight', 'image_encoder.blocks.11.attn.lora_B_v.67.weight', 'image_encoder.blocks.3.attn.lora_B_k.67.weight', 'image_encoder.blocks.3.attn.lora_B_v.67.weight', 'image_encoder.blocks.6.attn.lora_B_v.67.weight', 'image_encoder.blocks.6.attn.lora_B_k.67.weight', 'image_encoder.blocks.0.attn.lora_B_v.67.weight', 'image_encoder.blocks.7.attn.lora_B_v.67.weight', 'image_encoder.blocks.9.attn.lora_B_v.67.weight', 'classifier_pool.67.bias', 'image_encoder.blocks.1.attn.lora_B_k.67.weight', 'image_encoder.blocks.10.attn.lora_B_k.67.weight', 'image_encoder.blocks.7.attn.lora_B_k.67.weight', 'image_encoder.blocks.5.attn.lora_B_v.67.weight', 'image_encoder.blocks.2.attn.lora_B_k.67.weight', 'classifier_pool.67.weight', 'image_encoder.blocks.8.attn.lora_B_v.67.weight'}
2025-12-11 19:45:59,328 [inflora.py] => Task 67, Epoch 50/50 => Loss 0.064, Train_accy 97.52
Threshold:  0.9934
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 105/768 type remove
Layer 4 : 153/768 type remove
Layer 5 : 195/768 type remove
Layer 6 : 197/768 type remove
Layer 7 : 246/768 type remove
Layer 8 : 284/768 type remove
Layer 9 : 374/768 type remove
Layer 10 : 355/768 type retain
Layer 11 : 300/768 type remove
Layer 12 : 298/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:46:06,816 [trainer.py] => Time:159.26814436912537
4132 4132
4132 4132
2025-12-11 19:46:19,131 [trainer.py] => Time:12.315326690673828
2025-12-11 19:46:19,132 [inflora.py] => Exemplar size: 0
2025-12-11 19:46:19,132 [trainer.py] => CNN: {'total': np.float64(37.92), '00-01': np.float64(73.12), '02-03': np.float64(32.26), '04-05': np.float64(42.7), '06-07': np.float64(62.5), '08-09': np.float64(31.51), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(32.14), '16-17': np.float64(15.71), '18-19': np.float64(55.36), '20-21': np.float64(57.14), '22-23': np.float64(60.94), '24-25': np.float64(14.29), '26-27': np.float64(59.13), '28-29': np.float64(55.56), '30-31': np.float64(33.64), '32-33': np.float64(4.55), '34-35': np.float64(25.81), '36-37': np.float64(46.67), '38-39': np.float64(53.66), '40-41': np.float64(25.93), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(52.13), '50-51': np.float64(8.11), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(25.0), '58-59': np.float64(3.64), '60-61': np.float64(41.46), '62-63': np.float64(33.96), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(32.5), '74-75': np.float64(51.28), '76-77': np.float64(40.96), '78-79': np.float64(32.93), '80-81': np.float64(51.39), '82-83': np.float64(19.49), '84-85': np.float64(30.43), '86-87': np.float64(35.63), '88-89': np.float64(61.67), '90-91': np.float64(50.0), '92-93': np.float64(16.33), '94-95': np.float64(18.31), '96-97': np.float64(22.22), '98-99': np.float64(29.73), '100-101': np.float64(46.67), '102-103': np.float64(53.45), '104-105': np.float64(5.32), '106-107': np.float64(38.46), '108-109': np.float64(52.24), '110-111': np.float64(33.33), '112-113': np.float64(78.85), '114-115': np.float64(47.5), '116-117': np.float64(72.22), '118-119': np.float64(38.46), '120-121': np.float64(65.57), '122-123': np.float64(14.29), '124-125': np.float64(44.9), '126-127': np.float64(11.32), '128-129': np.float64(18.99), '130-131': np.float64(44.44), '132-133': np.float64(38.24), '134-135': np.float64(61.54), 'old': np.float64(37.47), 'new': np.float64(61.54)}
2025-12-11 19:46:19,132 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92)]
2025-12-11 19:46:19,132 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43)]
2025-12-11 19:46:19,132 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586]
2025-12-11 19:46:33,177 [trainer.py] => W-NCM: {'00-01': 61.29032258064516, '02-03': 72.58064516129032, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 65.75342465753424, '10-11': 53.65853658536586, '12-13': 88.88888888888889, '14-15': 69.64285714285714, '16-17': 72.85714285714285, '18-19': 76.78571428571429, '20-21': 73.62637362637363, '22-23': 64.84375, '24-25': 61.224489795918366, '26-27': 58.26086956521739, '28-29': 59.25925925925925, '30-31': 45.45454545454545, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 80.0, '38-39': 70.73170731707317, '40-41': 48.148148148148145, '42-43': 78.125, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 76.59574468085107, '50-51': 72.97297297297297, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 85.0, '58-59': 52.72727272727272, '60-61': 70.73170731707317, '62-63': 72.64150943396226, '64-65': 70.0, '66-67': 52.63157894736842, '68-69': 75.0, '70-71': 60.273972602739725, '72-73': 70.0, '74-75': 35.8974358974359, '76-77': 67.46987951807229, '78-79': 57.3170731707317, '80-81': 76.38888888888889, '82-83': 48.30508474576271, '84-85': 73.91304347826086, '86-87': 75.86206896551724, '88-89': 88.33333333333333, '90-91': 62.5, '92-93': 22.448979591836736, '94-95': 76.05633802816901, '96-97': 50.505050505050505, '98-99': 55.4054054054054, '100-101': 66.66666666666666, '102-103': 70.6896551724138, '104-105': 59.57446808510638, '106-107': 74.35897435897436, '108-109': 67.16417910447761, '110-111': 78.33333333333333, '112-113': 88.46153846153845, '114-115': 70.0, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 80.32786885245902, '122-123': 67.85714285714286, '124-125': 55.10204081632652, '126-127': 60.37735849056604, '128-129': 63.29113924050633, '130-131': 80.0, '132-133': 82.35294117647058, '134-135': 85.8974358974359}
2025-12-11 19:46:33,178 [trainer.py] => Ave Acc (W-NCM): 68.05%
2025-12-11 19:46:33,178 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 61.29% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 76.79% (best 94.64%); T11: W-NCM 73.63% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 45.45% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 48.15% (best 94.44%); T22: W-NCM 78.12% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 72.64% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 48.31% (best 89.83%); T43: W-NCM 73.91% (best 89.13%); T44: W-NCM 75.86% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 22.45% (best 85.71%); T48: W-NCM 76.06% (best 95.77%); T49: W-NCM 50.51% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 70.69% (best 87.93%); T53: W-NCM 59.57% (best 79.79%); T54: W-NCM 74.36% (best 89.74%); T55: W-NCM 67.16% (best 91.04%); T56: W-NCM 78.33% (best 96.67%); T57: W-NCM 88.46% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 67.86% (best 94.64%); T63: W-NCM 55.10% (best 93.88%); T64: W-NCM 60.38% (best 84.91%); T65: W-NCM 63.29% (best 92.41%); T66: W-NCM 80.00% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 85.90% (best 85.90%)
2025-12-11 19:46:33,178 [trainer.py] => Average forgetting (W-NCM): 23.99% | Max forgetting (W-NCM): 63.27%
2025-12-11 19:46:33,191 [trainer.py] => All params: 144526051
2025-12-11 19:46:33,203 [trainer.py] => Trainable params: 185858
2025-12-11 19:46:33,203 [inflora.py] => Learning on 136-138
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.68.weight', 'image_encoder.blocks.3.attn.lora_B_k.68.weight', 'image_encoder.blocks.1.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_k.68.weight', 'classifier_pool.68.weight', 'image_encoder.blocks.2.attn.lora_B_k.68.weight', 'classifier_pool.68.bias', 'image_encoder.blocks.0.attn.lora_B_k.68.weight', 'image_encoder.blocks.8.attn.lora_B_k.68.weight', 'image_encoder.blocks.5.attn.lora_B_v.68.weight', 'image_encoder.blocks.4.attn.lora_B_v.68.weight', 'image_encoder.blocks.2.attn.lora_B_v.68.weight', 'image_encoder.blocks.10.attn.lora_B_k.68.weight', 'image_encoder.blocks.8.attn.lora_B_v.68.weight', 'image_encoder.blocks.0.attn.lora_B_v.68.weight', 'image_encoder.blocks.6.attn.lora_B_v.68.weight', 'image_encoder.blocks.5.attn.lora_B_k.68.weight', 'image_encoder.blocks.11.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_v.68.weight', 'image_encoder.blocks.1.attn.lora_B_v.68.weight', 'image_encoder.blocks.6.attn.lora_B_k.68.weight', 'image_encoder.blocks.4.attn.lora_B_k.68.weight', 'image_encoder.blocks.9.attn.lora_B_v.68.weight', 'image_encoder.blocks.9.attn.lora_B_k.68.weight', 'image_encoder.blocks.3.attn.lora_B_v.68.weight', 'image_encoder.blocks.7.attn.lora_B_k.68.weight'}
2025-12-11 19:48:20,512 [inflora.py] => Task 68, Epoch 50/50 => Loss 0.022, Train_accy 99.03
Threshold:  0.9936
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 107/768 type remove
Layer 4 : 159/768 type remove
Layer 5 : 201/768 type remove
Layer 6 : 203/768 type remove
Layer 7 : 252/768 type remove
Layer 8 : 292/768 type remove
Layer 9 : 382/768 type remove
Layer 10 : 346/768 type retain
Layer 11 : 307/768 type remove
Layer 12 : 294/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:48:27,086 [trainer.py] => Time:113.88355207443237
4181 4181
4181 4181
2025-12-11 19:48:39,494 [trainer.py] => Time:12.407539367675781
2025-12-11 19:48:39,495 [inflora.py] => Exemplar size: 0
2025-12-11 19:48:39,495 [trainer.py] => CNN: {'total': np.float64(38.05), '00-01': np.float64(70.97), '02-03': np.float64(35.48), '04-05': np.float64(44.94), '06-07': np.float64(60.71), '08-09': np.float64(32.88), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(30.36), '16-17': np.float64(15.71), '18-19': np.float64(57.14), '20-21': np.float64(59.34), '22-23': np.float64(59.38), '24-25': np.float64(12.24), '26-27': np.float64(60.0), '28-29': np.float64(51.85), '30-31': np.float64(32.73), '32-33': np.float64(4.55), '34-35': np.float64(25.81), '36-37': np.float64(53.33), '38-39': np.float64(51.22), '40-41': np.float64(27.78), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(44.44), '48-49': np.float64(54.26), '50-51': np.float64(16.22), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(27.5), '58-59': np.float64(1.82), '60-61': np.float64(41.46), '62-63': np.float64(33.96), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(40.0), '74-75': np.float64(56.41), '76-77': np.float64(42.17), '78-79': np.float64(26.83), '80-81': np.float64(52.78), '82-83': np.float64(16.95), '84-85': np.float64(32.61), '86-87': np.float64(34.48), '88-89': np.float64(58.33), '90-91': np.float64(50.0), '92-93': np.float64(18.37), '94-95': np.float64(18.31), '96-97': np.float64(21.21), '98-99': np.float64(31.08), '100-101': np.float64(46.67), '102-103': np.float64(51.72), '104-105': np.float64(4.26), '106-107': np.float64(35.9), '108-109': np.float64(53.73), '110-111': np.float64(33.33), '112-113': np.float64(78.85), '114-115': np.float64(42.5), '116-117': np.float64(69.44), '118-119': np.float64(34.62), '120-121': np.float64(63.93), '122-123': np.float64(10.71), '124-125': np.float64(40.82), '126-127': np.float64(13.21), '128-129': np.float64(15.19), '130-131': np.float64(44.44), '132-133': np.float64(47.06), '134-135': np.float64(61.54), '136-137': np.float64(63.27), 'old': np.float64(37.75), 'new': np.float64(63.27)}
2025-12-11 19:48:39,495 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05)]
2025-12-11 19:48:39,495 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72)]
2025-12-11 19:48:39,495 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828]
2025-12-11 19:48:53,335 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 75.80645161290323, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 65.75342465753424, '10-11': 53.65853658536586, '12-13': 87.03703703703704, '14-15': 73.21428571428571, '16-17': 65.71428571428571, '18-19': 75.0, '20-21': 75.82417582417582, '22-23': 67.96875, '24-25': 61.224489795918366, '26-27': 58.26086956521739, '28-29': 62.96296296296296, '30-31': 47.27272727272727, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 53.70370370370371, '42-43': 75.0, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 77.6595744680851, '50-51': 72.97297297297297, '52-53': 77.77777777777779, '54-55': 52.0, '56-57': 85.0, '58-59': 56.36363636363636, '60-61': 70.73170731707317, '62-63': 73.58490566037736, '64-65': 66.66666666666666, '66-67': 52.63157894736842, '68-69': 70.83333333333334, '70-71': 61.64383561643836, '72-73': 70.0, '74-75': 33.33333333333333, '76-77': 69.87951807228916, '78-79': 58.536585365853654, '80-81': 77.77777777777779, '82-83': 50.0, '84-85': 76.08695652173914, '86-87': 77.01149425287356, '88-89': 88.33333333333333, '90-91': 65.0, '92-93': 24.489795918367346, '94-95': 78.87323943661971, '96-97': 54.54545454545454, '98-99': 55.4054054054054, '100-101': 68.33333333333333, '102-103': 68.96551724137932, '104-105': 58.51063829787234, '106-107': 66.66666666666666, '108-109': 68.65671641791045, '110-111': 80.0, '112-113': 76.92307692307693, '114-115': 70.0, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 78.68852459016394, '122-123': 64.28571428571429, '124-125': 48.97959183673469, '126-127': 62.264150943396224, '128-129': 55.69620253164557, '130-131': 75.55555555555556, '132-133': 82.35294117647058, '134-135': 75.64102564102564, '136-137': 93.87755102040816}
2025-12-11 19:48:53,336 [trainer.py] => Ave Acc (W-NCM): 68.03%
2025-12-11 19:48:53,336 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 75.81% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 65.71% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 47.27% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 77.66% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 77.78% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 73.58% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 61.64% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 33.33% (best 87.18%); T39: W-NCM 69.88% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 77.78% (best 97.22%); T42: W-NCM 50.00% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 24.49% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 54.55% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 68.33% (best 91.67%); T52: W-NCM 68.97% (best 87.93%); T53: W-NCM 58.51% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 68.66% (best 91.04%); T56: W-NCM 80.00% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 64.29% (best 94.64%); T63: W-NCM 48.98% (best 93.88%); T64: W-NCM 62.26% (best 84.91%); T65: W-NCM 55.70% (best 92.41%); T66: W-NCM 75.56% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 75.64% (best 85.90%); T69: W-NCM 93.88% (best 93.88%)
2025-12-11 19:48:53,336 [trainer.py] => Average forgetting (W-NCM): 24.04% | Max forgetting (W-NCM): 61.22%
2025-12-11 19:48:53,348 [trainer.py] => All params: 144526051
2025-12-11 19:48:53,360 [trainer.py] => Trainable params: 185858
2025-12-11 19:48:53,360 [inflora.py] => Learning on 138-140
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_v.69.weight', 'image_encoder.blocks.7.attn.lora_B_k.69.weight', 'image_encoder.blocks.2.attn.lora_B_v.69.weight', 'image_encoder.blocks.5.attn.lora_B_k.69.weight', 'image_encoder.blocks.1.attn.lora_B_v.69.weight', 'image_encoder.blocks.7.attn.lora_B_v.69.weight', 'image_encoder.blocks.9.attn.lora_B_k.69.weight', 'image_encoder.blocks.0.attn.lora_B_k.69.weight', 'image_encoder.blocks.9.attn.lora_B_v.69.weight', 'image_encoder.blocks.10.attn.lora_B_v.69.weight', 'image_encoder.blocks.3.attn.lora_B_k.69.weight', 'image_encoder.blocks.10.attn.lora_B_k.69.weight', 'image_encoder.blocks.4.attn.lora_B_v.69.weight', 'classifier_pool.69.weight', 'image_encoder.blocks.5.attn.lora_B_v.69.weight', 'image_encoder.blocks.4.attn.lora_B_k.69.weight', 'image_encoder.blocks.1.attn.lora_B_k.69.weight', 'image_encoder.blocks.3.attn.lora_B_v.69.weight', 'image_encoder.blocks.6.attn.lora_B_k.69.weight', 'image_encoder.blocks.8.attn.lora_B_k.69.weight', 'image_encoder.blocks.8.attn.lora_B_v.69.weight', 'classifier_pool.69.bias', 'image_encoder.blocks.11.attn.lora_B_k.69.weight', 'image_encoder.blocks.11.attn.lora_B_v.69.weight', 'image_encoder.blocks.2.attn.lora_B_k.69.weight'}
2025-12-11 19:50:46,075 [inflora.py] => Task 69, Epoch 50/50 => Loss 0.053, Train_accy 99.05
Threshold:  0.9938
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 56/768 type remove
Layer 3 : 112/768 type remove
Layer 4 : 165/768 type remove
Layer 5 : 211/768 type remove
Layer 6 : 213/768 type remove
Layer 7 : 260/768 type remove
Layer 8 : 301/768 type remove
Layer 9 : 376/768 type retain
Layer 10 : 334/768 type retain
Layer 11 : 319/768 type remove
Layer 12 : 291/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:50:52,987 [trainer.py] => Time:119.62699937820435
4234 4234
4234 4234
2025-12-11 19:51:05,546 [trainer.py] => Time:12.557989597320557
2025-12-11 19:51:05,546 [inflora.py] => Exemplar size: 0
2025-12-11 19:51:05,546 [trainer.py] => CNN: {'total': np.float64(37.62), '00-01': np.float64(72.04), '02-03': np.float64(33.87), '04-05': np.float64(47.19), '06-07': np.float64(57.14), '08-09': np.float64(34.25), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(25.0), '16-17': np.float64(11.43), '18-19': np.float64(57.14), '20-21': np.float64(53.85), '22-23': np.float64(58.59), '24-25': np.float64(4.08), '26-27': np.float64(57.39), '28-29': np.float64(53.09), '30-31': np.float64(32.73), '32-33': np.float64(6.82), '34-35': np.float64(16.13), '36-37': np.float64(53.33), '38-39': np.float64(53.66), '40-41': np.float64(27.78), '42-43': np.float64(37.5), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(51.06), '50-51': np.float64(18.92), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(36.59), '62-63': np.float64(30.19), '64-65': np.float64(6.67), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(32.5), '74-75': np.float64(51.28), '76-77': np.float64(44.58), '78-79': np.float64(25.61), '80-81': np.float64(47.22), '82-83': np.float64(21.19), '84-85': np.float64(30.43), '86-87': np.float64(33.33), '88-89': np.float64(60.0), '90-91': np.float64(45.0), '92-93': np.float64(16.33), '94-95': np.float64(19.72), '96-97': np.float64(20.2), '98-99': np.float64(32.43), '100-101': np.float64(43.33), '102-103': np.float64(50.0), '104-105': np.float64(7.45), '106-107': np.float64(33.33), '108-109': np.float64(56.72), '110-111': np.float64(38.33), '112-113': np.float64(76.92), '114-115': np.float64(42.5), '116-117': np.float64(72.22), '118-119': np.float64(32.69), '120-121': np.float64(59.02), '122-123': np.float64(10.71), '124-125': np.float64(42.86), '126-127': np.float64(9.43), '128-129': np.float64(13.92), '130-131': np.float64(46.67), '132-133': np.float64(41.18), '134-135': np.float64(57.69), '136-137': np.float64(65.31), '138-139': np.float64(50.94), 'old': np.float64(37.46), 'new': np.float64(50.94)}
2025-12-11 19:51:05,546 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62)]
2025-12-11 19:51:05,547 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47)]
2025-12-11 19:51:05,547 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546]
2025-12-11 19:51:19,610 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 77.41935483870968, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 63.013698630136986, '10-11': 53.65853658536586, '12-13': 87.03703703703704, '14-15': 69.64285714285714, '16-17': 65.71428571428571, '18-19': 71.42857142857143, '20-21': 72.52747252747253, '22-23': 66.40625, '24-25': 59.183673469387756, '26-27': 54.78260869565217, '28-29': 61.72839506172839, '30-31': 50.90909090909091, '32-33': 93.18181818181817, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 46.2962962962963, '42-43': 75.0, '44-45': 44.44444444444444, '46-47': 81.48148148148148, '48-49': 78.72340425531915, '50-51': 72.97297297297297, '52-53': 80.55555555555556, '54-55': 52.0, '56-57': 87.5, '58-59': 40.0, '60-61': 68.29268292682927, '62-63': 71.69811320754717, '64-65': 66.66666666666666, '66-67': 52.63157894736842, '68-69': 70.83333333333334, '70-71': 58.9041095890411, '72-73': 67.5, '74-75': 32.05128205128205, '76-77': 67.46987951807229, '78-79': 58.536585365853654, '80-81': 76.38888888888889, '82-83': 50.0, '84-85': 76.08695652173914, '86-87': 75.86206896551724, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 80.28169014084507, '96-97': 51.515151515151516, '98-99': 52.702702702702695, '100-101': 63.33333333333333, '102-103': 63.793103448275865, '104-105': 54.25531914893617, '106-107': 69.23076923076923, '108-109': 73.13432835820896, '110-111': 75.0, '112-113': 78.84615384615384, '114-115': 70.0, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 80.32786885245902, '122-123': 64.28571428571429, '124-125': 48.97959183673469, '126-127': 54.71698113207547, '128-129': 55.69620253164557, '130-131': 73.33333333333333, '132-133': 82.35294117647058, '134-135': 71.7948717948718, '136-137': 91.83673469387756, '138-139': 94.33962264150944}
2025-12-11 19:51:19,610 [trainer.py] => Ave Acc (W-NCM): 67.38%
2025-12-11 19:51:19,610 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 77.42% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 65.71% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 50.91% (best 93.64%); T17: W-NCM 93.18% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 75.00% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 40.00% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 71.70% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 58.90% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 32.05% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 50.00% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 75.86% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 51.52% (best 84.85%); T50: W-NCM 52.70% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 63.79% (best 87.93%); T53: W-NCM 54.26% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 73.13% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 94.44% (best 94.44%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 64.29% (best 94.64%); T63: W-NCM 48.98% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 55.70% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 71.79% (best 85.90%); T69: W-NCM 91.84% (best 93.88%); T70: W-NCM 94.34% (best 94.34%)
2025-12-11 19:51:19,610 [trainer.py] => Average forgetting (W-NCM): 24.74% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:51:19,623 [trainer.py] => All params: 144526051
2025-12-11 19:51:19,635 [trainer.py] => Trainable params: 185858
2025-12-11 19:51:19,635 [inflora.py] => Learning on 140-142
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.70.weight', 'classifier_pool.70.weight', 'image_encoder.blocks.10.attn.lora_B_k.70.weight', 'image_encoder.blocks.1.attn.lora_B_k.70.weight', 'image_encoder.blocks.4.attn.lora_B_k.70.weight', 'image_encoder.blocks.8.attn.lora_B_v.70.weight', 'image_encoder.blocks.5.attn.lora_B_v.70.weight', 'image_encoder.blocks.11.attn.lora_B_v.70.weight', 'image_encoder.blocks.5.attn.lora_B_k.70.weight', 'image_encoder.blocks.0.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_k.70.weight', 'image_encoder.blocks.4.attn.lora_B_v.70.weight', 'image_encoder.blocks.9.attn.lora_B_v.70.weight', 'classifier_pool.70.bias', 'image_encoder.blocks.2.attn.lora_B_v.70.weight', 'image_encoder.blocks.7.attn.lora_B_k.70.weight', 'image_encoder.blocks.7.attn.lora_B_v.70.weight', 'image_encoder.blocks.10.attn.lora_B_v.70.weight', 'image_encoder.blocks.8.attn.lora_B_k.70.weight', 'image_encoder.blocks.2.attn.lora_B_k.70.weight', 'image_encoder.blocks.1.attn.lora_B_v.70.weight', 'image_encoder.blocks.6.attn.lora_B_v.70.weight', 'image_encoder.blocks.3.attn.lora_B_v.70.weight', 'image_encoder.blocks.0.attn.lora_B_k.70.weight', 'image_encoder.blocks.11.attn.lora_B_k.70.weight', 'image_encoder.blocks.9.attn.lora_B_k.70.weight'}
2025-12-11 19:53:03,372 [inflora.py] => Task 70, Epoch 50/50 => Loss 0.057, Train_accy 96.91
Threshold:  0.994
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
Skip Updating DualGPM for layer: 3
Skip Updating DualGPM for layer: 4
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 56/768 type remove
Layer 3 : 112/768 type remove
Layer 4 : 165/768 type remove
Layer 5 : 212/768 type remove
Layer 6 : 214/768 type remove
Layer 7 : 261/768 type remove
Layer 8 : 302/768 type remove
Layer 9 : 375/768 type retain
Layer 10 : 332/768 type retain
Layer 11 : 321/768 type remove
Layer 12 : 288/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:53:10,260 [trainer.py] => Time:110.62533640861511
4281 4281
4281 4281
2025-12-11 19:53:22,905 [trainer.py] => Time:12.644470453262329
2025-12-11 19:53:22,905 [inflora.py] => Exemplar size: 0
2025-12-11 19:53:22,906 [trainer.py] => CNN: {'total': np.float64(37.75), '00-01': np.float64(77.42), '02-03': np.float64(33.87), '04-05': np.float64(46.07), '06-07': np.float64(53.57), '08-09': np.float64(36.99), '10-11': np.float64(14.63), '12-13': np.float64(44.44), '14-15': np.float64(21.43), '16-17': np.float64(7.14), '18-19': np.float64(51.79), '20-21': np.float64(57.14), '22-23': np.float64(60.16), '24-25': np.float64(14.29), '26-27': np.float64(59.13), '28-29': np.float64(53.09), '30-31': np.float64(34.55), '32-33': np.float64(2.27), '34-35': np.float64(22.58), '36-37': np.float64(51.67), '38-39': np.float64(51.22), '40-41': np.float64(25.93), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(52.13), '50-51': np.float64(21.62), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(3.64), '60-61': np.float64(36.59), '62-63': np.float64(29.25), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(21.92), '72-73': np.float64(32.5), '74-75': np.float64(53.85), '76-77': np.float64(44.58), '78-79': np.float64(29.27), '80-81': np.float64(54.17), '82-83': np.float64(18.64), '84-85': np.float64(36.96), '86-87': np.float64(26.44), '88-89': np.float64(51.67), '90-91': np.float64(50.0), '92-93': np.float64(18.37), '94-95': np.float64(21.13), '96-97': np.float64(17.17), '98-99': np.float64(32.43), '100-101': np.float64(45.0), '102-103': np.float64(50.0), '104-105': np.float64(8.51), '106-107': np.float64(30.77), '108-109': np.float64(58.21), '110-111': np.float64(30.0), '112-113': np.float64(71.15), '114-115': np.float64(47.5), '116-117': np.float64(66.67), '118-119': np.float64(36.54), '120-121': np.float64(57.38), '122-123': np.float64(12.5), '124-125': np.float64(38.78), '126-127': np.float64(7.55), '128-129': np.float64(13.92), '130-131': np.float64(51.11), '132-133': np.float64(44.12), '134-135': np.float64(60.26), '136-137': np.float64(67.35), '138-139': np.float64(58.49), '140-141': np.float64(36.17), 'old': np.float64(37.77), 'new': np.float64(36.17)}
2025-12-11 19:53:22,906 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75)]
2025-12-11 19:53:22,906 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77)]
2025-12-11 19:53:22,906 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507]
2025-12-11 19:53:37,264 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 62.903225806451616, '04-05': 49.43820224719101, '06-07': 75.0, '08-09': 61.64383561643836, '10-11': 51.21951219512195, '12-13': 79.62962962962963, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 69.64285714285714, '20-21': 69.23076923076923, '22-23': 60.9375, '24-25': 55.10204081632652, '26-27': 46.95652173913044, '28-29': 64.19753086419753, '30-31': 38.18181818181819, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 63.41463414634146, '40-41': 57.407407407407405, '42-43': 71.875, '44-45': 40.74074074074074, '46-47': 81.48148148148148, '48-49': 73.40425531914893, '50-51': 75.67567567567568, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 85.0, '58-59': 43.63636363636363, '60-61': 68.29268292682927, '62-63': 69.81132075471697, '64-65': 63.33333333333333, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 56.16438356164384, '72-73': 67.5, '74-75': 32.05128205128205, '76-77': 67.46987951807229, '78-79': 57.3170731707317, '80-81': 76.38888888888889, '82-83': 40.67796610169492, '84-85': 71.73913043478261, '86-87': 72.41379310344827, '88-89': 86.66666666666667, '90-91': 62.5, '92-93': 26.53061224489796, '94-95': 77.46478873239437, '96-97': 47.474747474747474, '98-99': 51.35135135135135, '100-101': 60.0, '102-103': 67.24137931034483, '104-105': 52.12765957446809, '106-107': 64.1025641025641, '108-109': 62.68656716417911, '110-111': 71.66666666666667, '112-113': 76.92307692307693, '114-115': 70.0, '116-117': 97.22222222222221, '118-119': 80.76923076923077, '120-121': 78.68852459016394, '122-123': 57.14285714285714, '124-125': 40.816326530612244, '126-127': 52.83018867924528, '128-129': 55.69620253164557, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 62.82051282051282, '136-137': 79.59183673469387, '138-139': 84.90566037735849, '140-141': 87.2340425531915}
2025-12-11 19:53:37,265 [trainer.py] => Ave Acc (W-NCM): 64.91%
2025-12-11 19:53:37,265 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 62.90% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 79.63% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 69.23% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 46.96% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 38.18% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 73.40% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 69.81% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 32.05% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 40.68% (best 89.83%); T43: W-NCM 71.74% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 47.47% (best 84.85%); T50: W-NCM 51.35% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 52.13% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 62.69% (best 91.04%); T56: W-NCM 71.67% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 97.22% (best 97.22%); T60: W-NCM 80.77% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 57.14% (best 94.64%); T63: W-NCM 40.82% (best 93.88%); T64: W-NCM 52.83% (best 84.91%); T65: W-NCM 55.70% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 62.82% (best 85.90%); T69: W-NCM 79.59% (best 93.88%); T70: W-NCM 84.91% (best 94.34%); T71: W-NCM 87.23% (best 87.23%)
2025-12-11 19:53:37,265 [trainer.py] => Average forgetting (W-NCM): 27.22% | Max forgetting (W-NCM): 59.18%
2025-12-11 19:53:37,278 [trainer.py] => All params: 144526051
2025-12-11 19:53:37,290 [trainer.py] => Trainable params: 185858
2025-12-11 19:53:37,290 [inflora.py] => Learning on 142-144
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.71.weight', 'classifier_pool.71.bias', 'image_encoder.blocks.4.attn.lora_B_v.71.weight', 'image_encoder.blocks.1.attn.lora_B_k.71.weight', 'image_encoder.blocks.0.attn.lora_B_v.71.weight', 'image_encoder.blocks.3.attn.lora_B_v.71.weight', 'image_encoder.blocks.9.attn.lora_B_k.71.weight', 'image_encoder.blocks.9.attn.lora_B_v.71.weight', 'image_encoder.blocks.6.attn.lora_B_k.71.weight', 'image_encoder.blocks.3.attn.lora_B_k.71.weight', 'image_encoder.blocks.1.attn.lora_B_v.71.weight', 'classifier_pool.71.weight', 'image_encoder.blocks.7.attn.lora_B_v.71.weight', 'image_encoder.blocks.2.attn.lora_B_k.71.weight', 'image_encoder.blocks.4.attn.lora_B_k.71.weight', 'image_encoder.blocks.8.attn.lora_B_k.71.weight', 'image_encoder.blocks.6.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_k.71.weight', 'image_encoder.blocks.5.attn.lora_B_v.71.weight', 'image_encoder.blocks.7.attn.lora_B_k.71.weight', 'image_encoder.blocks.10.attn.lora_B_k.71.weight', 'image_encoder.blocks.10.attn.lora_B_v.71.weight', 'image_encoder.blocks.8.attn.lora_B_v.71.weight', 'image_encoder.blocks.11.attn.lora_B_v.71.weight', 'image_encoder.blocks.2.attn.lora_B_v.71.weight', 'image_encoder.blocks.0.attn.lora_B_k.71.weight'}
2025-12-11 19:55:16,982 [inflora.py] => Task 71, Epoch 50/50 => Loss 0.034, Train_accy 99.37
Threshold:  0.9942
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 113/768 type remove
Layer 4 : 167/768 type remove
Layer 5 : 215/768 type remove
Layer 6 : 218/768 type remove
Layer 7 : 267/768 type remove
Layer 8 : 312/768 type remove
Layer 9 : 364/768 type retain
Layer 10 : 319/768 type retain
Layer 11 : 334/768 type remove
Layer 12 : 284/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:55:23,637 [trainer.py] => Time:106.34727168083191
4329 4329
4329 4329
2025-12-11 19:55:36,428 [trainer.py] => Time:12.790641784667969
2025-12-11 19:55:36,429 [inflora.py] => Exemplar size: 0
2025-12-11 19:55:36,429 [trainer.py] => CNN: {'total': np.float64(38.07), '00-01': np.float64(73.12), '02-03': np.float64(40.32), '04-05': np.float64(41.57), '06-07': np.float64(60.71), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(42.59), '14-15': np.float64(21.43), '16-17': np.float64(8.57), '18-19': np.float64(53.57), '20-21': np.float64(59.34), '22-23': np.float64(62.5), '24-25': np.float64(14.29), '26-27': np.float64(60.0), '28-29': np.float64(55.56), '30-31': np.float64(32.73), '32-33': np.float64(2.27), '34-35': np.float64(29.03), '36-37': np.float64(50.0), '38-39': np.float64(51.22), '40-41': np.float64(20.37), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(56.38), '50-51': np.float64(21.62), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(0.0), '60-61': np.float64(39.02), '62-63': np.float64(35.85), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(23.29), '72-73': np.float64(37.5), '74-75': np.float64(53.85), '76-77': np.float64(42.17), '78-79': np.float64(25.61), '80-81': np.float64(48.61), '82-83': np.float64(18.64), '84-85': np.float64(41.3), '86-87': np.float64(26.44), '88-89': np.float64(56.67), '90-91': np.float64(47.5), '92-93': np.float64(18.37), '94-95': np.float64(18.31), '96-97': np.float64(17.17), '98-99': np.float64(31.08), '100-101': np.float64(45.0), '102-103': np.float64(51.72), '104-105': np.float64(8.51), '106-107': np.float64(30.77), '108-109': np.float64(59.7), '110-111': np.float64(25.0), '112-113': np.float64(73.08), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(62.3), '122-123': np.float64(10.71), '124-125': np.float64(38.78), '126-127': np.float64(5.66), '128-129': np.float64(13.92), '130-131': np.float64(51.11), '132-133': np.float64(50.0), '134-135': np.float64(58.97), '136-137': np.float64(63.27), '138-139': np.float64(54.72), '140-141': np.float64(34.04), '142-143': np.float64(47.92), 'old': np.float64(37.96), 'new': np.float64(47.92)}
2025-12-11 19:55:36,429 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07)]
2025-12-11 19:55:36,429 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77)]
2025-12-11 19:55:36,429 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807]
2025-12-11 19:55:50,792 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 69.35483870967742, '04-05': 55.0561797752809, '06-07': 76.78571428571429, '08-09': 63.013698630136986, '10-11': 56.09756097560976, '12-13': 83.33333333333334, '14-15': 71.42857142857143, '16-17': 70.0, '18-19': 71.42857142857143, '20-21': 70.32967032967034, '22-23': 64.84375, '24-25': 59.183673469387756, '26-27': 53.91304347826087, '28-29': 62.96296296296296, '30-31': 36.36363636363637, '32-33': 90.9090909090909, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 75.60975609756098, '40-41': 53.70370370370371, '42-43': 71.875, '44-45': 44.44444444444444, '46-47': 81.48148148148148, '48-49': 75.53191489361703, '50-51': 72.97297297297297, '52-53': 80.55555555555556, '54-55': 56.00000000000001, '56-57': 87.5, '58-59': 50.90909090909091, '60-61': 63.41463414634146, '62-63': 71.69811320754717, '64-65': 66.66666666666666, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 57.534246575342465, '72-73': 70.0, '74-75': 33.33333333333333, '76-77': 69.87951807228916, '78-79': 58.536585365853654, '80-81': 77.77777777777779, '82-83': 47.45762711864407, '84-85': 76.08695652173914, '86-87': 73.5632183908046, '88-89': 86.66666666666667, '90-91': 67.5, '92-93': 22.448979591836736, '94-95': 78.87323943661971, '96-97': 53.535353535353536, '98-99': 54.054054054054056, '100-101': 65.0, '102-103': 67.24137931034483, '104-105': 52.12765957446809, '106-107': 64.1025641025641, '108-109': 64.17910447761194, '110-111': 71.66666666666667, '112-113': 76.92307692307693, '114-115': 70.0, '116-117': 97.22222222222221, '118-119': 80.76923076923077, '120-121': 80.32786885245902, '122-123': 62.5, '124-125': 38.775510204081634, '126-127': 49.056603773584904, '128-129': 55.69620253164557, '130-131': 73.33333333333333, '132-133': 82.35294117647058, '134-135': 62.82051282051282, '136-137': 77.55102040816327, '138-139': 84.90566037735849, '140-141': 78.72340425531915, '142-143': 91.66666666666666}
2025-12-11 19:55:50,793 [trainer.py] => Ave Acc (W-NCM): 66.78%
2025-12-11 19:55:50,793 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 55.06% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 53.91% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 36.36% (best 93.64%); T17: W-NCM 90.91% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 75.61% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 71.88% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 75.53% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 56.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 50.91% (best 90.91%); T31: W-NCM 63.41% (best 90.24%); T32: W-NCM 71.70% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 57.53% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 33.33% (best 87.18%); T39: W-NCM 69.88% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 77.78% (best 97.22%); T42: W-NCM 47.46% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 73.56% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 22.45% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 53.54% (best 84.85%); T50: W-NCM 54.05% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 52.13% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 71.67% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 97.22% (best 97.22%); T60: W-NCM 80.77% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 62.50% (best 94.64%); T63: W-NCM 38.78% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 55.70% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 62.82% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 84.91% (best 94.34%); T71: W-NCM 78.72% (best 87.23%); T72: W-NCM 91.67% (best 91.67%)
2025-12-11 19:55:50,793 [trainer.py] => Average forgetting (W-NCM): 25.31% | Max forgetting (W-NCM): 63.27%
2025-12-11 19:55:50,805 [trainer.py] => All params: 144526051
2025-12-11 19:55:50,817 [trainer.py] => Trainable params: 185858
2025-12-11 19:55:50,817 [inflora.py] => Learning on 144-146
Parameters to be updated: {'classifier_pool.72.weight', 'classifier_pool.72.bias', 'image_encoder.blocks.3.attn.lora_B_k.72.weight', 'image_encoder.blocks.0.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_k.72.weight', 'image_encoder.blocks.4.attn.lora_B_k.72.weight', 'image_encoder.blocks.10.attn.lora_B_v.72.weight', 'image_encoder.blocks.3.attn.lora_B_v.72.weight', 'image_encoder.blocks.0.attn.lora_B_v.72.weight', 'image_encoder.blocks.2.attn.lora_B_k.72.weight', 'image_encoder.blocks.2.attn.lora_B_v.72.weight', 'image_encoder.blocks.6.attn.lora_B_k.72.weight', 'image_encoder.blocks.8.attn.lora_B_v.72.weight', 'image_encoder.blocks.9.attn.lora_B_k.72.weight', 'image_encoder.blocks.7.attn.lora_B_k.72.weight', 'image_encoder.blocks.7.attn.lora_B_v.72.weight', 'image_encoder.blocks.1.attn.lora_B_v.72.weight', 'image_encoder.blocks.1.attn.lora_B_k.72.weight', 'image_encoder.blocks.5.attn.lora_B_v.72.weight', 'image_encoder.blocks.10.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_v.72.weight', 'image_encoder.blocks.5.attn.lora_B_k.72.weight', 'image_encoder.blocks.11.attn.lora_B_k.72.weight', 'image_encoder.blocks.6.attn.lora_B_v.72.weight', 'image_encoder.blocks.9.attn.lora_B_v.72.weight', 'image_encoder.blocks.4.attn.lora_B_v.72.weight'}
2025-12-11 19:57:34,541 [inflora.py] => Task 72, Epoch 50/50 => Loss 0.026, Train_accy 98.94
Threshold:  0.9944
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 114/768 type remove
Layer 4 : 169/768 type remove
Layer 5 : 218/768 type remove
Layer 6 : 223/768 type remove
Layer 7 : 273/768 type remove
Layer 8 : 316/768 type remove
Layer 9 : 360/768 type retain
Layer 10 : 314/768 type retain
Layer 11 : 341/768 type remove
Layer 12 : 279/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:57:41,473 [trainer.py] => Time:110.65586376190186
4375 4375
4375 4375
2025-12-11 19:57:54,447 [trainer.py] => Time:12.97301435470581
2025-12-11 19:57:54,447 [inflora.py] => Exemplar size: 0
2025-12-11 19:57:54,447 [trainer.py] => CNN: {'total': np.float64(37.33), '00-01': np.float64(59.14), '02-03': np.float64(35.48), '04-05': np.float64(42.7), '06-07': np.float64(62.5), '08-09': np.float64(36.99), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(23.21), '16-17': np.float64(12.86), '18-19': np.float64(50.0), '20-21': np.float64(54.95), '22-23': np.float64(62.5), '24-25': np.float64(12.24), '26-27': np.float64(61.74), '28-29': np.float64(55.56), '30-31': np.float64(35.45), '32-33': np.float64(0.0), '34-35': np.float64(22.58), '36-37': np.float64(51.67), '38-39': np.float64(56.1), '40-41': np.float64(25.93), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(47.87), '50-51': np.float64(21.62), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(1.82), '60-61': np.float64(34.15), '62-63': np.float64(40.57), '64-65': np.float64(0.0), '66-67': np.float64(31.58), '68-69': np.float64(4.17), '70-71': np.float64(24.66), '72-73': np.float64(32.5), '74-75': np.float64(60.26), '76-77': np.float64(42.17), '78-79': np.float64(24.39), '80-81': np.float64(51.39), '82-83': np.float64(16.1), '84-85': np.float64(45.65), '86-87': np.float64(24.14), '88-89': np.float64(53.33), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(15.49), '96-97': np.float64(18.18), '98-99': np.float64(32.43), '100-101': np.float64(41.67), '102-103': np.float64(50.0), '104-105': np.float64(10.64), '106-107': np.float64(28.21), '108-109': np.float64(55.22), '110-111': np.float64(20.0), '112-113': np.float64(73.08), '114-115': np.float64(47.5), '116-117': np.float64(63.89), '118-119': np.float64(38.46), '120-121': np.float64(59.02), '122-123': np.float64(17.86), '124-125': np.float64(28.57), '126-127': np.float64(5.66), '128-129': np.float64(7.59), '130-131': np.float64(44.44), '132-133': np.float64(47.06), '134-135': np.float64(60.26), '136-137': np.float64(57.14), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(45.83), '144-145': np.float64(65.22), 'old': np.float64(37.03), 'new': np.float64(65.22)}
2025-12-11 19:57:54,447 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33)]
2025-12-11 19:57:54,447 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98)]
2025-12-11 19:57:54,448 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284]
2025-12-11 19:58:08,794 [trainer.py] => W-NCM: {'00-01': 63.44086021505376, '02-03': 59.67741935483871, '04-05': 51.68539325842697, '06-07': 76.78571428571429, '08-09': 61.64383561643836, '10-11': 53.65853658536586, '12-13': 83.33333333333334, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 69.64285714285714, '20-21': 67.03296703296702, '22-23': 60.9375, '24-25': 57.14285714285714, '26-27': 46.95652173913044, '28-29': 61.72839506172839, '30-31': 30.909090909090907, '32-33': 86.36363636363636, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 73.17073170731707, '40-41': 50.0, '42-43': 62.5, '44-45': 40.74074074074074, '46-47': 74.07407407407408, '48-49': 67.02127659574468, '50-51': 75.67567567567568, '52-53': 86.11111111111111, '54-55': 52.0, '56-57': 87.5, '58-59': 45.45454545454545, '60-61': 70.73170731707317, '62-63': 67.9245283018868, '64-65': 66.66666666666666, '66-67': 36.84210526315789, '68-69': 66.66666666666666, '70-71': 56.16438356164384, '72-73': 67.5, '74-75': 34.61538461538461, '76-77': 68.67469879518072, '78-79': 54.87804878048781, '80-81': 76.38888888888889, '82-83': 43.22033898305085, '84-85': 76.08695652173914, '86-87': 72.41379310344827, '88-89': 86.66666666666667, '90-91': 67.5, '92-93': 20.408163265306122, '94-95': 76.05633802816901, '96-97': 48.484848484848484, '98-99': 51.35135135135135, '100-101': 63.33333333333333, '102-103': 62.06896551724138, '104-105': 51.06382978723404, '106-107': 61.53846153846154, '108-109': 58.2089552238806, '110-111': 65.0, '112-113': 76.92307692307693, '114-115': 67.5, '116-117': 91.66666666666666, '118-119': 80.76923076923077, '120-121': 80.32786885245902, '122-123': 60.71428571428571, '124-125': 36.734693877551024, '126-127': 49.056603773584904, '128-129': 48.10126582278481, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 57.692307692307686, '136-137': 75.51020408163265, '138-139': 79.24528301886792, '140-141': 53.191489361702125, '142-143': 91.66666666666666, '144-145': 95.65217391304348}
2025-12-11 19:58:08,794 [trainer.py] => Ave Acc (W-NCM): 64.26%
2025-12-11 19:58:08,794 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 63.44% (best 97.85%); T2: W-NCM 59.68% (best 90.32%); T3: W-NCM 51.69% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 67.03% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 46.96% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 30.91% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 50.00% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 67.02% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 45.45% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 67.92% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 36.84% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 34.62% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 54.88% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 43.22% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 76.06% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 51.35% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 62.07% (best 87.93%); T53: W-NCM 51.06% (best 79.79%); T54: W-NCM 61.54% (best 89.74%); T55: W-NCM 58.21% (best 91.04%); T56: W-NCM 65.00% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 80.77% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 60.71% (best 94.64%); T63: W-NCM 36.73% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 48.10% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 57.69% (best 85.90%); T69: W-NCM 75.51% (best 93.88%); T70: W-NCM 79.25% (best 94.34%); T71: W-NCM 53.19% (best 87.23%); T72: W-NCM 91.67% (best 91.67%); T73: W-NCM 95.65% (best 95.65%)
2025-12-11 19:58:08,794 [trainer.py] => Average forgetting (W-NCM): 27.91% | Max forgetting (W-NCM): 65.31%
2025-12-11 19:58:08,807 [trainer.py] => All params: 144526051
2025-12-11 19:58:08,819 [trainer.py] => Trainable params: 185858
2025-12-11 19:58:08,819 [inflora.py] => Learning on 146-148
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.73.weight', 'image_encoder.blocks.6.attn.lora_B_v.73.weight', 'image_encoder.blocks.6.attn.lora_B_k.73.weight', 'image_encoder.blocks.0.attn.lora_B_k.73.weight', 'image_encoder.blocks.9.attn.lora_B_v.73.weight', 'image_encoder.blocks.3.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_k.73.weight', 'classifier_pool.73.weight', 'image_encoder.blocks.0.attn.lora_B_v.73.weight', 'image_encoder.blocks.11.attn.lora_B_k.73.weight', 'image_encoder.blocks.8.attn.lora_B_k.73.weight', 'image_encoder.blocks.11.attn.lora_B_v.73.weight', 'image_encoder.blocks.10.attn.lora_B_v.73.weight', 'image_encoder.blocks.8.attn.lora_B_v.73.weight', 'image_encoder.blocks.2.attn.lora_B_v.73.weight', 'image_encoder.blocks.10.attn.lora_B_k.73.weight', 'image_encoder.blocks.3.attn.lora_B_k.73.weight', 'image_encoder.blocks.5.attn.lora_B_v.73.weight', 'image_encoder.blocks.5.attn.lora_B_k.73.weight', 'image_encoder.blocks.2.attn.lora_B_k.73.weight', 'image_encoder.blocks.4.attn.lora_B_v.73.weight', 'image_encoder.blocks.7.attn.lora_B_v.73.weight', 'image_encoder.blocks.9.attn.lora_B_k.73.weight', 'image_encoder.blocks.1.attn.lora_B_v.73.weight', 'image_encoder.blocks.4.attn.lora_B_k.73.weight', 'classifier_pool.73.bias'}
2025-12-11 19:59:42,130 [inflora.py] => Task 73, Epoch 50/50 => Loss 0.067, Train_accy 97.81
Threshold:  0.9946
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 115/768 type remove
Layer 4 : 170/768 type remove
Layer 5 : 219/768 type remove
Layer 6 : 225/768 type remove
Layer 7 : 275/768 type remove
Layer 8 : 319/768 type remove
Layer 9 : 356/768 type retain
Layer 10 : 310/768 type retain
Layer 11 : 349/768 type remove
Layer 12 : 270/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 19:59:48,796 [trainer.py] => Time:99.97733545303345
4402 4402
4402 4402
2025-12-11 20:00:01,830 [trainer.py] => Time:13.033303022384644
2025-12-11 20:00:01,830 [inflora.py] => Exemplar size: 0
2025-12-11 20:00:01,830 [trainer.py] => CNN: {'total': np.float64(37.78), '00-01': np.float64(63.44), '02-03': np.float64(37.1), '04-05': np.float64(40.45), '06-07': np.float64(62.5), '08-09': np.float64(39.73), '10-11': np.float64(14.63), '12-13': np.float64(40.74), '14-15': np.float64(21.43), '16-17': np.float64(12.86), '18-19': np.float64(55.36), '20-21': np.float64(53.85), '22-23': np.float64(62.5), '24-25': np.float64(14.29), '26-27': np.float64(62.61), '28-29': np.float64(56.79), '30-31': np.float64(34.55), '32-33': np.float64(6.82), '34-35': np.float64(16.13), '36-37': np.float64(51.67), '38-39': np.float64(56.1), '40-41': np.float64(25.93), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(46.81), '50-51': np.float64(21.62), '52-53': np.float64(47.22), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(1.82), '60-61': np.float64(34.15), '62-63': np.float64(42.45), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(27.4), '72-73': np.float64(35.0), '74-75': np.float64(58.97), '76-77': np.float64(44.58), '78-79': np.float64(23.17), '80-81': np.float64(54.17), '82-83': np.float64(16.1), '84-85': np.float64(45.65), '86-87': np.float64(20.69), '88-89': np.float64(53.33), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(18.31), '96-97': np.float64(20.2), '98-99': np.float64(28.38), '100-101': np.float64(43.33), '102-103': np.float64(48.28), '104-105': np.float64(8.51), '106-107': np.float64(28.21), '108-109': np.float64(55.22), '110-111': np.float64(18.33), '112-113': np.float64(78.85), '114-115': np.float64(50.0), '116-117': np.float64(63.89), '118-119': np.float64(36.54), '120-121': np.float64(60.66), '122-123': np.float64(17.86), '124-125': np.float64(28.57), '126-127': np.float64(5.66), '128-129': np.float64(7.59), '130-131': np.float64(48.89), '132-133': np.float64(44.12), '134-135': np.float64(58.97), '136-137': np.float64(59.18), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(45.83), '144-145': np.float64(67.39), '146-147': np.float64(48.15), 'old': np.float64(37.71), 'new': np.float64(48.15)}
2025-12-11 20:00:01,831 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78)]
2025-12-11 20:00:01,831 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84)]
2025-12-11 20:00:01,831 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872]
2025-12-11 20:00:16,319 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 58.06451612903226, '04-05': 50.56179775280899, '06-07': 76.78571428571429, '08-09': 61.64383561643836, '10-11': 51.21951219512195, '12-13': 83.33333333333334, '14-15': 69.64285714285714, '16-17': 67.14285714285714, '18-19': 67.85714285714286, '20-21': 69.23076923076923, '22-23': 62.5, '24-25': 55.10204081632652, '26-27': 47.82608695652174, '28-29': 61.72839506172839, '30-31': 30.0, '32-33': 84.0909090909091, '34-35': 61.29032258064516, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 48.148148148148145, '42-43': 62.5, '44-45': 40.74074074074074, '46-47': 70.37037037037037, '48-49': 68.08510638297872, '50-51': 75.67567567567568, '52-53': 83.33333333333334, '54-55': 52.0, '56-57': 80.0, '58-59': 43.63636363636363, '60-61': 68.29268292682927, '62-63': 68.86792452830188, '64-65': 63.33333333333333, '66-67': 31.57894736842105, '68-69': 66.66666666666666, '70-71': 53.42465753424658, '72-73': 60.0, '74-75': 25.64102564102564, '76-77': 66.26506024096386, '78-79': 52.4390243902439, '80-81': 75.0, '82-83': 39.83050847457627, '84-85': 76.08695652173914, '86-87': 73.5632183908046, '88-89': 86.66666666666667, '90-91': 62.5, '92-93': 20.408163265306122, '94-95': 76.05633802816901, '96-97': 44.44444444444444, '98-99': 52.702702702702695, '100-101': 65.0, '102-103': 62.06896551724138, '104-105': 51.06382978723404, '106-107': 56.41025641025641, '108-109': 62.68656716417911, '110-111': 65.0, '112-113': 76.92307692307693, '114-115': 62.5, '116-117': 91.66666666666666, '118-119': 76.92307692307693, '120-121': 78.68852459016394, '122-123': 57.14285714285714, '124-125': 32.6530612244898, '126-127': 41.509433962264154, '128-129': 41.77215189873418, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 52.56410256410257, '136-137': 69.38775510204081, '138-139': 73.58490566037736, '140-141': 57.446808510638306, '142-143': 89.58333333333334, '144-145': 91.30434782608695, '146-147': 96.29629629629629}
2025-12-11 20:00:16,320 [trainer.py] => Ave Acc (W-NCM): 62.94%
2025-12-11 20:00:16,320 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 58.06% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 69.23% (best 95.60%); T12: W-NCM 62.50% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 47.83% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 30.00% (best 93.64%); T17: W-NCM 84.09% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 48.15% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 68.09% (best 94.68%); T26: W-NCM 75.68% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 80.00% (best 95.00%); T30: W-NCM 43.64% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 68.87% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 31.58% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 53.42% (best 91.78%); T37: W-NCM 60.00% (best 92.50%); T38: W-NCM 25.64% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 52.44% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 39.83% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 73.56% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 76.06% (best 95.77%); T49: W-NCM 44.44% (best 84.85%); T50: W-NCM 52.70% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 62.07% (best 87.93%); T53: W-NCM 51.06% (best 79.79%); T54: W-NCM 56.41% (best 89.74%); T55: W-NCM 62.69% (best 91.04%); T56: W-NCM 65.00% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 62.50% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 57.14% (best 94.64%); T63: W-NCM 32.65% (best 93.88%); T64: W-NCM 41.51% (best 84.91%); T65: W-NCM 41.77% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 52.56% (best 85.90%); T69: W-NCM 69.39% (best 93.88%); T70: W-NCM 73.58% (best 94.34%); T71: W-NCM 57.45% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 91.30% (best 95.65%); T74: W-NCM 96.30% (best 96.30%)
2025-12-11 20:00:16,320 [trainer.py] => Average forgetting (W-NCM): 29.31% | Max forgetting (W-NCM): 68.42%
2025-12-11 20:00:16,332 [trainer.py] => All params: 144526051
2025-12-11 20:00:16,344 [trainer.py] => Trainable params: 185858
2025-12-11 20:00:16,344 [inflora.py] => Learning on 148-150
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.74.weight', 'image_encoder.blocks.6.attn.lora_B_v.74.weight', 'image_encoder.blocks.8.attn.lora_B_k.74.weight', 'image_encoder.blocks.11.attn.lora_B_v.74.weight', 'image_encoder.blocks.11.attn.lora_B_k.74.weight', 'image_encoder.blocks.4.attn.lora_B_v.74.weight', 'classifier_pool.74.bias', 'image_encoder.blocks.1.attn.lora_B_v.74.weight', 'image_encoder.blocks.2.attn.lora_B_k.74.weight', 'image_encoder.blocks.0.attn.lora_B_v.74.weight', 'image_encoder.blocks.2.attn.lora_B_v.74.weight', 'image_encoder.blocks.10.attn.lora_B_k.74.weight', 'image_encoder.blocks.5.attn.lora_B_k.74.weight', 'classifier_pool.74.weight', 'image_encoder.blocks.1.attn.lora_B_k.74.weight', 'image_encoder.blocks.10.attn.lora_B_v.74.weight', 'image_encoder.blocks.8.attn.lora_B_v.74.weight', 'image_encoder.blocks.9.attn.lora_B_k.74.weight', 'image_encoder.blocks.5.attn.lora_B_v.74.weight', 'image_encoder.blocks.3.attn.lora_B_v.74.weight', 'image_encoder.blocks.7.attn.lora_B_v.74.weight', 'image_encoder.blocks.6.attn.lora_B_k.74.weight', 'image_encoder.blocks.7.attn.lora_B_k.74.weight', 'image_encoder.blocks.3.attn.lora_B_k.74.weight', 'image_encoder.blocks.9.attn.lora_B_v.74.weight', 'image_encoder.blocks.4.attn.lora_B_k.74.weight'}
2025-12-11 20:01:42,197 [inflora.py] => Task 74, Epoch 50/50 => Loss 0.059, Train_accy 98.40
Threshold:  0.9948
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 3
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 115/768 type remove
Layer 4 : 171/768 type remove
Layer 5 : 220/768 type remove
Layer 6 : 227/768 type remove
Layer 7 : 278/768 type remove
Layer 8 : 321/768 type remove
Layer 9 : 353/768 type retain
Layer 10 : 306/768 type retain
Layer 11 : 356/768 type remove
Layer 12 : 266/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:01:48,105 [trainer.py] => Time:91.76124930381775
4435 4435
4435 4435
2025-12-11 20:02:01,223 [trainer.py] => Time:13.117063283920288
2025-12-11 20:02:01,223 [inflora.py] => Exemplar size: 0
2025-12-11 20:02:01,223 [trainer.py] => CNN: {'total': np.float64(36.64), '00-01': np.float64(58.06), '02-03': np.float64(35.48), '04-05': np.float64(38.2), '06-07': np.float64(64.29), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(37.04), '14-15': np.float64(19.64), '16-17': np.float64(14.29), '18-19': np.float64(55.36), '20-21': np.float64(51.65), '22-23': np.float64(61.72), '24-25': np.float64(16.33), '26-27': np.float64(63.48), '28-29': np.float64(50.62), '30-31': np.float64(32.73), '32-33': np.float64(4.55), '34-35': np.float64(29.03), '36-37': np.float64(55.0), '38-39': np.float64(51.22), '40-41': np.float64(25.93), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(44.68), '50-51': np.float64(24.32), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(27.5), '58-59': np.float64(1.82), '60-61': np.float64(34.15), '62-63': np.float64(42.45), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(30.0), '74-75': np.float64(56.41), '76-77': np.float64(38.55), '78-79': np.float64(23.17), '80-81': np.float64(51.39), '82-83': np.float64(16.95), '84-85': np.float64(54.35), '86-87': np.float64(13.79), '88-89': np.float64(46.67), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(11.27), '96-97': np.float64(17.17), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(44.83), '104-105': np.float64(11.7), '106-107': np.float64(30.77), '108-109': np.float64(55.22), '110-111': np.float64(16.67), '112-113': np.float64(76.92), '114-115': np.float64(47.5), '116-117': np.float64(52.78), '118-119': np.float64(34.62), '120-121': np.float64(63.93), '122-123': np.float64(19.64), '124-125': np.float64(24.49), '126-127': np.float64(11.32), '128-129': np.float64(8.86), '130-131': np.float64(44.44), '132-133': np.float64(44.12), '134-135': np.float64(61.54), '136-137': np.float64(53.06), '138-139': np.float64(56.6), '140-141': np.float64(34.04), '142-143': np.float64(45.83), '144-145': np.float64(65.22), '146-147': np.float64(51.85), '148-149': np.float64(33.33), 'old': np.float64(36.67), 'new': np.float64(33.33)}
2025-12-11 20:02:01,224 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64)]
2025-12-11 20:02:01,224 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81)]
2025-12-11 20:02:01,224 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909]
2025-12-11 20:02:15,663 [trainer.py] => W-NCM: {'00-01': 63.44086021505376, '02-03': 51.61290322580645, '04-05': 49.43820224719101, '06-07': 73.21428571428571, '08-09': 61.64383561643836, '10-11': 46.34146341463415, '12-13': 81.48148148148148, '14-15': 67.85714285714286, '16-17': 67.14285714285714, '18-19': 64.28571428571429, '20-21': 68.13186813186813, '22-23': 61.71875, '24-25': 59.183673469387756, '26-27': 40.869565217391305, '28-29': 61.72839506172839, '30-31': 26.36363636363636, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 75.0, '38-39': 73.17073170731707, '40-41': 53.70370370370371, '42-43': 53.125, '44-45': 40.74074074074074, '46-47': 74.07407407407408, '48-49': 65.95744680851064, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 82.5, '58-59': 45.45454545454545, '60-61': 68.29268292682927, '62-63': 66.98113207547169, '64-65': 63.33333333333333, '66-67': 42.10526315789473, '68-69': 62.5, '70-71': 54.794520547945204, '72-73': 65.0, '74-75': 20.51282051282051, '76-77': 63.85542168674698, '78-79': 50.0, '80-81': 70.83333333333334, '82-83': 38.13559322033898, '84-85': 76.08695652173914, '86-87': 72.41379310344827, '88-89': 85.0, '90-91': 65.0, '92-93': 20.408163265306122, '94-95': 71.83098591549296, '96-97': 44.44444444444444, '98-99': 50.0, '100-101': 61.66666666666667, '102-103': 63.793103448275865, '104-105': 48.93617021276596, '106-107': 53.84615384615385, '108-109': 61.19402985074627, '110-111': 65.0, '112-113': 76.92307692307693, '114-115': 60.0, '116-117': 86.11111111111111, '118-119': 75.0, '120-121': 77.04918032786885, '122-123': 57.14285714285714, '124-125': 30.612244897959183, '126-127': 35.84905660377358, '128-129': 27.848101265822784, '130-131': 68.88888888888889, '132-133': 79.41176470588235, '134-135': 52.56410256410257, '136-137': 63.26530612244898, '138-139': 66.0377358490566, '140-141': 53.191489361702125, '142-143': 87.5, '144-145': 91.30434782608695, '146-147': 88.88888888888889, '148-149': 90.9090909090909}
2025-12-11 20:02:15,664 [trainer.py] => Ave Acc (W-NCM): 61.86%
2025-12-11 20:02:15,664 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 63.44% (best 97.85%); T2: W-NCM 51.61% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 73.21% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 46.34% (best 80.49%); T7: W-NCM 81.48% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 64.29% (best 94.64%); T11: W-NCM 68.13% (best 95.60%); T12: W-NCM 61.72% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 40.87% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 26.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 73.17% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 53.12% (best 96.88%); T23: W-NCM 40.74% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 65.96% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 45.45% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 66.98% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 54.79% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 20.51% (best 87.18%); T39: W-NCM 63.86% (best 78.31%); T40: W-NCM 50.00% (best 89.02%); T41: W-NCM 70.83% (best 97.22%); T42: W-NCM 38.14% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 85.00% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 71.83% (best 95.77%); T49: W-NCM 44.44% (best 84.85%); T50: W-NCM 50.00% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 63.79% (best 87.93%); T53: W-NCM 48.94% (best 79.79%); T54: W-NCM 53.85% (best 89.74%); T55: W-NCM 61.19% (best 91.04%); T56: W-NCM 65.00% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 86.11% (best 97.22%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 57.14% (best 94.64%); T63: W-NCM 30.61% (best 93.88%); T64: W-NCM 35.85% (best 84.91%); T65: W-NCM 27.85% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 52.56% (best 85.90%); T69: W-NCM 63.27% (best 93.88%); T70: W-NCM 66.04% (best 94.34%); T71: W-NCM 53.19% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 91.30% (best 95.65%); T74: W-NCM 88.89% (best 96.30%); T75: W-NCM 90.91% (best 90.91%)
2025-12-11 20:02:15,664 [trainer.py] => Average forgetting (W-NCM): 30.38% | Max forgetting (W-NCM): 67.27%
2025-12-11 20:02:15,677 [trainer.py] => All params: 144526051
2025-12-11 20:02:15,689 [trainer.py] => Trainable params: 185858
2025-12-11 20:02:15,689 [inflora.py] => Learning on 150-152
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.75.weight', 'image_encoder.blocks.5.attn.lora_B_v.75.weight', 'image_encoder.blocks.0.attn.lora_B_v.75.weight', 'image_encoder.blocks.2.attn.lora_B_k.75.weight', 'image_encoder.blocks.8.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_k.75.weight', 'image_encoder.blocks.11.attn.lora_B_k.75.weight', 'image_encoder.blocks.1.attn.lora_B_v.75.weight', 'image_encoder.blocks.6.attn.lora_B_k.75.weight', 'image_encoder.blocks.1.attn.lora_B_k.75.weight', 'image_encoder.blocks.3.attn.lora_B_v.75.weight', 'image_encoder.blocks.7.attn.lora_B_k.75.weight', 'image_encoder.blocks.6.attn.lora_B_v.75.weight', 'image_encoder.blocks.9.attn.lora_B_k.75.weight', 'image_encoder.blocks.9.attn.lora_B_v.75.weight', 'image_encoder.blocks.11.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_k.75.weight', 'image_encoder.blocks.10.attn.lora_B_v.75.weight', 'image_encoder.blocks.2.attn.lora_B_v.75.weight', 'classifier_pool.75.bias', 'classifier_pool.75.weight', 'image_encoder.blocks.8.attn.lora_B_v.75.weight', 'image_encoder.blocks.0.attn.lora_B_k.75.weight', 'image_encoder.blocks.5.attn.lora_B_k.75.weight', 'image_encoder.blocks.7.attn.lora_B_v.75.weight', 'image_encoder.blocks.4.attn.lora_B_v.75.weight'}
2025-12-11 20:03:46,649 [inflora.py] => Task 75, Epoch 50/50 => Loss 0.084, Train_accy 96.38
Threshold:  0.995
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 116/768 type remove
Layer 4 : 172/768 type remove
Layer 5 : 222/768 type remove
Layer 6 : 231/768 type remove
Layer 7 : 284/768 type remove
Layer 8 : 329/768 type remove
Layer 9 : 343/768 type retain
Layer 10 : 296/768 type retain
Layer 11 : 366/768 type remove
Layer 12 : 257/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:03:53,299 [trainer.py] => Time:97.60985851287842
4473 4473
4473 4473
2025-12-11 20:04:06,517 [trainer.py] => Time:13.217212200164795
2025-12-11 20:04:06,517 [inflora.py] => Exemplar size: 0
2025-12-11 20:04:06,518 [trainer.py] => CNN: {'total': np.float64(37.07), '00-01': np.float64(63.44), '02-03': np.float64(35.48), '04-05': np.float64(41.57), '06-07': np.float64(64.29), '08-09': np.float64(35.62), '10-11': np.float64(14.63), '12-13': np.float64(38.89), '14-15': np.float64(23.21), '16-17': np.float64(12.86), '18-19': np.float64(55.36), '20-21': np.float64(58.24), '22-23': np.float64(60.94), '24-25': np.float64(14.29), '26-27': np.float64(60.0), '28-29': np.float64(53.09), '30-31': np.float64(33.64), '32-33': np.float64(0.0), '34-35': np.float64(22.58), '36-37': np.float64(48.33), '38-39': np.float64(43.9), '40-41': np.float64(25.93), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(46.81), '50-51': np.float64(24.32), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(0.0), '60-61': np.float64(36.59), '62-63': np.float64(44.34), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(32.5), '74-75': np.float64(55.13), '76-77': np.float64(37.35), '78-79': np.float64(23.17), '80-81': np.float64(52.78), '82-83': np.float64(18.64), '84-85': np.float64(43.48), '86-87': np.float64(26.44), '88-89': np.float64(48.33), '90-91': np.float64(50.0), '92-93': np.float64(12.24), '94-95': np.float64(16.9), '96-97': np.float64(15.15), '98-99': np.float64(27.03), '100-101': np.float64(45.0), '102-103': np.float64(48.28), '104-105': np.float64(9.57), '106-107': np.float64(30.77), '108-109': np.float64(56.72), '110-111': np.float64(18.33), '112-113': np.float64(75.0), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(38.46), '120-121': np.float64(60.66), '122-123': np.float64(19.64), '124-125': np.float64(28.57), '126-127': np.float64(5.66), '128-129': np.float64(12.66), '130-131': np.float64(48.89), '132-133': np.float64(47.06), '134-135': np.float64(58.97), '136-137': np.float64(51.02), '138-139': np.float64(58.49), '140-141': np.float64(38.3), '142-143': np.float64(43.75), '144-145': np.float64(65.22), '146-147': np.float64(48.15), '148-149': np.float64(36.36), '150-151': np.float64(7.89), 'old': np.float64(37.32), 'new': np.float64(7.89)}
2025-12-11 20:04:06,518 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07)]
2025-12-11 20:04:06,518 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62)]
2025-12-11 20:04:06,518 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275]
2025-12-11 20:04:21,250 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 59.67741935483871, '04-05': 51.68539325842697, '06-07': 75.0, '08-09': 63.013698630136986, '10-11': 48.78048780487805, '12-13': 85.18518518518519, '14-15': 69.64285714285714, '16-17': 72.85714285714285, '18-19': 67.85714285714286, '20-21': 71.42857142857143, '22-23': 67.96875, '24-25': 57.14285714285714, '26-27': 50.43478260869565, '28-29': 64.19753086419753, '30-31': 30.909090909090907, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 80.0, '38-39': 75.60975609756098, '40-41': 51.85185185185185, '42-43': 59.375, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 74.46808510638297, '50-51': 70.27027027027027, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 85.0, '58-59': 52.72727272727272, '60-61': 70.73170731707317, '62-63': 73.58490566037736, '64-65': 63.33333333333333, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 54.794520547945204, '72-73': 65.0, '74-75': 30.76923076923077, '76-77': 67.46987951807229, '78-79': 56.09756097560976, '80-81': 75.0, '82-83': 39.83050847457627, '84-85': 78.26086956521739, '86-87': 74.71264367816092, '88-89': 85.0, '90-91': 67.5, '92-93': 20.408163265306122, '94-95': 78.87323943661971, '96-97': 48.484848484848484, '98-99': 52.702702702702695, '100-101': 65.0, '102-103': 67.24137931034483, '104-105': 52.12765957446809, '106-107': 64.1025641025641, '108-109': 64.17910447761194, '110-111': 66.66666666666666, '112-113': 76.92307692307693, '114-115': 65.0, '116-117': 91.66666666666666, '118-119': 78.84615384615384, '120-121': 78.68852459016394, '122-123': 57.14285714285714, '124-125': 32.6530612244898, '126-127': 45.28301886792453, '128-129': 36.708860759493675, '130-131': 71.11111111111111, '132-133': 82.35294117647058, '134-135': 51.28205128205128, '136-137': 73.46938775510205, '138-139': 69.81132075471697, '140-141': 55.319148936170215, '142-143': 89.58333333333334, '144-145': 91.30434782608695, '146-147': 88.88888888888889, '148-149': 81.81818181818183, '150-151': 84.21052631578947}
2025-12-11 20:04:21,250 [trainer.py] => Ave Acc (W-NCM): 65.24%
2025-12-11 20:04:21,251 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 59.68% (best 90.32%); T3: W-NCM 51.69% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 48.78% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 67.97% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 50.43% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 30.91% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 75.61% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 74.47% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 73.58% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 54.79% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 30.77% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 39.83% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 85.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 52.70% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 52.13% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 66.67% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 57.14% (best 94.64%); T63: W-NCM 32.65% (best 93.88%); T64: W-NCM 45.28% (best 84.91%); T65: W-NCM 36.71% (best 92.41%); T66: W-NCM 71.11% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 51.28% (best 85.90%); T69: W-NCM 73.47% (best 93.88%); T70: W-NCM 69.81% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 91.30% (best 95.65%); T74: W-NCM 88.89% (best 96.30%); T75: W-NCM 81.82% (best 90.91%); T76: W-NCM 84.21% (best 84.21%)
2025-12-11 20:04:21,251 [trainer.py] => Average forgetting (W-NCM): 26.86% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:04:21,263 [trainer.py] => All params: 144526051
2025-12-11 20:04:21,276 [trainer.py] => Trainable params: 185858
2025-12-11 20:04:21,276 [inflora.py] => Learning on 152-154
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.76.weight', 'image_encoder.blocks.0.attn.lora_B_v.76.weight', 'image_encoder.blocks.6.attn.lora_B_k.76.weight', 'image_encoder.blocks.11.attn.lora_B_k.76.weight', 'image_encoder.blocks.6.attn.lora_B_v.76.weight', 'image_encoder.blocks.1.attn.lora_B_k.76.weight', 'image_encoder.blocks.7.attn.lora_B_k.76.weight', 'classifier_pool.76.weight', 'image_encoder.blocks.5.attn.lora_B_k.76.weight', 'image_encoder.blocks.10.attn.lora_B_k.76.weight', 'image_encoder.blocks.7.attn.lora_B_v.76.weight', 'image_encoder.blocks.9.attn.lora_B_k.76.weight', 'image_encoder.blocks.0.attn.lora_B_k.76.weight', 'classifier_pool.76.bias', 'image_encoder.blocks.2.attn.lora_B_k.76.weight', 'image_encoder.blocks.5.attn.lora_B_v.76.weight', 'image_encoder.blocks.10.attn.lora_B_v.76.weight', 'image_encoder.blocks.9.attn.lora_B_v.76.weight', 'image_encoder.blocks.4.attn.lora_B_v.76.weight', 'image_encoder.blocks.1.attn.lora_B_v.76.weight', 'image_encoder.blocks.4.attn.lora_B_k.76.weight', 'image_encoder.blocks.3.attn.lora_B_k.76.weight', 'image_encoder.blocks.11.attn.lora_B_v.76.weight', 'image_encoder.blocks.2.attn.lora_B_v.76.weight', 'image_encoder.blocks.8.attn.lora_B_k.76.weight', 'image_encoder.blocks.8.attn.lora_B_v.76.weight'}
2025-12-11 20:06:09,059 [inflora.py] => Task 76, Epoch 50/50 => Loss 0.096, Train_accy 96.93
Threshold:  0.9952
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 58/768 type remove
Layer 3 : 117/768 type remove
Layer 4 : 174/768 type remove
Layer 5 : 224/768 type remove
Layer 6 : 235/768 type remove
Layer 7 : 289/768 type remove
Layer 8 : 334/768 type remove
Layer 9 : 339/768 type retain
Layer 10 : 293/768 type retain
Layer 11 : 370/768 type remove
Layer 12 : 252/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:06:16,136 [trainer.py] => Time:114.8603789806366
4510 4510
4510 4510
2025-12-11 20:06:29,531 [trainer.py] => Time:13.394573211669922
2025-12-11 20:06:29,531 [inflora.py] => Exemplar size: 0
2025-12-11 20:06:29,532 [trainer.py] => CNN: {'total': np.float64(36.39), '00-01': np.float64(59.14), '02-03': np.float64(35.48), '04-05': np.float64(37.08), '06-07': np.float64(64.29), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(38.89), '14-15': np.float64(28.57), '16-17': np.float64(18.57), '18-19': np.float64(55.36), '20-21': np.float64(57.14), '22-23': np.float64(64.06), '24-25': np.float64(12.24), '26-27': np.float64(64.35), '28-29': np.float64(51.85), '30-31': np.float64(34.55), '32-33': np.float64(2.27), '34-35': np.float64(25.81), '36-37': np.float64(55.0), '38-39': np.float64(41.46), '40-41': np.float64(27.78), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(44.68), '50-51': np.float64(24.32), '52-53': np.float64(41.67), '54-55': np.float64(0.0), '56-57': np.float64(27.5), '58-59': np.float64(0.0), '60-61': np.float64(36.59), '62-63': np.float64(43.4), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(4.17), '70-71': np.float64(27.4), '72-73': np.float64(37.5), '74-75': np.float64(56.41), '76-77': np.float64(34.94), '78-79': np.float64(20.73), '80-81': np.float64(41.67), '82-83': np.float64(15.25), '84-85': np.float64(56.52), '86-87': np.float64(18.39), '88-89': np.float64(45.0), '90-91': np.float64(52.5), '92-93': np.float64(12.24), '94-95': np.float64(9.86), '96-97': np.float64(15.15), '98-99': np.float64(29.73), '100-101': np.float64(43.33), '102-103': np.float64(46.55), '104-105': np.float64(7.45), '106-107': np.float64(46.15), '108-109': np.float64(53.73), '110-111': np.float64(18.33), '112-113': np.float64(73.08), '114-115': np.float64(50.0), '116-117': np.float64(52.78), '118-119': np.float64(30.77), '120-121': np.float64(60.66), '122-123': np.float64(21.43), '124-125': np.float64(30.61), '126-127': np.float64(11.32), '128-129': np.float64(11.39), '130-131': np.float64(46.67), '132-133': np.float64(41.18), '134-135': np.float64(64.1), '136-137': np.float64(51.02), '138-139': np.float64(54.72), '140-141': np.float64(38.3), '142-143': np.float64(41.67), '144-145': np.float64(63.04), '146-147': np.float64(44.44), '148-149': np.float64(30.3), '150-151': np.float64(7.89), '152-153': np.float64(10.81), 'old': np.float64(36.6), 'new': np.float64(10.81)}
2025-12-11 20:06:29,532 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39)]
2025-12-11 20:06:29,532 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61)]
2025-12-11 20:06:29,532 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408]
2025-12-11 20:06:44,710 [trainer.py] => W-NCM: {'00-01': 55.91397849462365, '02-03': 50.0, '04-05': 48.31460674157304, '06-07': 73.21428571428571, '08-09': 61.64383561643836, '10-11': 43.90243902439025, '12-13': 81.48148148148148, '14-15': 66.07142857142857, '16-17': 61.42857142857143, '18-19': 57.14285714285714, '20-21': 68.13186813186813, '22-23': 57.8125, '24-25': 55.10204081632652, '26-27': 42.608695652173914, '28-29': 58.0246913580247, '30-31': 25.454545454545453, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 75.0, '38-39': 63.41463414634146, '40-41': 51.85185185185185, '42-43': 59.375, '44-45': 44.44444444444444, '46-47': 74.07407407407408, '48-49': 67.02127659574468, '50-51': 62.16216216216216, '52-53': 80.55555555555556, '54-55': 48.0, '56-57': 82.5, '58-59': 40.0, '60-61': 70.73170731707317, '62-63': 68.86792452830188, '64-65': 60.0, '66-67': 36.84210526315789, '68-69': 62.5, '70-71': 53.42465753424658, '72-73': 65.0, '74-75': 25.64102564102564, '76-77': 66.26506024096386, '78-79': 51.21951219512195, '80-81': 62.5, '82-83': 34.74576271186441, '84-85': 76.08695652173914, '86-87': 70.11494252873564, '88-89': 83.33333333333334, '90-91': 55.00000000000001, '92-93': 20.408163265306122, '94-95': 71.83098591549296, '96-97': 44.44444444444444, '98-99': 48.64864864864865, '100-101': 60.0, '102-103': 63.793103448275865, '104-105': 50.0, '106-107': 64.1025641025641, '108-109': 58.2089552238806, '110-111': 63.33333333333333, '112-113': 76.92307692307693, '114-115': 60.0, '116-117': 88.88888888888889, '118-119': 75.0, '120-121': 75.40983606557377, '122-123': 60.71428571428571, '124-125': 28.57142857142857, '126-127': 39.62264150943396, '128-129': 29.11392405063291, '130-131': 68.88888888888889, '132-133': 73.52941176470588, '134-135': 44.871794871794876, '136-137': 73.46938775510205, '138-139': 69.81132075471697, '140-141': 53.191489361702125, '142-143': 87.5, '144-145': 86.95652173913044, '146-147': 81.48148148148148, '148-149': 78.78787878787878, '150-151': 57.89473684210527, '152-153': 94.5945945945946}
2025-12-11 20:06:44,711 [trainer.py] => Ave Acc (W-NCM): 61.00%
2025-12-11 20:06:44,711 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 55.91% (best 97.85%); T2: W-NCM 50.00% (best 90.32%); T3: W-NCM 48.31% (best 91.01%); T4: W-NCM 73.21% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 43.90% (best 80.49%); T7: W-NCM 81.48% (best 92.59%); T8: W-NCM 66.07% (best 94.64%); T9: W-NCM 61.43% (best 98.57%); T10: W-NCM 57.14% (best 94.64%); T11: W-NCM 68.13% (best 95.60%); T12: W-NCM 57.81% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 42.61% (best 94.78%); T15: W-NCM 58.02% (best 96.30%); T16: W-NCM 25.45% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 67.02% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 40.00% (best 90.91%); T31: W-NCM 70.73% (best 90.24%); T32: W-NCM 68.87% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 36.84% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 53.42% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 25.64% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 51.22% (best 89.02%); T41: W-NCM 62.50% (best 97.22%); T42: W-NCM 34.75% (best 89.83%); T43: W-NCM 76.09% (best 89.13%); T44: W-NCM 70.11% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 55.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 71.83% (best 95.77%); T49: W-NCM 44.44% (best 84.85%); T50: W-NCM 48.65% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 63.79% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 58.21% (best 91.04%); T56: W-NCM 63.33% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 75.41% (best 95.08%); T62: W-NCM 60.71% (best 94.64%); T63: W-NCM 28.57% (best 93.88%); T64: W-NCM 39.62% (best 84.91%); T65: W-NCM 29.11% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 73.53% (best 94.12%); T68: W-NCM 44.87% (best 85.90%); T69: W-NCM 73.47% (best 93.88%); T70: W-NCM 69.81% (best 94.34%); T71: W-NCM 53.19% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 86.96% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 78.79% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 94.59% (best 94.59%)
2025-12-11 20:06:44,711 [trainer.py] => Average forgetting (W-NCM): 31.18% | Max forgetting (W-NCM): 68.18%
2025-12-11 20:06:44,724 [trainer.py] => All params: 144526051
2025-12-11 20:06:44,735 [trainer.py] => Trainable params: 185858
2025-12-11 20:06:44,735 [inflora.py] => Learning on 154-156
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.77.weight', 'image_encoder.blocks.0.attn.lora_B_v.77.weight', 'image_encoder.blocks.0.attn.lora_B_k.77.weight', 'image_encoder.blocks.2.attn.lora_B_v.77.weight', 'image_encoder.blocks.3.attn.lora_B_v.77.weight', 'image_encoder.blocks.1.attn.lora_B_v.77.weight', 'image_encoder.blocks.5.attn.lora_B_v.77.weight', 'image_encoder.blocks.6.attn.lora_B_v.77.weight', 'image_encoder.blocks.4.attn.lora_B_k.77.weight', 'classifier_pool.77.bias', 'image_encoder.blocks.9.attn.lora_B_k.77.weight', 'image_encoder.blocks.1.attn.lora_B_k.77.weight', 'image_encoder.blocks.2.attn.lora_B_k.77.weight', 'image_encoder.blocks.5.attn.lora_B_k.77.weight', 'image_encoder.blocks.4.attn.lora_B_v.77.weight', 'image_encoder.blocks.7.attn.lora_B_v.77.weight', 'image_encoder.blocks.6.attn.lora_B_k.77.weight', 'image_encoder.blocks.3.attn.lora_B_k.77.weight', 'image_encoder.blocks.8.attn.lora_B_v.77.weight', 'image_encoder.blocks.10.attn.lora_B_v.77.weight', 'image_encoder.blocks.9.attn.lora_B_v.77.weight', 'classifier_pool.77.weight', 'image_encoder.blocks.10.attn.lora_B_k.77.weight', 'image_encoder.blocks.8.attn.lora_B_k.77.weight', 'image_encoder.blocks.11.attn.lora_B_k.77.weight', 'image_encoder.blocks.7.attn.lora_B_k.77.weight'}
2025-12-11 20:08:33,146 [inflora.py] => Task 77, Epoch 50/50 => Loss 0.009, Train_accy 100.00
Threshold:  0.9954
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 59/768 type remove
Layer 3 : 127/768 type remove
Layer 4 : 189/768 type remove
Layer 5 : 243/768 type remove
Layer 6 : 255/768 type remove
Layer 7 : 304/768 type remove
Layer 8 : 347/768 type remove
Layer 9 : 326/768 type retain
Layer 10 : 278/768 type retain
Layer 11 : 383/768 type retain
Layer 12 : 245/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:08:40,344 [trainer.py] => Time:115.60888171195984
4560 4560
4560 4560
2025-12-11 20:08:53,855 [trainer.py] => Time:13.510154247283936
2025-12-11 20:08:53,855 [inflora.py] => Exemplar size: 0
2025-12-11 20:08:53,855 [trainer.py] => CNN: {'total': np.float64(35.94), '00-01': np.float64(60.22), '02-03': np.float64(37.1), '04-05': np.float64(39.33), '06-07': np.float64(67.86), '08-09': np.float64(35.62), '10-11': np.float64(9.76), '12-13': np.float64(40.74), '14-15': np.float64(25.0), '16-17': np.float64(18.57), '18-19': np.float64(53.57), '20-21': np.float64(62.64), '22-23': np.float64(63.28), '24-25': np.float64(8.16), '26-27': np.float64(60.87), '28-29': np.float64(56.79), '30-31': np.float64(37.27), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(55.0), '38-39': np.float64(46.34), '40-41': np.float64(25.93), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(44.68), '50-51': np.float64(32.43), '52-53': np.float64(33.33), '54-55': np.float64(0.0), '56-57': np.float64(25.0), '58-59': np.float64(0.0), '60-61': np.float64(41.46), '62-63': np.float64(42.45), '64-65': np.float64(0.0), '66-67': np.float64(36.84), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(37.5), '74-75': np.float64(48.72), '76-77': np.float64(36.14), '78-79': np.float64(20.73), '80-81': np.float64(44.44), '82-83': np.float64(14.41), '84-85': np.float64(54.35), '86-87': np.float64(17.24), '88-89': np.float64(45.0), '90-91': np.float64(47.5), '92-93': np.float64(12.24), '94-95': np.float64(15.49), '96-97': np.float64(15.15), '98-99': np.float64(28.38), '100-101': np.float64(43.33), '102-103': np.float64(44.83), '104-105': np.float64(7.45), '106-107': np.float64(43.59), '108-109': np.float64(53.73), '110-111': np.float64(18.33), '112-113': np.float64(69.23), '114-115': np.float64(45.0), '116-117': np.float64(55.56), '118-119': np.float64(36.54), '120-121': np.float64(60.66), '122-123': np.float64(21.43), '124-125': np.float64(24.49), '126-127': np.float64(11.32), '128-129': np.float64(8.86), '130-131': np.float64(48.89), '132-133': np.float64(38.24), '134-135': np.float64(62.82), '136-137': np.float64(53.06), '138-139': np.float64(49.06), '140-141': np.float64(36.17), '142-143': np.float64(41.67), '144-145': np.float64(65.22), '146-147': np.float64(48.15), '148-149': np.float64(36.36), '150-151': np.float64(7.89), '152-153': np.float64(8.11), '154-155': np.float64(4.0), 'old': np.float64(36.3), 'new': np.float64(4.0)}
2025-12-11 20:08:53,856 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94)]
2025-12-11 20:08:53,856 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46)]
2025-12-11 20:08:53,856 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755]
2025-12-11 20:09:08,955 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 50.0, '04-05': 49.43820224719101, '06-07': 75.0, '08-09': 60.273972602739725, '10-11': 48.78048780487805, '12-13': 81.48148148148148, '14-15': 67.85714285714286, '16-17': 62.857142857142854, '18-19': 62.5, '20-21': 71.42857142857143, '22-23': 59.375, '24-25': 59.183673469387756, '26-27': 43.47826086956522, '28-29': 61.72839506172839, '30-31': 30.0, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 68.29268292682927, '40-41': 55.55555555555556, '42-43': 59.375, '44-45': 44.44444444444444, '46-47': 70.37037037037037, '48-49': 67.02127659574468, '50-51': 59.45945945945946, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 82.5, '58-59': 49.09090909090909, '60-61': 65.85365853658537, '62-63': 71.69811320754717, '64-65': 66.66666666666666, '66-67': 42.10526315789473, '68-69': 62.5, '70-71': 56.16438356164384, '72-73': 65.0, '74-75': 25.64102564102564, '76-77': 63.85542168674698, '78-79': 56.09756097560976, '80-81': 66.66666666666666, '82-83': 41.52542372881356, '84-85': 78.26086956521739, '86-87': 70.11494252873564, '88-89': 83.33333333333334, '90-91': 62.5, '92-93': 18.367346938775512, '94-95': 76.05633802816901, '96-97': 47.474747474747474, '98-99': 56.75675675675676, '100-101': 63.33333333333333, '102-103': 65.51724137931035, '104-105': 46.808510638297875, '106-107': 64.1025641025641, '108-109': 56.71641791044776, '110-111': 63.33333333333333, '112-113': 78.84615384615384, '114-115': 60.0, '116-117': 88.88888888888889, '118-119': 75.0, '120-121': 73.77049180327869, '122-123': 62.5, '124-125': 34.69387755102041, '126-127': 45.28301886792453, '128-129': 31.645569620253166, '130-131': 60.0, '132-133': 73.52941176470588, '134-135': 50.0, '136-137': 73.46938775510205, '138-139': 67.9245283018868, '140-141': 59.57446808510638, '142-143': 89.58333333333334, '144-145': 78.26086956521739, '146-147': 85.18518518518519, '148-149': 81.81818181818183, '150-151': 57.89473684210527, '152-153': 94.5945945945946, '154-155': 94.0}
2025-12-11 20:09:08,955 [trainer.py] => Ave Acc (W-NCM): 63.06%
2025-12-11 20:09:08,955 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 50.00% (best 90.32%); T3: W-NCM 49.44% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 60.27% (best 83.56%); T6: W-NCM 48.78% (best 80.49%); T7: W-NCM 81.48% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 62.86% (best 98.57%); T10: W-NCM 62.50% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 59.38% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 43.48% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 30.00% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 44.44% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 67.02% (best 94.68%); T26: W-NCM 59.46% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 49.09% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 71.70% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 25.64% (best 87.18%); T39: W-NCM 63.86% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 66.67% (best 97.22%); T42: W-NCM 41.53% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 70.11% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 18.37% (best 85.71%); T48: W-NCM 76.06% (best 95.77%); T49: W-NCM 47.47% (best 84.85%); T50: W-NCM 56.76% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 65.52% (best 87.93%); T53: W-NCM 46.81% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 56.72% (best 91.04%); T56: W-NCM 63.33% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 73.77% (best 95.08%); T62: W-NCM 62.50% (best 94.64%); T63: W-NCM 34.69% (best 93.88%); T64: W-NCM 45.28% (best 84.91%); T65: W-NCM 31.65% (best 92.41%); T66: W-NCM 60.00% (best 95.56%); T67: W-NCM 73.53% (best 94.12%); T68: W-NCM 50.00% (best 85.90%); T69: W-NCM 73.47% (best 93.88%); T70: W-NCM 67.92% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 78.26% (best 95.65%); T74: W-NCM 85.19% (best 96.30%); T75: W-NCM 81.82% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 94.59% (best 94.59%); T78: W-NCM 94.00% (best 94.00%)
2025-12-11 20:09:08,955 [trainer.py] => Average forgetting (W-NCM): 29.12% | Max forgetting (W-NCM): 67.35%
2025-12-11 20:09:08,968 [trainer.py] => All params: 144526051
2025-12-11 20:09:08,980 [trainer.py] => Trainable params: 185858
2025-12-11 20:09:08,980 [inflora.py] => Learning on 156-158
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_k.78.weight', 'image_encoder.blocks.3.attn.lora_B_v.78.weight', 'image_encoder.blocks.7.attn.lora_B_k.78.weight', 'image_encoder.blocks.0.attn.lora_B_v.78.weight', 'image_encoder.blocks.9.attn.lora_B_v.78.weight', 'image_encoder.blocks.11.attn.lora_B_v.78.weight', 'image_encoder.blocks.6.attn.lora_B_k.78.weight', 'image_encoder.blocks.8.attn.lora_B_v.78.weight', 'image_encoder.blocks.1.attn.lora_B_k.78.weight', 'image_encoder.blocks.9.attn.lora_B_k.78.weight', 'image_encoder.blocks.5.attn.lora_B_k.78.weight', 'image_encoder.blocks.10.attn.lora_B_v.78.weight', 'classifier_pool.78.weight', 'image_encoder.blocks.7.attn.lora_B_v.78.weight', 'image_encoder.blocks.10.attn.lora_B_k.78.weight', 'image_encoder.blocks.3.attn.lora_B_k.78.weight', 'image_encoder.blocks.2.attn.lora_B_k.78.weight', 'image_encoder.blocks.1.attn.lora_B_v.78.weight', 'image_encoder.blocks.4.attn.lora_B_k.78.weight', 'image_encoder.blocks.5.attn.lora_B_v.78.weight', 'classifier_pool.78.bias', 'image_encoder.blocks.8.attn.lora_B_k.78.weight', 'image_encoder.blocks.0.attn.lora_B_k.78.weight', 'image_encoder.blocks.2.attn.lora_B_v.78.weight', 'image_encoder.blocks.4.attn.lora_B_v.78.weight'}
2025-12-11 20:11:03,022 [inflora.py] => Task 78, Epoch 50/50 => Loss 0.033, Train_accy 98.69
Threshold:  0.9956
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 59/768 type remove
Layer 3 : 128/768 type remove
Layer 4 : 190/768 type remove
Layer 5 : 244/768 type remove
Layer 6 : 257/768 type remove
Layer 7 : 307/768 type remove
Layer 8 : 353/768 type remove
Layer 9 : 321/768 type retain
Layer 10 : 274/768 type retain
Layer 11 : 378/768 type retain
Layer 12 : 240/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:11:10,236 [trainer.py] => Time:121.25530242919922
4616 4616
4616 4616
2025-12-11 20:11:23,936 [trainer.py] => Time:13.699882507324219
2025-12-11 20:11:23,936 [inflora.py] => Exemplar size: 0
2025-12-11 20:11:23,936 [trainer.py] => CNN: {'total': np.float64(35.38), '00-01': np.float64(60.22), '02-03': np.float64(35.48), '04-05': np.float64(38.2), '06-07': np.float64(62.5), '08-09': np.float64(32.88), '10-11': np.float64(9.76), '12-13': np.float64(38.89), '14-15': np.float64(33.93), '16-17': np.float64(18.57), '18-19': np.float64(55.36), '20-21': np.float64(59.34), '22-23': np.float64(57.81), '24-25': np.float64(10.2), '26-27': np.float64(60.87), '28-29': np.float64(56.79), '30-31': np.float64(36.36), '32-33': np.float64(2.27), '34-35': np.float64(22.58), '36-37': np.float64(51.67), '38-39': np.float64(53.66), '40-41': np.float64(25.93), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(45.74), '50-51': np.float64(18.92), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(25.0), '58-59': np.float64(0.0), '60-61': np.float64(39.02), '62-63': np.float64(42.45), '64-65': np.float64(0.0), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(37.5), '74-75': np.float64(42.31), '76-77': np.float64(31.33), '78-79': np.float64(20.73), '80-81': np.float64(44.44), '82-83': np.float64(17.8), '84-85': np.float64(54.35), '86-87': np.float64(19.54), '88-89': np.float64(45.0), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(11.27), '96-97': np.float64(14.14), '98-99': np.float64(33.78), '100-101': np.float64(41.67), '102-103': np.float64(43.1), '104-105': np.float64(8.51), '106-107': np.float64(41.03), '108-109': np.float64(52.24), '110-111': np.float64(15.0), '112-113': np.float64(73.08), '114-115': np.float64(45.0), '116-117': np.float64(55.56), '118-119': np.float64(28.85), '120-121': np.float64(60.66), '122-123': np.float64(23.21), '124-125': np.float64(26.53), '126-127': np.float64(11.32), '128-129': np.float64(10.13), '130-131': np.float64(51.11), '132-133': np.float64(35.29), '134-135': np.float64(69.23), '136-137': np.float64(51.02), '138-139': np.float64(49.06), '140-141': np.float64(34.04), '142-143': np.float64(41.67), '144-145': np.float64(63.04), '146-147': np.float64(44.44), '148-149': np.float64(27.27), '150-151': np.float64(10.53), '152-153': np.float64(10.81), '154-155': np.float64(2.0), '156-157': np.float64(37.5), 'old': np.float64(35.35), 'new': np.float64(37.5)}
2025-12-11 20:11:23,936 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38)]
2025-12-11 20:11:23,937 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43)]
2025-12-11 20:11:23,937 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575]
2025-12-11 20:11:39,409 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 51.61290322580645, '04-05': 52.80898876404494, '06-07': 75.0, '08-09': 63.013698630136986, '10-11': 56.09756097560976, '12-13': 83.33333333333334, '14-15': 69.64285714285714, '16-17': 64.28571428571429, '18-19': 64.28571428571429, '20-21': 70.32967032967034, '22-23': 60.9375, '24-25': 59.183673469387756, '26-27': 46.08695652173913, '28-29': 64.19753086419753, '30-31': 36.36363636363637, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 75.0, '38-39': 70.73170731707317, '40-41': 53.70370370370371, '42-43': 59.375, '44-45': 51.85185185185185, '46-47': 74.07407407407408, '48-49': 72.3404255319149, '50-51': 59.45945945945946, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 80.0, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 71.69811320754717, '64-65': 63.33333333333333, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 57.534246575342465, '72-73': 65.0, '74-75': 29.48717948717949, '76-77': 65.06024096385542, '78-79': 57.3170731707317, '80-81': 73.61111111111111, '82-83': 44.91525423728814, '84-85': 78.26086956521739, '86-87': 74.71264367816092, '88-89': 83.33333333333334, '90-91': 62.5, '92-93': 22.448979591836736, '94-95': 77.46478873239437, '96-97': 48.484848484848484, '98-99': 59.45945945945946, '100-101': 61.66666666666667, '102-103': 67.24137931034483, '104-105': 50.0, '106-107': 66.66666666666666, '108-109': 58.2089552238806, '110-111': 65.0, '112-113': 78.84615384615384, '114-115': 60.0, '116-117': 88.88888888888889, '118-119': 73.07692307692307, '120-121': 75.40983606557377, '122-123': 66.07142857142857, '124-125': 38.775510204081634, '126-127': 50.943396226415096, '128-129': 35.44303797468354, '130-131': 66.66666666666666, '132-133': 79.41176470588235, '134-135': 52.56410256410257, '136-137': 73.46938775510205, '138-139': 67.9245283018868, '140-141': 57.446808510638306, '142-143': 85.41666666666666, '144-145': 82.6086956521739, '146-147': 81.48148148148148, '148-149': 81.81818181818183, '150-151': 60.526315789473685, '152-153': 91.8918918918919, '154-155': 90.0, '156-157': 87.5}
2025-12-11 20:11:39,410 [trainer.py] => Ave Acc (W-NCM): 64.91%
2025-12-11 20:11:39,410 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 51.61% (best 90.32%); T3: W-NCM 52.81% (best 91.01%); T4: W-NCM 75.00% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 56.10% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 64.29% (best 98.57%); T10: W-NCM 64.29% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 46.09% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 36.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 72.34% (best 94.68%); T26: W-NCM 59.46% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 80.00% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 71.70% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 57.53% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 29.49% (best 87.18%); T39: W-NCM 65.06% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 73.61% (best 97.22%); T42: W-NCM 44.92% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 22.45% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 59.46% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 58.21% (best 91.04%); T56: W-NCM 65.00% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 73.08% (best 100.00%); T61: W-NCM 75.41% (best 95.08%); T62: W-NCM 66.07% (best 94.64%); T63: W-NCM 38.78% (best 93.88%); T64: W-NCM 50.94% (best 84.91%); T65: W-NCM 35.44% (best 92.41%); T66: W-NCM 66.67% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 52.56% (best 85.90%); T69: W-NCM 73.47% (best 93.88%); T70: W-NCM 67.92% (best 94.34%); T71: W-NCM 57.45% (best 87.23%); T72: W-NCM 85.42% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 81.82% (best 90.91%); T76: W-NCM 60.53% (best 84.21%); T77: W-NCM 91.89% (best 94.59%); T78: W-NCM 90.00% (best 94.00%); T79: W-NCM 87.50% (best 87.50%)
2025-12-11 20:11:39,410 [trainer.py] => Average forgetting (W-NCM): 27.19% | Max forgetting (W-NCM): 63.27%
2025-12-11 20:11:39,422 [trainer.py] => All params: 144526051
2025-12-11 20:11:39,434 [trainer.py] => Trainable params: 185858
2025-12-11 20:11:39,434 [inflora.py] => Learning on 158-160
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.79.weight', 'image_encoder.blocks.7.attn.lora_B_v.79.weight', 'image_encoder.blocks.6.attn.lora_B_v.79.weight', 'image_encoder.blocks.5.attn.lora_B_v.79.weight', 'image_encoder.blocks.1.attn.lora_B_k.79.weight', 'image_encoder.blocks.6.attn.lora_B_k.79.weight', 'image_encoder.blocks.8.attn.lora_B_k.79.weight', 'image_encoder.blocks.0.attn.lora_B_k.79.weight', 'image_encoder.blocks.7.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_v.79.weight', 'image_encoder.blocks.8.attn.lora_B_v.79.weight', 'image_encoder.blocks.9.attn.lora_B_k.79.weight', 'image_encoder.blocks.4.attn.lora_B_k.79.weight', 'image_encoder.blocks.11.attn.lora_B_k.79.weight', 'classifier_pool.79.bias', 'image_encoder.blocks.3.attn.lora_B_v.79.weight', 'image_encoder.blocks.3.attn.lora_B_k.79.weight', 'image_encoder.blocks.9.attn.lora_B_v.79.weight', 'image_encoder.blocks.11.attn.lora_B_v.79.weight', 'image_encoder.blocks.10.attn.lora_B_k.79.weight', 'classifier_pool.79.weight', 'image_encoder.blocks.2.attn.lora_B_v.79.weight', 'image_encoder.blocks.5.attn.lora_B_k.79.weight', 'image_encoder.blocks.10.attn.lora_B_v.79.weight', 'image_encoder.blocks.0.attn.lora_B_v.79.weight', 'image_encoder.blocks.1.attn.lora_B_v.79.weight'}
2025-12-11 20:13:53,017 [inflora.py] => Task 79, Epoch 50/50 => Loss 0.069, Train_accy 96.85
Threshold:  0.9958
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 60/768 type remove
Layer 3 : 129/768 type remove
Layer 4 : 193/768 type remove
Layer 5 : 248/768 type remove
Layer 6 : 260/768 type remove
Layer 7 : 310/768 type remove
Layer 8 : 359/768 type remove
Layer 9 : 317/768 type retain
Layer 10 : 272/768 type retain
Layer 11 : 376/768 type retain
Layer 12 : 236/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:14:00,672 [trainer.py] => Time:141.23759388923645
4679 4679
4679 4679
2025-12-11 20:14:14,523 [trainer.py] => Time:13.851082801818848
2025-12-11 20:14:14,524 [inflora.py] => Exemplar size: 0
2025-12-11 20:14:14,524 [trainer.py] => CNN: {'total': np.float64(33.7), '00-01': np.float64(59.14), '02-03': np.float64(35.48), '04-05': np.float64(37.08), '06-07': np.float64(67.86), '08-09': np.float64(35.62), '10-11': np.float64(7.32), '12-13': np.float64(33.33), '14-15': np.float64(30.36), '16-17': np.float64(21.43), '18-19': np.float64(62.5), '20-21': np.float64(60.44), '22-23': np.float64(57.81), '24-25': np.float64(10.2), '26-27': np.float64(60.0), '28-29': np.float64(55.56), '30-31': np.float64(35.45), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(53.33), '38-39': np.float64(39.02), '40-41': np.float64(27.78), '42-43': np.float64(21.88), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(40.43), '50-51': np.float64(27.03), '52-53': np.float64(30.56), '54-55': np.float64(0.0), '56-57': np.float64(20.0), '58-59': np.float64(0.0), '60-61': np.float64(36.59), '62-63': np.float64(41.51), '64-65': np.float64(3.33), '66-67': np.float64(31.58), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(37.5), '74-75': np.float64(37.18), '76-77': np.float64(22.89), '78-79': np.float64(21.95), '80-81': np.float64(36.11), '82-83': np.float64(16.1), '84-85': np.float64(58.7), '86-87': np.float64(16.09), '88-89': np.float64(38.33), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(9.86), '96-97': np.float64(13.13), '98-99': np.float64(25.68), '100-101': np.float64(38.33), '102-103': np.float64(37.93), '104-105': np.float64(6.38), '106-107': np.float64(38.46), '108-109': np.float64(53.73), '110-111': np.float64(20.0), '112-113': np.float64(69.23), '114-115': np.float64(42.5), '116-117': np.float64(50.0), '118-119': np.float64(26.92), '120-121': np.float64(62.3), '122-123': np.float64(25.0), '124-125': np.float64(20.41), '126-127': np.float64(9.43), '128-129': np.float64(8.86), '130-131': np.float64(46.67), '132-133': np.float64(26.47), '134-135': np.float64(60.26), '136-137': np.float64(53.06), '138-139': np.float64(47.17), '140-141': np.float64(31.91), '142-143': np.float64(47.92), '144-145': np.float64(65.22), '146-147': np.float64(44.44), '148-149': np.float64(24.24), '150-151': np.float64(7.89), '152-153': np.float64(5.41), '154-155': np.float64(0.0), '156-157': np.float64(30.36), '158-159': np.float64(17.46), 'old': np.float64(33.93), 'new': np.float64(17.46)}
2025-12-11 20:14:14,524 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7)]
2025-12-11 20:14:14,524 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28)]
2025-12-11 20:14:14,524 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386]
2025-12-11 20:14:30,126 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 48.38709677419355, '04-05': 50.56179775280899, '06-07': 78.57142857142857, '08-09': 61.64383561643836, '10-11': 53.65853658536586, '12-13': 85.18518518518519, '14-15': 67.85714285714286, '16-17': 67.14285714285714, '18-19': 64.28571428571429, '20-21': 70.32967032967034, '22-23': 60.9375, '24-25': 57.14285714285714, '26-27': 43.47826086956522, '28-29': 61.72839506172839, '30-31': 35.45454545454545, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 73.33333333333333, '38-39': 65.85365853658537, '40-41': 53.70370370370371, '42-43': 59.375, '44-45': 51.85185185185185, '46-47': 70.37037037037037, '48-49': 73.40425531914893, '50-51': 56.75675675675676, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 80.0, '58-59': 45.45454545454545, '60-61': 65.85365853658537, '62-63': 70.75471698113208, '64-65': 66.66666666666666, '66-67': 42.10526315789473, '68-69': 62.5, '70-71': 58.9041095890411, '72-73': 65.0, '74-75': 30.76923076923077, '76-77': 63.85542168674698, '78-79': 57.3170731707317, '80-81': 73.61111111111111, '82-83': 44.06779661016949, '84-85': 78.26086956521739, '86-87': 71.26436781609196, '88-89': 83.33333333333334, '90-91': 62.5, '92-93': 20.408163265306122, '94-95': 78.87323943661971, '96-97': 48.484848484848484, '98-99': 59.45945945945946, '100-101': 60.0, '102-103': 72.41379310344827, '104-105': 48.93617021276596, '106-107': 66.66666666666666, '108-109': 55.223880597014926, '110-111': 70.0, '112-113': 80.76923076923077, '114-115': 60.0, '116-117': 88.88888888888889, '118-119': 71.15384615384616, '120-121': 73.77049180327869, '122-123': 64.28571428571429, '124-125': 38.775510204081634, '126-127': 49.056603773584904, '128-129': 39.24050632911392, '130-131': 68.88888888888889, '132-133': 79.41176470588235, '134-135': 48.717948717948715, '136-137': 71.42857142857143, '138-139': 66.0377358490566, '140-141': 53.191489361702125, '142-143': 85.41666666666666, '144-145': 80.43478260869566, '146-147': 81.48148148148148, '148-149': 75.75757575757575, '150-151': 57.89473684210527, '152-153': 86.48648648648648, '154-155': 88.0, '156-157': 85.71428571428571, '158-159': 87.3015873015873}
2025-12-11 20:14:30,127 [trainer.py] => Ave Acc (W-NCM): 64.32%
2025-12-11 20:14:30,127 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 48.39% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 61.64% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 67.86% (best 94.64%); T9: W-NCM 67.14% (best 98.57%); T10: W-NCM 64.29% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 43.48% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 35.45% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 73.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 73.40% (best 94.68%); T26: W-NCM 56.76% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 80.00% (best 95.00%); T30: W-NCM 45.45% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 70.75% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 58.90% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 30.77% (best 87.18%); T39: W-NCM 63.86% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 73.61% (best 97.22%); T42: W-NCM 44.07% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 71.26% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 59.46% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 48.94% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 55.22% (best 91.04%); T56: W-NCM 70.00% (best 96.67%); T57: W-NCM 80.77% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 71.15% (best 100.00%); T61: W-NCM 73.77% (best 95.08%); T62: W-NCM 64.29% (best 94.64%); T63: W-NCM 38.78% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 39.24% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 48.72% (best 85.90%); T69: W-NCM 71.43% (best 93.88%); T70: W-NCM 66.04% (best 94.34%); T71: W-NCM 53.19% (best 87.23%); T72: W-NCM 85.42% (best 91.67%); T73: W-NCM 80.43% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 75.76% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 86.49% (best 94.59%); T78: W-NCM 88.00% (best 94.00%); T79: W-NCM 85.71% (best 87.50%); T80: W-NCM 87.30% (best 87.30%)
2025-12-11 20:14:30,127 [trainer.py] => Average forgetting (W-NCM): 27.72% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:14:30,140 [trainer.py] => All params: 144526051
2025-12-11 20:14:30,152 [trainer.py] => Trainable params: 185858
2025-12-11 20:14:30,152 [inflora.py] => Learning on 160-162
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.80.weight', 'classifier_pool.80.bias', 'image_encoder.blocks.2.attn.lora_B_k.80.weight', 'image_encoder.blocks.0.attn.lora_B_k.80.weight', 'image_encoder.blocks.5.attn.lora_B_v.80.weight', 'image_encoder.blocks.1.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_v.80.weight', 'image_encoder.blocks.2.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_k.80.weight', 'image_encoder.blocks.4.attn.lora_B_v.80.weight', 'image_encoder.blocks.9.attn.lora_B_v.80.weight', 'image_encoder.blocks.7.attn.lora_B_k.80.weight', 'image_encoder.blocks.4.attn.lora_B_k.80.weight', 'image_encoder.blocks.6.attn.lora_B_v.80.weight', 'image_encoder.blocks.11.attn.lora_B_v.80.weight', 'image_encoder.blocks.5.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_v.80.weight', 'image_encoder.blocks.10.attn.lora_B_k.80.weight', 'image_encoder.blocks.8.attn.lora_B_k.80.weight', 'image_encoder.blocks.0.attn.lora_B_v.80.weight', 'classifier_pool.80.weight', 'image_encoder.blocks.8.attn.lora_B_v.80.weight', 'image_encoder.blocks.1.attn.lora_B_k.80.weight', 'image_encoder.blocks.11.attn.lora_B_k.80.weight', 'image_encoder.blocks.3.attn.lora_B_k.80.weight', 'image_encoder.blocks.6.attn.lora_B_k.80.weight'}
2025-12-11 20:16:09,485 [inflora.py] => Task 80, Epoch 50/50 => Loss 0.015, Train_accy 98.35
Threshold:  0.996
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 61/768 type remove
Layer 3 : 135/768 type remove
Layer 4 : 205/768 type remove
Layer 5 : 265/768 type remove
Layer 6 : 277/768 type remove
Layer 7 : 326/768 type remove
Layer 8 : 375/768 type remove
Layer 9 : 301/768 type retain
Layer 10 : 258/768 type retain
Layer 11 : 366/768 type retain
Layer 12 : 230/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:16:16,160 [trainer.py] => Time:106.00838160514832
4725 4725
4725 4725
2025-12-11 20:16:30,176 [trainer.py] => Time:14.015025615692139
2025-12-11 20:16:30,176 [inflora.py] => Exemplar size: 0
2025-12-11 20:16:30,176 [trainer.py] => CNN: {'total': np.float64(34.73), '00-01': np.float64(62.37), '02-03': np.float64(37.1), '04-05': np.float64(37.08), '06-07': np.float64(64.29), '08-09': np.float64(35.62), '10-11': np.float64(7.32), '12-13': np.float64(37.04), '14-15': np.float64(25.0), '16-17': np.float64(22.86), '18-19': np.float64(60.71), '20-21': np.float64(61.54), '22-23': np.float64(57.81), '24-25': np.float64(8.16), '26-27': np.float64(60.0), '28-29': np.float64(60.49), '30-31': np.float64(38.18), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(56.67), '38-39': np.float64(46.34), '40-41': np.float64(24.07), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(39.36), '50-51': np.float64(27.03), '52-53': np.float64(22.22), '54-55': np.float64(0.0), '56-57': np.float64(25.0), '58-59': np.float64(0.0), '60-61': np.float64(36.59), '62-63': np.float64(41.51), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(31.51), '72-73': np.float64(37.5), '74-75': np.float64(37.18), '76-77': np.float64(22.89), '78-79': np.float64(21.95), '80-81': np.float64(38.89), '82-83': np.float64(17.8), '84-85': np.float64(54.35), '86-87': np.float64(18.39), '88-89': np.float64(43.33), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(11.27), '96-97': np.float64(14.14), '98-99': np.float64(25.68), '100-101': np.float64(38.33), '102-103': np.float64(39.66), '104-105': np.float64(6.38), '106-107': np.float64(35.9), '108-109': np.float64(53.73), '110-111': np.float64(16.67), '112-113': np.float64(75.0), '114-115': np.float64(47.5), '116-117': np.float64(50.0), '118-119': np.float64(28.85), '120-121': np.float64(60.66), '122-123': np.float64(25.0), '124-125': np.float64(24.49), '126-127': np.float64(9.43), '128-129': np.float64(11.39), '130-131': np.float64(44.44), '132-133': np.float64(29.41), '134-135': np.float64(65.38), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(52.08), '144-145': np.float64(65.22), '146-147': np.float64(51.85), '148-149': np.float64(30.3), '150-151': np.float64(7.89), '152-153': np.float64(10.81), '154-155': np.float64(0.0), '156-157': np.float64(33.93), '158-159': np.float64(23.81), '160-161': np.float64(34.78), 'old': np.float64(34.73), 'new': np.float64(34.78)}
2025-12-11 20:16:30,176 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73)]
2025-12-11 20:16:30,177 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53)]
2025-12-11 20:16:30,177 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275]
2025-12-11 20:16:45,603 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 50.0, '04-05': 55.0561797752809, '06-07': 78.57142857142857, '08-09': 64.38356164383562, '10-11': 58.536585365853654, '12-13': 87.03703703703704, '14-15': 71.42857142857143, '16-17': 68.57142857142857, '18-19': 71.42857142857143, '20-21': 72.52747252747253, '22-23': 64.84375, '24-25': 57.14285714285714, '26-27': 49.56521739130435, '28-29': 65.4320987654321, '30-31': 40.909090909090914, '32-33': 88.63636363636364, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 68.29268292682927, '40-41': 55.55555555555556, '42-43': 59.375, '44-45': 59.25925925925925, '46-47': 70.37037037037037, '48-49': 77.6595744680851, '50-51': 59.45945945945946, '52-53': 86.11111111111111, '54-55': 52.0, '56-57': 85.0, '58-59': 50.90909090909091, '60-61': 65.85365853658537, '62-63': 70.75471698113208, '64-65': 66.66666666666666, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 63.013698630136986, '72-73': 67.5, '74-75': 33.33333333333333, '76-77': 65.06024096385542, '78-79': 59.756097560975604, '80-81': 76.38888888888889, '82-83': 47.45762711864407, '84-85': 80.43478260869566, '86-87': 71.26436781609196, '88-89': 83.33333333333334, '90-91': 65.0, '92-93': 20.408163265306122, '94-95': 80.28169014084507, '96-97': 52.52525252525253, '98-99': 64.86486486486487, '100-101': 63.33333333333333, '102-103': 72.41379310344827, '104-105': 50.0, '106-107': 66.66666666666666, '108-109': 61.19402985074627, '110-111': 71.66666666666667, '112-113': 78.84615384615384, '114-115': 67.5, '116-117': 94.44444444444444, '118-119': 75.0, '120-121': 72.1311475409836, '122-123': 69.64285714285714, '124-125': 42.857142857142854, '126-127': 49.056603773584904, '128-129': 43.037974683544306, '130-131': 66.66666666666666, '132-133': 79.41176470588235, '134-135': 48.717948717948715, '136-137': 69.38775510204081, '138-139': 67.9245283018868, '140-141': 55.319148936170215, '142-143': 87.5, '144-145': 80.43478260869566, '146-147': 81.48148148148148, '148-149': 75.75757575757575, '150-151': 63.1578947368421, '152-153': 89.1891891891892, '154-155': 84.0, '156-157': 89.28571428571429, '158-159': 88.88888888888889, '160-161': 95.65217391304348}
2025-12-11 20:16:45,604 [trainer.py] => Ave Acc (W-NCM): 67.04%
2025-12-11 20:16:45,604 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 50.00% (best 90.32%); T3: W-NCM 55.06% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 68.57% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 49.57% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 40.91% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 59.26% (best 62.96%); T24: W-NCM 70.37% (best 92.59%); T25: W-NCM 77.66% (best 94.68%); T26: W-NCM 59.46% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 52.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 50.91% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 70.75% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 33.33% (best 87.18%); T39: W-NCM 65.06% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 76.39% (best 97.22%); T42: W-NCM 47.46% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 71.26% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 52.53% (best 84.85%); T50: W-NCM 64.86% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 61.19% (best 91.04%); T56: W-NCM 71.67% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 94.44% (best 97.22%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 72.13% (best 95.08%); T62: W-NCM 69.64% (best 94.64%); T63: W-NCM 42.86% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 66.67% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 48.72% (best 85.90%); T69: W-NCM 69.39% (best 93.88%); T70: W-NCM 67.92% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 80.43% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 75.76% (best 90.91%); T76: W-NCM 63.16% (best 84.21%); T77: W-NCM 89.19% (best 94.59%); T78: W-NCM 84.00% (best 94.00%); T79: W-NCM 89.29% (best 89.29%); T80: W-NCM 88.89% (best 88.89%); T81: W-NCM 95.65% (best 95.65%)
2025-12-11 20:16:45,604 [trainer.py] => Average forgetting (W-NCM): 25.05% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:16:45,617 [trainer.py] => All params: 144526051
2025-12-11 20:16:45,629 [trainer.py] => Trainable params: 185858
2025-12-11 20:16:45,629 [inflora.py] => Learning on 162-164
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.81.weight', 'image_encoder.blocks.9.attn.lora_B_v.81.weight', 'image_encoder.blocks.5.attn.lora_B_k.81.weight', 'image_encoder.blocks.11.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_k.81.weight', 'image_encoder.blocks.6.attn.lora_B_v.81.weight', 'image_encoder.blocks.7.attn.lora_B_k.81.weight', 'image_encoder.blocks.2.attn.lora_B_v.81.weight', 'image_encoder.blocks.1.attn.lora_B_v.81.weight', 'image_encoder.blocks.1.attn.lora_B_k.81.weight', 'image_encoder.blocks.6.attn.lora_B_k.81.weight', 'image_encoder.blocks.4.attn.lora_B_v.81.weight', 'image_encoder.blocks.7.attn.lora_B_v.81.weight', 'image_encoder.blocks.4.attn.lora_B_k.81.weight', 'image_encoder.blocks.3.attn.lora_B_v.81.weight', 'image_encoder.blocks.10.attn.lora_B_v.81.weight', 'image_encoder.blocks.11.attn.lora_B_v.81.weight', 'image_encoder.blocks.3.attn.lora_B_k.81.weight', 'image_encoder.blocks.5.attn.lora_B_v.81.weight', 'image_encoder.blocks.2.attn.lora_B_k.81.weight', 'image_encoder.blocks.0.attn.lora_B_v.81.weight', 'classifier_pool.81.weight', 'image_encoder.blocks.9.attn.lora_B_k.81.weight', 'image_encoder.blocks.10.attn.lora_B_k.81.weight', 'classifier_pool.81.bias', 'image_encoder.blocks.8.attn.lora_B_v.81.weight'}
2025-12-11 20:18:57,106 [inflora.py] => Task 81, Epoch 50/50 => Loss 0.030, Train_accy 98.50
Threshold:  0.9962
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 15/768 type remove
Layer 2 : 62/768 type remove
Layer 3 : 138/768 type remove
Layer 4 : 209/768 type remove
Layer 5 : 271/768 type remove
Layer 6 : 283/768 type remove
Layer 7 : 334/768 type remove
Layer 8 : 382/768 type retain
Layer 9 : 289/768 type retain
Layer 10 : 250/768 type retain
Layer 11 : 354/768 type retain
Layer 12 : 221/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:19:04,706 [trainer.py] => Time:139.0771861076355
4786 4786
4786 4786
2025-12-11 20:19:18,882 [trainer.py] => Time:14.17542028427124
2025-12-11 20:19:18,882 [inflora.py] => Exemplar size: 0
2025-12-11 20:19:18,882 [trainer.py] => CNN: {'total': np.float64(35.85), '00-01': np.float64(63.44), '02-03': np.float64(37.1), '04-05': np.float64(40.45), '06-07': np.float64(62.5), '08-09': np.float64(36.99), '10-11': np.float64(9.76), '12-13': np.float64(35.19), '14-15': np.float64(25.0), '16-17': np.float64(25.71), '18-19': np.float64(58.93), '20-21': np.float64(61.54), '22-23': np.float64(59.38), '24-25': np.float64(8.16), '26-27': np.float64(66.96), '28-29': np.float64(60.49), '30-31': np.float64(39.09), '32-33': np.float64(4.55), '34-35': np.float64(19.35), '36-37': np.float64(56.67), '38-39': np.float64(48.78), '40-41': np.float64(27.78), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(41.49), '50-51': np.float64(21.62), '52-53': np.float64(33.33), '54-55': np.float64(0.0), '56-57': np.float64(27.5), '58-59': np.float64(0.0), '60-61': np.float64(34.15), '62-63': np.float64(42.45), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(37.5), '74-75': np.float64(37.18), '76-77': np.float64(25.3), '78-79': np.float64(21.95), '80-81': np.float64(43.06), '82-83': np.float64(18.64), '84-85': np.float64(56.52), '86-87': np.float64(20.69), '88-89': np.float64(41.67), '90-91': np.float64(45.0), '92-93': np.float64(12.24), '94-95': np.float64(19.72), '96-97': np.float64(14.14), '98-99': np.float64(25.68), '100-101': np.float64(40.0), '102-103': np.float64(39.66), '104-105': np.float64(7.45), '106-107': np.float64(35.9), '108-109': np.float64(55.22), '110-111': np.float64(13.33), '112-113': np.float64(73.08), '114-115': np.float64(45.0), '116-117': np.float64(47.22), '118-119': np.float64(32.69), '120-121': np.float64(57.38), '122-123': np.float64(23.21), '124-125': np.float64(26.53), '126-127': np.float64(7.55), '128-129': np.float64(13.92), '130-131': np.float64(48.89), '132-133': np.float64(35.29), '134-135': np.float64(61.54), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(52.08), '144-145': np.float64(63.04), '146-147': np.float64(51.85), '148-149': np.float64(30.3), '150-151': np.float64(10.53), '152-153': np.float64(10.81), '154-155': np.float64(0.0), '156-157': np.float64(37.5), '158-159': np.float64(31.75), '160-161': np.float64(34.78), '162-163': np.float64(57.38), 'old': np.float64(35.58), 'new': np.float64(57.38)}
2025-12-11 20:19:18,882 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85)]
2025-12-11 20:19:18,883 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65)]
2025-12-11 20:19:18,883 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679]
2025-12-11 20:19:34,791 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 48.38709677419355, '04-05': 56.17977528089888, '06-07': 80.35714285714286, '08-09': 63.013698630136986, '10-11': 58.536585365853654, '12-13': 87.03703703703704, '14-15': 69.64285714285714, '16-17': 68.57142857142857, '18-19': 69.64285714285714, '20-21': 75.82417582417582, '22-23': 64.0625, '24-25': 57.14285714285714, '26-27': 49.56521739130435, '28-29': 65.4320987654321, '30-31': 39.09090909090909, '32-33': 88.63636363636364, '34-35': 70.96774193548387, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 53.70370370370371, '42-43': 59.375, '44-45': 59.25925925925925, '46-47': 74.07407407407408, '48-49': 76.59574468085107, '50-51': 62.16216216216216, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 87.5, '58-59': 52.72727272727272, '60-61': 65.85365853658537, '62-63': 70.75471698113208, '64-65': 73.33333333333333, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 60.273972602739725, '72-73': 65.0, '74-75': 35.8974358974359, '76-77': 66.26506024096386, '78-79': 58.536585365853654, '80-81': 75.0, '82-83': 47.45762711864407, '84-85': 82.6086956521739, '86-87': 72.41379310344827, '88-89': 83.33333333333334, '90-91': 67.5, '92-93': 20.408163265306122, '94-95': 80.28169014084507, '96-97': 49.494949494949495, '98-99': 55.4054054054054, '100-101': 60.0, '102-103': 72.41379310344827, '104-105': 50.0, '106-107': 66.66666666666666, '108-109': 64.17910447761194, '110-111': 71.66666666666667, '112-113': 78.84615384615384, '114-115': 70.0, '116-117': 91.66666666666666, '118-119': 78.84615384615384, '120-121': 72.1311475409836, '122-123': 67.85714285714286, '124-125': 46.93877551020408, '126-127': 47.16981132075472, '128-129': 41.77215189873418, '130-131': 68.88888888888889, '132-133': 79.41176470588235, '134-135': 47.43589743589743, '136-137': 69.38775510204081, '138-139': 67.9245283018868, '140-141': 55.319148936170215, '142-143': 85.41666666666666, '144-145': 82.6086956521739, '146-147': 85.18518518518519, '148-149': 75.75757575757575, '150-151': 60.526315789473685, '152-153': 83.78378378378379, '154-155': 78.0, '156-157': 82.14285714285714, '158-159': 87.3015873015873, '160-161': 95.65217391304348, '162-163': 91.80327868852459}
2025-12-11 20:19:34,792 [trainer.py] => Ave Acc (W-NCM): 67.21%
2025-12-11 20:19:34,792 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 48.39% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 69.64% (best 94.64%); T9: W-NCM 68.57% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 64.06% (best 95.31%); T13: W-NCM 57.14% (best 87.76%); T14: W-NCM 49.57% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 39.09% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 70.97% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 59.38% (best 96.88%); T23: W-NCM 59.26% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 70.75% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 47.46% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 49.49% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 71.67% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 72.13% (best 95.08%); T62: W-NCM 67.86% (best 94.64%); T63: W-NCM 46.94% (best 93.88%); T64: W-NCM 47.17% (best 84.91%); T65: W-NCM 41.77% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 47.44% (best 85.90%); T69: W-NCM 69.39% (best 93.88%); T70: W-NCM 67.92% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 85.42% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 85.19% (best 96.30%); T75: W-NCM 75.76% (best 90.91%); T76: W-NCM 60.53% (best 84.21%); T77: W-NCM 83.78% (best 94.59%); T78: W-NCM 78.00% (best 94.00%); T79: W-NCM 82.14% (best 89.29%); T80: W-NCM 87.30% (best 88.89%); T81: W-NCM 95.65% (best 95.65%); T82: W-NCM 91.80% (best 91.80%)
2025-12-11 20:19:34,792 [trainer.py] => Average forgetting (W-NCM): 24.88% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:19:34,804 [trainer.py] => All params: 144526051
2025-12-11 20:19:34,816 [trainer.py] => Trainable params: 185858
2025-12-11 20:19:34,817 [inflora.py] => Learning on 164-166
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.82.weight', 'image_encoder.blocks.10.attn.lora_B_k.82.weight', 'image_encoder.blocks.1.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_k.82.weight', 'image_encoder.blocks.1.attn.lora_B_k.82.weight', 'image_encoder.blocks.6.attn.lora_B_v.82.weight', 'image_encoder.blocks.0.attn.lora_B_v.82.weight', 'image_encoder.blocks.9.attn.lora_B_k.82.weight', 'image_encoder.blocks.2.attn.lora_B_v.82.weight', 'image_encoder.blocks.10.attn.lora_B_v.82.weight', 'image_encoder.blocks.2.attn.lora_B_k.82.weight', 'image_encoder.blocks.4.attn.lora_B_v.82.weight', 'image_encoder.blocks.5.attn.lora_B_v.82.weight', 'image_encoder.blocks.7.attn.lora_B_v.82.weight', 'classifier_pool.82.weight', 'image_encoder.blocks.8.attn.lora_B_k.82.weight', 'image_encoder.blocks.3.attn.lora_B_v.82.weight', 'image_encoder.blocks.0.attn.lora_B_k.82.weight', 'image_encoder.blocks.5.attn.lora_B_k.82.weight', 'image_encoder.blocks.9.attn.lora_B_v.82.weight', 'image_encoder.blocks.11.attn.lora_B_v.82.weight', 'image_encoder.blocks.4.attn.lora_B_k.82.weight', 'image_encoder.blocks.8.attn.lora_B_v.82.weight', 'image_encoder.blocks.3.attn.lora_B_k.82.weight', 'image_encoder.blocks.11.attn.lora_B_k.82.weight', 'classifier_pool.82.bias'}
2025-12-11 20:21:48,820 [inflora.py] => Task 82, Epoch 50/50 => Loss 0.023, Train_accy 98.61
Threshold:  0.9964
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 64/768 type remove
Layer 3 : 145/768 type remove
Layer 4 : 220/768 type remove
Layer 5 : 281/768 type remove
Layer 6 : 291/768 type remove
Layer 7 : 343/768 type remove
Layer 8 : 369/768 type retain
Layer 9 : 275/768 type retain
Layer 10 : 237/768 type retain
Layer 11 : 342/768 type retain
Layer 12 : 218/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:21:56,139 [trainer.py] => Time:141.3226158618927
4876 4876
4876 4876
2025-12-11 20:22:10,650 [trainer.py] => Time:14.510512828826904
2025-12-11 20:22:10,650 [inflora.py] => Exemplar size: 0
2025-12-11 20:22:10,651 [trainer.py] => CNN: {'total': np.float64(36.07), '00-01': np.float64(65.59), '02-03': np.float64(38.71), '04-05': np.float64(40.45), '06-07': np.float64(62.5), '08-09': np.float64(34.25), '10-11': np.float64(9.76), '12-13': np.float64(38.89), '14-15': np.float64(19.64), '16-17': np.float64(22.86), '18-19': np.float64(55.36), '20-21': np.float64(69.23), '22-23': np.float64(58.59), '24-25': np.float64(8.16), '26-27': np.float64(64.35), '28-29': np.float64(64.2), '30-31': np.float64(37.27), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(46.34), '40-41': np.float64(29.63), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(44.68), '50-51': np.float64(27.03), '52-53': np.float64(27.78), '54-55': np.float64(0.0), '56-57': np.float64(30.0), '58-59': np.float64(1.82), '60-61': np.float64(34.15), '62-63': np.float64(38.68), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(28.77), '72-73': np.float64(35.0), '74-75': np.float64(34.62), '76-77': np.float64(22.89), '78-79': np.float64(19.51), '80-81': np.float64(44.44), '82-83': np.float64(20.34), '84-85': np.float64(52.17), '86-87': np.float64(27.59), '88-89': np.float64(38.33), '90-91': np.float64(45.0), '92-93': np.float64(14.29), '94-95': np.float64(19.72), '96-97': np.float64(11.11), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(37.93), '104-105': np.float64(8.51), '106-107': np.float64(35.9), '108-109': np.float64(56.72), '110-111': np.float64(16.67), '112-113': np.float64(73.08), '114-115': np.float64(52.5), '116-117': np.float64(50.0), '118-119': np.float64(38.46), '120-121': np.float64(59.02), '122-123': np.float64(23.21), '124-125': np.float64(26.53), '126-127': np.float64(9.43), '128-129': np.float64(7.59), '130-131': np.float64(46.67), '132-133': np.float64(35.29), '134-135': np.float64(56.41), '136-137': np.float64(51.02), '138-139': np.float64(54.72), '140-141': np.float64(31.91), '142-143': np.float64(54.17), '144-145': np.float64(60.87), '146-147': np.float64(48.15), '148-149': np.float64(27.27), '150-151': np.float64(13.16), '152-153': np.float64(10.81), '154-155': np.float64(0.0), '156-157': np.float64(35.71), '158-159': np.float64(36.51), '160-161': np.float64(36.96), '162-163': np.float64(52.46), '164-165': np.float64(47.78), 'old': np.float64(35.85), 'new': np.float64(47.78)}
2025-12-11 20:22:10,651 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07)]
2025-12-11 20:22:10,651 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55)]
2025-12-11 20:22:10,651 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622]
2025-12-11 20:22:26,842 [trainer.py] => W-NCM: {'00-01': 64.51612903225806, '02-03': 51.61290322580645, '04-05': 52.80898876404494, '06-07': 80.35714285714286, '08-09': 63.013698630136986, '10-11': 60.97560975609756, '12-13': 87.03703703703704, '14-15': 73.21428571428571, '16-17': 68.57142857142857, '18-19': 67.85714285714286, '20-21': 75.82417582417582, '22-23': 64.0625, '24-25': 61.224489795918366, '26-27': 51.30434782608696, '28-29': 66.66666666666666, '30-31': 40.909090909090914, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 80.0, '38-39': 68.29268292682927, '40-41': 51.85185185185185, '42-43': 56.25, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 76.59574468085107, '50-51': 62.16216216216216, '52-53': 83.33333333333334, '54-55': 48.0, '56-57': 87.5, '58-59': 47.27272727272727, '60-61': 63.41463414634146, '62-63': 73.58490566037736, '64-65': 73.33333333333333, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 60.273972602739725, '72-73': 70.0, '74-75': 33.33333333333333, '76-77': 67.46987951807229, '78-79': 57.3170731707317, '80-81': 80.55555555555556, '82-83': 51.69491525423729, '84-85': 82.6086956521739, '86-87': 72.41379310344827, '88-89': 86.66666666666667, '90-91': 65.0, '92-93': 20.408163265306122, '94-95': 80.28169014084507, '96-97': 55.55555555555556, '98-99': 60.810810810810814, '100-101': 63.33333333333333, '102-103': 72.41379310344827, '104-105': 51.06382978723404, '106-107': 66.66666666666666, '108-109': 65.67164179104478, '110-111': 75.0, '112-113': 78.84615384615384, '114-115': 65.0, '116-117': 91.66666666666666, '118-119': 76.92307692307693, '120-121': 73.77049180327869, '122-123': 73.21428571428571, '124-125': 51.02040816326531, '126-127': 47.16981132075472, '128-129': 45.56962025316456, '130-131': 68.88888888888889, '132-133': 76.47058823529412, '134-135': 50.0, '136-137': 75.51020408163265, '138-139': 69.81132075471697, '140-141': 57.446808510638306, '142-143': 85.41666666666666, '144-145': 82.6086956521739, '146-147': 88.88888888888889, '148-149': 72.72727272727273, '150-151': 57.89473684210527, '152-153': 78.37837837837837, '154-155': 78.0, '156-157': 82.14285714285714, '158-159': 88.88888888888889, '160-161': 93.47826086956522, '162-163': 91.80327868852459, '164-165': 90.0}
2025-12-11 20:22:26,843 [trainer.py] => Ave Acc (W-NCM): 68.00%
2025-12-11 20:22:26,843 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 64.52% (best 97.85%); T2: W-NCM 51.61% (best 90.32%); T3: W-NCM 52.81% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 87.04% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 68.57% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 64.06% (best 95.31%); T13: W-NCM 61.22% (best 87.76%); T14: W-NCM 51.30% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 40.91% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 80.00% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 56.25% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 47.27% (best 90.91%); T31: W-NCM 63.41% (best 90.24%); T32: W-NCM 73.58% (best 90.57%); T33: W-NCM 73.33% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 33.33% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 80.56% (best 97.22%); T42: W-NCM 51.69% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 55.56% (best 84.85%); T50: W-NCM 60.81% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 51.06% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 65.67% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 78.85% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 73.77% (best 95.08%); T62: W-NCM 73.21% (best 94.64%); T63: W-NCM 51.02% (best 93.88%); T64: W-NCM 47.17% (best 84.91%); T65: W-NCM 45.57% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 76.47% (best 94.12%); T68: W-NCM 50.00% (best 85.90%); T69: W-NCM 75.51% (best 93.88%); T70: W-NCM 69.81% (best 94.34%); T71: W-NCM 57.45% (best 87.23%); T72: W-NCM 85.42% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 88.89% (best 96.30%); T75: W-NCM 72.73% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 78.38% (best 94.59%); T78: W-NCM 78.00% (best 94.00%); T79: W-NCM 82.14% (best 89.29%); T80: W-NCM 88.89% (best 88.89%); T81: W-NCM 93.48% (best 95.65%); T82: W-NCM 91.80% (best 91.80%); T83: W-NCM 90.00% (best 90.00%)
2025-12-11 20:22:26,843 [trainer.py] => Average forgetting (W-NCM): 24.06% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:22:26,856 [trainer.py] => All params: 144526051
2025-12-11 20:22:26,868 [trainer.py] => Trainable params: 185858
2025-12-11 20:22:26,868 [inflora.py] => Learning on 166-168
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.83.weight', 'image_encoder.blocks.8.attn.lora_B_v.83.weight', 'image_encoder.blocks.9.attn.lora_B_k.83.weight', 'image_encoder.blocks.7.attn.lora_B_k.83.weight', 'image_encoder.blocks.3.attn.lora_B_v.83.weight', 'image_encoder.blocks.4.attn.lora_B_k.83.weight', 'classifier_pool.83.weight', 'image_encoder.blocks.2.attn.lora_B_k.83.weight', 'image_encoder.blocks.1.attn.lora_B_k.83.weight', 'image_encoder.blocks.7.attn.lora_B_v.83.weight', 'image_encoder.blocks.5.attn.lora_B_v.83.weight', 'image_encoder.blocks.1.attn.lora_B_v.83.weight', 'image_encoder.blocks.0.attn.lora_B_k.83.weight', 'image_encoder.blocks.9.attn.lora_B_v.83.weight', 'image_encoder.blocks.2.attn.lora_B_v.83.weight', 'image_encoder.blocks.6.attn.lora_B_v.83.weight', 'image_encoder.blocks.6.attn.lora_B_k.83.weight', 'image_encoder.blocks.10.attn.lora_B_k.83.weight', 'image_encoder.blocks.8.attn.lora_B_k.83.weight', 'image_encoder.blocks.3.attn.lora_B_k.83.weight', 'image_encoder.blocks.5.attn.lora_B_k.83.weight', 'image_encoder.blocks.0.attn.lora_B_v.83.weight', 'classifier_pool.83.bias', 'image_encoder.blocks.4.attn.lora_B_v.83.weight', 'image_encoder.blocks.11.attn.lora_B_k.83.weight', 'image_encoder.blocks.11.attn.lora_B_v.83.weight'}
2025-12-11 20:24:30,647 [inflora.py] => Task 83, Epoch 50/50 => Loss 0.024, Train_accy 98.05
Threshold:  0.9966
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 65/768 type remove
Layer 3 : 152/768 type remove
Layer 4 : 233/768 type remove
Layer 5 : 294/768 type remove
Layer 6 : 304/768 type remove
Layer 7 : 354/768 type remove
Layer 8 : 362/768 type retain
Layer 9 : 266/768 type retain
Layer 10 : 220/768 type retain
Layer 11 : 323/768 type retain
Layer 12 : 215/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:24:37,721 [trainer.py] => Time:130.85281538963318
4933 4933
4933 4933
2025-12-11 20:24:52,342 [trainer.py] => Time:14.620905876159668
2025-12-11 20:24:52,342 [inflora.py] => Exemplar size: 0
2025-12-11 20:24:52,342 [trainer.py] => CNN: {'total': np.float64(36.41), '00-01': np.float64(64.52), '02-03': np.float64(35.48), '04-05': np.float64(46.07), '06-07': np.float64(58.93), '08-09': np.float64(35.62), '10-11': np.float64(9.76), '12-13': np.float64(46.3), '14-15': np.float64(21.43), '16-17': np.float64(24.29), '18-19': np.float64(51.79), '20-21': np.float64(65.93), '22-23': np.float64(56.25), '24-25': np.float64(4.08), '26-27': np.float64(66.09), '28-29': np.float64(66.67), '30-31': np.float64(36.36), '32-33': np.float64(2.27), '34-35': np.float64(19.35), '36-37': np.float64(55.0), '38-39': np.float64(41.46), '40-41': np.float64(29.63), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(46.81), '50-51': np.float64(27.03), '52-53': np.float64(27.78), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(1.82), '60-61': np.float64(34.15), '62-63': np.float64(35.85), '64-65': np.float64(3.33), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(26.03), '72-73': np.float64(35.0), '74-75': np.float64(34.62), '76-77': np.float64(28.92), '78-79': np.float64(20.73), '80-81': np.float64(48.61), '82-83': np.float64(19.49), '84-85': np.float64(50.0), '86-87': np.float64(29.89), '88-89': np.float64(43.33), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(21.13), '96-97': np.float64(11.11), '98-99': np.float64(27.03), '100-101': np.float64(41.67), '102-103': np.float64(41.38), '104-105': np.float64(8.51), '106-107': np.float64(35.9), '108-109': np.float64(56.72), '110-111': np.float64(18.33), '112-113': np.float64(73.08), '114-115': np.float64(55.0), '116-117': np.float64(50.0), '118-119': np.float64(38.46), '120-121': np.float64(57.38), '122-123': np.float64(21.43), '124-125': np.float64(26.53), '126-127': np.float64(9.43), '128-129': np.float64(7.59), '130-131': np.float64(46.67), '132-133': np.float64(38.24), '134-135': np.float64(56.41), '136-137': np.float64(48.98), '138-139': np.float64(52.83), '140-141': np.float64(34.04), '142-143': np.float64(50.0), '144-145': np.float64(65.22), '146-147': np.float64(48.15), '148-149': np.float64(27.27), '150-151': np.float64(13.16), '152-153': np.float64(16.22), '154-155': np.float64(2.0), '156-157': np.float64(30.36), '158-159': np.float64(38.1), '160-161': np.float64(36.96), '162-163': np.float64(50.82), '164-165': np.float64(47.78), '166-167': np.float64(52.63), 'old': np.float64(36.22), 'new': np.float64(52.63)}
2025-12-11 20:24:52,343 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41)]
2025-12-11 20:24:52,343 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52)]
2025-12-11 20:24:52,343 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234]
2025-12-11 20:25:08,647 [trainer.py] => W-NCM: {'00-01': 56.98924731182796, '02-03': 43.54838709677419, '04-05': 50.56179775280899, '06-07': 73.21428571428571, '08-09': 58.9041095890411, '10-11': 51.21951219512195, '12-13': 83.33333333333334, '14-15': 71.42857142857143, '16-17': 65.71428571428571, '18-19': 62.5, '20-21': 67.03296703296702, '22-23': 57.8125, '24-25': 55.10204081632652, '26-27': 40.869565217391305, '28-29': 61.72839506172839, '30-31': 27.27272727272727, '32-33': 86.36363636363636, '34-35': 58.06451612903226, '36-37': 70.0, '38-39': 51.21951219512195, '40-41': 46.2962962962963, '42-43': 56.25, '44-45': 51.85185185185185, '46-47': 74.07407407407408, '48-49': 68.08510638297872, '50-51': 62.16216216216216, '52-53': 75.0, '54-55': 44.0, '56-57': 72.5, '58-59': 20.0, '60-61': 56.09756097560976, '62-63': 64.15094339622641, '64-65': 60.0, '66-67': 31.57894736842105, '68-69': 62.5, '70-71': 52.054794520547944, '72-73': 65.0, '74-75': 23.076923076923077, '76-77': 62.65060240963856, '78-79': 43.90243902439025, '80-81': 70.83333333333334, '82-83': 34.74576271186441, '84-85': 78.26086956521739, '86-87': 66.66666666666666, '88-89': 83.33333333333334, '90-91': 62.5, '92-93': 18.367346938775512, '94-95': 71.83098591549296, '96-97': 36.36363636363637, '98-99': 52.702702702702695, '100-101': 56.666666666666664, '102-103': 63.793103448275865, '104-105': 44.680851063829785, '106-107': 56.41025641025641, '108-109': 46.26865671641791, '110-111': 66.66666666666666, '112-113': 75.0, '114-115': 60.0, '116-117': 88.88888888888889, '118-119': 71.15384615384616, '120-121': 73.77049180327869, '122-123': 62.5, '124-125': 34.69387755102041, '126-127': 45.28301886792453, '128-129': 37.9746835443038, '130-131': 66.66666666666666, '132-133': 79.41176470588235, '134-135': 39.743589743589745, '136-137': 67.3469387755102, '138-139': 69.81132075471697, '140-141': 53.191489361702125, '142-143': 83.33333333333334, '144-145': 82.6086956521739, '146-147': 77.77777777777779, '148-149': 66.66666666666666, '150-151': 52.63157894736842, '152-153': 62.16216216216216, '154-155': 74.0, '156-157': 71.42857142857143, '158-159': 80.95238095238095, '160-161': 89.13043478260869, '162-163': 81.9672131147541, '164-165': 86.66666666666667, '166-167': 94.73684210526315}
2025-12-11 20:25:08,648 [trainer.py] => Ave Acc (W-NCM): 61.00%
2025-12-11 20:25:08,648 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 56.99% (best 97.85%); T2: W-NCM 43.55% (best 90.32%); T3: W-NCM 50.56% (best 91.01%); T4: W-NCM 73.21% (best 92.86%); T5: W-NCM 58.90% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 71.43% (best 94.64%); T9: W-NCM 65.71% (best 98.57%); T10: W-NCM 62.50% (best 94.64%); T11: W-NCM 67.03% (best 95.60%); T12: W-NCM 57.81% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 40.87% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 27.27% (best 93.64%); T17: W-NCM 86.36% (best 97.73%); T18: W-NCM 58.06% (best 96.77%); T19: W-NCM 70.00% (best 86.67%); T20: W-NCM 51.22% (best 97.56%); T21: W-NCM 46.30% (best 94.44%); T22: W-NCM 56.25% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 68.09% (best 94.68%); T26: W-NCM 62.16% (best 100.00%); T27: W-NCM 75.00% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 72.50% (best 95.00%); T30: W-NCM 20.00% (best 90.91%); T31: W-NCM 56.10% (best 90.24%); T32: W-NCM 64.15% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 31.58% (best 100.00%); T35: W-NCM 62.50% (best 91.67%); T36: W-NCM 52.05% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 23.08% (best 87.18%); T39: W-NCM 62.65% (best 78.31%); T40: W-NCM 43.90% (best 89.02%); T41: W-NCM 70.83% (best 97.22%); T42: W-NCM 34.75% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 66.67% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 18.37% (best 85.71%); T48: W-NCM 71.83% (best 95.77%); T49: W-NCM 36.36% (best 84.85%); T50: W-NCM 52.70% (best 91.89%); T51: W-NCM 56.67% (best 91.67%); T52: W-NCM 63.79% (best 87.93%); T53: W-NCM 44.68% (best 79.79%); T54: W-NCM 56.41% (best 89.74%); T55: W-NCM 46.27% (best 91.04%); T56: W-NCM 66.67% (best 96.67%); T57: W-NCM 75.00% (best 94.23%); T58: W-NCM 60.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 71.15% (best 100.00%); T61: W-NCM 73.77% (best 95.08%); T62: W-NCM 62.50% (best 94.64%); T63: W-NCM 34.69% (best 93.88%); T64: W-NCM 45.28% (best 84.91%); T65: W-NCM 37.97% (best 92.41%); T66: W-NCM 66.67% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 39.74% (best 85.90%); T69: W-NCM 67.35% (best 93.88%); T70: W-NCM 69.81% (best 94.34%); T71: W-NCM 53.19% (best 87.23%); T72: W-NCM 83.33% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 62.16% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 71.43% (best 89.29%); T80: W-NCM 80.95% (best 88.89%); T81: W-NCM 89.13% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 86.67% (best 90.00%); T84: W-NCM 94.74% (best 94.74%)
2025-12-11 20:25:08,648 [trainer.py] => Average forgetting (W-NCM): 31.17% | Max forgetting (W-NCM): 70.91%
2025-12-11 20:25:08,660 [trainer.py] => All params: 144526051
2025-12-11 20:25:08,672 [trainer.py] => Trainable params: 185858
2025-12-11 20:25:08,672 [inflora.py] => Learning on 168-170
Parameters to be updated: {'classifier_pool.84.bias', 'classifier_pool.84.weight', 'image_encoder.blocks.11.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_v.84.weight', 'image_encoder.blocks.3.attn.lora_B_v.84.weight', 'image_encoder.blocks.2.attn.lora_B_k.84.weight', 'image_encoder.blocks.10.attn.lora_B_v.84.weight', 'image_encoder.blocks.1.attn.lora_B_k.84.weight', 'image_encoder.blocks.7.attn.lora_B_k.84.weight', 'image_encoder.blocks.4.attn.lora_B_v.84.weight', 'image_encoder.blocks.7.attn.lora_B_v.84.weight', 'image_encoder.blocks.1.attn.lora_B_v.84.weight', 'image_encoder.blocks.11.attn.lora_B_v.84.weight', 'image_encoder.blocks.0.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_k.84.weight', 'image_encoder.blocks.6.attn.lora_B_k.84.weight', 'image_encoder.blocks.10.attn.lora_B_k.84.weight', 'image_encoder.blocks.3.attn.lora_B_k.84.weight', 'image_encoder.blocks.4.attn.lora_B_k.84.weight', 'image_encoder.blocks.8.attn.lora_B_v.84.weight', 'image_encoder.blocks.9.attn.lora_B_v.84.weight', 'image_encoder.blocks.5.attn.lora_B_v.84.weight', 'image_encoder.blocks.8.attn.lora_B_k.84.weight', 'image_encoder.blocks.9.attn.lora_B_k.84.weight', 'image_encoder.blocks.2.attn.lora_B_v.84.weight', 'image_encoder.blocks.0.attn.lora_B_k.84.weight'}
2025-12-11 20:27:36,946 [inflora.py] => Task 84, Epoch 50/50 => Loss 0.054, Train_accy 98.52
Threshold:  0.9968
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 65/768 type remove
Layer 3 : 153/768 type remove
Layer 4 : 237/768 type remove
Layer 5 : 304/768 type remove
Layer 6 : 316/768 type remove
Layer 7 : 366/768 type remove
Layer 8 : 352/768 type retain
Layer 9 : 257/768 type retain
Layer 10 : 211/768 type retain
Layer 11 : 314/768 type retain
Layer 12 : 206/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:27:44,748 [trainer.py] => Time:156.07567715644836
5005 5005
5005 5005
2025-12-11 20:27:59,520 [trainer.py] => Time:14.771350383758545
2025-12-11 20:27:59,520 [inflora.py] => Exemplar size: 0
2025-12-11 20:27:59,520 [trainer.py] => CNN: {'total': np.float64(36.54), '00-01': np.float64(64.52), '02-03': np.float64(38.71), '04-05': np.float64(44.94), '06-07': np.float64(60.71), '08-09': np.float64(34.25), '10-11': np.float64(12.2), '12-13': np.float64(38.89), '14-15': np.float64(21.43), '16-17': np.float64(18.57), '18-19': np.float64(53.57), '20-21': np.float64(63.74), '22-23': np.float64(56.25), '24-25': np.float64(6.12), '26-27': np.float64(66.09), '28-29': np.float64(66.67), '30-31': np.float64(31.82), '32-33': np.float64(2.27), '34-35': np.float64(19.35), '36-37': np.float64(56.67), '38-39': np.float64(43.9), '40-41': np.float64(27.78), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(53.19), '50-51': np.float64(27.03), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(32.5), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(33.96), '64-65': np.float64(0.0), '66-67': np.float64(21.05), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(35.0), '74-75': np.float64(37.18), '76-77': np.float64(28.92), '78-79': np.float64(21.95), '80-81': np.float64(52.78), '82-83': np.float64(21.19), '84-85': np.float64(47.83), '86-87': np.float64(28.74), '88-89': np.float64(43.33), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(21.13), '96-97': np.float64(14.14), '98-99': np.float64(27.03), '100-101': np.float64(41.67), '102-103': np.float64(41.38), '104-105': np.float64(9.57), '106-107': np.float64(28.21), '108-109': np.float64(56.72), '110-111': np.float64(15.0), '112-113': np.float64(71.15), '114-115': np.float64(55.0), '116-117': np.float64(52.78), '118-119': np.float64(40.38), '120-121': np.float64(55.74), '122-123': np.float64(21.43), '124-125': np.float64(30.61), '126-127': np.float64(9.43), '128-129': np.float64(8.86), '130-131': np.float64(42.22), '132-133': np.float64(38.24), '134-135': np.float64(55.13), '136-137': np.float64(51.02), '138-139': np.float64(52.83), '140-141': np.float64(31.91), '142-143': np.float64(56.25), '144-145': np.float64(67.39), '146-147': np.float64(51.85), '148-149': np.float64(30.3), '150-151': np.float64(13.16), '152-153': np.float64(16.22), '154-155': np.float64(4.0), '156-157': np.float64(25.0), '158-159': np.float64(31.75), '160-161': np.float64(36.96), '162-163': np.float64(50.82), '164-165': np.float64(46.67), '166-167': np.float64(50.88), '168-169': np.float64(52.78), 'old': np.float64(36.31), 'new': np.float64(52.78)}
2025-12-11 20:27:59,520 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54)]
2025-12-11 20:27:59,521 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64)]
2025-12-11 20:27:59,521 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543]
2025-12-11 20:28:16,493 [trainer.py] => W-NCM: {'00-01': 62.365591397849464, '02-03': 51.61290322580645, '04-05': 51.68539325842697, '06-07': 78.57142857142857, '08-09': 60.273972602739725, '10-11': 51.21951219512195, '12-13': 83.33333333333334, '14-15': 73.21428571428571, '16-17': 70.0, '18-19': 67.85714285714286, '20-21': 70.32967032967034, '22-23': 60.9375, '24-25': 55.10204081632652, '26-27': 46.08695652173913, '28-29': 64.19753086419753, '30-31': 29.09090909090909, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 71.66666666666667, '38-39': 56.09756097560976, '40-41': 51.85185185185185, '42-43': 62.5, '44-45': 51.85185185185185, '46-47': 74.07407407407408, '48-49': 70.2127659574468, '50-51': 64.86486486486487, '52-53': 80.55555555555556, '54-55': 44.0, '56-57': 77.5, '58-59': 32.72727272727273, '60-61': 58.536585365853654, '62-63': 66.98113207547169, '64-65': 60.0, '66-67': 36.84210526315789, '68-69': 70.83333333333334, '70-71': 53.42465753424658, '72-73': 65.0, '74-75': 30.76923076923077, '76-77': 63.85542168674698, '78-79': 51.21951219512195, '80-81': 75.0, '82-83': 39.83050847457627, '84-85': 80.43478260869566, '86-87': 70.11494252873564, '88-89': 83.33333333333334, '90-91': 62.5, '92-93': 18.367346938775512, '94-95': 71.83098591549296, '96-97': 40.4040404040404, '98-99': 54.054054054054056, '100-101': 58.333333333333336, '102-103': 65.51724137931035, '104-105': 47.87234042553192, '106-107': 56.41025641025641, '108-109': 49.25373134328358, '110-111': 68.33333333333333, '112-113': 76.92307692307693, '114-115': 65.0, '116-117': 91.66666666666666, '118-119': 71.15384615384616, '120-121': 77.04918032786885, '122-123': 69.64285714285714, '124-125': 34.69387755102041, '126-127': 56.60377358490566, '128-129': 43.037974683544306, '130-131': 68.88888888888889, '132-133': 73.52941176470588, '134-135': 41.02564102564102, '136-137': 71.42857142857143, '138-139': 69.81132075471697, '140-141': 55.319148936170215, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 74.07407407407408, '148-149': 69.6969696969697, '150-151': 50.0, '152-153': 67.56756756756756, '154-155': 70.0, '156-157': 73.21428571428571, '158-159': 84.12698412698413, '160-161': 86.95652173913044, '162-163': 81.9672131147541, '164-165': 84.44444444444444, '166-167': 94.73684210526315, '168-169': 88.88888888888889}
2025-12-11 20:28:16,494 [trainer.py] => Ave Acc (W-NCM): 63.84%
2025-12-11 20:28:16,494 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 62.37% (best 97.85%); T2: W-NCM 51.61% (best 90.32%); T3: W-NCM 51.69% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 60.27% (best 83.56%); T6: W-NCM 51.22% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 70.00% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 70.33% (best 95.60%); T12: W-NCM 60.94% (best 95.31%); T13: W-NCM 55.10% (best 87.76%); T14: W-NCM 46.09% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 29.09% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 71.67% (best 86.67%); T20: W-NCM 56.10% (best 97.56%); T21: W-NCM 51.85% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 74.07% (best 92.59%); T25: W-NCM 70.21% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 80.56% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 77.50% (best 95.00%); T30: W-NCM 32.73% (best 90.91%); T31: W-NCM 58.54% (best 90.24%); T32: W-NCM 66.98% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 36.84% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 53.42% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 30.77% (best 87.18%); T39: W-NCM 63.86% (best 78.31%); T40: W-NCM 51.22% (best 89.02%); T41: W-NCM 75.00% (best 97.22%); T42: W-NCM 39.83% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 70.11% (best 91.95%); T45: W-NCM 83.33% (best 96.67%); T46: W-NCM 62.50% (best 92.50%); T47: W-NCM 18.37% (best 85.71%); T48: W-NCM 71.83% (best 95.77%); T49: W-NCM 40.40% (best 84.85%); T50: W-NCM 54.05% (best 91.89%); T51: W-NCM 58.33% (best 91.67%); T52: W-NCM 65.52% (best 87.93%); T53: W-NCM 47.87% (best 79.79%); T54: W-NCM 56.41% (best 89.74%); T55: W-NCM 49.25% (best 91.04%); T56: W-NCM 68.33% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 71.15% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 69.64% (best 94.64%); T63: W-NCM 34.69% (best 93.88%); T64: W-NCM 56.60% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 73.53% (best 94.12%); T68: W-NCM 41.03% (best 85.90%); T69: W-NCM 71.43% (best 93.88%); T70: W-NCM 69.81% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 74.07% (best 96.30%); T75: W-NCM 69.70% (best 90.91%); T76: W-NCM 50.00% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 70.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 84.13% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 84.44% (best 90.00%); T84: W-NCM 94.74% (best 94.74%); T85: W-NCM 88.89% (best 88.89%)
2025-12-11 20:28:16,494 [trainer.py] => Average forgetting (W-NCM): 28.26% | Max forgetting (W-NCM): 67.35%
2025-12-11 20:28:16,506 [trainer.py] => All params: 144526051
2025-12-11 20:28:16,518 [trainer.py] => Trainable params: 185858
2025-12-11 20:28:16,518 [inflora.py] => Learning on 170-172
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_v.85.weight', 'image_encoder.blocks.2.attn.lora_B_k.85.weight', 'image_encoder.blocks.11.attn.lora_B_k.85.weight', 'image_encoder.blocks.8.attn.lora_B_v.85.weight', 'image_encoder.blocks.4.attn.lora_B_k.85.weight', 'image_encoder.blocks.10.attn.lora_B_k.85.weight', 'image_encoder.blocks.5.attn.lora_B_v.85.weight', 'classifier_pool.85.weight', 'image_encoder.blocks.6.attn.lora_B_k.85.weight', 'image_encoder.blocks.9.attn.lora_B_k.85.weight', 'image_encoder.blocks.2.attn.lora_B_v.85.weight', 'image_encoder.blocks.10.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_v.85.weight', 'image_encoder.blocks.8.attn.lora_B_k.85.weight', 'image_encoder.blocks.11.attn.lora_B_v.85.weight', 'image_encoder.blocks.0.attn.lora_B_v.85.weight', 'image_encoder.blocks.1.attn.lora_B_k.85.weight', 'image_encoder.blocks.6.attn.lora_B_v.85.weight', 'image_encoder.blocks.3.attn.lora_B_v.85.weight', 'image_encoder.blocks.4.attn.lora_B_v.85.weight', 'image_encoder.blocks.5.attn.lora_B_k.85.weight', 'image_encoder.blocks.7.attn.lora_B_k.85.weight', 'classifier_pool.85.bias', 'image_encoder.blocks.7.attn.lora_B_v.85.weight', 'image_encoder.blocks.3.attn.lora_B_k.85.weight'}
2025-12-11 20:30:13,552 [inflora.py] => Task 85, Epoch 50/50 => Loss 0.038, Train_accy 98.73
Threshold:  0.997
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 66/768 type remove
Layer 3 : 156/768 type remove
Layer 4 : 241/768 type remove
Layer 5 : 312/768 type remove
Layer 6 : 323/768 type remove
Layer 7 : 375/768 type remove
Layer 8 : 342/768 type retain
Layer 9 : 248/768 type retain
Layer 10 : 202/768 type retain
Layer 11 : 298/768 type retain
Layer 12 : 190/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:30:21,441 [trainer.py] => Time:124.92259454727173
5072 5072
5072 5072
2025-12-11 20:30:36,357 [trainer.py] => Time:14.915684700012207
2025-12-11 20:30:36,357 [inflora.py] => Exemplar size: 0
2025-12-11 20:30:36,357 [trainer.py] => CNN: {'total': np.float64(37.44), '00-01': np.float64(64.52), '02-03': np.float64(38.71), '04-05': np.float64(44.94), '06-07': np.float64(58.93), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(42.59), '14-15': np.float64(23.21), '16-17': np.float64(18.57), '18-19': np.float64(55.36), '20-21': np.float64(62.64), '22-23': np.float64(57.03), '24-25': np.float64(2.04), '26-27': np.float64(66.96), '28-29': np.float64(70.37), '30-31': np.float64(34.55), '32-33': np.float64(2.27), '34-35': np.float64(19.35), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(29.63), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(54.26), '50-51': np.float64(32.43), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(33.02), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(27.4), '72-73': np.float64(32.5), '74-75': np.float64(38.46), '76-77': np.float64(30.12), '78-79': np.float64(23.17), '80-81': np.float64(56.94), '82-83': np.float64(22.03), '84-85': np.float64(45.65), '86-87': np.float64(34.48), '88-89': np.float64(45.0), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(23.94), '96-97': np.float64(12.12), '98-99': np.float64(27.03), '100-101': np.float64(43.33), '102-103': np.float64(41.38), '104-105': np.float64(11.7), '106-107': np.float64(28.21), '108-109': np.float64(53.73), '110-111': np.float64(18.33), '112-113': np.float64(71.15), '114-115': np.float64(50.0), '116-117': np.float64(58.33), '118-119': np.float64(40.38), '120-121': np.float64(57.38), '122-123': np.float64(19.64), '124-125': np.float64(30.61), '126-127': np.float64(9.43), '128-129': np.float64(10.13), '130-131': np.float64(44.44), '132-133': np.float64(38.24), '134-135': np.float64(58.97), '136-137': np.float64(53.06), '138-139': np.float64(54.72), '140-141': np.float64(36.17), '142-143': np.float64(56.25), '144-145': np.float64(67.39), '146-147': np.float64(51.85), '148-149': np.float64(27.27), '150-151': np.float64(10.53), '152-153': np.float64(16.22), '154-155': np.float64(4.0), '156-157': np.float64(35.71), '158-159': np.float64(30.16), '160-161': np.float64(34.78), '162-163': np.float64(50.82), '164-165': np.float64(47.78), '166-167': np.float64(50.88), '168-169': np.float64(52.78), '170-171': np.float64(34.33), 'old': np.float64(37.48), 'new': np.float64(34.33)}
2025-12-11 20:30:36,358 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44)]
2025-12-11 20:30:36,358 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64)]
2025-12-11 20:30:36,358 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773]
2025-12-11 20:30:52,942 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 56.451612903225815, '04-05': 53.93258426966292, '06-07': 78.57142857142857, '08-09': 63.013698630136986, '10-11': 53.65853658536586, '12-13': 83.33333333333334, '14-15': 75.0, '16-17': 71.42857142857143, '18-19': 67.85714285714286, '20-21': 71.42857142857143, '22-23': 61.71875, '24-25': 59.183673469387756, '26-27': 49.56521739130435, '28-29': 67.90123456790124, '30-31': 34.54545454545455, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 75.0, '38-39': 60.97560975609756, '40-41': 53.70370370370371, '42-43': 62.5, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 73.40425531914893, '50-51': 64.86486486486487, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 80.0, '58-59': 41.81818181818181, '60-61': 68.29268292682927, '62-63': 69.81132075471697, '64-65': 63.33333333333333, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 54.794520547945204, '72-73': 65.0, '74-75': 32.05128205128205, '76-77': 66.26506024096386, '78-79': 52.4390243902439, '80-81': 79.16666666666666, '82-83': 43.22033898305085, '84-85': 82.6086956521739, '86-87': 72.41379310344827, '88-89': 85.0, '90-91': 65.0, '92-93': 20.408163265306122, '94-95': 74.64788732394366, '96-97': 48.484848484848484, '98-99': 55.4054054054054, '100-101': 60.0, '102-103': 67.24137931034483, '104-105': 50.0, '106-107': 64.1025641025641, '108-109': 59.70149253731343, '110-111': 68.33333333333333, '112-113': 76.92307692307693, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 71.15384615384616, '120-121': 77.04918032786885, '122-123': 69.64285714285714, '124-125': 40.816326530612244, '126-127': 62.264150943396224, '128-129': 46.835443037974684, '130-131': 68.88888888888889, '132-133': 76.47058823529412, '134-135': 43.58974358974359, '136-137': 69.38775510204081, '138-139': 71.69811320754717, '140-141': 57.446808510638306, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 74.07407407407408, '148-149': 63.63636363636363, '150-151': 52.63157894736842, '152-153': 64.86486486486487, '154-155': 72.0, '156-157': 71.42857142857143, '158-159': 82.53968253968253, '160-161': 89.13043478260869, '162-163': 81.9672131147541, '164-165': 84.44444444444444, '166-167': 94.73684210526315, '168-169': 84.72222222222221, '170-171': 94.02985074626866}
2025-12-11 20:30:52,942 [trainer.py] => Ave Acc (W-NCM): 66.15%
2025-12-11 20:30:52,942 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 53.93% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 63.01% (best 83.56%); T6: W-NCM 53.66% (best 80.49%); T7: W-NCM 83.33% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 71.43% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 61.72% (best 95.31%); T13: W-NCM 59.18% (best 87.76%); T14: W-NCM 49.57% (best 94.78%); T15: W-NCM 67.90% (best 96.30%); T16: W-NCM 34.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 73.40% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 80.00% (best 95.00%); T30: W-NCM 41.82% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 69.81% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 54.79% (best 91.78%); T37: W-NCM 65.00% (best 92.50%); T38: W-NCM 32.05% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 52.44% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 43.22% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 85.00% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 74.65% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 55.41% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 50.00% (best 79.79%); T54: W-NCM 64.10% (best 89.74%); T55: W-NCM 59.70% (best 91.04%); T56: W-NCM 68.33% (best 96.67%); T57: W-NCM 76.92% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 71.15% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 69.64% (best 94.64%); T63: W-NCM 40.82% (best 93.88%); T64: W-NCM 62.26% (best 84.91%); T65: W-NCM 46.84% (best 92.41%); T66: W-NCM 68.89% (best 95.56%); T67: W-NCM 76.47% (best 94.12%); T68: W-NCM 43.59% (best 85.90%); T69: W-NCM 69.39% (best 93.88%); T70: W-NCM 71.70% (best 94.34%); T71: W-NCM 57.45% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 74.07% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 64.86% (best 94.59%); T78: W-NCM 72.00% (best 94.00%); T79: W-NCM 71.43% (best 89.29%); T80: W-NCM 82.54% (best 88.89%); T81: W-NCM 89.13% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 84.44% (best 90.00%); T84: W-NCM 94.74% (best 94.74%); T85: W-NCM 84.72% (best 88.89%); T86: W-NCM 94.03% (best 94.03%)
2025-12-11 20:30:52,942 [trainer.py] => Average forgetting (W-NCM): 25.94% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:30:52,955 [trainer.py] => All params: 144526051
2025-12-11 20:30:52,967 [trainer.py] => Trainable params: 185858
2025-12-11 20:30:52,967 [inflora.py] => Learning on 172-174
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.86.weight', 'image_encoder.blocks.8.attn.lora_B_k.86.weight', 'image_encoder.blocks.9.attn.lora_B_k.86.weight', 'image_encoder.blocks.3.attn.lora_B_k.86.weight', 'classifier_pool.86.bias', 'image_encoder.blocks.7.attn.lora_B_k.86.weight', 'image_encoder.blocks.10.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_k.86.weight', 'image_encoder.blocks.6.attn.lora_B_v.86.weight', 'image_encoder.blocks.7.attn.lora_B_v.86.weight', 'image_encoder.blocks.8.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_k.86.weight', 'image_encoder.blocks.0.attn.lora_B_k.86.weight', 'image_encoder.blocks.9.attn.lora_B_v.86.weight', 'image_encoder.blocks.11.attn.lora_B_k.86.weight', 'image_encoder.blocks.5.attn.lora_B_k.86.weight', 'image_encoder.blocks.3.attn.lora_B_v.86.weight', 'image_encoder.blocks.5.attn.lora_B_v.86.weight', 'image_encoder.blocks.1.attn.lora_B_k.86.weight', 'image_encoder.blocks.4.attn.lora_B_v.86.weight', 'image_encoder.blocks.6.attn.lora_B_k.86.weight', 'image_encoder.blocks.11.attn.lora_B_v.86.weight', 'image_encoder.blocks.2.attn.lora_B_v.86.weight', 'image_encoder.blocks.1.attn.lora_B_v.86.weight', 'classifier_pool.86.weight', 'image_encoder.blocks.0.attn.lora_B_v.86.weight'}
2025-12-11 20:32:43,860 [inflora.py] => Task 86, Epoch 50/50 => Loss 0.046, Train_accy 98.53
Threshold:  0.9972
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 67/768 type remove
Layer 3 : 157/768 type remove
Layer 4 : 242/768 type remove
Layer 5 : 315/768 type remove
Layer 6 : 326/768 type remove
Layer 7 : 379/768 type remove
Layer 8 : 338/768 type retain
Layer 9 : 244/768 type retain
Layer 10 : 197/768 type retain
Layer 11 : 285/768 type retain
Layer 12 : 171/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:32:51,632 [trainer.py] => Time:118.66507840156555
5125 5125
5125 5125
2025-12-11 20:33:06,786 [trainer.py] => Time:15.153753280639648
2025-12-11 20:33:06,787 [inflora.py] => Exemplar size: 0
2025-12-11 20:33:06,787 [trainer.py] => CNN: {'total': np.float64(37.11), '00-01': np.float64(62.37), '02-03': np.float64(37.1), '04-05': np.float64(43.82), '06-07': np.float64(58.93), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(40.74), '14-15': np.float64(25.0), '16-17': np.float64(24.29), '18-19': np.float64(53.57), '20-21': np.float64(64.84), '22-23': np.float64(58.59), '24-25': np.float64(0.0), '26-27': np.float64(65.22), '28-29': np.float64(69.14), '30-31': np.float64(33.64), '32-33': np.float64(4.55), '34-35': np.float64(16.13), '36-37': np.float64(56.67), '38-39': np.float64(48.78), '40-41': np.float64(27.78), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(59.26), '48-49': np.float64(52.13), '50-51': np.float64(29.73), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(34.91), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(32.5), '74-75': np.float64(37.18), '76-77': np.float64(31.33), '78-79': np.float64(24.39), '80-81': np.float64(52.78), '82-83': np.float64(21.19), '84-85': np.float64(43.48), '86-87': np.float64(33.33), '88-89': np.float64(45.0), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(23.94), '96-97': np.float64(16.16), '98-99': np.float64(25.68), '100-101': np.float64(38.33), '102-103': np.float64(39.66), '104-105': np.float64(9.57), '106-107': np.float64(23.08), '108-109': np.float64(53.73), '110-111': np.float64(21.67), '112-113': np.float64(69.23), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(38.46), '120-121': np.float64(54.1), '122-123': np.float64(19.64), '124-125': np.float64(30.61), '126-127': np.float64(9.43), '128-129': np.float64(13.92), '130-131': np.float64(44.44), '132-133': np.float64(38.24), '134-135': np.float64(58.97), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(60.42), '144-145': np.float64(65.22), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(16.22), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(25.4), '160-161': np.float64(34.78), '162-163': np.float64(54.1), '164-165': np.float64(44.44), '166-167': np.float64(50.88), '168-169': np.float64(56.94), '170-171': np.float64(37.31), '172-173': np.float64(37.74), 'old': np.float64(37.11), 'new': np.float64(37.74)}
2025-12-11 20:33:06,787 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11)]
2025-12-11 20:33:06,787 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57)]
2025-12-11 20:33:06,787 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122]
2025-12-11 20:33:23,518 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 54.83870967741935, '04-05': 56.17977528089888, '06-07': 76.78571428571429, '08-09': 65.75342465753424, '10-11': 58.536585365853654, '12-13': 85.18518518518519, '14-15': 73.21428571428571, '16-17': 72.85714285714285, '18-19': 67.85714285714286, '20-21': 71.42857142857143, '22-23': 64.84375, '24-25': 63.26530612244898, '26-27': 50.43478260869565, '28-29': 64.19753086419753, '30-31': 36.36363636363637, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 75.0, '38-39': 60.97560975609756, '40-41': 53.70370370370371, '42-43': 62.5, '44-45': 48.148148148148145, '46-47': 77.77777777777779, '48-49': 74.46808510638297, '50-51': 67.56756756756756, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 82.5, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 71.69811320754717, '64-65': 70.0, '66-67': 42.10526315789473, '68-69': 70.83333333333334, '70-71': 56.16438356164384, '72-73': 67.5, '74-75': 34.61538461538461, '76-77': 65.06024096385542, '78-79': 53.65853658536586, '80-81': 81.94444444444444, '82-83': 44.91525423728814, '84-85': 82.6086956521739, '86-87': 72.41379310344827, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 20.408163265306122, '94-95': 77.46478873239437, '96-97': 48.484848484848484, '98-99': 62.16216216216216, '100-101': 60.0, '102-103': 68.96551724137932, '104-105': 51.06382978723404, '106-107': 61.53846153846154, '108-109': 59.70149253731343, '110-111': 70.0, '112-113': 80.76923076923077, '114-115': 62.5, '116-117': 88.88888888888889, '118-119': 73.07692307692307, '120-121': 77.04918032786885, '122-123': 66.07142857142857, '124-125': 42.857142857142854, '126-127': 49.056603773584904, '128-129': 43.037974683544306, '130-131': 73.33333333333333, '132-133': 76.47058823529412, '134-135': 50.0, '136-137': 71.42857142857143, '138-139': 71.69811320754717, '140-141': 59.57446808510638, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 74.07407407407408, '148-149': 63.63636363636363, '150-151': 52.63157894736842, '152-153': 67.56756756756756, '154-155': 74.0, '156-157': 71.42857142857143, '158-159': 77.77777777777779, '160-161': 84.78260869565217, '162-163': 80.32786885245902, '164-165': 83.33333333333334, '166-167': 89.47368421052632, '168-169': 84.72222222222221, '170-171': 92.53731343283582, '172-173': 98.11320754716981}
2025-12-11 20:33:23,518 [trainer.py] => Ave Acc (W-NCM): 67.14%
2025-12-11 20:33:23,518 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 54.84% (best 90.32%); T3: W-NCM 56.18% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 65.75% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 71.43% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 50.43% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 36.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 75.00% (best 86.67%); T20: W-NCM 60.98% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 74.47% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 71.70% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 42.11% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 34.62% (best 87.18%); T39: W-NCM 65.06% (best 78.31%); T40: W-NCM 53.66% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 44.92% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 48.48% (best 84.85%); T50: W-NCM 62.16% (best 91.89%); T51: W-NCM 60.00% (best 91.67%); T52: W-NCM 68.97% (best 87.93%); T53: W-NCM 51.06% (best 79.79%); T54: W-NCM 61.54% (best 89.74%); T55: W-NCM 59.70% (best 91.04%); T56: W-NCM 70.00% (best 96.67%); T57: W-NCM 80.77% (best 94.23%); T58: W-NCM 62.50% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 73.08% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 66.07% (best 94.64%); T63: W-NCM 42.86% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 76.47% (best 94.12%); T68: W-NCM 50.00% (best 85.90%); T69: W-NCM 71.43% (best 93.88%); T70: W-NCM 71.70% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 74.07% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 71.43% (best 89.29%); T80: W-NCM 77.78% (best 88.89%); T81: W-NCM 84.78% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 83.33% (best 90.00%); T84: W-NCM 89.47% (best 94.74%); T85: W-NCM 84.72% (best 88.89%); T86: W-NCM 92.54% (best 94.03%); T87: W-NCM 98.11% (best 98.11%)
2025-12-11 20:33:23,518 [trainer.py] => Average forgetting (W-NCM): 25.01% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:33:23,531 [trainer.py] => All params: 144526051
2025-12-11 20:33:23,543 [trainer.py] => Trainable params: 185858
2025-12-11 20:33:23,543 [inflora.py] => Learning on 174-176
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.87.weight', 'image_encoder.blocks.8.attn.lora_B_k.87.weight', 'image_encoder.blocks.1.attn.lora_B_v.87.weight', 'image_encoder.blocks.11.attn.lora_B_k.87.weight', 'image_encoder.blocks.0.attn.lora_B_k.87.weight', 'classifier_pool.87.bias', 'image_encoder.blocks.5.attn.lora_B_k.87.weight', 'image_encoder.blocks.11.attn.lora_B_v.87.weight', 'image_encoder.blocks.4.attn.lora_B_v.87.weight', 'image_encoder.blocks.9.attn.lora_B_v.87.weight', 'image_encoder.blocks.7.attn.lora_B_k.87.weight', 'image_encoder.blocks.6.attn.lora_B_k.87.weight', 'classifier_pool.87.weight', 'image_encoder.blocks.3.attn.lora_B_v.87.weight', 'image_encoder.blocks.6.attn.lora_B_v.87.weight', 'image_encoder.blocks.8.attn.lora_B_v.87.weight', 'image_encoder.blocks.0.attn.lora_B_v.87.weight', 'image_encoder.blocks.1.attn.lora_B_k.87.weight', 'image_encoder.blocks.4.attn.lora_B_k.87.weight', 'image_encoder.blocks.5.attn.lora_B_v.87.weight', 'image_encoder.blocks.2.attn.lora_B_v.87.weight', 'image_encoder.blocks.2.attn.lora_B_k.87.weight', 'image_encoder.blocks.7.attn.lora_B_v.87.weight', 'image_encoder.blocks.10.attn.lora_B_v.87.weight', 'image_encoder.blocks.9.attn.lora_B_k.87.weight', 'image_encoder.blocks.3.attn.lora_B_k.87.weight'}
2025-12-11 20:35:29,561 [inflora.py] => Task 87, Epoch 50/50 => Loss 0.011, Train_accy 99.61
Threshold:  0.9974
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 67/768 type remove
Layer 3 : 159/768 type remove
Layer 4 : 249/768 type remove
Layer 5 : 325/768 type remove
Layer 6 : 336/768 type remove
Layer 7 : 379/768 type retain
Layer 8 : 327/768 type retain
Layer 9 : 234/768 type retain
Layer 10 : 191/768 type retain
Layer 11 : 279/768 type retain
Layer 12 : 168/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:35:36,843 [trainer.py] => Time:133.30048727989197
5201 5201
5201 5201
2025-12-11 20:35:52,342 [trainer.py] => Time:15.498054265975952
2025-12-11 20:35:52,342 [inflora.py] => Exemplar size: 0
2025-12-11 20:35:52,342 [trainer.py] => CNN: {'total': np.float64(36.7), '00-01': np.float64(64.52), '02-03': np.float64(37.1), '04-05': np.float64(41.57), '06-07': np.float64(55.36), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(40.74), '14-15': np.float64(17.86), '16-17': np.float64(21.43), '18-19': np.float64(53.57), '20-21': np.float64(65.93), '22-23': np.float64(60.94), '24-25': np.float64(4.08), '26-27': np.float64(63.48), '28-29': np.float64(65.43), '30-31': np.float64(33.64), '32-33': np.float64(4.55), '34-35': np.float64(16.13), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(29.63), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(47.87), '50-51': np.float64(21.62), '52-53': np.float64(36.11), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(39.62), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(30.0), '74-75': np.float64(39.74), '76-77': np.float64(30.12), '78-79': np.float64(20.73), '80-81': np.float64(52.78), '82-83': np.float64(22.88), '84-85': np.float64(47.83), '86-87': np.float64(31.03), '88-89': np.float64(36.67), '90-91': np.float64(42.5), '92-93': np.float64(12.24), '94-95': np.float64(23.94), '96-97': np.float64(13.13), '98-99': np.float64(25.68), '100-101': np.float64(40.0), '102-103': np.float64(37.93), '104-105': np.float64(9.57), '106-107': np.float64(25.64), '108-109': np.float64(53.73), '110-111': np.float64(20.0), '112-113': np.float64(71.15), '114-115': np.float64(50.0), '116-117': np.float64(61.11), '118-119': np.float64(40.38), '120-121': np.float64(54.1), '122-123': np.float64(19.64), '124-125': np.float64(30.61), '126-127': np.float64(9.43), '128-129': np.float64(12.66), '130-131': np.float64(40.0), '132-133': np.float64(38.24), '134-135': np.float64(61.54), '136-137': np.float64(53.06), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(60.42), '144-145': np.float64(67.39), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(21.62), '154-155': np.float64(6.0), '156-157': np.float64(30.36), '158-159': np.float64(26.98), '160-161': np.float64(36.96), '162-163': np.float64(55.74), '164-165': np.float64(46.67), '166-167': np.float64(50.88), '168-169': np.float64(51.39), '170-171': np.float64(34.33), '172-173': np.float64(35.85), '174-175': np.float64(32.89), 'old': np.float64(36.76), 'new': np.float64(32.89)}
2025-12-11 20:35:52,343 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7)]
2025-12-11 20:35:52,343 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52)]
2025-12-11 20:35:52,343 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006]
2025-12-11 20:36:09,421 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 56.451612903225815, '04-05': 58.42696629213483, '06-07': 76.78571428571429, '08-09': 64.38356164383562, '10-11': 58.536585365853654, '12-13': 85.18518518518519, '14-15': 73.21428571428571, '16-17': 72.85714285714285, '18-19': 67.85714285714286, '20-21': 72.52747252747253, '22-23': 65.625, '24-25': 63.26530612244898, '26-27': 53.04347826086957, '28-29': 62.96296296296296, '30-31': 38.18181818181819, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 76.66666666666667, '38-39': 63.41463414634146, '40-41': 53.70370370370371, '42-43': 62.5, '44-45': 55.55555555555556, '46-47': 77.77777777777779, '48-49': 76.59574468085107, '50-51': 67.56756756756756, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 82.5, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 74.52830188679245, '64-65': 70.0, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 57.534246575342465, '72-73': 67.5, '74-75': 35.8974358974359, '76-77': 67.46987951807229, '78-79': 56.09756097560976, '80-81': 81.94444444444444, '82-83': 45.76271186440678, '84-85': 82.6086956521739, '86-87': 72.41379310344827, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 20.408163265306122, '94-95': 78.87323943661971, '96-97': 47.474747474747474, '98-99': 59.45945945945946, '100-101': 63.33333333333333, '102-103': 68.96551724137932, '104-105': 52.12765957446809, '106-107': 61.53846153846154, '108-109': 59.70149253731343, '110-111': 71.66666666666667, '112-113': 80.76923076923077, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 73.07692307692307, '120-121': 77.04918032786885, '122-123': 67.85714285714286, '124-125': 40.816326530612244, '126-127': 52.83018867924528, '128-129': 40.50632911392405, '130-131': 77.77777777777779, '132-133': 79.41176470588235, '134-135': 51.28205128205128, '136-137': 71.42857142857143, '138-139': 73.58490566037736, '140-141': 59.57446808510638, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 74.07407407407408, '148-149': 63.63636363636363, '150-151': 55.26315789473685, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 73.21428571428571, '158-159': 76.19047619047619, '160-161': 86.95652173913044, '162-163': 81.9672131147541, '164-165': 80.0, '166-167': 91.22807017543859, '168-169': 86.11111111111111, '170-171': 92.53731343283582, '172-173': 94.33962264150944, '174-175': 93.42105263157895}
2025-12-11 20:36:09,422 [trainer.py] => Ave Acc (W-NCM): 68.13%
2025-12-11 20:36:09,422 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 56.45% (best 90.32%); T3: W-NCM 58.43% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 64.38% (best 83.56%); T6: W-NCM 58.54% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 72.86% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 72.53% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 53.04% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 38.18% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 76.67% (best 86.67%); T20: W-NCM 63.41% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 55.56% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 76.60% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 82.50% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 74.53% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 57.53% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 35.90% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 45.76% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 72.41% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 20.41% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 47.47% (best 84.85%); T50: W-NCM 59.46% (best 91.89%); T51: W-NCM 63.33% (best 91.67%); T52: W-NCM 68.97% (best 87.93%); T53: W-NCM 52.13% (best 79.79%); T54: W-NCM 61.54% (best 89.74%); T55: W-NCM 59.70% (best 91.04%); T56: W-NCM 71.67% (best 96.67%); T57: W-NCM 80.77% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 73.08% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 67.86% (best 94.64%); T63: W-NCM 40.82% (best 93.88%); T64: W-NCM 52.83% (best 84.91%); T65: W-NCM 40.51% (best 92.41%); T66: W-NCM 77.78% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 51.28% (best 85.90%); T69: W-NCM 71.43% (best 93.88%); T70: W-NCM 73.58% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 74.07% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 76.19% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 80.00% (best 90.00%); T84: W-NCM 91.23% (best 94.74%); T85: W-NCM 86.11% (best 88.89%); T86: W-NCM 92.54% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 93.42% (best 93.42%)
2025-12-11 20:36:09,422 [trainer.py] => Average forgetting (W-NCM): 24.03% | Max forgetting (W-NCM): 65.31%
2025-12-11 20:36:09,434 [trainer.py] => All params: 144526051
2025-12-11 20:36:09,446 [trainer.py] => Trainable params: 185858
2025-12-11 20:36:09,446 [inflora.py] => Learning on 176-178
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.88.weight', 'image_encoder.blocks.2.attn.lora_B_k.88.weight', 'classifier_pool.88.weight', 'image_encoder.blocks.6.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_v.88.weight', 'image_encoder.blocks.6.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_v.88.weight', 'image_encoder.blocks.9.attn.lora_B_k.88.weight', 'image_encoder.blocks.8.attn.lora_B_k.88.weight', 'image_encoder.blocks.8.attn.lora_B_v.88.weight', 'image_encoder.blocks.2.attn.lora_B_v.88.weight', 'image_encoder.blocks.4.attn.lora_B_k.88.weight', 'classifier_pool.88.bias', 'image_encoder.blocks.4.attn.lora_B_v.88.weight', 'image_encoder.blocks.9.attn.lora_B_v.88.weight', 'image_encoder.blocks.0.attn.lora_B_v.88.weight', 'image_encoder.blocks.10.attn.lora_B_v.88.weight', 'image_encoder.blocks.7.attn.lora_B_k.88.weight', 'image_encoder.blocks.10.attn.lora_B_k.88.weight', 'image_encoder.blocks.7.attn.lora_B_v.88.weight', 'image_encoder.blocks.3.attn.lora_B_k.88.weight', 'image_encoder.blocks.1.attn.lora_B_k.88.weight', 'image_encoder.blocks.11.attn.lora_B_v.88.weight', 'image_encoder.blocks.5.attn.lora_B_k.88.weight', 'image_encoder.blocks.5.attn.lora_B_v.88.weight', 'image_encoder.blocks.11.attn.lora_B_k.88.weight'}
2025-12-11 20:38:21,663 [inflora.py] => Task 88, Epoch 50/50 => Loss 0.062, Train_accy 97.87
Threshold:  0.9976
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 18/768 type remove
Layer 2 : 68/768 type remove
Layer 3 : 161/768 type remove
Layer 4 : 255/768 type remove
Layer 5 : 332/768 type remove
Layer 6 : 343/768 type remove
Layer 7 : 372/768 type retain
Layer 8 : 318/768 type retain
Layer 9 : 226/768 type retain
Layer 10 : 183/768 type retain
Layer 11 : 262/768 type retain
Layer 12 : 149/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:38:30,236 [trainer.py] => Time:140.78969049453735
5276 5276
5276 5276
2025-12-11 20:38:45,867 [trainer.py] => Time:15.630979061126709
2025-12-11 20:38:45,867 [inflora.py] => Exemplar size: 0
2025-12-11 20:38:45,868 [trainer.py] => CNN: {'total': np.float64(37.04), '00-01': np.float64(63.44), '02-03': np.float64(33.87), '04-05': np.float64(44.94), '06-07': np.float64(55.36), '08-09': np.float64(36.99), '10-11': np.float64(12.2), '12-13': np.float64(40.74), '14-15': np.float64(19.64), '16-17': np.float64(17.14), '18-19': np.float64(46.43), '20-21': np.float64(67.03), '22-23': np.float64(60.94), '24-25': np.float64(6.12), '26-27': np.float64(61.74), '28-29': np.float64(67.9), '30-31': np.float64(35.45), '32-33': np.float64(4.55), '34-35': np.float64(19.35), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(33.33), '42-43': np.float64(25.0), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(50.0), '50-51': np.float64(21.62), '52-53': np.float64(27.78), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(35.85), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(35.0), '74-75': np.float64(39.74), '76-77': np.float64(32.53), '78-79': np.float64(23.17), '80-81': np.float64(54.17), '82-83': np.float64(22.03), '84-85': np.float64(50.0), '86-87': np.float64(29.89), '88-89': np.float64(36.67), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(15.15), '98-99': np.float64(27.03), '100-101': np.float64(38.33), '102-103': np.float64(37.93), '104-105': np.float64(9.57), '106-107': np.float64(25.64), '108-109': np.float64(56.72), '110-111': np.float64(18.33), '112-113': np.float64(75.0), '114-115': np.float64(50.0), '116-117': np.float64(63.89), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(17.86), '124-125': np.float64(32.65), '126-127': np.float64(9.43), '128-129': np.float64(13.92), '130-131': np.float64(40.0), '132-133': np.float64(44.12), '134-135': np.float64(62.82), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(34.04), '142-143': np.float64(58.33), '144-145': np.float64(67.39), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(10.53), '152-153': np.float64(18.92), '154-155': np.float64(4.0), '156-157': np.float64(28.57), '158-159': np.float64(25.4), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(50.0), '166-167': np.float64(50.88), '168-169': np.float64(50.0), '170-171': np.float64(35.82), '172-173': np.float64(32.08), '174-175': np.float64(32.89), '176-177': np.float64(45.33), 'old': np.float64(36.92), 'new': np.float64(45.33)}
2025-12-11 20:38:45,868 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04)]
2025-12-11 20:38:45,868 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51)]
2025-12-11 20:38:45,868 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496]
2025-12-11 20:39:03,234 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 67.74193548387096, '04-05': 59.55056179775281, '06-07': 78.57142857142857, '08-09': 67.12328767123287, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 73.21428571428571, '16-17': 75.71428571428571, '18-19': 69.64285714285714, '20-21': 74.72527472527473, '22-23': 67.1875, '24-25': 63.26530612244898, '26-27': 55.65217391304348, '28-29': 64.19753086419753, '30-31': 41.81818181818181, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 79.7872340425532, '50-51': 64.86486486486487, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 85.0, '58-59': 56.36363636363636, '60-61': 68.29268292682927, '62-63': 74.52830188679245, '64-65': 70.0, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 60.273972602739725, '72-73': 70.0, '74-75': 39.743589743589745, '76-77': 67.46987951807229, '78-79': 56.09756097560976, '80-81': 83.33333333333334, '82-83': 51.69491525423729, '84-85': 82.6086956521739, '86-87': 74.71264367816092, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 22.448979591836736, '94-95': 78.87323943661971, '96-97': 49.494949494949495, '98-99': 58.108108108108105, '100-101': 61.66666666666667, '102-103': 72.41379310344827, '104-105': 54.25531914893617, '106-107': 66.66666666666666, '108-109': 62.68656716417911, '110-111': 75.0, '112-113': 80.76923076923077, '114-115': 70.0, '116-117': 88.88888888888889, '118-119': 76.92307692307693, '120-121': 75.40983606557377, '122-123': 71.42857142857143, '124-125': 42.857142857142854, '126-127': 56.60377358490566, '128-129': 43.037974683544306, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 55.12820512820513, '136-137': 77.55102040816327, '138-139': 75.47169811320755, '140-141': 59.57446808510638, '142-143': 89.58333333333334, '144-145': 82.6086956521739, '146-147': 77.77777777777779, '148-149': 63.63636363636363, '150-151': 55.26315789473685, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 75.0, '158-159': 77.77777777777779, '160-161': 86.95652173913044, '162-163': 83.60655737704919, '164-165': 81.11111111111111, '166-167': 89.47368421052632, '168-169': 81.94444444444444, '170-171': 89.55223880597015, '172-173': 94.33962264150944, '174-175': 94.73684210526315, '176-177': 93.33333333333333}
2025-12-11 20:39:03,235 [trainer.py] => Ave Acc (W-NCM): 69.64%
2025-12-11 20:39:03,235 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 75.71% (best 98.57%); T10: W-NCM 69.64% (best 94.64%); T11: W-NCM 74.73% (best 95.60%); T12: W-NCM 67.19% (best 95.31%); T13: W-NCM 63.27% (best 87.76%); T14: W-NCM 55.65% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 41.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 79.79% (best 94.68%); T26: W-NCM 64.86% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 74.53% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 39.74% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 56.10% (best 89.02%); T41: W-NCM 83.33% (best 97.22%); T42: W-NCM 51.69% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 22.45% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 49.49% (best 84.85%); T50: W-NCM 58.11% (best 91.89%); T51: W-NCM 61.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 54.26% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 62.69% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 80.77% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 75.41% (best 95.08%); T62: W-NCM 71.43% (best 94.64%); T63: W-NCM 42.86% (best 93.88%); T64: W-NCM 56.60% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 55.13% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 75.00% (best 89.29%); T80: W-NCM 77.78% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 83.61% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 89.47% (best 94.74%); T85: W-NCM 81.94% (best 88.89%); T86: W-NCM 89.55% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 94.74% (best 94.74%); T89: W-NCM 93.33% (best 93.33%)
2025-12-11 20:39:03,235 [trainer.py] => Average forgetting (W-NCM): 22.53% | Max forgetting (W-NCM): 63.27%
2025-12-11 20:39:03,248 [trainer.py] => All params: 144526051
2025-12-11 20:39:03,259 [trainer.py] => Trainable params: 185858
2025-12-11 20:39:03,260 [inflora.py] => Learning on 178-180
Parameters to be updated: {'classifier_pool.89.bias', 'image_encoder.blocks.8.attn.lora_B_k.89.weight', 'image_encoder.blocks.5.attn.lora_B_k.89.weight', 'image_encoder.blocks.3.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_v.89.weight', 'image_encoder.blocks.5.attn.lora_B_v.89.weight', 'image_encoder.blocks.10.attn.lora_B_v.89.weight', 'image_encoder.blocks.8.attn.lora_B_v.89.weight', 'image_encoder.blocks.9.attn.lora_B_v.89.weight', 'image_encoder.blocks.1.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_v.89.weight', 'image_encoder.blocks.2.attn.lora_B_k.89.weight', 'image_encoder.blocks.11.attn.lora_B_k.89.weight', 'image_encoder.blocks.7.attn.lora_B_k.89.weight', 'image_encoder.blocks.10.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_v.89.weight', 'image_encoder.blocks.6.attn.lora_B_k.89.weight', 'classifier_pool.89.weight', 'image_encoder.blocks.0.attn.lora_B_v.89.weight', 'image_encoder.blocks.9.attn.lora_B_k.89.weight', 'image_encoder.blocks.0.attn.lora_B_k.89.weight', 'image_encoder.blocks.4.attn.lora_B_k.89.weight', 'image_encoder.blocks.7.attn.lora_B_v.89.weight', 'image_encoder.blocks.1.attn.lora_B_k.89.weight', 'image_encoder.blocks.3.attn.lora_B_v.89.weight', 'image_encoder.blocks.2.attn.lora_B_v.89.weight'}
2025-12-11 20:40:46,066 [inflora.py] => Task 89, Epoch 50/50 => Loss 0.059, Train_accy 97.13
Threshold:  0.9978
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 25/768 type remove
Layer 2 : 71/768 type remove
Layer 3 : 166/768 type remove
Layer 4 : 262/768 type remove
Layer 5 : 342/768 type remove
Layer 6 : 356/768 type remove
Layer 7 : 355/768 type retain
Layer 8 : 298/768 type retain
Layer 9 : 207/768 type retain
Layer 10 : 168/768 type retain
Layer 11 : 236/768 type retain
Layer 12 : 137/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:40:53,646 [trainer.py] => Time:110.38652896881104
5306 5306
5306 5306
2025-12-11 20:41:09,351 [trainer.py] => Time:15.70446515083313
2025-12-11 20:41:09,351 [inflora.py] => Exemplar size: 0
2025-12-11 20:41:09,351 [trainer.py] => CNN: {'total': np.float64(37.43), '00-01': np.float64(62.37), '02-03': np.float64(33.87), '04-05': np.float64(43.82), '06-07': np.float64(53.57), '08-09': np.float64(38.36), '10-11': np.float64(12.2), '12-13': np.float64(42.59), '14-15': np.float64(19.64), '16-17': np.float64(20.0), '18-19': np.float64(48.21), '20-21': np.float64(68.13), '22-23': np.float64(60.16), '24-25': np.float64(6.12), '26-27': np.float64(65.22), '28-29': np.float64(66.67), '30-31': np.float64(34.55), '32-33': np.float64(4.55), '34-35': np.float64(19.35), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(37.04), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(51.06), '50-51': np.float64(18.92), '52-53': np.float64(33.33), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(0.0), '60-61': np.float64(31.71), '62-63': np.float64(33.96), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(32.5), '74-75': np.float64(39.74), '76-77': np.float64(33.73), '78-79': np.float64(23.17), '80-81': np.float64(55.56), '82-83': np.float64(22.88), '84-85': np.float64(45.65), '86-87': np.float64(32.18), '88-89': np.float64(40.0), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(16.16), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(39.66), '104-105': np.float64(10.64), '106-107': np.float64(28.21), '108-109': np.float64(56.72), '110-111': np.float64(16.67), '112-113': np.float64(75.0), '114-115': np.float64(45.0), '116-117': np.float64(63.89), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(19.64), '124-125': np.float64(34.69), '126-127': np.float64(7.55), '128-129': np.float64(15.19), '130-131': np.float64(37.78), '132-133': np.float64(41.18), '134-135': np.float64(61.54), '136-137': np.float64(53.06), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(60.42), '144-145': np.float64(67.39), '146-147': np.float64(51.85), '148-149': np.float64(21.21), '150-151': np.float64(7.89), '152-153': np.float64(24.32), '154-155': np.float64(4.0), '156-157': np.float64(28.57), '158-159': np.float64(23.81), '160-161': np.float64(34.78), '162-163': np.float64(55.74), '164-165': np.float64(48.89), '166-167': np.float64(50.88), '168-169': np.float64(50.0), '170-171': np.float64(35.82), '172-173': np.float64(35.85), '174-175': np.float64(31.58), '176-177': np.float64(48.0), '178-179': np.float64(63.33), 'old': np.float64(37.28), 'new': np.float64(63.33)}
2025-12-11 20:41:09,352 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43)]
2025-12-11 20:41:09,352 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61)]
2025-12-11 20:41:09,352 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213]
2025-12-11 20:41:26,460 [trainer.py] => W-NCM: {'00-01': 65.59139784946237, '02-03': 67.74193548387096, '04-05': 59.55056179775281, '06-07': 76.78571428571429, '08-09': 67.12328767123287, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 78.57142857142857, '18-19': 73.21428571428571, '20-21': 75.82417582417582, '22-23': 65.625, '24-25': 65.3061224489796, '26-27': 55.65217391304348, '28-29': 59.25925925925925, '30-31': 42.72727272727273, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 62.5, '44-45': 51.85185185185185, '46-47': 81.48148148148148, '48-49': 78.72340425531915, '50-51': 67.56756756756756, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 85.0, '58-59': 56.36363636363636, '60-61': 68.29268292682927, '62-63': 74.52830188679245, '64-65': 63.33333333333333, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 56.16438356164384, '72-73': 70.0, '74-75': 39.743589743589745, '76-77': 68.67469879518072, '78-79': 57.3170731707317, '80-81': 81.94444444444444, '82-83': 52.54237288135594, '84-85': 82.6086956521739, '86-87': 75.86206896551724, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 24.489795918367346, '94-95': 77.46478873239437, '96-97': 52.52525252525253, '98-99': 59.45945945945946, '100-101': 65.0, '102-103': 74.13793103448276, '104-105': 54.25531914893617, '106-107': 69.23076923076923, '108-109': 61.19402985074627, '110-111': 73.33333333333333, '112-113': 82.6923076923077, '114-115': 72.5, '116-117': 88.88888888888889, '118-119': 78.84615384615384, '120-121': 77.04918032786885, '122-123': 75.0, '124-125': 46.93877551020408, '126-127': 52.83018867924528, '128-129': 40.50632911392405, '130-131': 77.77777777777779, '132-133': 79.41176470588235, '134-135': 53.84615384615385, '136-137': 77.55102040816327, '138-139': 75.47169811320755, '140-141': 61.702127659574465, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 77.77777777777779, '148-149': 63.63636363636363, '150-151': 57.89473684210527, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 75.0, '158-159': 77.77777777777779, '160-161': 86.95652173913044, '162-163': 83.60655737704919, '164-165': 82.22222222222221, '166-167': 89.47368421052632, '168-169': 83.33333333333334, '170-171': 91.04477611940298, '172-173': 94.33962264150944, '174-175': 94.73684210526315, '176-177': 92.0, '178-179': 90.0}
2025-12-11 20:41:26,461 [trainer.py] => Ave Acc (W-NCM): 70.14%
2025-12-11 20:41:26,461 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 65.59% (best 97.85%); T2: W-NCM 67.74% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 75.82% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 55.65% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 42.73% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 78.72% (best 94.68%); T26: W-NCM 67.57% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 85.00% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 74.53% (best 90.57%); T33: W-NCM 63.33% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 56.16% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 39.74% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 52.54% (best 89.83%); T43: W-NCM 82.61% (best 89.13%); T44: W-NCM 75.86% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 24.49% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 52.53% (best 84.85%); T50: W-NCM 59.46% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 74.14% (best 87.93%); T53: W-NCM 54.26% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 61.19% (best 91.04%); T56: W-NCM 73.33% (best 96.67%); T57: W-NCM 82.69% (best 94.23%); T58: W-NCM 72.50% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 75.00% (best 94.64%); T63: W-NCM 46.94% (best 93.88%); T64: W-NCM 52.83% (best 84.91%); T65: W-NCM 40.51% (best 92.41%); T66: W-NCM 77.78% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 53.85% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 61.70% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 75.00% (best 89.29%); T80: W-NCM 77.78% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 83.61% (best 91.80%); T83: W-NCM 82.22% (best 90.00%); T84: W-NCM 89.47% (best 94.74%); T85: W-NCM 83.33% (best 88.89%); T86: W-NCM 91.04% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 94.74% (best 94.74%); T89: W-NCM 92.00% (best 93.33%); T90: W-NCM 90.00% (best 90.00%)
2025-12-11 20:41:26,461 [trainer.py] => Average forgetting (W-NCM): 21.99% | Max forgetting (W-NCM): 61.22%
2025-12-11 20:41:26,473 [trainer.py] => All params: 144526051
2025-12-11 20:41:26,485 [trainer.py] => Trainable params: 185858
2025-12-11 20:41:26,485 [inflora.py] => Learning on 180-182
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.90.weight', 'image_encoder.blocks.9.attn.lora_B_k.90.weight', 'image_encoder.blocks.0.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_v.90.weight', 'image_encoder.blocks.9.attn.lora_B_v.90.weight', 'image_encoder.blocks.2.attn.lora_B_k.90.weight', 'image_encoder.blocks.11.attn.lora_B_k.90.weight', 'image_encoder.blocks.7.attn.lora_B_k.90.weight', 'image_encoder.blocks.10.attn.lora_B_v.90.weight', 'image_encoder.blocks.10.attn.lora_B_k.90.weight', 'image_encoder.blocks.1.attn.lora_B_v.90.weight', 'image_encoder.blocks.11.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_v.90.weight', 'image_encoder.blocks.6.attn.lora_B_k.90.weight', 'classifier_pool.90.weight', 'image_encoder.blocks.4.attn.lora_B_k.90.weight', 'image_encoder.blocks.5.attn.lora_B_k.90.weight', 'image_encoder.blocks.2.attn.lora_B_v.90.weight', 'image_encoder.blocks.4.attn.lora_B_v.90.weight', 'image_encoder.blocks.0.attn.lora_B_v.90.weight', 'classifier_pool.90.bias', 'image_encoder.blocks.3.attn.lora_B_k.90.weight', 'image_encoder.blocks.1.attn.lora_B_k.90.weight', 'image_encoder.blocks.7.attn.lora_B_v.90.weight', 'image_encoder.blocks.3.attn.lora_B_v.90.weight', 'image_encoder.blocks.8.attn.lora_B_k.90.weight'}
2025-12-11 20:44:12,019 [inflora.py] => Task 90, Epoch 50/50 => Loss 0.087, Train_accy 96.26
Threshold:  0.998
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 25/768 type remove
Layer 2 : 72/768 type remove
Layer 3 : 173/768 type remove
Layer 4 : 270/768 type remove
Layer 5 : 353/768 type remove
Layer 6 : 368/768 type remove
Layer 7 : 345/768 type retain
Layer 8 : 291/768 type retain
Layer 9 : 202/768 type retain
Layer 10 : 161/768 type retain
Layer 11 : 224/768 type retain
Layer 12 : 131/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:44:21,606 [trainer.py] => Time:175.1210343837738
5400 5400
5400 5400
2025-12-11 20:44:37,554 [trainer.py] => Time:15.947881698608398
2025-12-11 20:44:37,555 [inflora.py] => Exemplar size: 0
2025-12-11 20:44:37,555 [trainer.py] => CNN: {'total': np.float64(37.65), '00-01': np.float64(66.67), '02-03': np.float64(33.87), '04-05': np.float64(44.94), '06-07': np.float64(55.36), '08-09': np.float64(42.47), '10-11': np.float64(12.2), '12-13': np.float64(48.15), '14-15': np.float64(19.64), '16-17': np.float64(22.86), '18-19': np.float64(48.21), '20-21': np.float64(68.13), '22-23': np.float64(60.16), '24-25': np.float64(2.04), '26-27': np.float64(64.35), '28-29': np.float64(67.9), '30-31': np.float64(32.73), '32-33': np.float64(9.09), '34-35': np.float64(22.58), '36-37': np.float64(60.0), '38-39': np.float64(48.78), '40-41': np.float64(37.04), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(55.56), '48-49': np.float64(47.87), '50-51': np.float64(21.62), '52-53': np.float64(33.33), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(1.82), '60-61': np.float64(31.71), '62-63': np.float64(34.91), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(27.5), '74-75': np.float64(43.59), '76-77': np.float64(34.94), '78-79': np.float64(23.17), '80-81': np.float64(56.94), '82-83': np.float64(23.73), '84-85': np.float64(41.3), '86-87': np.float64(37.93), '88-89': np.float64(40.0), '90-91': np.float64(37.5), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(17.17), '98-99': np.float64(29.73), '100-101': np.float64(38.33), '102-103': np.float64(43.1), '104-105': np.float64(11.7), '106-107': np.float64(25.64), '108-109': np.float64(56.72), '110-111': np.float64(21.67), '112-113': np.float64(73.08), '114-115': np.float64(47.5), '116-117': np.float64(66.67), '118-119': np.float64(38.46), '120-121': np.float64(60.66), '122-123': np.float64(19.64), '124-125': np.float64(32.65), '126-127': np.float64(7.55), '128-129': np.float64(13.92), '130-131': np.float64(37.78), '132-133': np.float64(41.18), '134-135': np.float64(55.13), '136-137': np.float64(53.06), '138-139': np.float64(50.94), '140-141': np.float64(34.04), '142-143': np.float64(56.25), '144-145': np.float64(63.04), '146-147': np.float64(55.56), '148-149': np.float64(21.21), '150-151': np.float64(7.89), '152-153': np.float64(18.92), '154-155': np.float64(6.0), '156-157': np.float64(30.36), '158-159': np.float64(22.22), '160-161': np.float64(34.78), '162-163': np.float64(54.1), '164-165': np.float64(46.67), '166-167': np.float64(49.12), '168-169': np.float64(51.39), '170-171': np.float64(32.84), '172-173': np.float64(35.85), '174-175': np.float64(32.89), '176-177': np.float64(50.67), '178-179': np.float64(60.0), '180-181': np.float64(37.23), 'old': np.float64(37.66), 'new': np.float64(37.23)}
2025-12-11 20:44:37,555 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65)]
2025-12-11 20:44:37,555 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48)]
2025-12-11 20:44:37,556 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815]
2025-12-11 20:44:55,453 [trainer.py] => W-NCM: {'00-01': 68.81720430107528, '02-03': 69.35483870967742, '04-05': 59.55056179775281, '06-07': 78.57142857142857, '08-09': 67.12328767123287, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 80.0, '18-19': 71.42857142857143, '20-21': 76.92307692307693, '22-23': 66.40625, '24-25': 65.3061224489796, '26-27': 54.78260869565217, '28-29': 61.72839506172839, '30-31': 40.909090909090914, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 68.29268292682927, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 48.148148148148145, '46-47': 81.48148148148148, '48-49': 81.91489361702128, '50-51': 70.27027027027027, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 87.5, '58-59': 56.36363636363636, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 60.0, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 60.273972602739725, '72-73': 67.5, '74-75': 39.743589743589745, '76-77': 66.26506024096386, '78-79': 57.3170731707317, '80-81': 81.94444444444444, '82-83': 52.54237288135594, '84-85': 80.43478260869566, '86-87': 75.86206896551724, '88-89': 90.0, '90-91': 67.5, '92-93': 28.57142857142857, '94-95': 77.46478873239437, '96-97': 53.535353535353536, '98-99': 62.16216216216216, '100-101': 66.66666666666666, '102-103': 75.86206896551724, '104-105': 54.25531914893617, '106-107': 71.7948717948718, '108-109': 61.19402985074627, '110-111': 75.0, '112-113': 82.6923076923077, '114-115': 67.5, '116-117': 88.88888888888889, '118-119': 75.0, '120-121': 77.04918032786885, '122-123': 75.0, '124-125': 53.06122448979592, '126-127': 54.71698113207547, '128-129': 44.303797468354425, '130-131': 80.0, '132-133': 79.41176470588235, '134-135': 53.84615384615385, '136-137': 77.55102040816327, '138-139': 75.47169811320755, '140-141': 61.702127659574465, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 81.48148148148148, '148-149': 63.63636363636363, '150-151': 55.26315789473685, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 75.0, '158-159': 76.19047619047619, '160-161': 86.95652173913044, '162-163': 81.9672131147541, '164-165': 81.11111111111111, '166-167': 85.96491228070175, '168-169': 83.33333333333334, '170-171': 91.04477611940298, '172-173': 96.22641509433963, '174-175': 94.73684210526315, '176-177': 85.33333333333334, '178-179': 86.66666666666667, '180-181': 88.29787234042553}
2025-12-11 20:44:55,453 [trainer.py] => Ave Acc (W-NCM): 70.61%
2025-12-11 20:44:55,454 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 68.82% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 67.12% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 80.00% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 76.92% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 61.73% (best 96.30%); T16: W-NCM 40.91% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 68.29% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 81.48% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 39.74% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 57.32% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 52.54% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 75.86% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 77.46% (best 95.77%); T49: W-NCM 53.54% (best 84.85%); T50: W-NCM 62.16% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 54.26% (best 79.79%); T54: W-NCM 71.79% (best 89.74%); T55: W-NCM 61.19% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 82.69% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 75.00% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 75.00% (best 94.64%); T63: W-NCM 53.06% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 44.30% (best 92.41%); T66: W-NCM 80.00% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 53.85% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 61.70% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 75.00% (best 89.29%); T80: W-NCM 76.19% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 85.96% (best 94.74%); T85: W-NCM 83.33% (best 88.89%); T86: W-NCM 91.04% (best 94.03%); T87: W-NCM 96.23% (best 98.11%); T88: W-NCM 94.74% (best 94.74%); T89: W-NCM 85.33% (best 93.33%); T90: W-NCM 86.67% (best 90.00%); T91: W-NCM 88.30% (best 88.30%)
2025-12-11 20:44:55,454 [trainer.py] => Average forgetting (W-NCM): 21.48% | Max forgetting (W-NCM): 57.14%
2025-12-11 20:44:55,466 [trainer.py] => All params: 144526051
2025-12-11 20:44:55,478 [trainer.py] => Trainable params: 185858
2025-12-11 20:44:55,478 [inflora.py] => Learning on 182-184
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_k.91.weight', 'image_encoder.blocks.8.attn.lora_B_k.91.weight', 'image_encoder.blocks.0.attn.lora_B_v.91.weight', 'image_encoder.blocks.0.attn.lora_B_k.91.weight', 'image_encoder.blocks.1.attn.lora_B_k.91.weight', 'image_encoder.blocks.2.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_v.91.weight', 'image_encoder.blocks.10.attn.lora_B_k.91.weight', 'classifier_pool.91.bias', 'image_encoder.blocks.8.attn.lora_B_v.91.weight', 'image_encoder.blocks.7.attn.lora_B_k.91.weight', 'image_encoder.blocks.10.attn.lora_B_v.91.weight', 'image_encoder.blocks.9.attn.lora_B_k.91.weight', 'image_encoder.blocks.4.attn.lora_B_k.91.weight', 'image_encoder.blocks.7.attn.lora_B_v.91.weight', 'image_encoder.blocks.4.attn.lora_B_v.91.weight', 'image_encoder.blocks.3.attn.lora_B_v.91.weight', 'image_encoder.blocks.5.attn.lora_B_v.91.weight', 'image_encoder.blocks.1.attn.lora_B_v.91.weight', 'image_encoder.blocks.11.attn.lora_B_k.91.weight', 'image_encoder.blocks.6.attn.lora_B_k.91.weight', 'classifier_pool.91.weight', 'image_encoder.blocks.2.attn.lora_B_v.91.weight', 'image_encoder.blocks.5.attn.lora_B_k.91.weight'}
2025-12-11 20:46:44,306 [inflora.py] => Task 91, Epoch 50/50 => Loss 0.050, Train_accy 98.97
Threshold:  0.9982
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 34/768 type remove
Layer 2 : 76/768 type remove
Layer 3 : 184/768 type remove
Layer 4 : 287/768 type remove
Layer 5 : 371/768 type remove
Layer 6 : 384/768 type remove
Layer 7 : 330/768 type retain
Layer 8 : 275/768 type retain
Layer 9 : 191/768 type retain
Layer 10 : 151/768 type retain
Layer 11 : 210/768 type retain
Layer 12 : 121/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:46:52,756 [trainer.py] => Time:117.2780921459198
5441 5441
5441 5441
2025-12-11 20:47:08,872 [trainer.py] => Time:16.115037202835083
2025-12-11 20:47:08,872 [inflora.py] => Exemplar size: 0
2025-12-11 20:47:08,872 [trainer.py] => CNN: {'total': np.float64(37.95), '00-01': np.float64(65.59), '02-03': np.float64(35.48), '04-05': np.float64(46.07), '06-07': np.float64(57.14), '08-09': np.float64(43.84), '10-11': np.float64(12.2), '12-13': np.float64(50.0), '14-15': np.float64(21.43), '16-17': np.float64(18.57), '18-19': np.float64(48.21), '20-21': np.float64(67.03), '22-23': np.float64(60.94), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(65.43), '30-31': np.float64(31.82), '32-33': np.float64(9.09), '34-35': np.float64(22.58), '36-37': np.float64(60.0), '38-39': np.float64(48.78), '40-41': np.float64(37.04), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(50.0), '50-51': np.float64(21.62), '52-53': np.float64(27.78), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(1.82), '60-61': np.float64(31.71), '62-63': np.float64(39.62), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(30.0), '74-75': np.float64(44.87), '76-77': np.float64(33.73), '78-79': np.float64(23.17), '80-81': np.float64(56.94), '82-83': np.float64(26.27), '84-85': np.float64(41.3), '86-87': np.float64(40.23), '88-89': np.float64(41.67), '90-91': np.float64(37.5), '92-93': np.float64(14.29), '94-95': np.float64(25.35), '96-97': np.float64(17.17), '98-99': np.float64(29.73), '100-101': np.float64(38.33), '102-103': np.float64(46.55), '104-105': np.float64(9.57), '106-107': np.float64(25.64), '108-109': np.float64(55.22), '110-111': np.float64(21.67), '112-113': np.float64(75.0), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(38.46), '120-121': np.float64(60.66), '122-123': np.float64(19.64), '124-125': np.float64(36.73), '126-127': np.float64(11.32), '128-129': np.float64(15.19), '130-131': np.float64(40.0), '132-133': np.float64(44.12), '134-135': np.float64(53.85), '136-137': np.float64(51.02), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(56.25), '144-145': np.float64(65.22), '146-147': np.float64(55.56), '148-149': np.float64(18.18), '150-151': np.float64(7.89), '152-153': np.float64(21.62), '154-155': np.float64(6.0), '156-157': np.float64(30.36), '158-159': np.float64(25.4), '160-161': np.float64(32.61), '162-163': np.float64(52.46), '164-165': np.float64(45.56), '166-167': np.float64(49.12), '168-169': np.float64(50.0), '170-171': np.float64(34.33), '172-173': np.float64(37.74), '174-175': np.float64(38.16), '176-177': np.float64(50.67), '178-179': np.float64(60.0), '180-181': np.float64(35.11), '182-183': np.float64(19.51), 'old': np.float64(38.09), 'new': np.float64(19.51)}
2025-12-11 20:47:08,872 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95)]
2025-12-11 20:47:08,872 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48)]
2025-12-11 20:47:08,873 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677]
2025-12-11 20:47:26,389 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 72.58064516129032, '04-05': 58.42696629213483, '06-07': 80.35714285714286, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 78.57142857142857, '18-19': 66.07142857142857, '20-21': 78.02197802197803, '22-23': 66.40625, '24-25': 65.3061224489796, '26-27': 53.91304347826087, '28-29': 60.49382716049383, '30-31': 41.81818181818181, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 48.148148148148145, '46-47': 77.77777777777779, '48-49': 82.97872340425532, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 87.5, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 60.0, '66-67': 47.368421052631575, '68-69': 66.66666666666666, '70-71': 60.273972602739725, '72-73': 67.5, '74-75': 39.743589743589745, '76-77': 67.46987951807229, '78-79': 59.756097560975604, '80-81': 80.55555555555556, '82-83': 54.23728813559322, '84-85': 80.43478260869566, '86-87': 74.71264367816092, '88-89': 90.0, '90-91': 67.5, '92-93': 28.57142857142857, '94-95': 78.87323943661971, '96-97': 58.58585858585859, '98-99': 60.810810810810814, '100-101': 66.66666666666666, '102-103': 72.41379310344827, '104-105': 54.25531914893617, '106-107': 69.23076923076923, '108-109': 64.17910447761194, '110-111': 75.0, '112-113': 84.61538461538461, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 76.92307692307693, '120-121': 78.68852459016394, '122-123': 76.78571428571429, '124-125': 53.06122448979592, '126-127': 54.71698113207547, '128-129': 46.835443037974684, '130-131': 77.77777777777779, '132-133': 79.41176470588235, '134-135': 53.84615384615385, '136-137': 79.59183673469387, '138-139': 75.47169811320755, '140-141': 59.57446808510638, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 81.48148148148148, '148-149': 66.66666666666666, '150-151': 52.63157894736842, '152-153': 67.56756756756756, '154-155': 74.0, '156-157': 73.21428571428571, '158-159': 74.60317460317461, '160-161': 84.78260869565217, '162-163': 80.32786885245902, '164-165': 81.11111111111111, '166-167': 85.96491228070175, '168-169': 79.16666666666666, '170-171': 88.05970149253731, '172-173': 98.11320754716981, '174-175': 93.42105263157895, '176-177': 84.0, '178-179': 86.66666666666667, '180-181': 86.17021276595744, '182-183': 95.1219512195122}
2025-12-11 20:47:26,389 [trainer.py] => Ave Acc (W-NCM): 70.76%
2025-12-11 20:47:26,389 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 58.43% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 66.07% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 65.31% (best 87.76%); T14: W-NCM 53.91% (best 94.78%); T15: W-NCM 60.49% (best 96.30%); T16: W-NCM 41.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 60.27% (best 91.78%); T37: W-NCM 67.50% (best 92.50%); T38: W-NCM 39.74% (best 87.18%); T39: W-NCM 67.47% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 80.56% (best 97.22%); T42: W-NCM 54.24% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 74.71% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 58.59% (best 84.85%); T50: W-NCM 60.81% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 54.26% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 76.79% (best 94.64%); T63: W-NCM 53.06% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 46.84% (best 92.41%); T66: W-NCM 77.78% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 53.85% (best 85.90%); T69: W-NCM 79.59% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 74.60% (best 88.89%); T81: W-NCM 84.78% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 85.96% (best 94.74%); T85: W-NCM 79.17% (best 88.89%); T86: W-NCM 88.06% (best 94.03%); T87: W-NCM 98.11% (best 98.11%); T88: W-NCM 93.42% (best 94.74%); T89: W-NCM 84.00% (best 93.33%); T90: W-NCM 86.67% (best 90.00%); T91: W-NCM 86.17% (best 88.30%); T92: W-NCM 95.12% (best 95.12%)
2025-12-11 20:47:26,390 [trainer.py] => Average forgetting (W-NCM): 21.36% | Max forgetting (W-NCM): 57.14%
2025-12-11 20:47:26,402 [trainer.py] => All params: 144526051
2025-12-11 20:47:26,414 [trainer.py] => Trainable params: 185858
2025-12-11 20:47:26,414 [inflora.py] => Learning on 184-186
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.92.weight', 'image_encoder.blocks.0.attn.lora_B_v.92.weight', 'image_encoder.blocks.2.attn.lora_B_v.92.weight', 'image_encoder.blocks.4.attn.lora_B_k.92.weight', 'image_encoder.blocks.7.attn.lora_B_k.92.weight', 'image_encoder.blocks.1.attn.lora_B_v.92.weight', 'classifier_pool.92.bias', 'image_encoder.blocks.5.attn.lora_B_v.92.weight', 'image_encoder.blocks.0.attn.lora_B_k.92.weight', 'classifier_pool.92.weight', 'image_encoder.blocks.5.attn.lora_B_k.92.weight', 'image_encoder.blocks.9.attn.lora_B_v.92.weight', 'image_encoder.blocks.4.attn.lora_B_v.92.weight', 'image_encoder.blocks.3.attn.lora_B_v.92.weight', 'image_encoder.blocks.10.attn.lora_B_k.92.weight', 'image_encoder.blocks.11.attn.lora_B_v.92.weight', 'image_encoder.blocks.11.attn.lora_B_k.92.weight', 'image_encoder.blocks.8.attn.lora_B_v.92.weight', 'image_encoder.blocks.6.attn.lora_B_k.92.weight', 'image_encoder.blocks.10.attn.lora_B_v.92.weight', 'image_encoder.blocks.1.attn.lora_B_k.92.weight', 'image_encoder.blocks.9.attn.lora_B_k.92.weight', 'image_encoder.blocks.6.attn.lora_B_v.92.weight', 'image_encoder.blocks.8.attn.lora_B_k.92.weight', 'image_encoder.blocks.3.attn.lora_B_k.92.weight', 'image_encoder.blocks.2.attn.lora_B_k.92.weight'}
2025-12-11 20:49:12,783 [inflora.py] => Task 92, Epoch 50/50 => Loss 0.031, Train_accy 98.91
Threshold:  0.9984
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 34/768 type remove
Layer 2 : 76/768 type remove
Layer 3 : 185/768 type remove
Layer 4 : 290/768 type remove
Layer 5 : 378/768 type remove
Layer 6 : 373/768 type retain
Layer 7 : 317/768 type retain
Layer 8 : 257/768 type retain
Layer 9 : 177/768 type retain
Layer 10 : 142/768 type retain
Layer 11 : 198/768 type retain
Layer 12 : 115/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:49:21,020 [trainer.py] => Time:114.60620546340942
5480 5480
5480 5480
2025-12-11 20:49:37,240 [trainer.py] => Time:16.21938943862915
2025-12-11 20:49:37,240 [inflora.py] => Exemplar size: 0
2025-12-11 20:49:37,241 [trainer.py] => CNN: {'total': np.float64(38.18), '00-01': np.float64(68.82), '02-03': np.float64(35.48), '04-05': np.float64(46.07), '06-07': np.float64(53.57), '08-09': np.float64(42.47), '10-11': np.float64(14.63), '12-13': np.float64(51.85), '14-15': np.float64(21.43), '16-17': np.float64(20.0), '18-19': np.float64(48.21), '20-21': np.float64(68.13), '22-23': np.float64(61.72), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(65.43), '30-31': np.float64(33.64), '32-33': np.float64(11.36), '34-35': np.float64(22.58), '36-37': np.float64(60.0), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(51.06), '50-51': np.float64(21.62), '52-53': np.float64(27.78), '54-55': np.float64(0.0), '56-57': np.float64(35.0), '58-59': np.float64(3.64), '60-61': np.float64(31.71), '62-63': np.float64(40.57), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(32.5), '74-75': np.float64(47.44), '76-77': np.float64(34.94), '78-79': np.float64(23.17), '80-81': np.float64(56.94), '82-83': np.float64(25.42), '84-85': np.float64(41.3), '86-87': np.float64(40.23), '88-89': np.float64(41.67), '90-91': np.float64(37.5), '92-93': np.float64(14.29), '94-95': np.float64(25.35), '96-97': np.float64(17.17), '98-99': np.float64(28.38), '100-101': np.float64(38.33), '102-103': np.float64(48.28), '104-105': np.float64(9.57), '106-107': np.float64(25.64), '108-109': np.float64(55.22), '110-111': np.float64(20.0), '112-113': np.float64(73.08), '114-115': np.float64(45.0), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(19.64), '124-125': np.float64(36.73), '126-127': np.float64(11.32), '128-129': np.float64(15.19), '130-131': np.float64(37.78), '132-133': np.float64(47.06), '134-135': np.float64(56.41), '136-137': np.float64(48.98), '138-139': np.float64(49.06), '140-141': np.float64(34.04), '142-143': np.float64(50.0), '144-145': np.float64(63.04), '146-147': np.float64(51.85), '148-149': np.float64(18.18), '150-151': np.float64(7.89), '152-153': np.float64(21.62), '154-155': np.float64(4.0), '156-157': np.float64(32.14), '158-159': np.float64(22.22), '160-161': np.float64(30.43), '162-163': np.float64(54.1), '164-165': np.float64(46.67), '166-167': np.float64(50.88), '168-169': np.float64(50.0), '170-171': np.float64(35.82), '172-173': np.float64(37.74), '174-175': np.float64(43.42), '176-177': np.float64(50.67), '178-179': np.float64(60.0), '180-181': np.float64(34.04), '182-183': np.float64(19.51), '184-185': np.float64(51.28), 'old': np.float64(38.08), 'new': np.float64(51.28)}
2025-12-11 20:49:37,241 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18)]
2025-12-11 20:49:37,241 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51)]
2025-12-11 20:49:37,241 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824]
2025-12-11 20:49:54,909 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 70.96774193548387, '04-05': 59.55056179775281, '06-07': 78.57142857142857, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 73.21428571428571, '16-17': 78.57142857142857, '18-19': 67.85714285714286, '20-21': 78.02197802197803, '22-23': 66.40625, '24-25': 67.3469387755102, '26-27': 54.78260869565217, '28-29': 59.25925925925925, '30-31': 42.72727272727273, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 53.70370370370371, '42-43': 65.625, '44-45': 48.148148148148145, '46-47': 77.77777777777779, '48-49': 84.04255319148936, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 87.5, '58-59': 56.36363636363636, '60-61': 65.85365853658537, '62-63': 75.47169811320755, '64-65': 60.0, '66-67': 47.368421052631575, '68-69': 66.66666666666666, '70-71': 65.75342465753424, '72-73': 70.0, '74-75': 38.46153846153847, '76-77': 66.26506024096386, '78-79': 60.97560975609756, '80-81': 81.94444444444444, '82-83': 54.23728813559322, '84-85': 78.26086956521739, '86-87': 78.16091954022988, '88-89': 91.66666666666666, '90-91': 67.5, '92-93': 28.57142857142857, '94-95': 78.87323943661971, '96-97': 57.57575757575758, '98-99': 66.21621621621621, '100-101': 65.0, '102-103': 75.86206896551724, '104-105': 55.319148936170215, '106-107': 69.23076923076923, '108-109': 62.68656716417911, '110-111': 75.0, '112-113': 84.61538461538461, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 76.92307692307693, '120-121': 78.68852459016394, '122-123': 76.78571428571429, '124-125': 53.06122448979592, '126-127': 54.71698113207547, '128-129': 43.037974683544306, '130-131': 75.55555555555556, '132-133': 79.41176470588235, '134-135': 55.12820512820513, '136-137': 81.63265306122449, '138-139': 77.35849056603774, '140-141': 61.702127659574465, '142-143': 89.58333333333334, '144-145': 82.6086956521739, '146-147': 81.48148148148148, '148-149': 66.66666666666666, '150-151': 55.26315789473685, '152-153': 67.56756756756756, '154-155': 74.0, '156-157': 73.21428571428571, '158-159': 74.60317460317461, '160-161': 82.6086956521739, '162-163': 81.9672131147541, '164-165': 81.11111111111111, '166-167': 80.7017543859649, '168-169': 77.77777777777779, '170-171': 88.05970149253731, '172-173': 94.33962264150944, '174-175': 94.73684210526315, '176-177': 85.33333333333334, '178-179': 86.66666666666667, '180-181': 84.04255319148936, '182-183': 90.2439024390244, '184-185': 94.87179487179486}
2025-12-11 20:49:54,909 [trainer.py] => Ave Acc (W-NCM): 70.94%
2025-12-11 20:49:54,910 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 73.21% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 78.02% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 59.26% (best 96.30%); T16: W-NCM 42.73% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 53.70% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 48.15% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 84.04% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 87.50% (best 95.00%); T30: W-NCM 56.36% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 75.47% (best 90.57%); T33: W-NCM 60.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 66.67% (best 91.67%); T36: W-NCM 65.75% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 66.27% (best 78.31%); T40: W-NCM 60.98% (best 89.02%); T41: W-NCM 81.94% (best 97.22%); T42: W-NCM 54.24% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 78.16% (best 91.95%); T45: W-NCM 91.67% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 57.58% (best 84.85%); T50: W-NCM 66.22% (best 91.89%); T51: W-NCM 65.00% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 55.32% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 62.69% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 76.92% (best 100.00%); T61: W-NCM 78.69% (best 95.08%); T62: W-NCM 76.79% (best 94.64%); T63: W-NCM 53.06% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 75.56% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 55.13% (best 85.90%); T69: W-NCM 81.63% (best 93.88%); T70: W-NCM 77.36% (best 94.34%); T71: W-NCM 61.70% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 74.60% (best 88.89%); T81: W-NCM 82.61% (best 95.65%); T82: W-NCM 81.97% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 80.70% (best 94.74%); T85: W-NCM 77.78% (best 88.89%); T86: W-NCM 88.06% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 94.74% (best 94.74%); T89: W-NCM 85.33% (best 93.33%); T90: W-NCM 86.67% (best 90.00%); T91: W-NCM 84.04% (best 88.30%); T92: W-NCM 90.24% (best 95.12%); T93: W-NCM 94.87% (best 94.87%)
2025-12-11 20:49:54,910 [trainer.py] => Average forgetting (W-NCM): 21.21% | Max forgetting (W-NCM): 57.14%
2025-12-11 20:49:54,922 [trainer.py] => All params: 144526051
2025-12-11 20:49:54,934 [trainer.py] => Trainable params: 185858
2025-12-11 20:49:54,934 [inflora.py] => Learning on 186-188
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.93.weight', 'image_encoder.blocks.6.attn.lora_B_v.93.weight', 'image_encoder.blocks.10.attn.lora_B_k.93.weight', 'image_encoder.blocks.4.attn.lora_B_v.93.weight', 'image_encoder.blocks.7.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_v.93.weight', 'image_encoder.blocks.2.attn.lora_B_v.93.weight', 'classifier_pool.93.weight', 'image_encoder.blocks.5.attn.lora_B_v.93.weight', 'image_encoder.blocks.6.attn.lora_B_k.93.weight', 'image_encoder.blocks.4.attn.lora_B_k.93.weight', 'image_encoder.blocks.8.attn.lora_B_v.93.weight', 'image_encoder.blocks.11.attn.lora_B_k.93.weight', 'image_encoder.blocks.9.attn.lora_B_k.93.weight', 'image_encoder.blocks.0.attn.lora_B_k.93.weight', 'image_encoder.blocks.5.attn.lora_B_k.93.weight', 'classifier_pool.93.bias', 'image_encoder.blocks.7.attn.lora_B_v.93.weight', 'image_encoder.blocks.3.attn.lora_B_k.93.weight', 'image_encoder.blocks.10.attn.lora_B_v.93.weight', 'image_encoder.blocks.11.attn.lora_B_v.93.weight', 'image_encoder.blocks.1.attn.lora_B_k.93.weight', 'image_encoder.blocks.1.attn.lora_B_v.93.weight', 'image_encoder.blocks.3.attn.lora_B_v.93.weight', 'image_encoder.blocks.0.attn.lora_B_v.93.weight', 'image_encoder.blocks.2.attn.lora_B_k.93.weight'}
2025-12-11 20:52:48,396 [inflora.py] => Task 93, Epoch 50/50 => Loss 0.055, Train_accy 97.88
Threshold:  0.9986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 34/768 type remove
Layer 2 : 80/768 type remove
Layer 3 : 192/768 type remove
Layer 4 : 305/768 type remove
Layer 5 : 373/768 type retain
Layer 6 : 353/768 type retain
Layer 7 : 299/768 type retain
Layer 8 : 241/768 type retain
Layer 9 : 164/768 type retain
Layer 10 : 128/768 type retain
Layer 11 : 180/768 type retain
Layer 12 : 101/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:52:56,811 [trainer.py] => Time:181.87723112106323
5592 5592
5592 5592
2025-12-11 20:53:13,479 [trainer.py] => Time:16.667392253875732
2025-12-11 20:53:13,479 [inflora.py] => Exemplar size: 0
2025-12-11 20:53:13,479 [trainer.py] => CNN: {'total': np.float64(38.38), '00-01': np.float64(68.82), '02-03': np.float64(33.87), '04-05': np.float64(49.44), '06-07': np.float64(55.36), '08-09': np.float64(43.84), '10-11': np.float64(14.63), '12-13': np.float64(51.85), '14-15': np.float64(19.64), '16-17': np.float64(21.43), '18-19': np.float64(50.0), '20-21': np.float64(68.13), '22-23': np.float64(60.94), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(65.43), '30-31': np.float64(30.0), '32-33': np.float64(9.09), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(28.12), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(47.87), '50-51': np.float64(21.62), '52-53': np.float64(33.33), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(34.15), '62-63': np.float64(41.51), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(32.5), '74-75': np.float64(50.0), '76-77': np.float64(36.14), '78-79': np.float64(23.17), '80-81': np.float64(56.94), '82-83': np.float64(24.58), '84-85': np.float64(45.65), '86-87': np.float64(42.53), '88-89': np.float64(40.0), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(26.76), '96-97': np.float64(19.19), '98-99': np.float64(31.08), '100-101': np.float64(40.0), '102-103': np.float64(44.83), '104-105': np.float64(10.64), '106-107': np.float64(30.77), '108-109': np.float64(56.72), '110-111': np.float64(21.67), '112-113': np.float64(75.0), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(59.02), '122-123': np.float64(16.07), '124-125': np.float64(36.73), '126-127': np.float64(7.55), '128-129': np.float64(13.92), '130-131': np.float64(37.78), '132-133': np.float64(41.18), '134-135': np.float64(52.56), '136-137': np.float64(51.02), '138-139': np.float64(49.06), '140-141': np.float64(36.17), '142-143': np.float64(52.08), '144-145': np.float64(63.04), '146-147': np.float64(51.85), '148-149': np.float64(15.15), '150-151': np.float64(5.26), '152-153': np.float64(16.22), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(25.4), '160-161': np.float64(30.43), '162-163': np.float64(52.46), '164-165': np.float64(46.67), '166-167': np.float64(52.63), '168-169': np.float64(52.78), '170-171': np.float64(34.33), '172-173': np.float64(39.62), '174-175': np.float64(36.84), '176-177': np.float64(46.67), '178-179': np.float64(63.33), '180-181': np.float64(35.11), '182-183': np.float64(17.07), '184-185': np.float64(51.28), '186-187': np.float64(45.54), 'old': np.float64(38.23), 'new': np.float64(45.54)}
2025-12-11 20:53:13,480 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38)]
2025-12-11 20:53:13,480 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55)]
2025-12-11 20:53:13,480 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956]
2025-12-11 20:53:31,929 [trainer.py] => W-NCM: {'00-01': 66.66666666666666, '02-03': 69.35483870967742, '04-05': 57.30337078651685, '06-07': 76.78571428571429, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 85.18518518518519, '14-15': 75.0, '16-17': 81.42857142857143, '18-19': 71.42857142857143, '20-21': 80.21978021978022, '22-23': 66.40625, '24-25': 67.3469387755102, '26-27': 53.91304347826087, '28-29': 64.19753086419753, '30-31': 44.54545454545455, '32-33': 88.63636363636364, '34-35': 61.29032258064516, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 82.97872340425532, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 90.0, '58-59': 58.18181818181818, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 70.0, '66-67': 47.368421052631575, '68-69': 70.83333333333334, '70-71': 61.64383561643836, '72-73': 70.0, '74-75': 37.17948717948718, '76-77': 68.67469879518072, '78-79': 60.97560975609756, '80-81': 77.77777777777779, '82-83': 52.54237288135594, '84-85': 78.26086956521739, '86-87': 77.01149425287356, '88-89': 91.66666666666666, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 80.28169014084507, '96-97': 61.61616161616161, '98-99': 66.21621621621621, '100-101': 66.66666666666666, '102-103': 74.13793103448276, '104-105': 55.319148936170215, '106-107': 69.23076923076923, '108-109': 65.67164179104478, '110-111': 75.0, '112-113': 84.61538461538461, '114-115': 70.0, '116-117': 88.88888888888889, '118-119': 78.84615384615384, '120-121': 80.32786885245902, '122-123': 75.0, '124-125': 53.06122448979592, '126-127': 58.490566037735846, '128-129': 43.037974683544306, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 53.84615384615385, '136-137': 81.63265306122449, '138-139': 73.58490566037736, '140-141': 61.702127659574465, '142-143': 89.58333333333334, '144-145': 84.78260869565217, '146-147': 77.77777777777779, '148-149': 66.66666666666666, '150-151': 52.63157894736842, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 73.21428571428571, '158-159': 74.60317460317461, '160-161': 84.78260869565217, '162-163': 80.32786885245902, '164-165': 81.11111111111111, '166-167': 84.21052631578947, '168-169': 79.16666666666666, '170-171': 88.05970149253731, '172-173': 94.33962264150944, '174-175': 93.42105263157895, '176-177': 84.0, '178-179': 86.66666666666667, '180-181': 80.85106382978722, '182-183': 87.8048780487805, '184-185': 89.74358974358975, '186-187': 86.60714285714286}
2025-12-11 20:53:31,930 [trainer.py] => Ave Acc (W-NCM): 71.35%
2025-12-11 20:53:31,930 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 66.67% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 57.30% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 85.19% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 71.43% (best 94.64%); T11: W-NCM 80.22% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 53.91% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 44.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 61.29% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 58.18% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 70.83% (best 91.67%); T36: W-NCM 61.64% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 68.67% (best 78.31%); T40: W-NCM 60.98% (best 89.02%); T41: W-NCM 77.78% (best 97.22%); T42: W-NCM 52.54% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 91.67% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 61.62% (best 84.85%); T50: W-NCM 66.22% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 74.14% (best 87.93%); T53: W-NCM 55.32% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 65.67% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 70.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 75.00% (best 94.64%); T63: W-NCM 53.06% (best 93.88%); T64: W-NCM 58.49% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 53.85% (best 85.90%); T69: W-NCM 81.63% (best 93.88%); T70: W-NCM 73.58% (best 94.34%); T71: W-NCM 61.70% (best 87.23%); T72: W-NCM 89.58% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 74.60% (best 88.89%); T81: W-NCM 84.78% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 84.21% (best 94.74%); T85: W-NCM 79.17% (best 88.89%); T86: W-NCM 88.06% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 93.42% (best 94.74%); T89: W-NCM 84.00% (best 93.33%); T90: W-NCM 86.67% (best 90.00%); T91: W-NCM 80.85% (best 88.30%); T92: W-NCM 87.80% (best 95.12%); T93: W-NCM 89.74% (best 94.87%); T94: W-NCM 86.61% (best 86.61%)
2025-12-11 20:53:31,930 [trainer.py] => Average forgetting (W-NCM): 20.73% | Max forgetting (W-NCM): 59.18%
2025-12-11 20:53:31,942 [trainer.py] => All params: 144526051
2025-12-11 20:53:31,954 [trainer.py] => Trainable params: 185858
2025-12-11 20:53:31,954 [inflora.py] => Learning on 188-190
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.94.weight', 'image_encoder.blocks.7.attn.lora_B_v.94.weight', 'image_encoder.blocks.8.attn.lora_B_k.94.weight', 'image_encoder.blocks.1.attn.lora_B_v.94.weight', 'image_encoder.blocks.6.attn.lora_B_k.94.weight', 'image_encoder.blocks.8.attn.lora_B_v.94.weight', 'image_encoder.blocks.6.attn.lora_B_v.94.weight', 'image_encoder.blocks.10.attn.lora_B_v.94.weight', 'image_encoder.blocks.2.attn.lora_B_v.94.weight', 'image_encoder.blocks.0.attn.lora_B_k.94.weight', 'image_encoder.blocks.1.attn.lora_B_k.94.weight', 'image_encoder.blocks.9.attn.lora_B_v.94.weight', 'image_encoder.blocks.9.attn.lora_B_k.94.weight', 'classifier_pool.94.weight', 'classifier_pool.94.bias', 'image_encoder.blocks.5.attn.lora_B_v.94.weight', 'image_encoder.blocks.7.attn.lora_B_k.94.weight', 'image_encoder.blocks.5.attn.lora_B_k.94.weight', 'image_encoder.blocks.3.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_k.94.weight', 'image_encoder.blocks.0.attn.lora_B_v.94.weight', 'image_encoder.blocks.11.attn.lora_B_k.94.weight', 'image_encoder.blocks.4.attn.lora_B_v.94.weight', 'image_encoder.blocks.2.attn.lora_B_k.94.weight', 'image_encoder.blocks.3.attn.lora_B_v.94.weight', 'image_encoder.blocks.11.attn.lora_B_v.94.weight'}
2025-12-11 20:56:05,971 [inflora.py] => Task 94, Epoch 50/50 => Loss 0.049, Train_accy 97.51
Threshold:  0.9988
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 34/768 type remove
Layer 2 : 83/768 type remove
Layer 3 : 202/768 type remove
Layer 4 : 321/768 type remove
Layer 5 : 355/768 type retain
Layer 6 : 334/768 type retain
Layer 7 : 281/768 type retain
Layer 8 : 221/768 type retain
Layer 9 : 147/768 type retain
Layer 10 : 113/768 type retain
Layer 11 : 159/768 type retain
Layer 12 : 94/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:56:14,433 [trainer.py] => Time:162.47907900810242
5683 5683
5683 5683
2025-12-11 20:56:31,189 [trainer.py] => Time:16.755754470825195
2025-12-11 20:56:31,190 [inflora.py] => Exemplar size: 0
2025-12-11 20:56:31,190 [trainer.py] => CNN: {'total': np.float64(39.2), '00-01': np.float64(67.74), '02-03': np.float64(37.1), '04-05': np.float64(49.44), '06-07': np.float64(60.71), '08-09': np.float64(43.84), '10-11': np.float64(14.63), '12-13': np.float64(51.85), '14-15': np.float64(23.21), '16-17': np.float64(21.43), '18-19': np.float64(53.57), '20-21': np.float64(70.33), '22-23': np.float64(61.72), '24-25': np.float64(6.12), '26-27': np.float64(65.22), '28-29': np.float64(62.96), '30-31': np.float64(32.73), '32-33': np.float64(6.82), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(46.81), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(3.64), '60-61': np.float64(36.59), '62-63': np.float64(41.51), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(23.29), '72-73': np.float64(32.5), '74-75': np.float64(50.0), '76-77': np.float64(37.35), '78-79': np.float64(24.39), '80-81': np.float64(61.11), '82-83': np.float64(25.42), '84-85': np.float64(45.65), '86-87': np.float64(42.53), '88-89': np.float64(43.33), '90-91': np.float64(37.5), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(18.18), '98-99': np.float64(28.38), '100-101': np.float64(41.67), '102-103': np.float64(48.28), '104-105': np.float64(10.64), '106-107': np.float64(30.77), '108-109': np.float64(58.21), '110-111': np.float64(20.0), '112-113': np.float64(80.77), '114-115': np.float64(47.5), '116-117': np.float64(66.67), '118-119': np.float64(40.38), '120-121': np.float64(60.66), '122-123': np.float64(17.86), '124-125': np.float64(32.65), '126-127': np.float64(11.32), '128-129': np.float64(12.66), '130-131': np.float64(37.78), '132-133': np.float64(44.12), '134-135': np.float64(53.85), '136-137': np.float64(48.98), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(54.17), '144-145': np.float64(60.87), '146-147': np.float64(51.85), '148-149': np.float64(18.18), '150-151': np.float64(7.89), '152-153': np.float64(18.92), '154-155': np.float64(6.0), '156-157': np.float64(32.14), '158-159': np.float64(22.22), '160-161': np.float64(30.43), '162-163': np.float64(52.46), '164-165': np.float64(47.78), '166-167': np.float64(52.63), '168-169': np.float64(51.39), '170-171': np.float64(32.84), '172-173': np.float64(39.62), '174-175': np.float64(36.84), '176-177': np.float64(46.67), '178-179': np.float64(60.0), '180-181': np.float64(36.17), '182-183': np.float64(17.07), '184-185': np.float64(48.72), '186-187': np.float64(46.43), '188-189': np.float64(60.44), 'old': np.float64(38.86), 'new': np.float64(60.44)}
2025-12-11 20:56:31,190 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2)]
2025-12-11 20:56:31,190 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72)]
2025-12-11 20:56:31,190 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978]
2025-12-11 20:56:49,764 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 70.96774193548387, '04-05': 60.67415730337079, '06-07': 76.78571428571429, '08-09': 71.23287671232876, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 76.78571428571429, '16-17': 81.42857142857143, '18-19': 75.0, '20-21': 81.31868131868131, '22-23': 66.40625, '24-25': 67.3469387755102, '26-27': 57.391304347826086, '28-29': 66.66666666666666, '30-31': 46.36363636363636, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 57.407407407407405, '42-43': 62.5, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 81.91489361702128, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 90.0, '58-59': 60.0, '60-61': 68.29268292682927, '62-63': 77.35849056603774, '64-65': 80.0, '66-67': 52.63157894736842, '68-69': 75.0, '70-71': 65.75342465753424, '72-73': 72.5, '74-75': 37.17948717948718, '76-77': 72.28915662650603, '78-79': 59.756097560975604, '80-81': 80.55555555555556, '82-83': 55.08474576271186, '84-85': 80.43478260869566, '86-87': 77.01149425287356, '88-89': 90.0, '90-91': 67.5, '92-93': 28.57142857142857, '94-95': 80.28169014084507, '96-97': 61.61616161616161, '98-99': 68.91891891891892, '100-101': 70.0, '102-103': 75.86206896551724, '104-105': 57.446808510638306, '106-107': 69.23076923076923, '108-109': 68.65671641791045, '110-111': 75.0, '112-113': 84.61538461538461, '114-115': 72.5, '116-117': 88.88888888888889, '118-119': 78.84615384615384, '120-121': 80.32786885245902, '122-123': 73.21428571428571, '124-125': 51.02040816326531, '126-127': 56.60377358490566, '128-129': 46.835443037974684, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 55.12820512820513, '136-137': 79.59183673469387, '138-139': 73.58490566037736, '140-141': 59.57446808510638, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 77.77777777777779, '148-149': 66.66666666666666, '150-151': 55.26315789473685, '152-153': 70.27027027027027, '154-155': 76.0, '156-157': 73.21428571428571, '158-159': 76.19047619047619, '160-161': 84.78260869565217, '162-163': 80.32786885245902, '164-165': 81.11111111111111, '166-167': 80.7017543859649, '168-169': 77.77777777777779, '170-171': 88.05970149253731, '172-173': 96.22641509433963, '174-175': 92.10526315789474, '176-177': 82.66666666666667, '178-179': 86.66666666666667, '180-181': 82.97872340425532, '182-183': 87.8048780487805, '184-185': 89.74358974358975, '186-187': 81.25, '188-189': 82.41758241758241}
2025-12-11 20:56:49,764 [trainer.py] => Ave Acc (W-NCM): 72.27%
2025-12-11 20:56:49,764 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 60.67% (best 91.01%); T4: W-NCM 76.79% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 76.79% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 75.00% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 57.39% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 46.36% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 60.00% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 77.36% (best 90.57%); T33: W-NCM 80.00% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 65.75% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 80.56% (best 97.22%); T42: W-NCM 55.08% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 61.62% (best 84.85%); T50: W-NCM 68.92% (best 91.89%); T51: W-NCM 70.00% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 57.45% (best 79.79%); T54: W-NCM 69.23% (best 89.74%); T55: W-NCM 68.66% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 72.50% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 73.21% (best 94.64%); T63: W-NCM 51.02% (best 93.88%); T64: W-NCM 56.60% (best 84.91%); T65: W-NCM 46.84% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 55.13% (best 85.90%); T69: W-NCM 79.59% (best 93.88%); T70: W-NCM 73.58% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 76.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 76.19% (best 88.89%); T81: W-NCM 84.78% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 80.70% (best 94.74%); T85: W-NCM 77.78% (best 88.89%); T86: W-NCM 88.06% (best 94.03%); T87: W-NCM 96.23% (best 98.11%); T88: W-NCM 92.11% (best 94.74%); T89: W-NCM 82.67% (best 93.33%); T90: W-NCM 86.67% (best 90.00%); T91: W-NCM 82.98% (best 88.30%); T92: W-NCM 87.80% (best 95.12%); T93: W-NCM 89.74% (best 94.87%); T94: W-NCM 81.25% (best 86.61%); T95: W-NCM 82.42% (best 82.42%)
2025-12-11 20:56:49,765 [trainer.py] => Average forgetting (W-NCM): 19.70% | Max forgetting (W-NCM): 57.14%
2025-12-11 20:56:49,777 [trainer.py] => All params: 144526051
2025-12-11 20:56:49,789 [trainer.py] => Trainable params: 185858
2025-12-11 20:56:49,789 [inflora.py] => Learning on 190-192
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.95.weight', 'image_encoder.blocks.4.attn.lora_B_v.95.weight', 'image_encoder.blocks.11.attn.lora_B_v.95.weight', 'image_encoder.blocks.0.attn.lora_B_k.95.weight', 'image_encoder.blocks.2.attn.lora_B_k.95.weight', 'image_encoder.blocks.10.attn.lora_B_k.95.weight', 'image_encoder.blocks.2.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_v.95.weight', 'image_encoder.blocks.8.attn.lora_B_k.95.weight', 'classifier_pool.95.bias', 'image_encoder.blocks.9.attn.lora_B_k.95.weight', 'image_encoder.blocks.1.attn.lora_B_k.95.weight', 'image_encoder.blocks.1.attn.lora_B_v.95.weight', 'image_encoder.blocks.9.attn.lora_B_v.95.weight', 'image_encoder.blocks.3.attn.lora_B_v.95.weight', 'image_encoder.blocks.6.attn.lora_B_v.95.weight', 'classifier_pool.95.weight', 'image_encoder.blocks.5.attn.lora_B_v.95.weight', 'image_encoder.blocks.7.attn.lora_B_k.95.weight', 'image_encoder.blocks.11.attn.lora_B_k.95.weight', 'image_encoder.blocks.3.attn.lora_B_k.95.weight', 'image_encoder.blocks.5.attn.lora_B_k.95.weight', 'image_encoder.blocks.7.attn.lora_B_v.95.weight', 'image_encoder.blocks.4.attn.lora_B_k.95.weight', 'image_encoder.blocks.0.attn.lora_B_v.95.weight', 'image_encoder.blocks.6.attn.lora_B_k.95.weight'}
2025-12-11 20:59:23,435 [inflora.py] => Task 95, Epoch 50/50 => Loss 0.044, Train_accy 98.33
Threshold:  0.999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 34/768 type remove
Layer 2 : 86/768 type remove
Layer 3 : 204/768 type remove
Layer 4 : 326/768 type remove
Layer 5 : 348/768 type retain
Layer 6 : 325/768 type retain
Layer 7 : 270/768 type retain
Layer 8 : 213/768 type retain
Layer 9 : 141/768 type retain
Layer 10 : 107/768 type retain
Layer 11 : 152/768 type retain
Layer 12 : 90/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 20:59:32,887 [trainer.py] => Time:163.0983443260193
5771 5771
5771 5771
2025-12-11 20:59:49,882 [trainer.py] => Time:16.994643926620483
2025-12-11 20:59:49,883 [inflora.py] => Exemplar size: 0
2025-12-11 20:59:49,883 [trainer.py] => CNN: {'total': np.float64(39.35), '00-01': np.float64(65.59), '02-03': np.float64(37.1), '04-05': np.float64(48.31), '06-07': np.float64(55.36), '08-09': np.float64(43.84), '10-11': np.float64(14.63), '12-13': np.float64(53.7), '14-15': np.float64(21.43), '16-17': np.float64(21.43), '18-19': np.float64(51.79), '20-21': np.float64(67.03), '22-23': np.float64(61.72), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(62.96), '30-31': np.float64(33.64), '32-33': np.float64(6.82), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(31.48), '42-43': np.float64(34.38), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(47.87), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(5.45), '60-61': np.float64(39.02), '62-63': np.float64(41.51), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(12.5), '70-71': np.float64(26.03), '72-73': np.float64(32.5), '74-75': np.float64(50.0), '76-77': np.float64(37.35), '78-79': np.float64(21.95), '80-81': np.float64(55.56), '82-83': np.float64(26.27), '84-85': np.float64(43.48), '86-87': np.float64(41.38), '88-89': np.float64(46.67), '90-91': np.float64(42.5), '92-93': np.float64(14.29), '94-95': np.float64(21.13), '96-97': np.float64(18.18), '98-99': np.float64(27.03), '100-101': np.float64(40.0), '102-103': np.float64(48.28), '104-105': np.float64(12.77), '106-107': np.float64(28.21), '108-109': np.float64(55.22), '110-111': np.float64(20.0), '112-113': np.float64(76.92), '114-115': np.float64(47.5), '116-117': np.float64(61.11), '118-119': np.float64(42.31), '120-121': np.float64(59.02), '122-123': np.float64(17.86), '124-125': np.float64(32.65), '126-127': np.float64(13.21), '128-129': np.float64(13.92), '130-131': np.float64(37.78), '132-133': np.float64(47.06), '134-135': np.float64(52.56), '136-137': np.float64(48.98), '138-139': np.float64(50.94), '140-141': np.float64(34.04), '142-143': np.float64(54.17), '144-145': np.float64(63.04), '146-147': np.float64(44.44), '148-149': np.float64(21.21), '150-151': np.float64(7.89), '152-153': np.float64(18.92), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(22.22), '160-161': np.float64(34.78), '162-163': np.float64(50.82), '164-165': np.float64(46.67), '166-167': np.float64(52.63), '168-169': np.float64(54.17), '170-171': np.float64(35.82), '172-173': np.float64(39.62), '174-175': np.float64(38.16), '176-177': np.float64(48.0), '178-179': np.float64(63.33), '180-181': np.float64(35.11), '182-183': np.float64(19.51), '184-185': np.float64(48.72), '186-187': np.float64(46.43), '188-189': np.float64(59.34), '190-191': np.float64(62.5), 'old': np.float64(38.99), 'new': np.float64(62.5)}
2025-12-11 20:59:49,883 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2), np.float64(39.35)]
2025-12-11 20:59:49,883 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72), np.float64(95.72)]
2025-12-11 20:59:49,883 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978, 0.39351932074163926]
2025-12-11 21:00:08,808 [trainer.py] => W-NCM: {'00-01': 68.81720430107528, '02-03': 72.58064516129032, '04-05': 60.67415730337079, '06-07': 78.57142857142857, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 76.78571428571429, '16-17': 81.42857142857143, '18-19': 73.21428571428571, '20-21': 82.41758241758241, '22-23': 66.40625, '24-25': 67.3469387755102, '26-27': 58.26086956521739, '28-29': 66.66666666666666, '30-31': 44.54545454545455, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 57.407407407407405, '42-43': 62.5, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 81.91489361702128, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 48.0, '56-57': 90.0, '58-59': 58.18181818181818, '60-61': 65.85365853658537, '62-63': 76.41509433962264, '64-65': 76.66666666666667, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 65.75342465753424, '72-73': 72.5, '74-75': 41.02564102564102, '76-77': 72.28915662650603, '78-79': 59.756097560975604, '80-81': 80.55555555555556, '82-83': 55.932203389830505, '84-85': 78.26086956521739, '86-87': 79.3103448275862, '88-89': 90.0, '90-91': 67.5, '92-93': 28.57142857142857, '94-95': 80.28169014084507, '96-97': 61.61616161616161, '98-99': 71.62162162162163, '100-101': 70.0, '102-103': 77.58620689655173, '104-105': 57.446808510638306, '106-107': 66.66666666666666, '108-109': 68.65671641791045, '110-111': 75.0, '112-113': 82.6923076923077, '114-115': 72.5, '116-117': 91.66666666666666, '118-119': 78.84615384615384, '120-121': 81.9672131147541, '122-123': 75.0, '124-125': 51.02040816326531, '126-127': 54.71698113207547, '128-129': 48.10126582278481, '130-131': 73.33333333333333, '132-133': 79.41176470588235, '134-135': 55.12820512820513, '136-137': 77.55102040816327, '138-139': 75.47169811320755, '140-141': 57.446808510638306, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 81.48148148148148, '148-149': 66.66666666666666, '150-151': 57.89473684210527, '152-153': 70.27027027027027, '154-155': 74.0, '156-157': 73.21428571428571, '158-159': 76.19047619047619, '160-161': 84.78260869565217, '162-163': 80.32786885245902, '164-165': 81.11111111111111, '166-167': 80.7017543859649, '168-169': 79.16666666666666, '170-171': 88.05970149253731, '172-173': 96.22641509433963, '174-175': 92.10526315789474, '176-177': 82.66666666666667, '178-179': 83.33333333333334, '180-181': 81.91489361702128, '182-183': 85.36585365853658, '184-185': 84.61538461538461, '186-187': 81.25, '188-189': 81.31868131868131, '190-191': 81.81818181818183}
2025-12-11 21:00:08,808 [trainer.py] => Ave Acc (W-NCM): 72.29%
2025-12-11 21:00:08,808 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 68.82% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 60.67% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 76.79% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 73.21% (best 94.64%); T11: W-NCM 82.42% (best 95.60%); T12: W-NCM 66.41% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 44.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 57.41% (best 94.44%); T22: W-NCM 62.50% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 58.18% (best 90.91%); T31: W-NCM 65.85% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 65.75% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 41.03% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 80.56% (best 97.22%); T42: W-NCM 55.93% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 79.31% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 28.57% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 61.62% (best 84.85%); T50: W-NCM 71.62% (best 91.89%); T51: W-NCM 70.00% (best 91.67%); T52: W-NCM 77.59% (best 87.93%); T53: W-NCM 57.45% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 68.66% (best 91.04%); T56: W-NCM 75.00% (best 96.67%); T57: W-NCM 82.69% (best 94.23%); T58: W-NCM 72.50% (best 92.50%); T59: W-NCM 91.67% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 81.97% (best 95.08%); T62: W-NCM 75.00% (best 94.64%); T63: W-NCM 51.02% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 48.10% (best 92.41%); T66: W-NCM 73.33% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 55.13% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 57.45% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 81.48% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 70.27% (best 94.59%); T78: W-NCM 74.00% (best 94.00%); T79: W-NCM 73.21% (best 89.29%); T80: W-NCM 76.19% (best 88.89%); T81: W-NCM 84.78% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 80.70% (best 94.74%); T85: W-NCM 79.17% (best 88.89%); T86: W-NCM 88.06% (best 94.03%); T87: W-NCM 96.23% (best 98.11%); T88: W-NCM 92.11% (best 94.74%); T89: W-NCM 82.67% (best 93.33%); T90: W-NCM 83.33% (best 90.00%); T91: W-NCM 81.91% (best 88.30%); T92: W-NCM 85.37% (best 95.12%); T93: W-NCM 84.62% (best 94.87%); T94: W-NCM 81.25% (best 86.61%); T95: W-NCM 81.32% (best 82.42%); T96: W-NCM 81.82% (best 81.82%)
2025-12-11 21:00:08,809 [trainer.py] => Average forgetting (W-NCM): 19.58% | Max forgetting (W-NCM): 57.14%
2025-12-11 21:00:08,821 [trainer.py] => All params: 144526051
2025-12-11 21:00:08,833 [trainer.py] => Trainable params: 185858
2025-12-11 21:00:08,833 [inflora.py] => Learning on 192-194
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.96.weight', 'image_encoder.blocks.11.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_k.96.weight', 'image_encoder.blocks.9.attn.lora_B_k.96.weight', 'image_encoder.blocks.7.attn.lora_B_v.96.weight', 'image_encoder.blocks.1.attn.lora_B_v.96.weight', 'image_encoder.blocks.3.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_k.96.weight', 'image_encoder.blocks.5.attn.lora_B_k.96.weight', 'image_encoder.blocks.3.attn.lora_B_k.96.weight', 'image_encoder.blocks.9.attn.lora_B_v.96.weight', 'image_encoder.blocks.6.attn.lora_B_v.96.weight', 'image_encoder.blocks.5.attn.lora_B_v.96.weight', 'image_encoder.blocks.8.attn.lora_B_k.96.weight', 'image_encoder.blocks.1.attn.lora_B_k.96.weight', 'image_encoder.blocks.2.attn.lora_B_v.96.weight', 'image_encoder.blocks.8.attn.lora_B_v.96.weight', 'image_encoder.blocks.10.attn.lora_B_v.96.weight', 'classifier_pool.96.weight', 'image_encoder.blocks.11.attn.lora_B_k.96.weight', 'image_encoder.blocks.7.attn.lora_B_k.96.weight', 'classifier_pool.96.bias', 'image_encoder.blocks.0.attn.lora_B_v.96.weight', 'image_encoder.blocks.4.attn.lora_B_v.96.weight', 'image_encoder.blocks.0.attn.lora_B_k.96.weight', 'image_encoder.blocks.4.attn.lora_B_k.96.weight'}
2025-12-11 21:02:01,361 [inflora.py] => Task 96, Epoch 50/50 => Loss 0.065, Train_accy 98.10
Threshold:  0.9992
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 51/768 type remove
Layer 2 : 99/768 type remove
Layer 3 : 232/768 type remove
Layer 4 : 360/768 type remove
Layer 5 : 313/768 type retain
Layer 6 : 288/768 type retain
Layer 7 : 238/768 type retain
Layer 8 : 183/768 type retain
Layer 9 : 113/768 type retain
Layer 10 : 82/768 type retain
Layer 11 : 113/768 type retain
Layer 12 : 70/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 21:02:08,219 [trainer.py] => Time:119.38579249382019
5825 5825
5825 5825
2025-12-11 21:02:25,422 [trainer.py] => Time:17.202531576156616
2025-12-11 21:02:25,422 [inflora.py] => Exemplar size: 0
2025-12-11 21:02:25,423 [trainer.py] => CNN: {'total': np.float64(39.19), '00-01': np.float64(69.89), '02-03': np.float64(37.1), '04-05': np.float64(49.44), '06-07': np.float64(53.57), '08-09': np.float64(45.21), '10-11': np.float64(17.07), '12-13': np.float64(53.7), '14-15': np.float64(21.43), '16-17': np.float64(15.71), '18-19': np.float64(55.36), '20-21': np.float64(68.13), '22-23': np.float64(60.16), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(62.96), '30-31': np.float64(30.91), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(48.94), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(41.46), '62-63': np.float64(42.45), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(30.0), '74-75': np.float64(46.15), '76-77': np.float64(36.14), '78-79': np.float64(23.17), '80-81': np.float64(58.33), '82-83': np.float64(27.97), '84-85': np.float64(41.3), '86-87': np.float64(43.68), '88-89': np.float64(46.67), '90-91': np.float64(40.0), '92-93': np.float64(14.29), '94-95': np.float64(26.76), '96-97': np.float64(19.19), '98-99': np.float64(29.73), '100-101': np.float64(41.67), '102-103': np.float64(53.45), '104-105': np.float64(13.83), '106-107': np.float64(30.77), '108-109': np.float64(53.73), '110-111': np.float64(21.67), '112-113': np.float64(78.85), '114-115': np.float64(45.0), '116-117': np.float64(69.44), '118-119': np.float64(40.38), '120-121': np.float64(57.38), '122-123': np.float64(17.86), '124-125': np.float64(36.73), '126-127': np.float64(11.32), '128-129': np.float64(12.66), '130-131': np.float64(37.78), '132-133': np.float64(44.12), '134-135': np.float64(50.0), '136-137': np.float64(48.98), '138-139': np.float64(52.83), '140-141': np.float64(31.91), '142-143': np.float64(54.17), '144-145': np.float64(60.87), '146-147': np.float64(44.44), '148-149': np.float64(18.18), '150-151': np.float64(5.26), '152-153': np.float64(18.92), '154-155': np.float64(8.0), '156-157': np.float64(30.36), '158-159': np.float64(25.4), '160-161': np.float64(30.43), '162-163': np.float64(49.18), '164-165': np.float64(46.67), '166-167': np.float64(50.88), '168-169': np.float64(54.17), '170-171': np.float64(35.82), '172-173': np.float64(39.62), '174-175': np.float64(36.84), '176-177': np.float64(48.0), '178-179': np.float64(60.0), '180-181': np.float64(36.17), '182-183': np.float64(17.07), '184-185': np.float64(51.28), '186-187': np.float64(46.43), '188-189': np.float64(58.24), '190-191': np.float64(61.36), '192-193': np.float64(9.26), 'old': np.float64(39.47), 'new': np.float64(9.26)}
2025-12-11 21:02:25,423 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2), np.float64(39.35), np.float64(39.19)]
2025-12-11 21:02:25,423 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72), np.float64(95.72), np.float64(95.67)]
2025-12-11 21:02:25,423 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978, 0.39351932074163926, 0.391931330472103]
2025-12-11 21:02:44,309 [trainer.py] => W-NCM: {'00-01': 69.89247311827957, '02-03': 70.96774193548387, '04-05': 60.67415730337079, '06-07': 78.57142857142857, '08-09': 69.86301369863014, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 75.0, '16-17': 81.42857142857143, '18-19': 64.28571428571429, '20-21': 81.31868131868131, '22-23': 64.84375, '24-25': 69.38775510204081, '26-27': 58.26086956521739, '28-29': 66.66666666666666, '30-31': 44.54545454545455, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 84.04255319148936, '50-51': 72.97297297297297, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 90.0, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 75.47169811320755, '64-65': 76.66666666666667, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 67.12328767123287, '72-73': 72.5, '74-75': 38.46153846153847, '76-77': 72.28915662650603, '78-79': 59.756097560975604, '80-81': 79.16666666666666, '82-83': 55.932203389830505, '84-85': 78.26086956521739, '86-87': 77.01149425287356, '88-89': 90.0, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 80.28169014084507, '96-97': 59.59595959595959, '98-99': 68.91891891891892, '100-101': 70.0, '102-103': 75.86206896551724, '104-105': 58.51063829787234, '106-107': 66.66666666666666, '108-109': 68.65671641791045, '110-111': 73.33333333333333, '112-113': 84.61538461538461, '114-115': 67.5, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 80.32786885245902, '122-123': 73.21428571428571, '124-125': 51.02040816326531, '126-127': 56.60377358490566, '128-129': 48.10126582278481, '130-131': 75.55555555555556, '132-133': 82.35294117647058, '134-135': 55.12820512820513, '136-137': 77.55102040816327, '138-139': 75.47169811320755, '140-141': 59.57446808510638, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 77.77777777777779, '148-149': 66.66666666666666, '150-151': 55.26315789473685, '152-153': 72.97297297297297, '154-155': 72.0, '156-157': 67.85714285714286, '158-159': 74.60317460317461, '160-161': 86.95652173913044, '162-163': 80.32786885245902, '164-165': 81.11111111111111, '166-167': 80.7017543859649, '168-169': 77.77777777777779, '170-171': 86.56716417910447, '172-173': 96.22641509433963, '174-175': 92.10526315789474, '176-177': 81.33333333333333, '178-179': 83.33333333333334, '180-181': 79.7872340425532, '182-183': 85.36585365853658, '184-185': 84.61538461538461, '186-187': 78.57142857142857, '188-189': 79.12087912087912, '190-191': 81.81818181818183, '192-193': 85.18518518518519}
2025-12-11 21:02:44,310 [trainer.py] => Ave Acc (W-NCM): 71.99%
2025-12-11 21:02:44,310 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 69.89% (best 97.85%); T2: W-NCM 70.97% (best 90.32%); T3: W-NCM 60.67% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 69.86% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 64.29% (best 94.64%); T11: W-NCM 81.32% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 58.26% (best 94.78%); T15: W-NCM 66.67% (best 96.30%); T16: W-NCM 44.55% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 84.04% (best 94.68%); T26: W-NCM 72.97% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 75.47% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 67.12% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 55.93% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 90.00% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 59.60% (best 84.85%); T50: W-NCM 68.92% (best 91.89%); T51: W-NCM 70.00% (best 91.67%); T52: W-NCM 75.86% (best 87.93%); T53: W-NCM 58.51% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 68.66% (best 91.04%); T56: W-NCM 73.33% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 67.50% (best 92.50%); T59: W-NCM 94.44% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 80.33% (best 95.08%); T62: W-NCM 73.21% (best 94.64%); T63: W-NCM 51.02% (best 93.88%); T64: W-NCM 56.60% (best 84.91%); T65: W-NCM 48.10% (best 92.41%); T66: W-NCM 75.56% (best 95.56%); T67: W-NCM 82.35% (best 94.12%); T68: W-NCM 55.13% (best 85.90%); T69: W-NCM 77.55% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 72.97% (best 94.59%); T78: W-NCM 72.00% (best 94.00%); T79: W-NCM 67.86% (best 89.29%); T80: W-NCM 74.60% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 80.33% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 80.70% (best 94.74%); T85: W-NCM 77.78% (best 88.89%); T86: W-NCM 86.57% (best 94.03%); T87: W-NCM 96.23% (best 98.11%); T88: W-NCM 92.11% (best 94.74%); T89: W-NCM 81.33% (best 93.33%); T90: W-NCM 83.33% (best 90.00%); T91: W-NCM 79.79% (best 88.30%); T92: W-NCM 85.37% (best 95.12%); T93: W-NCM 84.62% (best 94.87%); T94: W-NCM 78.57% (best 86.61%); T95: W-NCM 79.12% (best 82.42%); T96: W-NCM 81.82% (best 81.82%); T97: W-NCM 85.19% (best 85.19%)
2025-12-11 21:02:44,310 [trainer.py] => Average forgetting (W-NCM): 19.80% | Max forgetting (W-NCM): 59.18%
2025-12-11 21:02:44,323 [trainer.py] => All params: 144526051
2025-12-11 21:02:44,335 [trainer.py] => Trainable params: 185858
2025-12-11 21:02:44,335 [inflora.py] => Learning on 194-196
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.97.weight', 'image_encoder.blocks.2.attn.lora_B_k.97.weight', 'image_encoder.blocks.7.attn.lora_B_k.97.weight', 'image_encoder.blocks.3.attn.lora_B_v.97.weight', 'image_encoder.blocks.10.attn.lora_B_k.97.weight', 'image_encoder.blocks.0.attn.lora_B_v.97.weight', 'image_encoder.blocks.9.attn.lora_B_v.97.weight', 'image_encoder.blocks.11.attn.lora_B_v.97.weight', 'image_encoder.blocks.2.attn.lora_B_v.97.weight', 'image_encoder.blocks.1.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_v.97.weight', 'image_encoder.blocks.4.attn.lora_B_v.97.weight', 'image_encoder.blocks.3.attn.lora_B_k.97.weight', 'classifier_pool.97.weight', 'image_encoder.blocks.1.attn.lora_B_k.97.weight', 'image_encoder.blocks.11.attn.lora_B_k.97.weight', 'image_encoder.blocks.7.attn.lora_B_v.97.weight', 'image_encoder.blocks.4.attn.lora_B_k.97.weight', 'image_encoder.blocks.9.attn.lora_B_k.97.weight', 'image_encoder.blocks.8.attn.lora_B_k.97.weight', 'image_encoder.blocks.8.attn.lora_B_v.97.weight', 'image_encoder.blocks.6.attn.lora_B_k.97.weight', 'image_encoder.blocks.0.attn.lora_B_k.97.weight', 'classifier_pool.97.bias', 'image_encoder.blocks.5.attn.lora_B_k.97.weight', 'image_encoder.blocks.5.attn.lora_B_v.97.weight'}
2025-12-11 21:04:14,688 [inflora.py] => Task 97, Epoch 50/50 => Loss 0.264, Train_accy 90.84
Threshold:  0.9994
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 59/768 type remove
Layer 2 : 102/768 type remove
Layer 3 : 234/768 type remove
Layer 4 : 367/768 type remove
Layer 5 : 302/768 type retain
Layer 6 : 277/768 type retain
Layer 7 : 225/768 type retain
Layer 8 : 175/768 type retain
Layer 9 : 104/768 type retain
Layer 10 : 72/768 type retain
Layer 11 : 100/768 type retain
Layer 12 : 57/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 21:04:23,034 [trainer.py] => Time:98.69965982437134
5854 5854
5854 5854
2025-12-11 21:04:40,381 [trainer.py] => Time:17.34680485725403
2025-12-11 21:04:40,382 [inflora.py] => Exemplar size: 0
2025-12-11 21:04:40,382 [trainer.py] => CNN: {'total': np.float64(38.78), '00-01': np.float64(68.82), '02-03': np.float64(37.1), '04-05': np.float64(48.31), '06-07': np.float64(53.57), '08-09': np.float64(45.21), '10-11': np.float64(17.07), '12-13': np.float64(55.56), '14-15': np.float64(19.64), '16-17': np.float64(14.29), '18-19': np.float64(53.57), '20-21': np.float64(67.03), '22-23': np.float64(59.38), '24-25': np.float64(4.08), '26-27': np.float64(65.22), '28-29': np.float64(60.49), '30-31': np.float64(30.91), '32-33': np.float64(4.55), '34-35': np.float64(22.58), '36-37': np.float64(58.33), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(51.85), '48-49': np.float64(48.94), '50-51': np.float64(21.62), '52-53': np.float64(38.89), '54-55': np.float64(0.0), '56-57': np.float64(37.5), '58-59': np.float64(7.27), '60-61': np.float64(39.02), '62-63': np.float64(39.62), '64-65': np.float64(0.0), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(27.5), '74-75': np.float64(47.44), '76-77': np.float64(37.35), '78-79': np.float64(23.17), '80-81': np.float64(58.33), '82-83': np.float64(28.81), '84-85': np.float64(41.3), '86-87': np.float64(44.83), '88-89': np.float64(46.67), '90-91': np.float64(40.0), '92-93': np.float64(12.24), '94-95': np.float64(26.76), '96-97': np.float64(18.18), '98-99': np.float64(29.73), '100-101': np.float64(40.0), '102-103': np.float64(48.28), '104-105': np.float64(12.77), '106-107': np.float64(28.21), '108-109': np.float64(53.73), '110-111': np.float64(20.0), '112-113': np.float64(76.92), '114-115': np.float64(47.5), '116-117': np.float64(69.44), '118-119': np.float64(36.54), '120-121': np.float64(57.38), '122-123': np.float64(16.07), '124-125': np.float64(34.69), '126-127': np.float64(9.43), '128-129': np.float64(11.39), '130-131': np.float64(40.0), '132-133': np.float64(44.12), '134-135': np.float64(51.28), '136-137': np.float64(48.98), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(54.17), '144-145': np.float64(63.04), '146-147': np.float64(44.44), '148-149': np.float64(18.18), '150-151': np.float64(7.89), '152-153': np.float64(18.92), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(25.4), '160-161': np.float64(30.43), '162-163': np.float64(49.18), '164-165': np.float64(45.56), '166-167': np.float64(52.63), '168-169': np.float64(52.78), '170-171': np.float64(35.82), '172-173': np.float64(39.62), '174-175': np.float64(39.47), '176-177': np.float64(48.0), '178-179': np.float64(60.0), '180-181': np.float64(37.23), '182-183': np.float64(17.07), '184-185': np.float64(51.28), '186-187': np.float64(46.43), '188-189': np.float64(58.24), '190-191': np.float64(62.5), '192-193': np.float64(9.26), '194-195': np.float64(13.79), 'old': np.float64(38.9), 'new': np.float64(13.79)}
2025-12-11 21:04:40,382 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2), np.float64(39.35), np.float64(39.19), np.float64(38.78)]
2025-12-11 21:04:40,382 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72), np.float64(95.72), np.float64(95.67), np.float64(95.78)]
2025-12-11 21:04:40,382 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978, 0.39351932074163926, 0.391931330472103, 0.387769046805603]
2025-12-11 21:04:58,980 [trainer.py] => W-NCM: {'00-01': 70.96774193548387, '02-03': 72.58064516129032, '04-05': 59.55056179775281, '06-07': 78.57142857142857, '08-09': 71.23287671232876, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 75.0, '16-17': 81.42857142857143, '18-19': 66.07142857142857, '20-21': 80.21978021978022, '22-23': 64.84375, '24-25': 69.38775510204081, '26-27': 57.391304347826086, '28-29': 65.4320987654321, '30-31': 43.63636363636363, '32-33': 88.63636363636364, '34-35': 64.51612903225806, '36-37': 78.33333333333333, '38-39': 70.73170731707317, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 82.97872340425532, '50-51': 70.27027027027027, '52-53': 86.11111111111111, '54-55': 44.0, '56-57': 90.0, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 75.47169811320755, '64-65': 76.66666666666667, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 67.12328767123287, '72-73': 72.5, '74-75': 38.46153846153847, '76-77': 72.28915662650603, '78-79': 58.536585365853654, '80-81': 79.16666666666666, '82-83': 54.23728813559322, '84-85': 80.43478260869566, '86-87': 78.16091954022988, '88-89': 88.33333333333333, '90-91': 67.5, '92-93': 26.53061224489796, '94-95': 80.28169014084507, '96-97': 57.57575757575758, '98-99': 67.56756756756756, '100-101': 66.66666666666666, '102-103': 70.6896551724138, '104-105': 59.57446808510638, '106-107': 66.66666666666666, '108-109': 67.16417910447761, '110-111': 73.33333333333333, '112-113': 86.53846153846155, '114-115': 65.0, '116-117': 88.88888888888889, '118-119': 80.76923076923077, '120-121': 77.04918032786885, '122-123': 73.21428571428571, '124-125': 46.93877551020408, '126-127': 54.71698113207547, '128-129': 43.037974683544306, '130-131': 71.11111111111111, '132-133': 79.41176470588235, '134-135': 52.56410256410257, '136-137': 79.59183673469387, '138-139': 77.35849056603774, '140-141': 59.57446808510638, '142-143': 87.5, '144-145': 84.78260869565217, '146-147': 77.77777777777779, '148-149': 63.63636363636363, '150-151': 57.89473684210527, '152-153': 67.56756756756756, '154-155': 72.0, '156-157': 69.64285714285714, '158-159': 71.42857142857143, '160-161': 86.95652173913044, '162-163': 78.68852459016394, '164-165': 81.11111111111111, '166-167': 78.94736842105263, '168-169': 79.16666666666666, '170-171': 86.56716417910447, '172-173': 94.33962264150944, '174-175': 92.10526315789474, '176-177': 82.66666666666667, '178-179': 83.33333333333334, '180-181': 75.53191489361703, '182-183': 85.36585365853658, '184-185': 79.48717948717949, '186-187': 77.67857142857143, '188-189': 78.02197802197803, '190-191': 81.81818181818183, '192-193': 81.48148148148148, '194-195': 65.51724137931035}
2025-12-11 21:04:58,981 [trainer.py] => Ave Acc (W-NCM): 71.22%
2025-12-11 21:04:58,981 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 70.97% (best 97.85%); T2: W-NCM 72.58% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 78.57% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 81.43% (best 98.57%); T10: W-NCM 66.07% (best 94.64%); T11: W-NCM 80.22% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 57.39% (best 94.78%); T15: W-NCM 65.43% (best 96.30%); T16: W-NCM 43.64% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 64.52% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 70.73% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 82.98% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 86.11% (best 94.44%); T28: W-NCM 44.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 75.47% (best 90.57%); T33: W-NCM 76.67% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 67.12% (best 91.78%); T37: W-NCM 72.50% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 58.54% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 54.24% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 78.16% (best 91.95%); T45: W-NCM 88.33% (best 96.67%); T46: W-NCM 67.50% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 80.28% (best 95.77%); T49: W-NCM 57.58% (best 84.85%); T50: W-NCM 67.57% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 70.69% (best 87.93%); T53: W-NCM 59.57% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 67.16% (best 91.04%); T56: W-NCM 73.33% (best 96.67%); T57: W-NCM 86.54% (best 94.23%); T58: W-NCM 65.00% (best 92.50%); T59: W-NCM 88.89% (best 97.22%); T60: W-NCM 80.77% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 73.21% (best 94.64%); T63: W-NCM 46.94% (best 93.88%); T64: W-NCM 54.72% (best 84.91%); T65: W-NCM 43.04% (best 92.41%); T66: W-NCM 71.11% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 52.56% (best 85.90%); T69: W-NCM 79.59% (best 93.88%); T70: W-NCM 77.36% (best 94.34%); T71: W-NCM 59.57% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 84.78% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 57.89% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 72.00% (best 94.00%); T79: W-NCM 69.64% (best 89.29%); T80: W-NCM 71.43% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 78.69% (best 91.80%); T83: W-NCM 81.11% (best 90.00%); T84: W-NCM 78.95% (best 94.74%); T85: W-NCM 79.17% (best 88.89%); T86: W-NCM 86.57% (best 94.03%); T87: W-NCM 94.34% (best 98.11%); T88: W-NCM 92.11% (best 94.74%); T89: W-NCM 82.67% (best 93.33%); T90: W-NCM 83.33% (best 90.00%); T91: W-NCM 75.53% (best 88.30%); T92: W-NCM 85.37% (best 95.12%); T93: W-NCM 79.49% (best 94.87%); T94: W-NCM 77.68% (best 86.61%); T95: W-NCM 78.02% (best 82.42%); T96: W-NCM 81.82% (best 81.82%); T97: W-NCM 81.48% (best 85.19%); T98: W-NCM 65.52% (best 65.52%)
2025-12-11 21:04:58,981 [trainer.py] => Average forgetting (W-NCM): 20.32% | Max forgetting (W-NCM): 59.18%
2025-12-11 21:04:58,994 [trainer.py] => All params: 144526051
2025-12-11 21:04:59,005 [trainer.py] => Trainable params: 185858
2025-12-11 21:04:59,005 [inflora.py] => Learning on 196-198
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.98.weight', 'image_encoder.blocks.11.attn.lora_B_v.98.weight', 'classifier_pool.98.weight', 'image_encoder.blocks.2.attn.lora_B_k.98.weight', 'image_encoder.blocks.0.attn.lora_B_k.98.weight', 'image_encoder.blocks.0.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_k.98.weight', 'image_encoder.blocks.5.attn.lora_B_v.98.weight', 'image_encoder.blocks.8.attn.lora_B_k.98.weight', 'image_encoder.blocks.8.attn.lora_B_v.98.weight', 'image_encoder.blocks.6.attn.lora_B_k.98.weight', 'classifier_pool.98.bias', 'image_encoder.blocks.10.attn.lora_B_v.98.weight', 'image_encoder.blocks.7.attn.lora_B_k.98.weight', 'image_encoder.blocks.4.attn.lora_B_v.98.weight', 'image_encoder.blocks.4.attn.lora_B_k.98.weight', 'image_encoder.blocks.9.attn.lora_B_v.98.weight', 'image_encoder.blocks.5.attn.lora_B_k.98.weight', 'image_encoder.blocks.11.attn.lora_B_k.98.weight', 'image_encoder.blocks.10.attn.lora_B_k.98.weight', 'image_encoder.blocks.2.attn.lora_B_v.98.weight', 'image_encoder.blocks.1.attn.lora_B_v.98.weight', 'image_encoder.blocks.3.attn.lora_B_k.98.weight', 'image_encoder.blocks.6.attn.lora_B_v.98.weight', 'image_encoder.blocks.9.attn.lora_B_k.98.weight', 'image_encoder.blocks.7.attn.lora_B_v.98.weight'}
2025-12-11 21:07:16,133 [inflora.py] => Task 98, Epoch 50/50 => Loss 0.052, Train_accy 98.33
Threshold:  0.9996
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 59/768 type remove
Layer 2 : 115/768 type remove
Layer 3 : 265/768 type remove
Layer 4 : 354/768 type retain
Layer 5 : 250/768 type retain
Layer 6 : 225/768 type retain
Layer 7 : 177/768 type retain
Layer 8 : 134/768 type retain
Layer 9 : 71/768 type retain
Layer 10 : 44/768 type retain
Layer 11 : 63/768 type retain
Layer 12 : 33/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 21:07:24,058 [trainer.py] => Time:145.0526728630066
5936 5936
5936 5936
2025-12-11 21:07:41,671 [trainer.py] => Time:17.612261533737183
2025-12-11 21:07:41,671 [inflora.py] => Exemplar size: 0
2025-12-11 21:07:41,671 [trainer.py] => CNN: {'total': np.float64(38.8), '00-01': np.float64(70.97), '02-03': np.float64(38.71), '04-05': np.float64(48.31), '06-07': np.float64(53.57), '08-09': np.float64(45.21), '10-11': np.float64(17.07), '12-13': np.float64(51.85), '14-15': np.float64(23.21), '16-17': np.float64(12.86), '18-19': np.float64(50.0), '20-21': np.float64(64.84), '22-23': np.float64(60.94), '24-25': np.float64(4.08), '26-27': np.float64(63.48), '28-29': np.float64(59.26), '30-31': np.float64(30.0), '32-33': np.float64(13.64), '34-35': np.float64(19.35), '36-37': np.float64(60.0), '38-39': np.float64(51.22), '40-41': np.float64(37.04), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(50.0), '50-51': np.float64(21.62), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(7.27), '60-61': np.float64(34.15), '62-63': np.float64(39.62), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(24.66), '72-73': np.float64(27.5), '74-75': np.float64(52.56), '76-77': np.float64(37.35), '78-79': np.float64(21.95), '80-81': np.float64(58.33), '82-83': np.float64(28.81), '84-85': np.float64(39.13), '86-87': np.float64(43.68), '88-89': np.float64(50.0), '90-91': np.float64(37.5), '92-93': np.float64(12.24), '94-95': np.float64(25.35), '96-97': np.float64(20.2), '98-99': np.float64(28.38), '100-101': np.float64(40.0), '102-103': np.float64(46.55), '104-105': np.float64(13.83), '106-107': np.float64(25.64), '108-109': np.float64(53.73), '110-111': np.float64(21.67), '112-113': np.float64(73.08), '114-115': np.float64(45.0), '116-117': np.float64(72.22), '118-119': np.float64(32.69), '120-121': np.float64(55.74), '122-123': np.float64(17.86), '124-125': np.float64(36.73), '126-127': np.float64(11.32), '128-129': np.float64(13.92), '130-131': np.float64(40.0), '132-133': np.float64(41.18), '134-135': np.float64(52.56), '136-137': np.float64(48.98), '138-139': np.float64(50.94), '140-141': np.float64(36.17), '142-143': np.float64(54.17), '144-145': np.float64(60.87), '146-147': np.float64(40.74), '148-149': np.float64(21.21), '150-151': np.float64(5.26), '152-153': np.float64(18.92), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(26.98), '160-161': np.float64(28.26), '162-163': np.float64(47.54), '164-165': np.float64(44.44), '166-167': np.float64(50.88), '168-169': np.float64(52.78), '170-171': np.float64(37.31), '172-173': np.float64(39.62), '174-175': np.float64(40.79), '176-177': np.float64(48.0), '178-179': np.float64(60.0), '180-181': np.float64(34.04), '182-183': np.float64(14.63), '184-185': np.float64(51.28), '186-187': np.float64(45.54), '188-189': np.float64(56.04), '190-191': np.float64(61.36), '192-193': np.float64(9.26), '194-195': np.float64(17.24), '196-197': np.float64(46.34), 'old': np.float64(38.69), 'new': np.float64(46.34)}
2025-12-11 21:07:41,672 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2), np.float64(39.35), np.float64(39.19), np.float64(38.78), np.float64(38.8)]
2025-12-11 21:07:41,672 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72), np.float64(95.72), np.float64(95.67), np.float64(95.78), np.float64(95.75)]
2025-12-11 21:07:41,672 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978, 0.39351932074163926, 0.391931330472103, 0.387769046805603, 0.38797169811320753]
2025-12-11 21:08:00,974 [trainer.py] => W-NCM: {'00-01': 70.96774193548387, '02-03': 69.35483870967742, '04-05': 59.55056179775281, '06-07': 80.35714285714286, '08-09': 71.23287671232876, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 75.0, '16-17': 78.57142857142857, '18-19': 66.07142857142857, '20-21': 79.12087912087912, '22-23': 65.625, '24-25': 69.38775510204081, '26-27': 54.78260869565217, '28-29': 62.96296296296296, '30-31': 41.81818181818181, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 81.91489361702128, '50-51': 70.27027027027027, '52-53': 83.33333333333334, '54-55': 48.0, '56-57': 90.0, '58-59': 52.72727272727272, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 70.0, '66-67': 52.63157894736842, '68-69': 75.0, '70-71': 63.013698630136986, '72-73': 70.0, '74-75': 38.46153846153847, '76-77': 72.28915662650603, '78-79': 59.756097560975604, '80-81': 79.16666666666666, '82-83': 52.54237288135594, '84-85': 78.26086956521739, '86-87': 77.01149425287356, '88-89': 86.66666666666667, '90-91': 65.0, '92-93': 26.53061224489796, '94-95': 78.87323943661971, '96-97': 57.57575757575758, '98-99': 62.16216216216216, '100-101': 66.66666666666666, '102-103': 67.24137931034483, '104-105': 55.319148936170215, '106-107': 66.66666666666666, '108-109': 64.17910447761194, '110-111': 73.33333333333333, '112-113': 86.53846153846155, '114-115': 62.5, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 77.04918032786885, '122-123': 73.21428571428571, '124-125': 46.93877551020408, '126-127': 49.056603773584904, '128-129': 36.708860759493675, '130-131': 66.66666666666666, '132-133': 79.41176470588235, '134-135': 52.56410256410257, '136-137': 73.46938775510205, '138-139': 75.47169811320755, '140-141': 55.319148936170215, '142-143': 85.41666666666666, '144-145': 82.6086956521739, '146-147': 77.77777777777779, '148-149': 63.63636363636363, '150-151': 52.63157894736842, '152-153': 67.56756756756756, '154-155': 72.0, '156-157': 67.85714285714286, '158-159': 71.42857142857143, '160-161': 86.95652173913044, '162-163': 77.04918032786885, '164-165': 77.77777777777779, '166-167': 71.9298245614035, '168-169': 76.38888888888889, '170-171': 86.56716417910447, '172-173': 92.45283018867924, '174-175': 90.78947368421053, '176-177': 80.0, '178-179': 83.33333333333334, '180-181': 76.59574468085107, '182-183': 85.36585365853658, '184-185': 79.48717948717949, '186-187': 74.10714285714286, '188-189': 76.92307692307693, '190-191': 81.81818181818183, '192-193': 79.62962962962963, '194-195': 65.51724137931035, '196-197': 92.6829268292683}
2025-12-11 21:08:00,974 [trainer.py] => Ave Acc (W-NCM): 70.30%
2025-12-11 21:08:00,975 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 70.97% (best 97.85%); T2: W-NCM 69.35% (best 90.32%); T3: W-NCM 59.55% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 66.07% (best 94.64%); T11: W-NCM 79.12% (best 95.60%); T12: W-NCM 65.62% (best 95.31%); T13: W-NCM 69.39% (best 87.76%); T14: W-NCM 54.78% (best 94.78%); T15: W-NCM 62.96% (best 96.30%); T16: W-NCM 41.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 52.73% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 70.00% (best 93.33%); T34: W-NCM 52.63% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 63.01% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 38.46% (best 87.18%); T39: W-NCM 72.29% (best 78.31%); T40: W-NCM 59.76% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 52.54% (best 89.83%); T43: W-NCM 78.26% (best 89.13%); T44: W-NCM 77.01% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 26.53% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 57.58% (best 84.85%); T50: W-NCM 62.16% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 67.24% (best 87.93%); T53: W-NCM 55.32% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 73.33% (best 96.67%); T57: W-NCM 86.54% (best 94.23%); T58: W-NCM 62.50% (best 92.50%); T59: W-NCM 94.44% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 73.21% (best 94.64%); T63: W-NCM 46.94% (best 93.88%); T64: W-NCM 49.06% (best 84.91%); T65: W-NCM 36.71% (best 92.41%); T66: W-NCM 66.67% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 52.56% (best 85.90%); T69: W-NCM 73.47% (best 93.88%); T70: W-NCM 75.47% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 85.42% (best 91.67%); T73: W-NCM 82.61% (best 95.65%); T74: W-NCM 77.78% (best 96.30%); T75: W-NCM 63.64% (best 90.91%); T76: W-NCM 52.63% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 72.00% (best 94.00%); T79: W-NCM 67.86% (best 89.29%); T80: W-NCM 71.43% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 77.05% (best 91.80%); T83: W-NCM 77.78% (best 90.00%); T84: W-NCM 71.93% (best 94.74%); T85: W-NCM 76.39% (best 88.89%); T86: W-NCM 86.57% (best 94.03%); T87: W-NCM 92.45% (best 98.11%); T88: W-NCM 90.79% (best 94.74%); T89: W-NCM 80.00% (best 93.33%); T90: W-NCM 83.33% (best 90.00%); T91: W-NCM 76.60% (best 88.30%); T92: W-NCM 85.37% (best 95.12%); T93: W-NCM 79.49% (best 94.87%); T94: W-NCM 74.11% (best 86.61%); T95: W-NCM 76.92% (best 82.42%); T96: W-NCM 81.82% (best 81.82%); T97: W-NCM 79.63% (best 85.19%); T98: W-NCM 65.52% (best 65.52%); T99: W-NCM 92.68% (best 92.68%)
2025-12-11 21:08:00,975 [trainer.py] => Average forgetting (W-NCM): 21.25% | Max forgetting (W-NCM): 59.18%
2025-12-11 21:08:00,987 [trainer.py] => All params: 144526051
2025-12-11 21:08:00,999 [trainer.py] => Trainable params: 185858
2025-12-11 21:08:00,999 [inflora.py] => Learning on 198-200
Parameters to be updated: {'classifier_pool.99.bias', 'image_encoder.blocks.3.attn.lora_B_k.99.weight', 'image_encoder.blocks.8.attn.lora_B_v.99.weight', 'image_encoder.blocks.2.attn.lora_B_k.99.weight', 'image_encoder.blocks.0.attn.lora_B_v.99.weight', 'image_encoder.blocks.9.attn.lora_B_v.99.weight', 'image_encoder.blocks.6.attn.lora_B_v.99.weight', 'image_encoder.blocks.5.attn.lora_B_v.99.weight', 'image_encoder.blocks.4.attn.lora_B_v.99.weight', 'image_encoder.blocks.4.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_v.99.weight', 'image_encoder.blocks.1.attn.lora_B_k.99.weight', 'image_encoder.blocks.0.attn.lora_B_k.99.weight', 'image_encoder.blocks.1.attn.lora_B_v.99.weight', 'image_encoder.blocks.10.attn.lora_B_v.99.weight', 'image_encoder.blocks.3.attn.lora_B_v.99.weight', 'image_encoder.blocks.8.attn.lora_B_k.99.weight', 'image_encoder.blocks.7.attn.lora_B_k.99.weight', 'image_encoder.blocks.9.attn.lora_B_k.99.weight', 'image_encoder.blocks.2.attn.lora_B_v.99.weight', 'image_encoder.blocks.10.attn.lora_B_k.99.weight', 'classifier_pool.99.weight', 'image_encoder.blocks.5.attn.lora_B_k.99.weight', 'image_encoder.blocks.11.attn.lora_B_k.99.weight', 'image_encoder.blocks.6.attn.lora_B_k.99.weight', 'image_encoder.blocks.7.attn.lora_B_v.99.weight'}
2025-12-11 21:10:13,735 [inflora.py] => Task 99, Epoch 50/50 => Loss 0.041, Train_accy 98.92
Threshold:  0.9998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 70/768 type remove
Layer 2 : 132/768 type remove
Layer 3 : 305/768 type remove
Layer 4 : 299/768 type retain
Layer 5 : 199/768 type retain
Layer 6 : 177/768 type retain
Layer 7 : 133/768 type retain
Layer 8 : 96/768 type retain
Layer 9 : 45/768 type retain
Layer 10 : 28/768 type retain
Layer 11 : 39/768 type retain
Layer 12 : 18/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 21:10:22,761 [trainer.py] => Time:141.76214027404785
6000 6000
6000 6000
2025-12-11 21:10:40,470 [trainer.py] => Time:17.708805084228516
2025-12-11 21:10:40,471 [inflora.py] => Exemplar size: 0
2025-12-11 21:10:40,471 [trainer.py] => CNN: {'total': np.float64(38.32), '00-01': np.float64(69.89), '02-03': np.float64(38.71), '04-05': np.float64(48.31), '06-07': np.float64(53.57), '08-09': np.float64(45.21), '10-11': np.float64(19.51), '12-13': np.float64(53.7), '14-15': np.float64(21.43), '16-17': np.float64(12.86), '18-19': np.float64(50.0), '20-21': np.float64(64.84), '22-23': np.float64(60.94), '24-25': np.float64(4.08), '26-27': np.float64(61.74), '28-29': np.float64(55.56), '30-31': np.float64(30.0), '32-33': np.float64(13.64), '34-35': np.float64(22.58), '36-37': np.float64(60.0), '38-39': np.float64(48.78), '40-41': np.float64(35.19), '42-43': np.float64(31.25), '44-45': np.float64(0.0), '46-47': np.float64(48.15), '48-49': np.float64(54.26), '50-51': np.float64(18.92), '52-53': np.float64(44.44), '54-55': np.float64(0.0), '56-57': np.float64(40.0), '58-59': np.float64(7.27), '60-61': np.float64(34.15), '62-63': np.float64(39.62), '64-65': np.float64(3.33), '66-67': np.float64(26.32), '68-69': np.float64(8.33), '70-71': np.float64(21.92), '72-73': np.float64(27.5), '74-75': np.float64(51.28), '76-77': np.float64(38.55), '78-79': np.float64(21.95), '80-81': np.float64(58.33), '82-83': np.float64(28.81), '84-85': np.float64(39.13), '86-87': np.float64(43.68), '88-89': np.float64(50.0), '90-91': np.float64(37.5), '92-93': np.float64(12.24), '94-95': np.float64(26.76), '96-97': np.float64(19.19), '98-99': np.float64(24.32), '100-101': np.float64(40.0), '102-103': np.float64(46.55), '104-105': np.float64(12.77), '106-107': np.float64(25.64), '108-109': np.float64(53.73), '110-111': np.float64(20.0), '112-113': np.float64(73.08), '114-115': np.float64(45.0), '116-117': np.float64(72.22), '118-119': np.float64(32.69), '120-121': np.float64(55.74), '122-123': np.float64(17.86), '124-125': np.float64(36.73), '126-127': np.float64(11.32), '128-129': np.float64(15.19), '130-131': np.float64(40.0), '132-133': np.float64(38.24), '134-135': np.float64(53.85), '136-137': np.float64(48.98), '138-139': np.float64(52.83), '140-141': np.float64(36.17), '142-143': np.float64(52.08), '144-145': np.float64(60.87), '146-147': np.float64(40.74), '148-149': np.float64(21.21), '150-151': np.float64(5.26), '152-153': np.float64(16.22), '154-155': np.float64(6.0), '156-157': np.float64(28.57), '158-159': np.float64(23.81), '160-161': np.float64(30.43), '162-163': np.float64(49.18), '164-165': np.float64(44.44), '166-167': np.float64(50.88), '168-169': np.float64(52.78), '170-171': np.float64(35.82), '172-173': np.float64(39.62), '174-175': np.float64(39.47), '176-177': np.float64(44.0), '178-179': np.float64(60.0), '180-181': np.float64(32.98), '182-183': np.float64(14.63), '184-185': np.float64(51.28), '186-187': np.float64(43.75), '188-189': np.float64(56.04), '190-191': np.float64(61.36), '192-193': np.float64(9.26), '194-195': np.float64(20.69), '196-197': np.float64(47.56), '198-199': np.float64(18.75), 'old': np.float64(38.53), 'new': np.float64(18.75)}
2025-12-11 21:10:40,471 [trainer.py] => CNN top1 curve: [np.float64(98.92), np.float64(79.35), np.float64(69.26), np.float64(71.67), np.float64(66.22), np.float64(63.77), np.float64(63.68), np.float64(61.83), np.float64(61.11), np.float64(58.62), np.float64(58.03), np.float64(58.11), np.float64(56.75), np.float64(59.54), np.float64(61.85), np.float64(59.07), np.float64(58.04), np.float64(57.97), np.float64(58.2), np.float64(59.07), np.float64(59.22), np.float64(57.94), np.float64(55.19), np.float64(55.58), np.float64(56.06), np.float64(54.82), np.float64(54.77), np.float64(47.69), np.float64(50.9), np.float64(48.55), np.float64(49.95), np.float64(51.72), np.float64(49.35), np.float64(49.13), np.float64(48.95), np.float64(48.07), np.float64(49.58), np.float64(45.98), np.float64(46.57), np.float64(45.19), np.float64(47.92), np.float64(49.06), np.float64(48.58), np.float64(47.4), np.float64(48.06), np.float64(47.24), np.float64(45.98), np.float64(47.15), np.float64(45.29), np.float64(45.21), np.float64(43.98), np.float64(45.07), np.float64(42.39), np.float64(41.96), np.float64(43.38), np.float64(42.26), np.float64(42.32), np.float64(42.8), np.float64(42.1), np.float64(41.66), np.float64(41.95), np.float64(40.35), np.float64(39.89), np.float64(39.01), np.float64(38.39), np.float64(38.63), np.float64(38.9), np.float64(37.92), np.float64(38.05), np.float64(37.62), np.float64(37.75), np.float64(38.07), np.float64(37.33), np.float64(37.78), np.float64(36.64), np.float64(37.07), np.float64(36.39), np.float64(35.94), np.float64(35.38), np.float64(33.7), np.float64(34.73), np.float64(35.85), np.float64(36.07), np.float64(36.41), np.float64(36.54), np.float64(37.44), np.float64(37.11), np.float64(36.7), np.float64(37.04), np.float64(37.43), np.float64(37.65), np.float64(37.95), np.float64(38.18), np.float64(38.38), np.float64(39.2), np.float64(39.35), np.float64(39.19), np.float64(38.78), np.float64(38.8), np.float64(38.32)]
2025-12-11 21:10:40,471 [trainer.py] => CNN top1 with task curve: [np.float64(98.92), np.float64(96.77), np.float64(95.08), np.float64(96.0), np.float64(95.98), np.float64(94.93), np.float64(94.66), np.float64(95.42), np.float64(96.13), np.float64(96.0), np.float64(96.49), np.float64(96.2), np.float64(96.08), np.float64(96.03), np.float64(96.05), np.float64(96.65), np.float64(96.53), np.float64(96.15), np.float64(96.54), np.float64(96.43), np.float64(96.7), np.float64(96.43), np.float64(96.36), np.float64(96.43), np.float64(96.14), np.float64(96.11), np.float64(95.9), np.float64(95.32), np.float64(95.49), np.float64(95.18), np.float64(95.61), np.float64(95.8), np.float64(96.06), np.float64(96.44), np.float64(96.04), np.float64(95.47), np.float64(95.88), np.float64(95.62), np.float64(95.78), np.float64(95.92), np.float64(96.2), np.float64(96.22), np.float64(96.51), np.float64(95.82), np.float64(96.02), np.float64(96.25), np.float64(96.03), np.float64(96.23), np.float64(95.67), np.float64(96.09), np.float64(96.38), np.float64(96.11), np.float64(95.86), np.float64(95.67), np.float64(95.66), np.float64(95.62), np.float64(95.49), np.float64(95.76), np.float64(95.97), np.float64(95.92), np.float64(96.04), np.float64(95.97), np.float64(95.86), np.float64(95.79), np.float64(95.6), np.float64(95.65), np.float64(95.68), np.float64(95.43), np.float64(95.72), np.float64(95.47), np.float64(95.77), np.float64(95.77), np.float64(95.98), np.float64(95.84), np.float64(95.81), np.float64(95.62), np.float64(95.61), np.float64(95.46), np.float64(95.43), np.float64(95.28), np.float64(95.53), np.float64(95.65), np.float64(95.55), np.float64(95.52), np.float64(95.64), np.float64(95.64), np.float64(95.57), np.float64(95.52), np.float64(95.51), np.float64(95.61), np.float64(95.48), np.float64(95.48), np.float64(95.51), np.float64(95.55), np.float64(95.72), np.float64(95.72), np.float64(95.67), np.float64(95.78), np.float64(95.75), np.float64(95.78)]
2025-12-11 21:10:40,471 [trainer.py] => CNN top1 task curve: [1.0, 0.8, 0.6926229508196722, 0.7166666666666667, 0.6621983914209115, 0.6376811594202898, 0.6367521367521367, 0.6202290076335878, 0.6161616161616161, 0.5892307692307692, 0.581646423751687, 0.5822784810126582, 0.5686274509803921, 0.5953533397870281, 0.618491921005386, 0.5906862745098039, 0.580441640378549, 0.5804464973056197, 0.5820456217807212, 0.5907142857142857, 0.5921595598349381, 0.5800807537012113, 0.5525446133509584, 0.5564935064935065, 0.5618115055079559, 0.5487731897067624, 0.5483304042179262, 0.476905311778291, 0.5090293453724605, 0.48549534756431306, 0.5, 0.5177304964539007, 0.4935129740518962, 0.4913494809688581, 0.48949682462139715, 0.48066037735849054, 0.49583333333333335, 0.4597855227882037, 0.46574752261956054, 0.4519350811485643, 0.47919191919191917, 0.4913227921326649, 0.4861690034103827, 0.4746881878209831, 0.4809763101220388, 0.4723991507430998, 0.46017391304347827, 0.47182620502376105, 0.45320197044334976, 0.45238858608528376, 0.44007549543881724, 0.45072598084646276, 0.42419693785649953, 0.4198813056379822, 0.4340995053826011, 0.4232199027738061, 0.42378134685826996, 0.4288102535525216, 0.42124137931034483, 0.41691596410116943, 0.41974317817014445, 0.40432261465471797, 0.3994275305750716, 0.3906570841889117, 0.3846540880503145, 0.3870646766169154, 0.38973852984706464, 0.3799612778315586, 0.3810093279119828, 0.37623996221067546, 0.37748189675309507, 0.3806883806883807, 0.37325714285714284, 0.3777828259881872, 0.3664036076662909, 0.37089201877934275, 0.364079822616408, 0.35964912280701755, 0.35398613518197575, 0.33725154947638386, 0.3475132275132275, 0.3587547012118679, 0.3609515996718622, 0.36428137036286234, 0.36543456543456543, 0.37440851735015773, 0.3711219512195122, 0.3670447990771006, 0.3703563305534496, 0.3742932529212213, 0.3764814814814815, 0.37952582245910677, 0.38175182481751824, 0.38376251788268956, 0.3920464543374978, 0.39351932074163926, 0.391931330472103, 0.387769046805603, 0.38797169811320753, 0.38316666666666666]
2025-12-11 21:10:59,820 [trainer.py] => W-NCM: {'00-01': 67.74193548387096, '02-03': 66.12903225806451, '04-05': 58.42696629213483, '06-07': 80.35714285714286, '08-09': 71.23287671232876, '10-11': 60.97560975609756, '12-13': 88.88888888888889, '14-15': 75.0, '16-17': 78.57142857142857, '18-19': 67.85714285714286, '20-21': 80.21978021978022, '22-23': 64.84375, '24-25': 67.3469387755102, '26-27': 55.65217391304348, '28-29': 64.19753086419753, '30-31': 41.81818181818181, '32-33': 88.63636363636364, '34-35': 67.74193548387096, '36-37': 78.33333333333333, '38-39': 65.85365853658537, '40-41': 55.55555555555556, '42-43': 65.625, '44-45': 51.85185185185185, '46-47': 77.77777777777779, '48-49': 81.91489361702128, '50-51': 70.27027027027027, '52-53': 83.33333333333334, '54-55': 48.0, '56-57': 90.0, '58-59': 54.54545454545454, '60-61': 68.29268292682927, '62-63': 76.41509433962264, '64-65': 66.66666666666666, '66-67': 47.368421052631575, '68-69': 75.0, '70-71': 65.75342465753424, '72-73': 70.0, '74-75': 37.17948717948718, '76-77': 71.08433734939759, '78-79': 60.97560975609756, '80-81': 79.16666666666666, '82-83': 53.38983050847458, '84-85': 80.43478260869566, '86-87': 78.16091954022988, '88-89': 86.66666666666667, '90-91': 65.0, '92-93': 24.489795918367346, '94-95': 78.87323943661971, '96-97': 55.55555555555556, '98-99': 62.16216216216216, '100-101': 66.66666666666666, '102-103': 72.41379310344827, '104-105': 56.38297872340425, '106-107': 66.66666666666666, '108-109': 64.17910447761194, '110-111': 73.33333333333333, '112-113': 84.61538461538461, '114-115': 62.5, '116-117': 94.44444444444444, '118-119': 78.84615384615384, '120-121': 77.04918032786885, '122-123': 71.42857142857143, '124-125': 44.89795918367347, '126-127': 45.28301886792453, '128-129': 35.44303797468354, '130-131': 64.44444444444444, '132-133': 79.41176470588235, '134-135': 51.28205128205128, '136-137': 75.51020408163265, '138-139': 73.58490566037736, '140-141': 55.319148936170215, '142-143': 87.5, '144-145': 80.43478260869566, '146-147': 74.07407407407408, '148-149': 66.66666666666666, '150-151': 55.26315789473685, '152-153': 67.56756756756756, '154-155': 72.0, '156-157': 67.85714285714286, '158-159': 71.42857142857143, '160-161': 86.95652173913044, '162-163': 77.04918032786885, '164-165': 80.0, '166-167': 70.17543859649122, '168-169': 77.77777777777779, '170-171': 85.07462686567165, '172-173': 92.45283018867924, '174-175': 90.78947368421053, '176-177': 80.0, '178-179': 83.33333333333334, '180-181': 75.53191489361703, '182-183': 85.36585365853658, '184-185': 79.48717948717949, '186-187': 71.42857142857143, '188-189': 80.21978021978022, '190-191': 80.68181818181817, '192-193': 79.62962962962963, '194-195': 62.06896551724138, '196-197': 92.6829268292683, '198-199': 85.9375}
2025-12-11 21:10:59,821 [trainer.py] => Ave Acc (W-NCM): 70.24%
2025-12-11 21:10:59,821 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 67.74% (best 97.85%); T2: W-NCM 66.13% (best 90.32%); T3: W-NCM 58.43% (best 91.01%); T4: W-NCM 80.36% (best 92.86%); T5: W-NCM 71.23% (best 83.56%); T6: W-NCM 60.98% (best 80.49%); T7: W-NCM 88.89% (best 92.59%); T8: W-NCM 75.00% (best 94.64%); T9: W-NCM 78.57% (best 98.57%); T10: W-NCM 67.86% (best 94.64%); T11: W-NCM 80.22% (best 95.60%); T12: W-NCM 64.84% (best 95.31%); T13: W-NCM 67.35% (best 87.76%); T14: W-NCM 55.65% (best 94.78%); T15: W-NCM 64.20% (best 96.30%); T16: W-NCM 41.82% (best 93.64%); T17: W-NCM 88.64% (best 97.73%); T18: W-NCM 67.74% (best 96.77%); T19: W-NCM 78.33% (best 86.67%); T20: W-NCM 65.85% (best 97.56%); T21: W-NCM 55.56% (best 94.44%); T22: W-NCM 65.62% (best 96.88%); T23: W-NCM 51.85% (best 62.96%); T24: W-NCM 77.78% (best 92.59%); T25: W-NCM 81.91% (best 94.68%); T26: W-NCM 70.27% (best 100.00%); T27: W-NCM 83.33% (best 94.44%); T28: W-NCM 48.00% (best 80.00%); T29: W-NCM 90.00% (best 95.00%); T30: W-NCM 54.55% (best 90.91%); T31: W-NCM 68.29% (best 90.24%); T32: W-NCM 76.42% (best 90.57%); T33: W-NCM 66.67% (best 93.33%); T34: W-NCM 47.37% (best 100.00%); T35: W-NCM 75.00% (best 91.67%); T36: W-NCM 65.75% (best 91.78%); T37: W-NCM 70.00% (best 92.50%); T38: W-NCM 37.18% (best 87.18%); T39: W-NCM 71.08% (best 78.31%); T40: W-NCM 60.98% (best 89.02%); T41: W-NCM 79.17% (best 97.22%); T42: W-NCM 53.39% (best 89.83%); T43: W-NCM 80.43% (best 89.13%); T44: W-NCM 78.16% (best 91.95%); T45: W-NCM 86.67% (best 96.67%); T46: W-NCM 65.00% (best 92.50%); T47: W-NCM 24.49% (best 85.71%); T48: W-NCM 78.87% (best 95.77%); T49: W-NCM 55.56% (best 84.85%); T50: W-NCM 62.16% (best 91.89%); T51: W-NCM 66.67% (best 91.67%); T52: W-NCM 72.41% (best 87.93%); T53: W-NCM 56.38% (best 79.79%); T54: W-NCM 66.67% (best 89.74%); T55: W-NCM 64.18% (best 91.04%); T56: W-NCM 73.33% (best 96.67%); T57: W-NCM 84.62% (best 94.23%); T58: W-NCM 62.50% (best 92.50%); T59: W-NCM 94.44% (best 97.22%); T60: W-NCM 78.85% (best 100.00%); T61: W-NCM 77.05% (best 95.08%); T62: W-NCM 71.43% (best 94.64%); T63: W-NCM 44.90% (best 93.88%); T64: W-NCM 45.28% (best 84.91%); T65: W-NCM 35.44% (best 92.41%); T66: W-NCM 64.44% (best 95.56%); T67: W-NCM 79.41% (best 94.12%); T68: W-NCM 51.28% (best 85.90%); T69: W-NCM 75.51% (best 93.88%); T70: W-NCM 73.58% (best 94.34%); T71: W-NCM 55.32% (best 87.23%); T72: W-NCM 87.50% (best 91.67%); T73: W-NCM 80.43% (best 95.65%); T74: W-NCM 74.07% (best 96.30%); T75: W-NCM 66.67% (best 90.91%); T76: W-NCM 55.26% (best 84.21%); T77: W-NCM 67.57% (best 94.59%); T78: W-NCM 72.00% (best 94.00%); T79: W-NCM 67.86% (best 89.29%); T80: W-NCM 71.43% (best 88.89%); T81: W-NCM 86.96% (best 95.65%); T82: W-NCM 77.05% (best 91.80%); T83: W-NCM 80.00% (best 90.00%); T84: W-NCM 70.18% (best 94.74%); T85: W-NCM 77.78% (best 88.89%); T86: W-NCM 85.07% (best 94.03%); T87: W-NCM 92.45% (best 98.11%); T88: W-NCM 90.79% (best 94.74%); T89: W-NCM 80.00% (best 93.33%); T90: W-NCM 83.33% (best 90.00%); T91: W-NCM 75.53% (best 88.30%); T92: W-NCM 85.37% (best 95.12%); T93: W-NCM 79.49% (best 94.87%); T94: W-NCM 71.43% (best 86.61%); T95: W-NCM 80.22% (best 82.42%); T96: W-NCM 80.68% (best 81.82%); T97: W-NCM 79.63% (best 85.19%); T98: W-NCM 62.07% (best 65.52%); T99: W-NCM 92.68% (best 92.68%); T100: W-NCM 85.94% (best 85.94%)
2025-12-11 21:10:59,821 [trainer.py] => Average forgetting (W-NCM): 21.25% | Max forgetting (W-NCM): 61.22%
2025-12-11 21:10:59,821 [trainer.py] => 
===== Summary =====
2025-12-11 21:10:59,821 [trainer.py] => Final average accuracy: 38.32%
2025-12-11 21:10:59,821 [trainer.py] => Average accuracy over tasks: 47.14%
2025-12-11 21:10:59,822 [trainer.py] => Final average forgetting: 14.89%
2025-12-11 21:10:59,822 [trainer.py] => Final max forgetting: 46.67%
2025-12-11 21:10:59,822 [trainer.py] => W-NCM final average accuracy: 70.24%
2025-12-11 21:10:59,822 [trainer.py] => W-NCM average accuracy over tasks: 71.14%
2025-12-11 21:10:59,822 [trainer.py] => W-NCM final average forgetting: 21.25%
2025-12-11 21:10:59,822 [trainer.py] => W-NCM final max forgetting: 61.22%
