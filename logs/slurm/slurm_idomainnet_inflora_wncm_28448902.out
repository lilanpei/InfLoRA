logs/idomainnet/17_17_sip/InfLoRA/adam/10/0.95_1.0-0.0005/2025121206
2025-12-12 02:28:21,600 [trainer.py] => config: configs/idomainnet_inflora_seed42_wncm_10ep.json
2025-12-12 02:28:21,601 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-12 02:28:21,601 [trainer.py] => prefix: reproduce
2025-12-12 02:28:21,601 [trainer.py] => dataset: idomainnet
2025-12-12 02:28:21,601 [trainer.py] => data_path: /leonardo_scratch/large/userexternal/lli00001/domainnet
2025-12-12 02:28:21,601 [trainer.py] => memory_size: 0
2025-12-12 02:28:21,601 [trainer.py] => memory_per_class: 0
2025-12-12 02:28:21,601 [trainer.py] => fixed_memory: True
2025-12-12 02:28:21,601 [trainer.py] => shuffle: False
2025-12-12 02:28:21,601 [trainer.py] => init_cls: 17
2025-12-12 02:28:21,601 [trainer.py] => increment: 17
2025-12-12 02:28:21,602 [trainer.py] => model_name: InfLoRA
2025-12-12 02:28:21,602 [trainer.py] => net_type: sip
2025-12-12 02:28:21,602 [trainer.py] => embd_dim: 768
2025-12-12 02:28:21,602 [trainer.py] => num_heads: 12
2025-12-12 02:28:21,602 [trainer.py] => total_sessions: 10
2025-12-12 02:28:21,602 [trainer.py] => idomainnet_tasks_per_domain: 1
2025-12-12 02:28:21,602 [trainer.py] => seed: 2025121206
2025-12-12 02:28:21,602 [trainer.py] => EPSILON: 1e-08
2025-12-12 02:28:21,602 [trainer.py] => init_epoch: 10
2025-12-12 02:28:21,602 [trainer.py] => optim: adam
2025-12-12 02:28:21,602 [trainer.py] => init_lr: 0.0005
2025-12-12 02:28:21,602 [trainer.py] => init_lr_decay: 0.1
2025-12-12 02:28:21,602 [trainer.py] => init_weight_decay: 0.0
2025-12-12 02:28:21,602 [trainer.py] => epochs: 10
2025-12-12 02:28:21,602 [trainer.py] => lrate: 0.0005
2025-12-12 02:28:21,602 [trainer.py] => lrate_decay: 0.1
2025-12-12 02:28:21,602 [trainer.py] => batch_size: 128
2025-12-12 02:28:21,602 [trainer.py] => weight_decay: 0.0
2025-12-12 02:28:21,602 [trainer.py] => rank: 10
2025-12-12 02:28:21,602 [trainer.py] => lamb: 0.95
2025-12-12 02:28:21,602 [trainer.py] => lame: 1.0
2025-12-12 02:28:21,602 [trainer.py] => num_workers: 8
2025-12-12 02:28:21,602 [trainer.py] => use_wncm: True
2025-12-12 02:28:21,602 [trainer.py] => wncm_lambda: 0.07
2025-12-12 02:28:21,602 [trainer.py] => save_checkpoints: False
2025-12-12 02:28:21,920 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2025-12-12 02:28:22,318 [data_manager.py] => iDomainNet: using idomainnet_tasks_per_domain=1 -> 6 tasks, increments=[17, 17, 17, 17, 16, 16], task_domains=['real', 'painting', 'clipart', 'sketch', 'quickdraw', 'infograph']
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 290, unexpected 0
2025-12-12 02:28:23,737 [whitened_ncm_head.py] => WhitenedNCM: Using CPU
2025-12-12 02:28:23,739 [trainer.py] => All params: 109723167
2025-12-12 02:28:23,740 [trainer.py] => Trainable params: 109723167
2025-12-12 02:28:23,740 [inflora.py] => Learning on 0-17
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight'}
2025-12-12 02:35:23,403 [inflora.py] => Task 0, Epoch 10/10 => Loss 0.067, Train_accy 97.98
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 11/768 type remove
Layer 5 : 15/768 type remove
Layer 6 : 13/768 type remove
Layer 7 : 13/768 type remove
Layer 8 : 16/768 type remove
Layer 9 : 22/768 type remove
Layer 10 : 25/768 type remove
Layer 11 : 9/768 type remove
Layer 12 : 13/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 02:36:08,786 [trainer.py] => Time:465.0457215309143
3076 3076
3076 3076
2025-12-12 02:36:17,698 [trainer.py] => Time:8.911828756332397
2025-12-12 02:36:17,698 [inflora.py] => Exemplar size: 0
2025-12-12 02:36:17,698 [trainer.py] => CNN: {'total': np.float64(97.2), '00-16': np.float64(97.2), 'old': 0, 'new': np.float64(97.2)}
2025-12-12 02:36:17,698 [trainer.py] => CNN top1 curve: [np.float64(97.2)]
2025-12-12 02:36:17,698 [trainer.py] => CNN top1 with task curve: [np.float64(97.2)]
2025-12-12 02:36:17,698 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-12 02:36:45,720 [trainer.py] => W-NCM: {'00-16': 97.10663198959688}
2025-12-12 02:36:45,721 [trainer.py] => Ave Acc (W-NCM): 97.11%
2025-12-12 02:36:45,721 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 97.11% (best 97.11%)
2025-12-12 02:36:45,721 [trainer.py] => Average forgetting (W-NCM): 0.00% | Max forgetting (W-NCM): 0.00%
2025-12-12 02:36:45,722 [trainer.py] => All params: 109723167
2025-12-12 02:36:45,724 [trainer.py] => Trainable params: 197393
2025-12-12 02:36:45,724 [inflora.py] => Learning on 17-34
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight'}
2025-12-12 02:40:10,846 [inflora.py] => Task 1, Epoch 10/10 => Loss 0.211, Train_accy 93.63
Threshold:  0.9583333333333333
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 16/768 type remove
Layer 5 : 23/768 type remove
Layer 6 : 23/768 type remove
Layer 7 : 24/768 type remove
Layer 8 : 30/768 type remove
Layer 9 : 42/768 type remove
Layer 10 : 42/768 type remove
Layer 11 : 14/768 type remove
Layer 12 : 27/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 02:40:35,530 [trainer.py] => Time:229.80617952346802
9011 9011
9011 9011
2025-12-12 02:40:59,619 [trainer.py] => Time:24.088788509368896
2025-12-12 02:40:59,619 [inflora.py] => Exemplar size: 0
2025-12-12 02:40:59,619 [trainer.py] => CNN: {'total': np.float64(86.78), '00-16': np.float64(88.98), '17-33': np.float64(84.51), 'old': np.float64(88.98), 'new': np.float64(84.51)}
2025-12-12 02:40:59,619 [trainer.py] => CNN top1 curve: [np.float64(97.2), np.float64(86.78)]
2025-12-12 02:40:59,620 [trainer.py] => CNN top1 with task curve: [np.float64(97.2), np.float64(92.82)]
2025-12-12 02:40:59,620 [trainer.py] => CNN top1 task curve: [1.0, 0.9042281655754079]
2025-12-12 02:41:32,750 [trainer.py] => W-NCM: {'00-16': 82.97267759562843, '17-33': 91.92966636609559}
2025-12-12 02:41:32,750 [trainer.py] => Ave Acc (W-NCM): 87.45%
2025-12-12 02:41:32,750 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 82.97% (best 97.11%); T2: W-NCM 91.93% (best 91.93%)
2025-12-12 02:41:32,750 [trainer.py] => Average forgetting (W-NCM): 14.13% | Max forgetting (W-NCM): 14.13%
2025-12-12 02:41:32,751 [trainer.py] => All params: 109723167
2025-12-12 02:41:32,753 [trainer.py] => Trainable params: 197393
2025-12-12 02:41:32,753 [inflora.py] => Learning on 34-51
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight'}
2025-12-12 02:43:17,003 [inflora.py] => Task 2, Epoch 10/10 => Loss 0.336, Train_accy 90.27
Threshold:  0.9666666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 26/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 30/768 type remove
Layer 8 : 33/768 type remove
Layer 9 : 45/768 type remove
Layer 10 : 45/768 type remove
Layer 11 : 17/768 type remove
Layer 12 : 38/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 02:43:31,781 [trainer.py] => Time:119.02858805656433
16335 16335
16335 16335
2025-12-12 02:44:14,254 [trainer.py] => Time:42.47191643714905
2025-12-12 02:44:14,254 [inflora.py] => Exemplar size: 0
2025-12-12 02:44:14,254 [trainer.py] => CNN: {'total': np.float64(70.09), '00-16': np.float64(86.4), '17-33': np.float64(80.37), '34-50': np.float64(44.58), 'old': np.float64(83.4), 'new': np.float64(44.58)}
2025-12-12 02:44:14,254 [trainer.py] => CNN top1 curve: [np.float64(97.2), np.float64(86.78), np.float64(70.09)]
2025-12-12 02:44:14,254 [trainer.py] => CNN top1 with task curve: [np.float64(97.2), np.float64(92.82), np.float64(84.16)]
2025-12-12 02:44:14,254 [trainer.py] => CNN top1 task curve: [1.0, 0.9042281655754079, 0.7370064279155188]
2025-12-12 02:45:00,962 [trainer.py] => W-NCM: {'00-16': 73.83246849518162, '17-33': 86.7740726863994, '34-50': 78.41456882699518}
2025-12-12 02:45:00,962 [trainer.py] => Ave Acc (W-NCM): 79.67%
2025-12-12 02:45:00,962 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 73.83% (best 97.11%); T2: W-NCM 86.77% (best 91.93%); T3: W-NCM 78.41% (best 78.41%)
2025-12-12 02:45:00,962 [trainer.py] => Average forgetting (W-NCM): 14.21% | Max forgetting (W-NCM): 23.27%
2025-12-12 02:45:00,964 [trainer.py] => All params: 109723167
2025-12-12 02:45:00,965 [trainer.py] => Trainable params: 197393
2025-12-12 02:45:00,965 [inflora.py] => Learning on 51-68
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'classifier_pool.3.weight'}
2025-12-12 02:49:34,613 [inflora.py] => Task 3, Epoch 10/10 => Loss 0.504, Train_accy 84.20
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 20/768 type remove
Layer 5 : 31/768 type remove
Layer 6 : 32/768 type remove
Layer 7 : 36/768 type remove
Layer 8 : 39/768 type remove
Layer 9 : 52/768 type remove
Layer 10 : 52/768 type remove
Layer 11 : 21/768 type remove
Layer 12 : 48/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 02:50:06,123 [trainer.py] => Time:305.1579909324646
26665 26665
26665 26665
2025-12-12 02:51:14,937 [trainer.py] => Time:68.813725233078
2025-12-12 02:51:14,937 [inflora.py] => Exemplar size: 0
2025-12-12 02:51:14,937 [trainer.py] => CNN: {'total': np.float64(67.35), '00-16': np.float64(84.93), '17-33': np.float64(76.18), '34-50': np.float64(43.08), '51-67': np.float64(64.66), 'old': np.float64(68.29), 'new': np.float64(64.66)}
2025-12-12 02:51:14,937 [trainer.py] => CNN top1 curve: [np.float64(97.2), np.float64(86.78), np.float64(70.09), np.float64(67.35)]
2025-12-12 02:51:14,937 [trainer.py] => CNN top1 with task curve: [np.float64(97.2), np.float64(92.82), np.float64(84.16), np.float64(84.47)]
2025-12-12 02:51:14,937 [trainer.py] => CNN top1 task curve: [1.0, 0.9042281655754079, 0.7370064279155188, 0.7065066566660416]
2025-12-12 02:52:35,033 [trainer.py] => W-NCM: {'00-16': 70.52408361102697, '17-33': 82.14658904314078, '34-50': 73.84090556675453, '51-67': 84.20824295010846}
2025-12-12 02:52:35,033 [trainer.py] => Ave Acc (W-NCM): 77.68%
2025-12-12 02:52:35,034 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 70.52% (best 97.11%); T2: W-NCM 82.15% (best 91.93%); T3: W-NCM 73.84% (best 78.41%); T4: W-NCM 84.21% (best 84.21%)
2025-12-12 02:52:35,034 [trainer.py] => Average forgetting (W-NCM): 13.65% | Max forgetting (W-NCM): 26.58%
2025-12-12 02:52:35,035 [trainer.py] => All params: 109723167
2025-12-12 02:52:35,036 [trainer.py] => Trainable params: 197393
2025-12-12 02:52:35,036 [inflora.py] => Learning on 68-84
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight'}
2025-12-12 02:58:06,098 [inflora.py] => Task 4, Epoch 10/10 => Loss 0.561, Train_accy 81.82
Threshold:  0.9833333333333333
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 36/768 type remove
Layer 6 : 36/768 type remove
Layer 7 : 41/768 type remove
Layer 8 : 45/768 type remove
Layer 9 : 59/768 type remove
Layer 10 : 58/768 type remove
Layer 11 : 26/768 type remove
Layer 12 : 64/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 02:58:42,372 [trainer.py] => Time:367.3360958099365
45447 45447
45447 45447
2025-12-12 03:00:38,296 [trainer.py] => Time:115.92315912246704
2025-12-12 03:00:38,296 [inflora.py] => Exemplar size: 0
2025-12-12 03:00:38,296 [trainer.py] => CNN: {'total': np.float64(50.22), '00-16': np.float64(65.07), '17-33': np.float64(57.95), '34-50': np.float64(36.34), '51-67': np.float64(49.03), '68-84': np.float64(41.9), 'old': np.float64(52.15), 'new': np.float64(41.9)}
2025-12-12 03:00:38,296 [trainer.py] => CNN top1 curve: [np.float64(97.2), np.float64(86.78), np.float64(70.09), np.float64(67.35), np.float64(50.22)]
2025-12-12 03:00:38,296 [trainer.py] => CNN top1 with task curve: [np.float64(97.2), np.float64(92.82), np.float64(84.16), np.float64(84.47), np.float64(70.25)]
2025-12-12 03:00:38,296 [trainer.py] => CNN top1 task curve: [1.0, 0.9042281655754079, 0.7370064279155188, 0.7065066566660416, 0.5443263581754572]
2025-12-12 03:02:47,230 [trainer.py] => W-NCM: {'00-16': 52.17438811188811, '17-33': 61.844523732295386, '34-50': 58.23980442271364, '51-67': 63.993660855784476, '68-83': 48.566767653227686}
2025-12-12 03:02:47,231 [trainer.py] => Ave Acc (W-NCM): 56.96%
2025-12-12 03:02:47,231 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 52.17% (best 97.11%); T2: W-NCM 61.84% (best 91.93%); T3: W-NCM 58.24% (best 78.41%); T4: W-NCM 63.99% (best 84.21%); T5: W-NCM 48.57% (best 48.57%)
2025-12-12 03:02:47,231 [trainer.py] => Average forgetting (W-NCM): 28.85% | Max forgetting (W-NCM): 44.93%
2025-12-12 03:02:47,232 [trainer.py] => All params: 109723167
2025-12-12 03:02:47,234 [trainer.py] => Trainable params: 197393
2025-12-12 03:02:47,234 [inflora.py] => Learning on 84-100
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'classifier_pool.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight'}
2025-12-12 03:07:09,730 [inflora.py] => Task 5, Epoch 10/10 => Loss 1.066, Train_accy 66.71
Threshold:  0.9916666666666667
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 45/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 73/768 type remove
Layer 6 : 71/768 type remove
Layer 7 : 84/768 type remove
Layer 8 : 86/768 type remove
Layer 9 : 114/768 type remove
Layer 10 : 95/768 type remove
Layer 11 : 58/768 type remove
Layer 12 : 95/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-12 03:07:40,490 [trainer.py] => Time:293.2563579082489
60622 60622
60622 60622
2025-12-12 03:10:16,041 [trainer.py] => Time:155.5505063533783
2025-12-12 03:10:16,045 [inflora.py] => Exemplar size: 0
2025-12-12 03:10:16,045 [trainer.py] => CNN: {'total': np.float64(39.76), '00-16': np.float64(63.93), '17-33': np.float64(55.3), '34-50': np.float64(35.17), '51-67': np.float64(48.04), '68-84': np.float64(34.41), '85-101': np.float64(0.06), 'old': np.float64(47.87), 'new': np.float64(0.06)}
2025-12-12 03:10:16,045 [trainer.py] => CNN top1 curve: [np.float64(97.2), np.float64(86.78), np.float64(70.09), np.float64(67.35), np.float64(50.22), np.float64(39.76)]
2025-12-12 03:10:16,045 [trainer.py] => CNN top1 with task curve: [np.float64(97.2), np.float64(92.82), np.float64(84.16), np.float64(84.47), np.float64(70.25), np.float64(57.22)]
2025-12-12 03:10:16,045 [trainer.py] => CNN top1 task curve: [1.0, 0.9042281655754079, 0.7370064279155188, 0.7065066566660416, 0.5443263581754572, 0.5037610108541454]
2025-12-12 03:13:00,617 [trainer.py] => W-NCM: {'00-16': 48.69677676084361, '17-33': 58.29732914375491, '34-50': 55.2341873498799, '51-67': 60.90394377196407, '68-83': 38.987521558283454, '84-99': 61.62440493539299}
2025-12-12 03:13:00,617 [trainer.py] => Ave Acc (W-NCM): 53.96%
2025-12-12 03:13:00,617 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 48.70% (best 97.11%); T2: W-NCM 58.30% (best 91.93%); T3: W-NCM 55.23% (best 78.41%); T4: W-NCM 60.90% (best 84.21%); T5: W-NCM 38.99% (best 48.57%); T6: W-NCM 61.62% (best 61.62%)
2025-12-12 03:13:00,617 [trainer.py] => Average forgetting (W-NCM): 27.62% | Max forgetting (W-NCM): 48.41%
2025-12-12 03:13:00,617 [trainer.py] => 
===== Summary =====
2025-12-12 03:13:00,618 [trainer.py] => Final average accuracy: 39.76%
2025-12-12 03:13:00,618 [trainer.py] => Average accuracy over tasks: 68.57%
2025-12-12 03:13:00,618 [trainer.py] => Final average forgetting: 19.20%
2025-12-12 03:13:00,618 [trainer.py] => Final max forgetting: 33.27%
2025-12-12 03:13:00,618 [trainer.py] => W-NCM final average accuracy: 53.96%
2025-12-12 03:13:00,618 [trainer.py] => W-NCM average accuracy over tasks: 75.47%
2025-12-12 03:13:00,618 [trainer.py] => W-NCM final average forgetting: 27.62%
2025-12-12 03:13:00,618 [trainer.py] => W-NCM final max forgetting: 48.41%
