logs/cifar100/2_2_sip/InfLoRA/adam/10/0.95_1.0-0.0005/42
2025-12-10 13:29:05,493 [trainer.py] => config: configs/cifar100_50tasks_inflora_seed42.json
2025-12-10 13:29:05,495 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-10 13:29:05,495 [trainer.py] => prefix: reproduce
2025-12-10 13:29:05,495 [trainer.py] => dataset: cifar100
2025-12-10 13:29:05,495 [trainer.py] => data_path: data/
2025-12-10 13:29:05,495 [trainer.py] => memory_size: 0
2025-12-10 13:29:05,495 [trainer.py] => memory_per_class: 0
2025-12-10 13:29:05,495 [trainer.py] => fixed_memory: True
2025-12-10 13:29:05,495 [trainer.py] => shuffle: False
2025-12-10 13:29:05,495 [trainer.py] => init_cls: 2
2025-12-10 13:29:05,495 [trainer.py] => increment: 2
2025-12-10 13:29:05,495 [trainer.py] => model_name: InfLoRA
2025-12-10 13:29:05,495 [trainer.py] => net_type: sip
2025-12-10 13:29:05,495 [trainer.py] => embd_dim: 768
2025-12-10 13:29:05,495 [trainer.py] => num_heads: 12
2025-12-10 13:29:05,495 [trainer.py] => total_sessions: 50
2025-12-10 13:29:05,495 [trainer.py] => seed: 42
2025-12-10 13:29:05,495 [trainer.py] => EPSILON: 1e-08
2025-12-10 13:29:05,496 [trainer.py] => init_epoch: 20
2025-12-10 13:29:05,496 [trainer.py] => optim: adam
2025-12-10 13:29:05,496 [trainer.py] => init_lr: 0.0005
2025-12-10 13:29:05,496 [trainer.py] => init_lr_decay: 0.1
2025-12-10 13:29:05,496 [trainer.py] => init_weight_decay: 0.0
2025-12-10 13:29:05,496 [trainer.py] => epochs: 20
2025-12-10 13:29:05,496 [trainer.py] => lrate: 0.0005
2025-12-10 13:29:05,496 [trainer.py] => lrate_decay: 0.1
2025-12-10 13:29:05,496 [trainer.py] => batch_size: 128
2025-12-10 13:29:05,496 [trainer.py] => weight_decay: 0.0
2025-12-10 13:29:05,496 [trainer.py] => rank: 10
2025-12-10 13:29:05,496 [trainer.py] => lamb: 0.95
2025-12-10 13:29:05,496 [trainer.py] => lame: 1.0
2025-12-10 13:29:05,496 [trainer.py] => num_workers: 8
2025-12-10 13:29:07,463 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 2402, unexpected 0
2025-12-10 13:29:10,846 [trainer.py] => All params: 125940251
2025-12-10 13:29:10,853 [trainer.py] => Trainable params: 125940251
2025-12-10 13:29:10,853 [inflora.py] => Learning on 0-2
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'classifier_pool.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight'}
2025-12-10 13:31:26,142 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.009, Train_accy 99.50
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 10/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 9/768 type remove
Layer 6 : 10/768 type remove
Layer 7 : 12/768 type remove
Layer 8 : 14/768 type remove
Layer 9 : 10/768 type remove
Layer 10 : 5/768 type remove
Layer 11 : 3/768 type remove
Layer 12 : 2/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:31:34,661 [trainer.py] => Time:143.8082594871521
200 200
200 200
2025-12-10 13:31:35,810 [trainer.py] => Time:1.148163080215454
2025-12-10 13:31:35,810 [inflora.py] => Exemplar size: 0
2025-12-10 13:31:35,810 [trainer.py] => CNN: {'total': np.float64(100.0), '00-01': np.float64(100.0), 'old': 0, 'new': np.float64(100.0)}
2025-12-10 13:31:35,810 [trainer.py] => CNN top1 curve: [np.float64(100.0)]
2025-12-10 13:31:35,810 [trainer.py] => CNN top1 with task curve: [np.float64(100.0)]
2025-12-10 13:31:35,810 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-10 13:31:36,525 [trainer.py] => All params: 125940251
2025-12-10 13:31:36,532 [trainer.py] => Trainable params: 185858
2025-12-10 13:31:36,532 [inflora.py] => Learning on 2-4
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'classifier_pool.14.bias', 'classifier_pool.17.bias', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'classifier_pool.19.bias', 'classifier_pool.11.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight'}
2025-12-10 13:33:42,951 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.030, Train_accy 98.60
Threshold:  0.951
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 13/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 13/768 type remove
Layer 6 : 15/768 type remove
Layer 7 : 18/768 type remove
Layer 8 : 22/768 type remove
Layer 9 : 20/768 type remove
Layer 10 : 14/768 type remove
Layer 11 : 7/768 type remove
Layer 12 : 7/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:33:53,139 [trainer.py] => Time:136.60681295394897
400 400
400 400
2025-12-10 13:33:54,886 [trainer.py] => Time:1.746863842010498
2025-12-10 13:33:54,886 [inflora.py] => Exemplar size: 0
2025-12-10 13:33:54,886 [trainer.py] => CNN: {'total': np.float64(96.75), '00-01': np.float64(100.0), '02-03': np.float64(93.5), 'old': np.float64(100.0), 'new': np.float64(93.5)}
2025-12-10 13:33:54,886 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75)]
2025-12-10 13:33:54,886 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0)]
2025-12-10 13:33:54,886 [trainer.py] => CNN top1 task curve: [1.0, 0.9675]
2025-12-10 13:33:55,611 [trainer.py] => All params: 125940251
2025-12-10 13:33:55,618 [trainer.py] => Trainable params: 2044438
2025-12-10 13:33:55,618 [inflora.py] => Learning on 4-6
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight'}
2025-12-10 13:36:02,088 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.022, Train_accy 99.20
Threshold:  0.952
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 14/768 type remove
Layer 6 : 16/768 type remove
Layer 7 : 20/768 type remove
Layer 8 : 27/768 type remove
Layer 9 : 25/768 type remove
Layer 10 : 19/768 type remove
Layer 11 : 9/768 type remove
Layer 12 : 10/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:36:12,422 [trainer.py] => Time:136.80397200584412
600 600
600 600
2025-12-10 13:36:14,622 [trainer.py] => Time:2.1997900009155273
2025-12-10 13:36:14,623 [inflora.py] => Exemplar size: 0
2025-12-10 13:36:14,623 [trainer.py] => CNN: {'total': np.float64(91.5), '00-01': np.float64(99.5), '02-03': np.float64(91.5), '04-05': np.float64(83.5), 'old': np.float64(95.5), 'new': np.float64(83.5)}
2025-12-10 13:36:14,623 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5)]
2025-12-10 13:36:14,623 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0)]
2025-12-10 13:36:14,623 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915]
2025-12-10 13:36:15,377 [trainer.py] => All params: 125940251
2025-12-10 13:36:15,384 [trainer.py] => Trainable params: 2044438
2025-12-10 13:36:15,384 [inflora.py] => Learning on 6-8
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'classifier_pool.33.bias', 'classifier_pool.36.bias', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'classifier_pool.39.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight', 'classifier_pool.34.weight'}
2025-12-10 13:38:21,552 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.051, Train_accy 98.20
Threshold:  0.953
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 15/768 type remove
Layer 5 : 16/768 type remove
Layer 6 : 18/768 type remove
Layer 7 : 22/768 type remove
Layer 8 : 33/768 type remove
Layer 9 : 29/768 type remove
Layer 10 : 23/768 type remove
Layer 11 : 11/768 type remove
Layer 12 : 13/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:38:31,662 [trainer.py] => Time:136.2778091430664
800 800
800 800
2025-12-10 13:38:34,431 [trainer.py] => Time:2.7691566944122314
2025-12-10 13:38:34,431 [inflora.py] => Exemplar size: 0
2025-12-10 13:38:34,431 [trainer.py] => CNN: {'total': np.float64(85.5), '00-01': np.float64(99.5), '02-03': np.float64(91.0), '04-05': np.float64(83.5), '06-07': np.float64(68.0), 'old': np.float64(91.33), 'new': np.float64(68.0)}
2025-12-10 13:38:34,432 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5)]
2025-12-10 13:38:34,432 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5)]
2025-12-10 13:38:34,432 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855]
2025-12-10 13:38:35,173 [trainer.py] => All params: 125940251
2025-12-10 13:38:35,180 [trainer.py] => Trainable params: 2044438
2025-12-10 13:38:35,180 [inflora.py] => Learning on 8-10
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight', 'classifier_pool.41.bias', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'classifier_pool.41.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'classifier_pool.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'classifier_pool.43.bias', 'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'classifier_pool.49.bias', 'image_encoder.blocks.7.attn.lora_B_v.45.weight'}
2025-12-10 13:40:41,697 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.014, Train_accy 99.50
Threshold:  0.954
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 17/768 type remove
Layer 5 : 19/768 type remove
Layer 6 : 22/768 type remove
Layer 7 : 26/768 type remove
Layer 8 : 39/768 type remove
Layer 9 : 34/768 type remove
Layer 10 : 27/768 type remove
Layer 11 : 13/768 type remove
Layer 12 : 16/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:40:51,676 [trainer.py] => Time:136.49567246437073
1000 1000
1000 1000
2025-12-10 13:40:54,909 [trainer.py] => Time:3.2323811054229736
2025-12-10 13:40:54,909 [inflora.py] => Exemplar size: 0
2025-12-10 13:40:54,909 [trainer.py] => CNN: {'total': np.float64(91.0), '00-01': np.float64(99.5), '02-03': np.float64(90.5), '04-05': np.float64(91.5), '06-07': np.float64(75.5), '08-09': np.float64(98.0), 'old': np.float64(89.25), 'new': np.float64(98.0)}
2025-12-10 13:40:54,909 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0)]
2025-12-10 13:40:54,909 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6)]
2025-12-10 13:40:54,909 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91]
2025-12-10 13:40:55,643 [trainer.py] => All params: 125940251
2025-12-10 13:40:55,649 [trainer.py] => Trainable params: 2044438
2025-12-10 13:40:55,649 [inflora.py] => Learning on 10-12
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'classifier_pool.5.bias'}
2025-12-10 13:43:02,547 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.038, Train_accy 98.50
Threshold:  0.955
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 20/768 type remove
Layer 6 : 23/768 type remove
Layer 7 : 28/768 type remove
Layer 8 : 40/768 type remove
Layer 9 : 36/768 type remove
Layer 10 : 30/768 type remove
Layer 11 : 15/768 type remove
Layer 12 : 23/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:43:12,529 [trainer.py] => Time:136.88030409812927
1200 1200
1200 1200
2025-12-10 13:43:16,297 [trainer.py] => Time:3.767549514770508
2025-12-10 13:43:16,297 [inflora.py] => Exemplar size: 0
2025-12-10 13:43:16,298 [trainer.py] => CNN: {'total': np.float64(86.92), '00-01': np.float64(94.5), '02-03': np.float64(77.0), '04-05': np.float64(97.5), '06-07': np.float64(78.0), '08-09': np.float64(97.5), '10-11': np.float64(77.0), 'old': np.float64(88.9), 'new': np.float64(77.0)}
2025-12-10 13:43:16,298 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92)]
2025-12-10 13:43:16,298 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58)]
2025-12-10 13:43:16,298 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666]
2025-12-10 13:43:17,018 [trainer.py] => All params: 125940251
2025-12-10 13:43:17,024 [trainer.py] => Trainable params: 185858
2025-12-10 13:43:17,025 [inflora.py] => Learning on 12-14
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight'}
2025-12-10 13:45:23,684 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.050, Train_accy 97.70
Threshold:  0.956
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 20/768 type remove
Layer 5 : 24/768 type remove
Layer 6 : 26/768 type remove
Layer 7 : 31/768 type remove
Layer 8 : 43/768 type remove
Layer 9 : 41/768 type remove
Layer 10 : 36/768 type remove
Layer 11 : 18/768 type remove
Layer 12 : 25/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:45:34,111 [trainer.py] => Time:137.08677983283997
1400 1400
1400 1400
2025-12-10 13:45:38,373 [trainer.py] => Time:4.261615037918091
2025-12-10 13:45:38,374 [inflora.py] => Exemplar size: 0
2025-12-10 13:45:38,374 [trainer.py] => CNN: {'total': np.float64(88.71), '00-01': np.float64(96.0), '02-03': np.float64(81.0), '04-05': np.float64(94.0), '06-07': np.float64(76.0), '08-09': np.float64(96.5), '10-11': np.float64(79.5), '12-13': np.float64(98.0), 'old': np.float64(87.17), 'new': np.float64(98.0)}
2025-12-10 13:45:38,374 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71)]
2025-12-10 13:45:38,374 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57)]
2025-12-10 13:45:38,374 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429]
2025-12-10 13:45:39,086 [trainer.py] => All params: 125940251
2025-12-10 13:45:39,092 [trainer.py] => Trainable params: 185858
2025-12-10 13:45:39,093 [inflora.py] => Learning on 14-16
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight'}
2025-12-10 13:47:45,856 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.036, Train_accy 99.00
Threshold:  0.957
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 28/768 type remove
Layer 6 : 30/768 type remove
Layer 7 : 34/768 type remove
Layer 8 : 49/768 type remove
Layer 9 : 47/768 type remove
Layer 10 : 41/768 type remove
Layer 11 : 20/768 type remove
Layer 12 : 27/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:47:56,080 [trainer.py] => Time:136.98695039749146
1600 1600
1600 1600
2025-12-10 13:48:00,861 [trainer.py] => Time:4.781665086746216
2025-12-10 13:48:00,862 [inflora.py] => Exemplar size: 0
2025-12-10 13:48:00,862 [trainer.py] => CNN: {'total': np.float64(88.62), '00-01': np.float64(99.5), '02-03': np.float64(83.5), '04-05': np.float64(92.0), '06-07': np.float64(73.0), '08-09': np.float64(98.0), '10-11': np.float64(78.0), '12-13': np.float64(94.5), '14-15': np.float64(90.5), 'old': np.float64(88.36), 'new': np.float64(90.5)}
2025-12-10 13:48:00,862 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62)]
2025-12-10 13:48:00,862 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62)]
2025-12-10 13:48:00,862 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625]
2025-12-10 13:48:01,726 [trainer.py] => All params: 125940251
2025-12-10 13:48:01,732 [trainer.py] => Trainable params: 185858
2025-12-10 13:48:01,732 [inflora.py] => Learning on 16-18
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight'}
2025-12-10 13:50:08,582 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.018, Train_accy 99.40
Threshold:  0.958
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 22/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 33/768 type remove
Layer 7 : 38/768 type remove
Layer 8 : 54/768 type remove
Layer 9 : 52/768 type remove
Layer 10 : 44/768 type remove
Layer 11 : 23/768 type remove
Layer 12 : 30/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:50:18,753 [trainer.py] => Time:137.0207142829895
1800 1800
1800 1800
2025-12-10 13:50:24,064 [trainer.py] => Time:5.3108556270599365
2025-12-10 13:50:24,064 [inflora.py] => Exemplar size: 0
2025-12-10 13:50:24,064 [trainer.py] => CNN: {'total': np.float64(87.83), '00-01': np.float64(99.0), '02-03': np.float64(87.0), '04-05': np.float64(91.0), '06-07': np.float64(74.5), '08-09': np.float64(94.5), '10-11': np.float64(67.5), '12-13': np.float64(92.5), '14-15': np.float64(93.5), '16-17': np.float64(91.0), 'old': np.float64(87.44), 'new': np.float64(91.0)}
2025-12-10 13:50:24,065 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83)]
2025-12-10 13:50:24,065 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78)]
2025-12-10 13:50:24,065 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333]
2025-12-10 13:50:24,787 [trainer.py] => All params: 125940251
2025-12-10 13:50:24,794 [trainer.py] => Trainable params: 185858
2025-12-10 13:50:24,794 [inflora.py] => Learning on 18-20
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight'}
2025-12-10 13:52:31,674 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.025, Train_accy 99.00
Threshold:  0.959
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 31/768 type remove
Layer 6 : 36/768 type remove
Layer 7 : 42/768 type remove
Layer 8 : 61/768 type remove
Layer 9 : 60/768 type remove
Layer 10 : 52/768 type remove
Layer 11 : 26/768 type remove
Layer 12 : 32/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:52:41,674 [trainer.py] => Time:136.8795862197876
2000 2000
2000 2000
2025-12-10 13:52:47,534 [trainer.py] => Time:5.860520601272583
2025-12-10 13:52:47,535 [inflora.py] => Exemplar size: 0
2025-12-10 13:52:47,535 [trainer.py] => CNN: {'total': np.float64(87.15), '00-01': np.float64(99.5), '02-03': np.float64(87.5), '04-05': np.float64(87.5), '06-07': np.float64(70.5), '08-09': np.float64(97.0), '10-11': np.float64(68.5), '12-13': np.float64(90.5), '14-15': np.float64(91.0), '16-17': np.float64(89.0), '18-19': np.float64(90.5), 'old': np.float64(86.78), 'new': np.float64(90.5)}
2025-12-10 13:52:47,535 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15)]
2025-12-10 13:52:47,535 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75)]
2025-12-10 13:52:47,535 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715]
2025-12-10 13:52:49,835 [trainer.py] => All params: 125940251
2025-12-10 13:52:49,842 [trainer.py] => Trainable params: 185858
2025-12-10 13:52:49,842 [inflora.py] => Learning on 20-22
Parameters to be updated: {'classifier_pool.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight'}
2025-12-10 13:54:56,505 [inflora.py] => Task 10, Epoch 20/20 => Loss 0.011, Train_accy 99.60
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 23/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 33/768 type remove
Layer 6 : 39/768 type remove
Layer 7 : 46/768 type remove
Layer 8 : 65/768 type remove
Layer 9 : 66/768 type remove
Layer 10 : 56/768 type remove
Layer 11 : 30/768 type remove
Layer 12 : 34/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:55:06,730 [trainer.py] => Time:136.8881413936615
2200 2200
2200 2200
2025-12-10 13:55:13,165 [trainer.py] => Time:6.434467554092407
2025-12-10 13:55:13,165 [inflora.py] => Exemplar size: 0
2025-12-10 13:55:13,165 [trainer.py] => CNN: {'total': np.float64(85.41), '00-01': np.float64(98.0), '02-03': np.float64(81.5), '04-05': np.float64(86.5), '06-07': np.float64(66.5), '08-09': np.float64(93.5), '10-11': np.float64(64.5), '12-13': np.float64(90.0), '14-15': np.float64(89.0), '16-17': np.float64(94.5), '18-19': np.float64(88.0), '20-21': np.float64(87.5), 'old': np.float64(85.2), 'new': np.float64(87.5)}
2025-12-10 13:55:13,165 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41)]
2025-12-10 13:55:13,165 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73)]
2025-12-10 13:55:13,165 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091]
2025-12-10 13:55:18,267 [trainer.py] => All params: 125940251
2025-12-10 13:55:18,274 [trainer.py] => Trainable params: 185858
2025-12-10 13:55:18,274 [inflora.py] => Learning on 22-24
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight'}
2025-12-10 13:57:24,947 [inflora.py] => Task 11, Epoch 20/20 => Loss 0.014, Train_accy 99.50
Threshold:  0.961
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 34/768 type remove
Layer 6 : 41/768 type remove
Layer 7 : 50/768 type remove
Layer 8 : 70/768 type remove
Layer 9 : 70/768 type remove
Layer 10 : 59/768 type remove
Layer 11 : 32/768 type remove
Layer 12 : 36/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 13:57:35,049 [trainer.py] => Time:136.7752344608307
2400 2400
2400 2400
2025-12-10 13:57:41,941 [trainer.py] => Time:6.891244649887085
2025-12-10 13:57:41,941 [inflora.py] => Exemplar size: 0
2025-12-10 13:57:41,941 [trainer.py] => CNN: {'total': np.float64(84.58), '00-01': np.float64(98.0), '02-03': np.float64(77.5), '04-05': np.float64(90.0), '06-07': np.float64(64.5), '08-09': np.float64(91.5), '10-11': np.float64(65.5), '12-13': np.float64(88.5), '14-15': np.float64(85.5), '16-17': np.float64(91.5), '18-19': np.float64(85.5), '20-21': np.float64(85.0), '22-23': np.float64(92.0), 'old': np.float64(83.91), 'new': np.float64(92.0)}
2025-12-10 13:57:41,941 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58)]
2025-12-10 13:57:41,941 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71)]
2025-12-10 13:57:41,941 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333]
2025-12-10 13:57:45,336 [trainer.py] => All params: 125940251
2025-12-10 13:57:45,343 [trainer.py] => Trainable params: 185858
2025-12-10 13:57:45,343 [inflora.py] => Learning on 24-26
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight'}
2025-12-10 13:59:52,198 [inflora.py] => Task 12, Epoch 20/20 => Loss 0.033, Train_accy 98.70
Threshold:  0.962
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 25/768 type remove
Layer 4 : 27/768 type remove
Layer 5 : 36/768 type remove
Layer 6 : 43/768 type remove
Layer 7 : 52/768 type remove
Layer 8 : 73/768 type remove
Layer 9 : 73/768 type remove
Layer 10 : 62/768 type remove
Layer 11 : 34/768 type remove
Layer 12 : 38/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:00:02,151 [trainer.py] => Time:136.80728697776794
2600 2600
2600 2600
2025-12-10 14:00:09,611 [trainer.py] => Time:7.460017681121826
2025-12-10 14:00:09,611 [inflora.py] => Exemplar size: 0
2025-12-10 14:00:09,611 [trainer.py] => CNN: {'total': np.float64(83.08), '00-01': np.float64(97.0), '02-03': np.float64(79.5), '04-05': np.float64(88.5), '06-07': np.float64(57.5), '08-09': np.float64(90.5), '10-11': np.float64(68.0), '12-13': np.float64(86.5), '14-15': np.float64(86.5), '16-17': np.float64(89.5), '18-19': np.float64(87.5), '20-21': np.float64(85.0), '22-23': np.float64(92.0), '24-25': np.float64(72.0), 'old': np.float64(84.0), 'new': np.float64(72.0)}
2025-12-10 14:00:09,611 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08)]
2025-12-10 14:00:09,611 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77)]
2025-12-10 14:00:09,611 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308]
2025-12-10 14:00:16,001 [trainer.py] => All params: 125940251
2025-12-10 14:00:16,007 [trainer.py] => Trainable params: 185858
2025-12-10 14:00:16,008 [inflora.py] => Learning on 26-28
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight'}
2025-12-10 14:02:23,007 [inflora.py] => Task 13, Epoch 20/20 => Loss 0.063, Train_accy 97.60
Threshold:  0.963
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 26/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 39/768 type remove
Layer 6 : 46/768 type remove
Layer 7 : 54/768 type remove
Layer 8 : 78/768 type remove
Layer 9 : 78/768 type remove
Layer 10 : 65/768 type remove
Layer 11 : 36/768 type remove
Layer 12 : 40/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:02:33,103 [trainer.py] => Time:137.09540700912476
2800 2800
2800 2800
2025-12-10 14:02:41,029 [trainer.py] => Time:7.926149606704712
2025-12-10 14:02:41,030 [inflora.py] => Exemplar size: 0
2025-12-10 14:02:41,030 [trainer.py] => CNN: {'total': np.float64(82.25), '00-01': np.float64(98.0), '02-03': np.float64(78.5), '04-05': np.float64(88.0), '06-07': np.float64(52.5), '08-09': np.float64(91.5), '10-11': np.float64(66.5), '12-13': np.float64(81.0), '14-15': np.float64(88.0), '16-17': np.float64(91.5), '18-19': np.float64(88.5), '20-21': np.float64(85.5), '22-23': np.float64(87.0), '24-25': np.float64(73.5), '26-27': np.float64(81.5), 'old': np.float64(82.31), 'new': np.float64(81.5)}
2025-12-10 14:02:41,030 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25)]
2025-12-10 14:02:41,030 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71)]
2025-12-10 14:02:41,030 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225]
2025-12-10 14:02:46,260 [trainer.py] => All params: 125940251
2025-12-10 14:02:46,267 [trainer.py] => Trainable params: 185858
2025-12-10 14:02:46,267 [inflora.py] => Learning on 28-30
Parameters to be updated: {'classifier_pool.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight'}
2025-12-10 14:04:53,338 [inflora.py] => Task 14, Epoch 20/20 => Loss 0.038, Train_accy 98.60
Threshold:  0.964
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 29/768 type remove
Layer 5 : 40/768 type remove
Layer 6 : 47/768 type remove
Layer 7 : 56/768 type remove
Layer 8 : 80/768 type remove
Layer 9 : 81/768 type remove
Layer 10 : 69/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 42/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:05:03,342 [trainer.py] => Time:137.0751085281372
3000 3000
3000 3000
2025-12-10 14:05:11,846 [trainer.py] => Time:8.503044605255127
2025-12-10 14:05:11,846 [inflora.py] => Exemplar size: 0
2025-12-10 14:05:11,846 [trainer.py] => CNN: {'total': np.float64(82.33), '00-01': np.float64(98.0), '02-03': np.float64(79.0), '04-05': np.float64(84.0), '06-07': np.float64(54.5), '08-09': np.float64(95.5), '10-11': np.float64(62.5), '12-13': np.float64(85.5), '14-15': np.float64(87.5), '16-17': np.float64(90.5), '18-19': np.float64(90.5), '20-21': np.float64(85.5), '22-23': np.float64(87.5), '24-25': np.float64(70.0), '26-27': np.float64(79.5), '28-29': np.float64(85.0), 'old': np.float64(82.14), 'new': np.float64(85.0)}
2025-12-10 14:05:11,846 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33)]
2025-12-10 14:05:11,846 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73)]
2025-12-10 14:05:11,846 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334]
2025-12-10 14:05:14,992 [trainer.py] => All params: 125940251
2025-12-10 14:05:14,999 [trainer.py] => Trainable params: 185858
2025-12-10 14:05:14,999 [inflora.py] => Learning on 30-32
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight'}
2025-12-10 14:07:22,324 [inflora.py] => Task 15, Epoch 20/20 => Loss 0.007, Train_accy 99.80
Threshold:  0.965
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 41/768 type remove
Layer 6 : 49/768 type remove
Layer 7 : 59/768 type remove
Layer 8 : 83/768 type remove
Layer 9 : 84/768 type remove
Layer 10 : 72/768 type remove
Layer 11 : 41/768 type remove
Layer 12 : 44/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:07:32,593 [trainer.py] => Time:137.593683719635
3200 3200
3200 3200
2025-12-10 14:07:41,619 [trainer.py] => Time:9.026241540908813
2025-12-10 14:07:41,619 [inflora.py] => Exemplar size: 0
2025-12-10 14:07:41,619 [trainer.py] => CNN: {'total': np.float64(81.34), '00-01': np.float64(95.5), '02-03': np.float64(83.0), '04-05': np.float64(85.0), '06-07': np.float64(56.5), '08-09': np.float64(92.0), '10-11': np.float64(59.5), '12-13': np.float64(84.5), '14-15': np.float64(88.0), '16-17': np.float64(87.0), '18-19': np.float64(86.0), '20-21': np.float64(83.0), '22-23': np.float64(87.5), '24-25': np.float64(69.0), '26-27': np.float64(76.5), '28-29': np.float64(83.0), '30-31': np.float64(85.5), 'old': np.float64(81.07), 'new': np.float64(85.5)}
2025-12-10 14:07:41,620 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34)]
2025-12-10 14:07:41,620 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69)]
2025-12-10 14:07:41,620 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375]
2025-12-10 14:07:45,743 [trainer.py] => All params: 125940251
2025-12-10 14:07:45,749 [trainer.py] => Trainable params: 185858
2025-12-10 14:07:45,749 [inflora.py] => Learning on 32-34
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight'}
2025-12-10 14:09:52,774 [inflora.py] => Task 16, Epoch 20/20 => Loss 0.048, Train_accy 97.80
Threshold:  0.966
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 31/768 type remove
Layer 5 : 42/768 type remove
Layer 6 : 53/768 type remove
Layer 7 : 64/768 type remove
Layer 8 : 90/768 type remove
Layer 9 : 92/768 type remove
Layer 10 : 78/768 type remove
Layer 11 : 43/768 type remove
Layer 12 : 47/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:10:02,678 [trainer.py] => Time:136.92863249778748
3400 3400
3400 3400
2025-12-10 14:10:12,161 [trainer.py] => Time:9.48280119895935
2025-12-10 14:10:12,161 [inflora.py] => Exemplar size: 0
2025-12-10 14:10:12,161 [trainer.py] => CNN: {'total': np.float64(79.68), '00-01': np.float64(92.0), '02-03': np.float64(78.0), '04-05': np.float64(87.5), '06-07': np.float64(48.5), '08-09': np.float64(87.5), '10-11': np.float64(59.0), '12-13': np.float64(82.0), '14-15': np.float64(83.0), '16-17': np.float64(81.5), '18-19': np.float64(85.0), '20-21': np.float64(83.0), '22-23': np.float64(84.5), '24-25': np.float64(66.0), '26-27': np.float64(77.0), '28-29': np.float64(83.0), '30-31': np.float64(89.0), '32-33': np.float64(88.0), 'old': np.float64(79.16), 'new': np.float64(88.0)}
2025-12-10 14:10:12,162 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68)]
2025-12-10 14:10:12,162 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56)]
2025-12-10 14:10:12,162 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529]
2025-12-10 14:10:19,923 [trainer.py] => All params: 125940251
2025-12-10 14:10:19,930 [trainer.py] => Trainable params: 185858
2025-12-10 14:10:19,930 [inflora.py] => Learning on 34-36
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'classifier_pool.17.weight'}
2025-12-10 14:12:27,045 [inflora.py] => Task 17, Epoch 20/20 => Loss 0.027, Train_accy 98.70
Threshold:  0.967
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 30/768 type remove
Layer 4 : 33/768 type remove
Layer 5 : 45/768 type remove
Layer 6 : 58/768 type remove
Layer 7 : 70/768 type remove
Layer 8 : 98/768 type remove
Layer 9 : 103/768 type remove
Layer 10 : 88/768 type remove
Layer 11 : 48/768 type remove
Layer 12 : 49/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:12:37,047 [trainer.py] => Time:137.116858959198
3600 3600
3600 3600
2025-12-10 14:12:47,096 [trainer.py] => Time:10.048686742782593
2025-12-10 14:12:47,096 [inflora.py] => Exemplar size: 0
2025-12-10 14:12:47,096 [trainer.py] => CNN: {'total': np.float64(76.92), '00-01': np.float64(94.5), '02-03': np.float64(78.0), '04-05': np.float64(89.5), '06-07': np.float64(48.0), '08-09': np.float64(88.0), '10-11': np.float64(50.5), '12-13': np.float64(79.0), '14-15': np.float64(80.5), '16-17': np.float64(82.0), '18-19': np.float64(82.5), '20-21': np.float64(82.5), '22-23': np.float64(84.0), '24-25': np.float64(65.5), '26-27': np.float64(69.5), '28-29': np.float64(83.5), '30-31': np.float64(85.5), '32-33': np.float64(86.0), '34-35': np.float64(55.5), 'old': np.float64(78.18), 'new': np.float64(55.5)}
2025-12-10 14:12:47,096 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92)]
2025-12-10 14:12:47,096 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64)]
2025-12-10 14:12:47,096 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667]
2025-12-10 14:12:50,317 [trainer.py] => All params: 125940251
2025-12-10 14:12:50,324 [trainer.py] => Trainable params: 185858
2025-12-10 14:12:50,324 [inflora.py] => Learning on 36-38
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight'}
2025-12-10 14:14:57,672 [inflora.py] => Task 18, Epoch 20/20 => Loss 0.015, Train_accy 99.40
Threshold:  0.968
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 31/768 type remove
Layer 4 : 34/768 type remove
Layer 5 : 46/768 type remove
Layer 6 : 59/768 type remove
Layer 7 : 73/768 type remove
Layer 8 : 104/768 type remove
Layer 9 : 113/768 type remove
Layer 10 : 96/768 type remove
Layer 11 : 52/768 type remove
Layer 12 : 53/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:15:07,784 [trainer.py] => Time:137.45975852012634
3800 3800
3800 3800
2025-12-10 14:15:18,245 [trainer.py] => Time:10.460428476333618
2025-12-10 14:15:18,245 [inflora.py] => Exemplar size: 0
2025-12-10 14:15:18,245 [trainer.py] => CNN: {'total': np.float64(75.68), '00-01': np.float64(92.0), '02-03': np.float64(77.0), '04-05': np.float64(88.0), '06-07': np.float64(45.0), '08-09': np.float64(81.5), '10-11': np.float64(47.0), '12-13': np.float64(76.5), '14-15': np.float64(78.0), '16-17': np.float64(78.5), '18-19': np.float64(83.5), '20-21': np.float64(84.0), '22-23': np.float64(76.5), '24-25': np.float64(63.0), '26-27': np.float64(70.0), '28-29': np.float64(81.5), '30-31': np.float64(86.5), '32-33': np.float64(86.0), '34-35': np.float64(57.0), '36-37': np.float64(86.5), 'old': np.float64(75.08), 'new': np.float64(86.5)}
2025-12-10 14:15:18,245 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68)]
2025-12-10 14:15:18,245 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66)]
2025-12-10 14:15:18,245 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579]
2025-12-10 14:15:27,302 [trainer.py] => All params: 125940251
2025-12-10 14:15:27,308 [trainer.py] => Trainable params: 185858
2025-12-10 14:15:27,309 [inflora.py] => Learning on 38-40
Parameters to be updated: {'classifier_pool.19.bias', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight'}
2025-12-10 14:17:34,568 [inflora.py] => Task 19, Epoch 20/20 => Loss 0.010, Train_accy 99.70
Threshold:  0.969
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 32/768 type remove
Layer 4 : 36/768 type remove
Layer 5 : 48/768 type remove
Layer 6 : 64/768 type remove
Layer 7 : 80/768 type remove
Layer 8 : 118/768 type remove
Layer 9 : 127/768 type remove
Layer 10 : 105/768 type remove
Layer 11 : 56/768 type remove
Layer 12 : 56/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:17:44,609 [trainer.py] => Time:137.29994010925293
4000 4000
4000 4000
2025-12-10 14:17:55,777 [trainer.py] => Time:11.168036937713623
2025-12-10 14:17:55,777 [inflora.py] => Exemplar size: 0
2025-12-10 14:17:55,777 [trainer.py] => CNN: {'total': np.float64(75.4), '00-01': np.float64(94.0), '02-03': np.float64(78.5), '04-05': np.float64(86.5), '06-07': np.float64(42.5), '08-09': np.float64(87.0), '10-11': np.float64(53.5), '12-13': np.float64(78.5), '14-15': np.float64(82.5), '16-17': np.float64(81.5), '18-19': np.float64(76.5), '20-21': np.float64(81.0), '22-23': np.float64(71.5), '24-25': np.float64(63.5), '26-27': np.float64(68.5), '28-29': np.float64(81.0), '30-31': np.float64(76.5), '32-33': np.float64(80.5), '34-35': np.float64(56.5), '36-37': np.float64(83.5), '38-39': np.float64(84.5), 'old': np.float64(74.92), 'new': np.float64(84.5)}
2025-12-10 14:17:55,777 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4)]
2025-12-10 14:17:55,777 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7)]
2025-12-10 14:17:55,777 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754]
2025-12-10 14:17:59,180 [trainer.py] => All params: 125940251
2025-12-10 14:17:59,186 [trainer.py] => Trainable params: 185858
2025-12-10 14:17:59,186 [inflora.py] => Learning on 40-42
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight'}
2025-12-10 14:20:05,923 [inflora.py] => Task 20, Epoch 20/20 => Loss 0.027, Train_accy 99.10
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 33/768 type remove
Layer 4 : 37/768 type remove
Layer 5 : 50/768 type remove
Layer 6 : 66/768 type remove
Layer 7 : 82/768 type remove
Layer 8 : 121/768 type remove
Layer 9 : 130/768 type remove
Layer 10 : 109/768 type remove
Layer 11 : 59/768 type remove
Layer 12 : 59/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:20:16,260 [trainer.py] => Time:137.07350969314575
4200 4200
4200 4200
2025-12-10 14:20:27,878 [trainer.py] => Time:11.617595434188843
2025-12-10 14:20:27,878 [inflora.py] => Exemplar size: 0
2025-12-10 14:20:27,878 [trainer.py] => CNN: {'total': np.float64(75.9), '00-01': np.float64(94.5), '02-03': np.float64(81.0), '04-05': np.float64(88.5), '06-07': np.float64(52.0), '08-09': np.float64(87.0), '10-11': np.float64(56.0), '12-13': np.float64(81.0), '14-15': np.float64(84.0), '16-17': np.float64(80.5), '18-19': np.float64(76.0), '20-21': np.float64(78.0), '22-23': np.float64(77.0), '24-25': np.float64(58.5), '26-27': np.float64(67.0), '28-29': np.float64(78.5), '30-31': np.float64(75.5), '32-33': np.float64(79.0), '34-35': np.float64(52.5), '36-37': np.float64(77.0), '38-39': np.float64(85.0), '40-41': np.float64(85.5), 'old': np.float64(75.42), 'new': np.float64(85.5)}
2025-12-10 14:20:27,878 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9)]
2025-12-10 14:20:27,878 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69)]
2025-12-10 14:20:27,878 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191]
2025-12-10 14:20:34,251 [trainer.py] => All params: 125940251
2025-12-10 14:20:34,258 [trainer.py] => Trainable params: 185858
2025-12-10 14:20:34,258 [inflora.py] => Learning on 42-44
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight'}
2025-12-10 14:22:41,418 [inflora.py] => Task 21, Epoch 20/20 => Loss 0.067, Train_accy 97.20
Threshold:  0.971
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 35/768 type remove
Layer 4 : 40/768 type remove
Layer 5 : 54/768 type remove
Layer 6 : 70/768 type remove
Layer 7 : 85/768 type remove
Layer 8 : 124/768 type remove
Layer 9 : 133/768 type remove
Layer 10 : 112/768 type remove
Layer 11 : 62/768 type remove
Layer 12 : 61/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:22:51,693 [trainer.py] => Time:137.43465662002563
4400 4400
4400 4400
2025-12-10 14:23:03,784 [trainer.py] => Time:12.091088771820068
2025-12-10 14:23:03,784 [inflora.py] => Exemplar size: 0
2025-12-10 14:23:03,784 [trainer.py] => CNN: {'total': np.float64(75.41), '00-01': np.float64(95.0), '02-03': np.float64(83.0), '04-05': np.float64(87.0), '06-07': np.float64(54.5), '08-09': np.float64(90.0), '10-11': np.float64(56.5), '12-13': np.float64(80.0), '14-15': np.float64(85.5), '16-17': np.float64(82.0), '18-19': np.float64(82.5), '20-21': np.float64(81.5), '22-23': np.float64(78.5), '24-25': np.float64(65.0), '26-27': np.float64(70.5), '28-29': np.float64(77.5), '30-31': np.float64(72.5), '32-33': np.float64(73.5), '34-35': np.float64(51.5), '36-37': np.float64(72.0), '38-39': np.float64(85.5), '40-41': np.float64(85.0), '42-43': np.float64(50.0), 'old': np.float64(76.62), 'new': np.float64(50.0)}
2025-12-10 14:23:03,785 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41)]
2025-12-10 14:23:03,785 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59)]
2025-12-10 14:23:03,785 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091]
2025-12-10 14:23:08,416 [trainer.py] => All params: 125940251
2025-12-10 14:23:08,423 [trainer.py] => Trainable params: 185858
2025-12-10 14:23:08,424 [inflora.py] => Learning on 44-46
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight'}
2025-12-10 14:25:15,751 [inflora.py] => Task 22, Epoch 20/20 => Loss 0.055, Train_accy 98.00
Threshold:  0.972
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 36/768 type remove
Layer 4 : 41/768 type remove
Layer 5 : 55/768 type remove
Layer 6 : 71/768 type remove
Layer 7 : 87/768 type remove
Layer 8 : 129/768 type remove
Layer 9 : 136/768 type remove
Layer 10 : 117/768 type remove
Layer 11 : 64/768 type remove
Layer 12 : 64/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:25:25,833 [trainer.py] => Time:137.4093577861786
4600 4600
4600 4600
2025-12-10 14:25:38,531 [trainer.py] => Time:12.697608232498169
2025-12-10 14:25:38,532 [inflora.py] => Exemplar size: 0
2025-12-10 14:25:38,532 [trainer.py] => CNN: {'total': np.float64(73.41), '00-01': np.float64(95.0), '02-03': np.float64(84.0), '04-05': np.float64(85.5), '06-07': np.float64(52.5), '08-09': np.float64(90.5), '10-11': np.float64(61.0), '12-13': np.float64(87.0), '14-15': np.float64(86.5), '16-17': np.float64(76.0), '18-19': np.float64(79.5), '20-21': np.float64(76.0), '22-23': np.float64(80.5), '24-25': np.float64(59.0), '26-27': np.float64(74.0), '28-29': np.float64(74.5), '30-31': np.float64(65.0), '32-33': np.float64(75.0), '34-35': np.float64(50.0), '36-37': np.float64(65.0), '38-39': np.float64(85.0), '40-41': np.float64(87.0), '42-43': np.float64(41.5), '44-45': np.float64(58.5), 'old': np.float64(74.09), 'new': np.float64(58.5)}
2025-12-10 14:25:38,532 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41)]
2025-12-10 14:25:38,532 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57)]
2025-12-10 14:25:38,532 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086]
2025-12-10 14:25:44,000 [trainer.py] => All params: 125940251
2025-12-10 14:25:44,006 [trainer.py] => Trainable params: 185858
2025-12-10 14:25:44,006 [inflora.py] => Learning on 46-48
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight'}
2025-12-10 14:27:51,454 [inflora.py] => Task 23, Epoch 20/20 => Loss 0.023, Train_accy 99.20
Threshold:  0.973
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 37/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 56/768 type remove
Layer 6 : 73/768 type remove
Layer 7 : 90/768 type remove
Layer 8 : 132/768 type remove
Layer 9 : 139/768 type remove
Layer 10 : 120/768 type remove
Layer 11 : 66/768 type remove
Layer 12 : 67/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:28:01,802 [trainer.py] => Time:137.79572439193726
4800 4800
4800 4800
2025-12-10 14:28:14,983 [trainer.py] => Time:13.180698871612549
2025-12-10 14:28:14,983 [inflora.py] => Exemplar size: 0
2025-12-10 14:28:14,983 [trainer.py] => CNN: {'total': np.float64(72.56), '00-01': np.float64(85.0), '02-03': np.float64(72.5), '04-05': np.float64(87.5), '06-07': np.float64(55.0), '08-09': np.float64(88.5), '10-11': np.float64(65.0), '12-13': np.float64(88.5), '14-15': np.float64(86.0), '16-17': np.float64(71.0), '18-19': np.float64(82.5), '20-21': np.float64(74.0), '22-23': np.float64(85.5), '24-25': np.float64(62.5), '26-27': np.float64(74.0), '28-29': np.float64(78.0), '30-31': np.float64(66.5), '32-33': np.float64(71.5), '34-35': np.float64(48.5), '36-37': np.float64(62.0), '38-39': np.float64(83.0), '40-41': np.float64(87.5), '42-43': np.float64(42.5), '44-45': np.float64(58.5), '46-47': np.float64(66.0), 'old': np.float64(72.85), 'new': np.float64(66.0)}
2025-12-10 14:28:14,983 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56)]
2025-12-10 14:28:14,983 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71)]
2025-12-10 14:28:14,984 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625]
2025-12-10 14:28:17,311 [trainer.py] => All params: 125940251
2025-12-10 14:28:17,318 [trainer.py] => Trainable params: 185858
2025-12-10 14:28:17,318 [inflora.py] => Learning on 48-50
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight'}
2025-12-10 14:30:24,697 [inflora.py] => Task 24, Epoch 20/20 => Loss 0.031, Train_accy 98.90
Threshold:  0.974
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 38/768 type remove
Layer 4 : 46/768 type remove
Layer 5 : 59/768 type remove
Layer 6 : 77/768 type remove
Layer 7 : 93/768 type remove
Layer 8 : 136/768 type remove
Layer 9 : 142/768 type remove
Layer 10 : 123/768 type remove
Layer 11 : 68/768 type remove
Layer 12 : 69/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:30:34,921 [trainer.py] => Time:137.60282278060913
5000 5000
5000 5000
2025-12-10 14:30:48,627 [trainer.py] => Time:13.705641031265259
2025-12-10 14:30:48,627 [inflora.py] => Exemplar size: 0
2025-12-10 14:30:48,627 [trainer.py] => CNN: {'total': np.float64(72.58), '00-01': np.float64(90.5), '02-03': np.float64(80.5), '04-05': np.float64(83.5), '06-07': np.float64(48.0), '08-09': np.float64(88.0), '10-11': np.float64(54.0), '12-13': np.float64(83.0), '14-15': np.float64(85.0), '16-17': np.float64(74.0), '18-19': np.float64(77.0), '20-21': np.float64(77.0), '22-23': np.float64(68.5), '24-25': np.float64(67.5), '26-27': np.float64(75.5), '28-29': np.float64(74.5), '30-31': np.float64(72.5), '32-33': np.float64(71.0), '34-35': np.float64(53.0), '36-37': np.float64(63.5), '38-39': np.float64(86.5), '40-41': np.float64(84.0), '42-43': np.float64(43.5), '44-45': np.float64(55.0), '46-47': np.float64(66.0), '48-49': np.float64(93.0), 'old': np.float64(71.73), 'new': np.float64(93.0)}
2025-12-10 14:30:48,627 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58)]
2025-12-10 14:30:48,627 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6)]
2025-12-10 14:30:48,627 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258]
2025-12-10 14:30:53,447 [trainer.py] => All params: 125940251
2025-12-10 14:30:53,454 [trainer.py] => Trainable params: 185858
2025-12-10 14:30:53,454 [inflora.py] => Learning on 50-52
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'classifier_pool.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight'}
2025-12-10 14:33:00,748 [inflora.py] => Task 25, Epoch 20/20 => Loss 0.049, Train_accy 98.50
Threshold:  0.975
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 39/768 type remove
Layer 4 : 48/768 type remove
Layer 5 : 61/768 type remove
Layer 6 : 82/768 type remove
Layer 7 : 96/768 type remove
Layer 8 : 140/768 type remove
Layer 9 : 145/768 type remove
Layer 10 : 129/768 type remove
Layer 11 : 71/768 type remove
Layer 12 : 72/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:33:10,725 [trainer.py] => Time:137.27065348625183
5200 5200
5200 5200
2025-12-10 14:33:24,895 [trainer.py] => Time:14.170310974121094
2025-12-10 14:33:24,895 [inflora.py] => Exemplar size: 0
2025-12-10 14:33:24,896 [trainer.py] => CNN: {'total': np.float64(72.67), '00-01': np.float64(95.0), '02-03': np.float64(82.0), '04-05': np.float64(78.5), '06-07': np.float64(48.5), '08-09': np.float64(92.0), '10-11': np.float64(52.0), '12-13': np.float64(81.0), '14-15': np.float64(85.0), '16-17': np.float64(79.0), '18-19': np.float64(78.5), '20-21': np.float64(79.0), '22-23': np.float64(62.0), '24-25': np.float64(67.5), '26-27': np.float64(68.0), '28-29': np.float64(74.0), '30-31': np.float64(70.0), '32-33': np.float64(70.5), '34-35': np.float64(52.0), '36-37': np.float64(65.0), '38-39': np.float64(87.0), '40-41': np.float64(84.0), '42-43': np.float64(45.0), '44-45': np.float64(55.0), '46-47': np.float64(63.5), '48-49': np.float64(91.0), '50-51': np.float64(84.5), 'old': np.float64(72.2), 'new': np.float64(84.5)}
2025-12-10 14:33:24,896 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67)]
2025-12-10 14:33:24,896 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54)]
2025-12-10 14:33:24,896 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692]
2025-12-10 14:33:31,923 [trainer.py] => All params: 125940251
2025-12-10 14:33:31,930 [trainer.py] => Trainable params: 185858
2025-12-10 14:33:31,930 [inflora.py] => Learning on 52-54
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight'}
2025-12-10 14:35:39,818 [inflora.py] => Task 26, Epoch 20/20 => Loss 0.004, Train_accy 99.80
Threshold:  0.976
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 40/768 type remove
Layer 4 : 49/768 type remove
Layer 5 : 65/768 type remove
Layer 6 : 87/768 type remove
Layer 7 : 103/768 type remove
Layer 8 : 148/768 type remove
Layer 9 : 150/768 type remove
Layer 10 : 132/768 type remove
Layer 11 : 74/768 type remove
Layer 12 : 75/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:35:49,888 [trainer.py] => Time:137.9570655822754
5400 5400
5400 5400
2025-12-10 14:36:04,733 [trainer.py] => Time:14.845344305038452
2025-12-10 14:36:04,733 [inflora.py] => Exemplar size: 0
2025-12-10 14:36:04,733 [trainer.py] => CNN: {'total': np.float64(72.09), '00-01': np.float64(92.5), '02-03': np.float64(80.5), '04-05': np.float64(78.0), '06-07': np.float64(49.0), '08-09': np.float64(91.0), '10-11': np.float64(50.5), '12-13': np.float64(82.0), '14-15': np.float64(85.5), '16-17': np.float64(70.0), '18-19': np.float64(77.0), '20-21': np.float64(78.0), '22-23': np.float64(73.0), '24-25': np.float64(62.5), '26-27': np.float64(72.5), '28-29': np.float64(75.0), '30-31': np.float64(69.5), '32-33': np.float64(67.5), '34-35': np.float64(48.5), '36-37': np.float64(66.5), '38-39': np.float64(87.0), '40-41': np.float64(83.0), '42-43': np.float64(43.5), '44-45': np.float64(56.5), '46-47': np.float64(44.5), '48-49': np.float64(89.5), '50-51': np.float64(82.0), '52-53': np.float64(91.5), 'old': np.float64(71.35), 'new': np.float64(91.5)}
2025-12-10 14:36:04,734 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09)]
2025-12-10 14:36:04,734 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56)]
2025-12-10 14:36:04,734 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926]
2025-12-10 14:36:10,829 [trainer.py] => All params: 125940251
2025-12-10 14:36:10,835 [trainer.py] => Trainable params: 185858
2025-12-10 14:36:10,836 [inflora.py] => Learning on 54-56
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight'}
2025-12-10 14:38:18,359 [inflora.py] => Task 27, Epoch 20/20 => Loss 0.011, Train_accy 99.60
Threshold:  0.977
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 41/768 type remove
Layer 4 : 50/768 type remove
Layer 5 : 68/768 type remove
Layer 6 : 91/768 type remove
Layer 7 : 108/768 type remove
Layer 8 : 157/768 type remove
Layer 9 : 162/768 type remove
Layer 10 : 143/768 type remove
Layer 11 : 79/768 type remove
Layer 12 : 79/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:38:28,759 [trainer.py] => Time:137.92343759536743
5600 5600
5600 5600
2025-12-10 14:38:44,137 [trainer.py] => Time:15.37737226486206
2025-12-10 14:38:44,137 [inflora.py] => Exemplar size: 0
2025-12-10 14:38:44,137 [trainer.py] => CNN: {'total': np.float64(71.59), '00-01': np.float64(94.0), '02-03': np.float64(82.5), '04-05': np.float64(74.0), '06-07': np.float64(48.0), '08-09': np.float64(93.5), '10-11': np.float64(50.0), '12-13': np.float64(81.0), '14-15': np.float64(87.0), '16-17': np.float64(71.5), '18-19': np.float64(76.5), '20-21': np.float64(79.0), '22-23': np.float64(70.5), '24-25': np.float64(62.0), '26-27': np.float64(73.0), '28-29': np.float64(74.5), '30-31': np.float64(65.0), '32-33': np.float64(66.0), '34-35': np.float64(50.0), '36-37': np.float64(65.5), '38-39': np.float64(82.0), '40-41': np.float64(86.5), '42-43': np.float64(37.0), '44-45': np.float64(58.0), '46-47': np.float64(46.0), '48-49': np.float64(80.5), '50-51': np.float64(85.0), '52-53': np.float64(90.5), '54-55': np.float64(75.5), 'old': np.float64(71.44), 'new': np.float64(75.5)}
2025-12-10 14:38:44,137 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59)]
2025-12-10 14:38:44,137 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54)]
2025-12-10 14:38:44,137 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571]
2025-12-10 14:38:45,571 [trainer.py] => All params: 125940251
2025-12-10 14:38:45,579 [trainer.py] => Trainable params: 185858
2025-12-10 14:38:45,579 [inflora.py] => Learning on 56-58
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight'}
2025-12-10 14:40:53,225 [inflora.py] => Task 28, Epoch 20/20 => Loss 0.019, Train_accy 99.20
Threshold:  0.978
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 42/768 type remove
Layer 4 : 52/768 type remove
Layer 5 : 70/768 type remove
Layer 6 : 94/768 type remove
Layer 7 : 113/768 type remove
Layer 8 : 167/768 type remove
Layer 9 : 171/768 type remove
Layer 10 : 150/768 type remove
Layer 11 : 82/768 type remove
Layer 12 : 83/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:41:03,286 [trainer.py] => Time:137.70751667022705
5800 5800
5800 5800
2025-12-10 14:41:19,226 [trainer.py] => Time:15.939399480819702
2025-12-10 14:41:19,226 [inflora.py] => Exemplar size: 0
2025-12-10 14:41:19,226 [trainer.py] => CNN: {'total': np.float64(71.84), '00-01': np.float64(88.5), '02-03': np.float64(79.5), '04-05': np.float64(75.5), '06-07': np.float64(46.5), '08-09': np.float64(86.0), '10-11': np.float64(45.5), '12-13': np.float64(78.5), '14-15': np.float64(86.5), '16-17': np.float64(72.0), '18-19': np.float64(76.0), '20-21': np.float64(83.0), '22-23': np.float64(72.0), '24-25': np.float64(66.5), '26-27': np.float64(72.0), '28-29': np.float64(76.0), '30-31': np.float64(69.5), '32-33': np.float64(66.5), '34-35': np.float64(51.0), '36-37': np.float64(66.5), '38-39': np.float64(85.5), '40-41': np.float64(82.5), '42-43': np.float64(39.0), '44-45': np.float64(57.5), '46-47': np.float64(47.5), '48-49': np.float64(87.0), '50-51': np.float64(83.0), '52-53': np.float64(90.5), '54-55': np.float64(74.0), '56-57': np.float64(79.5), 'old': np.float64(71.57), 'new': np.float64(79.5)}
2025-12-10 14:41:19,226 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84)]
2025-12-10 14:41:19,226 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59)]
2025-12-10 14:41:19,227 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689]
2025-12-10 14:41:22,603 [trainer.py] => All params: 125940251
2025-12-10 14:41:22,609 [trainer.py] => Trainable params: 185858
2025-12-10 14:41:22,610 [inflora.py] => Learning on 58-60
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'classifier_pool.29.bias', 'classifier_pool.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight'}
2025-12-10 14:43:30,609 [inflora.py] => Task 29, Epoch 20/20 => Loss 0.019, Train_accy 99.40
Threshold:  0.979
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 23/768 type remove
Layer 3 : 44/768 type remove
Layer 4 : 55/768 type remove
Layer 5 : 74/768 type remove
Layer 6 : 99/768 type remove
Layer 7 : 117/768 type remove
Layer 8 : 171/768 type remove
Layer 9 : 174/768 type remove
Layer 10 : 153/768 type remove
Layer 11 : 85/768 type remove
Layer 12 : 86/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:43:40,981 [trainer.py] => Time:138.37172031402588
6000 6000
6000 6000
2025-12-10 14:43:57,371 [trainer.py] => Time:16.389040231704712
2025-12-10 14:43:57,371 [inflora.py] => Exemplar size: 0
2025-12-10 14:43:57,371 [trainer.py] => CNN: {'total': np.float64(70.3), '00-01': np.float64(85.0), '02-03': np.float64(77.5), '04-05': np.float64(85.0), '06-07': np.float64(47.0), '08-09': np.float64(89.0), '10-11': np.float64(49.0), '12-13': np.float64(57.5), '14-15': np.float64(82.5), '16-17': np.float64(76.0), '18-19': np.float64(72.5), '20-21': np.float64(79.0), '22-23': np.float64(64.5), '24-25': np.float64(67.5), '26-27': np.float64(76.0), '28-29': np.float64(78.5), '30-31': np.float64(71.5), '32-33': np.float64(57.5), '34-35': np.float64(53.0), '36-37': np.float64(63.5), '38-39': np.float64(83.5), '40-41': np.float64(82.0), '42-43': np.float64(38.5), '44-45': np.float64(54.5), '46-47': np.float64(43.5), '48-49': np.float64(82.0), '50-51': np.float64(78.5), '52-53': np.float64(82.5), '54-55': np.float64(72.0), '56-57': np.float64(76.5), '58-59': np.float64(83.5), 'old': np.float64(69.84), 'new': np.float64(83.5)}
2025-12-10 14:43:57,371 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3)]
2025-12-10 14:43:57,371 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52)]
2025-12-10 14:43:57,371 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703]
2025-12-10 14:44:01,603 [trainer.py] => All params: 125940251
2025-12-10 14:44:01,610 [trainer.py] => Trainable params: 185858
2025-12-10 14:44:01,610 [inflora.py] => Learning on 60-62
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.30.weight', 'image_encoder.blocks.10.attn.lora_B_v.30.weight', 'image_encoder.blocks.0.attn.lora_B_k.30.weight', 'classifier_pool.30.bias', 'image_encoder.blocks.5.attn.lora_B_v.30.weight', 'image_encoder.blocks.2.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_k.30.weight', 'image_encoder.blocks.1.attn.lora_B_v.30.weight', 'image_encoder.blocks.8.attn.lora_B_v.30.weight', 'image_encoder.blocks.6.attn.lora_B_v.30.weight', 'image_encoder.blocks.11.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_k.30.weight', 'image_encoder.blocks.1.attn.lora_B_k.30.weight', 'image_encoder.blocks.6.attn.lora_B_k.30.weight', 'image_encoder.blocks.0.attn.lora_B_v.30.weight', 'image_encoder.blocks.5.attn.lora_B_k.30.weight', 'image_encoder.blocks.7.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_k.30.weight', 'classifier_pool.30.weight', 'image_encoder.blocks.11.attn.lora_B_v.30.weight', 'image_encoder.blocks.2.attn.lora_B_k.30.weight', 'image_encoder.blocks.3.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_k.30.weight', 'image_encoder.blocks.9.attn.lora_B_v.30.weight', 'image_encoder.blocks.4.attn.lora_B_v.30.weight', 'image_encoder.blocks.7.attn.lora_B_v.30.weight'}
2025-12-10 14:46:09,919 [inflora.py] => Task 30, Epoch 20/20 => Loss 0.008, Train_accy 99.70
Threshold:  0.98
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 45/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 77/768 type remove
Layer 6 : 103/768 type remove
Layer 7 : 124/768 type remove
Layer 8 : 177/768 type remove
Layer 9 : 180/768 type remove
Layer 10 : 159/768 type remove
Layer 11 : 89/768 type remove
Layer 12 : 90/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:46:19,962 [trainer.py] => Time:138.3519434928894
6200 6200
6200 6200
2025-12-10 14:46:36,898 [trainer.py] => Time:16.935214042663574
2025-12-10 14:46:36,898 [inflora.py] => Exemplar size: 0
2025-12-10 14:46:36,898 [trainer.py] => CNN: {'total': np.float64(68.77), '00-01': np.float64(91.0), '02-03': np.float64(77.5), '04-05': np.float64(85.5), '06-07': np.float64(48.5), '08-09': np.float64(85.5), '10-11': np.float64(32.0), '12-13': np.float64(57.5), '14-15': np.float64(84.5), '16-17': np.float64(63.0), '18-19': np.float64(71.5), '20-21': np.float64(83.0), '22-23': np.float64(54.0), '24-25': np.float64(63.0), '26-27': np.float64(71.0), '28-29': np.float64(76.0), '30-31': np.float64(68.5), '32-33': np.float64(57.0), '34-35': np.float64(50.5), '36-37': np.float64(64.5), '38-39': np.float64(86.0), '40-41': np.float64(81.5), '42-43': np.float64(33.5), '44-45': np.float64(53.0), '46-47': np.float64(35.0), '48-49': np.float64(74.0), '50-51': np.float64(77.5), '52-53': np.float64(83.0), '54-55': np.float64(70.0), '56-57': np.float64(76.5), '58-59': np.float64(85.0), '60-61': np.float64(93.0), 'old': np.float64(67.97), 'new': np.float64(93.0)}
2025-12-10 14:46:36,898 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77)]
2025-12-10 14:46:36,898 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5)]
2025-12-10 14:46:36,898 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871]
2025-12-10 14:46:46,063 [trainer.py] => All params: 125940251
2025-12-10 14:46:46,069 [trainer.py] => Trainable params: 185858
2025-12-10 14:46:46,069 [inflora.py] => Learning on 62-64
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.31.weight', 'image_encoder.blocks.3.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_v.31.weight', 'image_encoder.blocks.4.attn.lora_B_v.31.weight', 'classifier_pool.31.weight', 'image_encoder.blocks.11.attn.lora_B_k.31.weight', 'image_encoder.blocks.10.attn.lora_B_k.31.weight', 'image_encoder.blocks.5.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_v.31.weight', 'image_encoder.blocks.1.attn.lora_B_k.31.weight', 'image_encoder.blocks.9.attn.lora_B_v.31.weight', 'image_encoder.blocks.5.attn.lora_B_v.31.weight', 'image_encoder.blocks.2.attn.lora_B_k.31.weight', 'image_encoder.blocks.9.attn.lora_B_k.31.weight', 'image_encoder.blocks.7.attn.lora_B_k.31.weight', 'image_encoder.blocks.8.attn.lora_B_k.31.weight', 'image_encoder.blocks.11.attn.lora_B_v.31.weight', 'classifier_pool.31.bias', 'image_encoder.blocks.8.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_v.31.weight', 'image_encoder.blocks.6.attn.lora_B_k.31.weight', 'image_encoder.blocks.10.attn.lora_B_v.31.weight', 'image_encoder.blocks.4.attn.lora_B_k.31.weight', 'image_encoder.blocks.2.attn.lora_B_v.31.weight', 'image_encoder.blocks.0.attn.lora_B_k.31.weight'}
2025-12-10 14:48:54,234 [inflora.py] => Task 31, Epoch 20/20 => Loss 0.008, Train_accy 99.50
Threshold:  0.981
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 48/768 type remove
Layer 4 : 59/768 type remove
Layer 5 : 83/768 type remove
Layer 6 : 111/768 type remove
Layer 7 : 131/768 type remove
Layer 8 : 186/768 type remove
Layer 9 : 187/768 type remove
Layer 10 : 167/768 type remove
Layer 11 : 95/768 type remove
Layer 12 : 96/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:49:04,720 [trainer.py] => Time:138.650395154953
6400 6400
6400 6400
2025-12-10 14:49:22,209 [trainer.py] => Time:17.48946976661682
2025-12-10 14:49:22,210 [inflora.py] => Exemplar size: 0
2025-12-10 14:49:22,210 [trainer.py] => CNN: {'total': np.float64(67.39), '00-01': np.float64(87.0), '02-03': np.float64(71.5), '04-05': np.float64(76.5), '06-07': np.float64(48.0), '08-09': np.float64(87.0), '10-11': np.float64(47.5), '12-13': np.float64(61.0), '14-15': np.float64(81.5), '16-17': np.float64(62.0), '18-19': np.float64(67.5), '20-21': np.float64(80.0), '22-23': np.float64(53.5), '24-25': np.float64(61.0), '26-27': np.float64(68.0), '28-29': np.float64(73.0), '30-31': np.float64(64.0), '32-33': np.float64(58.5), '34-35': np.float64(50.0), '36-37': np.float64(59.5), '38-39': np.float64(82.0), '40-41': np.float64(81.0), '42-43': np.float64(27.5), '44-45': np.float64(52.0), '46-47': np.float64(43.5), '48-49': np.float64(77.0), '50-51': np.float64(74.5), '52-53': np.float64(85.0), '54-55': np.float64(60.0), '56-57': np.float64(77.0), '58-59': np.float64(75.0), '60-61': np.float64(89.5), '62-63': np.float64(75.5), 'old': np.float64(67.13), 'new': np.float64(75.5)}
2025-12-10 14:49:22,210 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39)]
2025-12-10 14:49:22,210 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52)]
2025-12-10 14:49:22,210 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625]
2025-12-10 14:49:26,650 [trainer.py] => All params: 125940251
2025-12-10 14:49:26,657 [trainer.py] => Trainable params: 185858
2025-12-10 14:49:26,658 [inflora.py] => Learning on 64-66
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.32.weight', 'image_encoder.blocks.1.attn.lora_B_k.32.weight', 'image_encoder.blocks.7.attn.lora_B_v.32.weight', 'image_encoder.blocks.9.attn.lora_B_v.32.weight', 'image_encoder.blocks.2.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_v.32.weight', 'image_encoder.blocks.1.attn.lora_B_v.32.weight', 'image_encoder.blocks.10.attn.lora_B_k.32.weight', 'classifier_pool.32.bias', 'image_encoder.blocks.5.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_v.32.weight', 'image_encoder.blocks.7.attn.lora_B_k.32.weight', 'image_encoder.blocks.9.attn.lora_B_k.32.weight', 'image_encoder.blocks.3.attn.lora_B_v.32.weight', 'image_encoder.blocks.0.attn.lora_B_k.32.weight', 'image_encoder.blocks.5.attn.lora_B_k.32.weight', 'image_encoder.blocks.0.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_k.32.weight', 'image_encoder.blocks.11.attn.lora_B_k.32.weight', 'classifier_pool.32.weight', 'image_encoder.blocks.6.attn.lora_B_v.32.weight', 'image_encoder.blocks.8.attn.lora_B_v.32.weight', 'image_encoder.blocks.3.attn.lora_B_k.32.weight', 'image_encoder.blocks.2.attn.lora_B_v.32.weight', 'image_encoder.blocks.10.attn.lora_B_v.32.weight', 'image_encoder.blocks.4.attn.lora_B_k.32.weight'}
2025-12-10 14:51:34,966 [inflora.py] => Task 32, Epoch 20/20 => Loss 0.092, Train_accy 96.60
Threshold:  0.982
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 49/768 type remove
Layer 4 : 61/768 type remove
Layer 5 : 86/768 type remove
Layer 6 : 114/768 type remove
Layer 7 : 137/768 type remove
Layer 8 : 191/768 type remove
Layer 9 : 195/768 type remove
Layer 10 : 176/768 type remove
Layer 11 : 102/768 type remove
Layer 12 : 99/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:51:45,104 [trainer.py] => Time:138.44682478904724
6600 6600
6600 6600
2025-12-10 14:52:03,130 [trainer.py] => Time:18.02540636062622
2025-12-10 14:52:03,130 [inflora.py] => Exemplar size: 0
2025-12-10 14:52:03,130 [trainer.py] => CNN: {'total': np.float64(67.68), '00-01': np.float64(91.0), '02-03': np.float64(74.0), '04-05': np.float64(63.0), '06-07': np.float64(43.0), '08-09': np.float64(91.0), '10-11': np.float64(41.0), '12-13': np.float64(69.0), '14-15': np.float64(85.0), '16-17': np.float64(66.0), '18-19': np.float64(73.5), '20-21': np.float64(79.0), '22-23': np.float64(57.0), '24-25': np.float64(61.0), '26-27': np.float64(57.5), '28-29': np.float64(70.5), '30-31': np.float64(61.0), '32-33': np.float64(63.0), '34-35': np.float64(50.5), '36-37': np.float64(57.5), '38-39': np.float64(80.5), '40-41': np.float64(84.0), '42-43': np.float64(28.0), '44-45': np.float64(55.0), '46-47': np.float64(41.0), '48-49': np.float64(73.0), '50-51': np.float64(79.5), '52-53': np.float64(83.5), '54-55': np.float64(70.0), '56-57': np.float64(81.5), '58-59': np.float64(75.0), '60-61': np.float64(89.0), '62-63': np.float64(74.5), '64-65': np.float64(65.5), 'old': np.float64(67.75), 'new': np.float64(65.5)}
2025-12-10 14:52:03,130 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68)]
2025-12-10 14:52:03,131 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52)]
2025-12-10 14:52:03,131 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818]
2025-12-10 14:52:05,303 [trainer.py] => All params: 125940251
2025-12-10 14:52:05,310 [trainer.py] => Trainable params: 185858
2025-12-10 14:52:05,310 [inflora.py] => Learning on 66-68
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_k.33.weight', 'image_encoder.blocks.3.attn.lora_B_k.33.weight', 'image_encoder.blocks.10.attn.lora_B_k.33.weight', 'image_encoder.blocks.7.attn.lora_B_v.33.weight', 'image_encoder.blocks.9.attn.lora_B_k.33.weight', 'classifier_pool.33.bias', 'image_encoder.blocks.11.attn.lora_B_k.33.weight', 'image_encoder.blocks.11.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_v.33.weight', 'image_encoder.blocks.4.attn.lora_B_v.33.weight', 'classifier_pool.33.weight', 'image_encoder.blocks.10.attn.lora_B_v.33.weight', 'image_encoder.blocks.2.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_v.33.weight', 'image_encoder.blocks.5.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_v.33.weight', 'image_encoder.blocks.9.attn.lora_B_v.33.weight', 'image_encoder.blocks.7.attn.lora_B_k.33.weight', 'image_encoder.blocks.0.attn.lora_B_k.33.weight', 'image_encoder.blocks.2.attn.lora_B_k.33.weight', 'image_encoder.blocks.1.attn.lora_B_v.33.weight', 'image_encoder.blocks.3.attn.lora_B_v.33.weight', 'image_encoder.blocks.8.attn.lora_B_k.33.weight', 'image_encoder.blocks.6.attn.lora_B_k.33.weight'}
2025-12-10 14:54:13,582 [inflora.py] => Task 33, Epoch 20/20 => Loss 0.038, Train_accy 98.80
Threshold:  0.983
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 50/768 type remove
Layer 4 : 62/768 type remove
Layer 5 : 88/768 type remove
Layer 6 : 118/768 type remove
Layer 7 : 141/768 type remove
Layer 8 : 196/768 type remove
Layer 9 : 204/768 type remove
Layer 10 : 183/768 type remove
Layer 11 : 107/768 type remove
Layer 12 : 101/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:54:23,646 [trainer.py] => Time:138.33621311187744
6800 6800
6800 6800
2025-12-10 14:54:42,229 [trainer.py] => Time:18.582699060440063
2025-12-10 14:54:42,230 [inflora.py] => Exemplar size: 0
2025-12-10 14:54:42,230 [trainer.py] => CNN: {'total': np.float64(68.07), '00-01': np.float64(89.0), '02-03': np.float64(75.5), '04-05': np.float64(69.5), '06-07': np.float64(45.5), '08-09': np.float64(86.0), '10-11': np.float64(43.0), '12-13': np.float64(67.0), '14-15': np.float64(83.5), '16-17': np.float64(73.0), '18-19': np.float64(76.0), '20-21': np.float64(82.0), '22-23': np.float64(56.0), '24-25': np.float64(60.5), '26-27': np.float64(62.5), '28-29': np.float64(73.0), '30-31': np.float64(65.5), '32-33': np.float64(64.5), '34-35': np.float64(49.5), '36-37': np.float64(61.5), '38-39': np.float64(81.5), '40-41': np.float64(83.5), '42-43': np.float64(34.0), '44-45': np.float64(53.5), '46-47': np.float64(42.0), '48-49': np.float64(78.0), '50-51': np.float64(80.5), '52-53': np.float64(79.5), '54-55': np.float64(62.5), '56-57': np.float64(82.0), '58-59': np.float64(74.5), '60-61': np.float64(85.0), '62-63': np.float64(73.5), '64-65': np.float64(67.0), '66-67': np.float64(54.5), 'old': np.float64(68.48), 'new': np.float64(54.5)}
2025-12-10 14:54:42,230 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07)]
2025-12-10 14:54:42,230 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49)]
2025-12-10 14:54:42,230 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471]
2025-12-10 14:54:46,864 [trainer.py] => All params: 125940251
2025-12-10 14:54:46,870 [trainer.py] => Trainable params: 185858
2025-12-10 14:54:46,870 [inflora.py] => Learning on 68-70
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.34.weight', 'image_encoder.blocks.0.attn.lora_B_v.34.weight', 'image_encoder.blocks.11.attn.lora_B_v.34.weight', 'image_encoder.blocks.8.attn.lora_B_v.34.weight', 'image_encoder.blocks.9.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_v.34.weight', 'image_encoder.blocks.3.attn.lora_B_v.34.weight', 'image_encoder.blocks.7.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_k.34.weight', 'image_encoder.blocks.10.attn.lora_B_k.34.weight', 'image_encoder.blocks.2.attn.lora_B_v.34.weight', 'image_encoder.blocks.6.attn.lora_B_k.34.weight', 'image_encoder.blocks.0.attn.lora_B_k.34.weight', 'classifier_pool.34.bias', 'image_encoder.blocks.9.attn.lora_B_k.34.weight', 'image_encoder.blocks.11.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_v.34.weight', 'image_encoder.blocks.2.attn.lora_B_k.34.weight', 'image_encoder.blocks.3.attn.lora_B_k.34.weight', 'image_encoder.blocks.1.attn.lora_B_k.34.weight', 'image_encoder.blocks.8.attn.lora_B_k.34.weight', 'image_encoder.blocks.7.attn.lora_B_k.34.weight', 'image_encoder.blocks.10.attn.lora_B_v.34.weight', 'image_encoder.blocks.5.attn.lora_B_v.34.weight', 'image_encoder.blocks.4.attn.lora_B_k.34.weight', 'classifier_pool.34.weight'}
2025-12-10 14:56:55,461 [inflora.py] => Task 34, Epoch 20/20 => Loss 0.018, Train_accy 99.40
Threshold:  0.984
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 27/768 type remove
Layer 3 : 51/768 type remove
Layer 4 : 63/768 type remove
Layer 5 : 90/768 type remove
Layer 6 : 121/768 type remove
Layer 7 : 146/768 type remove
Layer 8 : 201/768 type remove
Layer 9 : 209/768 type remove
Layer 10 : 188/768 type remove
Layer 11 : 111/768 type remove
Layer 12 : 105/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:57:05,351 [trainer.py] => Time:138.4808247089386
7000 7000
7000 7000
2025-12-10 14:57:24,457 [trainer.py] => Time:19.105866193771362
2025-12-10 14:57:24,458 [inflora.py] => Exemplar size: 0
2025-12-10 14:57:24,458 [trainer.py] => CNN: {'total': np.float64(68.53), '00-01': np.float64(86.5), '02-03': np.float64(72.0), '04-05': np.float64(69.0), '06-07': np.float64(45.0), '08-09': np.float64(87.0), '10-11': np.float64(45.5), '12-13': np.float64(63.5), '14-15': np.float64(82.0), '16-17': np.float64(71.0), '18-19': np.float64(75.5), '20-21': np.float64(81.0), '22-23': np.float64(54.5), '24-25': np.float64(60.5), '26-27': np.float64(66.0), '28-29': np.float64(75.0), '30-31': np.float64(71.0), '32-33': np.float64(66.0), '34-35': np.float64(49.5), '36-37': np.float64(60.0), '38-39': np.float64(82.0), '40-41': np.float64(86.0), '42-43': np.float64(32.0), '44-45': np.float64(51.5), '46-47': np.float64(46.0), '48-49': np.float64(85.0), '50-51': np.float64(75.5), '52-53': np.float64(80.0), '54-55': np.float64(62.5), '56-57': np.float64(78.0), '58-59': np.float64(72.0), '60-61': np.float64(87.0), '62-63': np.float64(73.5), '64-65': np.float64(63.0), '66-67': np.float64(50.0), '68-69': np.float64(94.0), 'old': np.float64(67.78), 'new': np.float64(94.0)}
2025-12-10 14:57:24,458 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53)]
2025-12-10 14:57:24,458 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53)]
2025-12-10 14:57:24,458 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143]
2025-12-10 14:57:32,212 [trainer.py] => All params: 125940251
2025-12-10 14:57:32,219 [trainer.py] => Trainable params: 185858
2025-12-10 14:57:32,219 [inflora.py] => Learning on 70-72
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.35.weight', 'image_encoder.blocks.6.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_v.35.weight', 'image_encoder.blocks.10.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_k.35.weight', 'image_encoder.blocks.9.attn.lora_B_k.35.weight', 'image_encoder.blocks.2.attn.lora_B_k.35.weight', 'classifier_pool.35.bias', 'image_encoder.blocks.11.attn.lora_B_k.35.weight', 'classifier_pool.35.weight', 'image_encoder.blocks.2.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_v.35.weight', 'image_encoder.blocks.0.attn.lora_B_v.35.weight', 'image_encoder.blocks.7.attn.lora_B_k.35.weight', 'image_encoder.blocks.5.attn.lora_B_k.35.weight', 'image_encoder.blocks.11.attn.lora_B_v.35.weight', 'image_encoder.blocks.5.attn.lora_B_v.35.weight', 'image_encoder.blocks.1.attn.lora_B_v.35.weight', 'image_encoder.blocks.3.attn.lora_B_k.35.weight', 'image_encoder.blocks.7.attn.lora_B_v.35.weight', 'image_encoder.blocks.4.attn.lora_B_k.35.weight', 'image_encoder.blocks.10.attn.lora_B_k.35.weight', 'image_encoder.blocks.8.attn.lora_B_v.35.weight', 'image_encoder.blocks.6.attn.lora_B_v.35.weight'}
2025-12-10 14:59:40,790 [inflora.py] => Task 35, Epoch 20/20 => Loss 0.009, Train_accy 99.80
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 28/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 93/768 type remove
Layer 6 : 126/768 type remove
Layer 7 : 154/768 type remove
Layer 8 : 208/768 type remove
Layer 9 : 217/768 type remove
Layer 10 : 196/768 type remove
Layer 11 : 117/768 type remove
Layer 12 : 115/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 14:59:51,362 [trainer.py] => Time:139.14334917068481
7200 7200
7200 7200
2025-12-10 15:00:10,989 [trainer.py] => Time:19.62657070159912
2025-12-10 15:00:10,989 [inflora.py] => Exemplar size: 0
2025-12-10 15:00:10,990 [trainer.py] => CNN: {'total': np.float64(66.18), '00-01': np.float64(81.0), '02-03': np.float64(68.5), '04-05': np.float64(64.5), '06-07': np.float64(40.5), '08-09': np.float64(86.0), '10-11': np.float64(47.0), '12-13': np.float64(59.0), '14-15': np.float64(79.5), '16-17': np.float64(70.5), '18-19': np.float64(74.5), '20-21': np.float64(81.0), '22-23': np.float64(51.5), '24-25': np.float64(63.5), '26-27': np.float64(66.0), '28-29': np.float64(70.5), '30-31': np.float64(67.0), '32-33': np.float64(62.0), '34-35': np.float64(47.5), '36-37': np.float64(57.5), '38-39': np.float64(82.5), '40-41': np.float64(86.0), '42-43': np.float64(30.0), '44-45': np.float64(47.5), '46-47': np.float64(41.0), '48-49': np.float64(73.5), '50-51': np.float64(70.0), '52-53': np.float64(81.0), '54-55': np.float64(57.5), '56-57': np.float64(71.5), '58-59': np.float64(71.0), '60-61': np.float64(84.0), '62-63': np.float64(70.5), '64-65': np.float64(64.0), '66-67': np.float64(44.0), '68-69': np.float64(85.5), '70-71': np.float64(85.5), 'old': np.float64(65.63), 'new': np.float64(85.5)}
2025-12-10 15:00:10,990 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18)]
2025-12-10 15:00:10,990 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51)]
2025-12-10 15:00:10,990 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555]
2025-12-10 15:00:15,021 [trainer.py] => All params: 125940251
2025-12-10 15:00:15,027 [trainer.py] => Trainable params: 185858
2025-12-10 15:00:15,027 [inflora.py] => Learning on 72-74
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_k.36.weight', 'image_encoder.blocks.7.attn.lora_B_v.36.weight', 'image_encoder.blocks.6.attn.lora_B_k.36.weight', 'classifier_pool.36.weight', 'image_encoder.blocks.7.attn.lora_B_k.36.weight', 'image_encoder.blocks.0.attn.lora_B_v.36.weight', 'image_encoder.blocks.9.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_v.36.weight', 'image_encoder.blocks.3.attn.lora_B_v.36.weight', 'image_encoder.blocks.8.attn.lora_B_v.36.weight', 'classifier_pool.36.bias', 'image_encoder.blocks.11.attn.lora_B_k.36.weight', 'image_encoder.blocks.9.attn.lora_B_k.36.weight', 'image_encoder.blocks.1.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_v.36.weight', 'image_encoder.blocks.2.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_v.36.weight', 'image_encoder.blocks.10.attn.lora_B_k.36.weight', 'image_encoder.blocks.5.attn.lora_B_k.36.weight', 'image_encoder.blocks.4.attn.lora_B_k.36.weight', 'image_encoder.blocks.8.attn.lora_B_k.36.weight', 'image_encoder.blocks.6.attn.lora_B_v.36.weight', 'image_encoder.blocks.1.attn.lora_B_v.36.weight', 'image_encoder.blocks.0.attn.lora_B_k.36.weight'}
2025-12-10 15:02:23,162 [inflora.py] => Task 36, Epoch 20/20 => Loss 0.082, Train_accy 96.50
Threshold:  0.986
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 29/768 type remove
Layer 3 : 53/768 type remove
Layer 4 : 67/768 type remove
Layer 5 : 96/768 type remove
Layer 6 : 129/768 type remove
Layer 7 : 159/768 type remove
Layer 8 : 216/768 type remove
Layer 9 : 229/768 type remove
Layer 10 : 206/768 type remove
Layer 11 : 125/768 type remove
Layer 12 : 120/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:02:33,568 [trainer.py] => Time:138.5402638912201
7400 7400
7400 7400
2025-12-10 15:02:53,908 [trainer.py] => Time:20.34042501449585
2025-12-10 15:02:53,909 [inflora.py] => Exemplar size: 0
2025-12-10 15:02:53,909 [trainer.py] => CNN: {'total': np.float64(67.14), '00-01': np.float64(78.5), '02-03': np.float64(70.0), '04-05': np.float64(75.0), '06-07': np.float64(42.0), '08-09': np.float64(86.5), '10-11': np.float64(57.0), '12-13': np.float64(60.5), '14-15': np.float64(77.0), '16-17': np.float64(72.5), '18-19': np.float64(75.0), '20-21': np.float64(78.5), '22-23': np.float64(55.0), '24-25': np.float64(64.5), '26-27': np.float64(63.0), '28-29': np.float64(70.5), '30-31': np.float64(73.5), '32-33': np.float64(69.0), '34-35': np.float64(48.0), '36-37': np.float64(56.5), '38-39': np.float64(80.5), '40-41': np.float64(87.5), '42-43': np.float64(34.0), '44-45': np.float64(50.0), '46-47': np.float64(44.5), '48-49': np.float64(81.0), '50-51': np.float64(72.0), '52-53': np.float64(85.5), '54-55': np.float64(54.0), '56-57': np.float64(73.5), '58-59': np.float64(68.5), '60-61': np.float64(82.0), '62-63': np.float64(70.0), '64-65': np.float64(66.5), '66-67': np.float64(44.0), '68-69': np.float64(89.5), '70-71': np.float64(84.5), '72-73': np.float64(44.0), 'old': np.float64(67.78), 'new': np.float64(44.0)}
2025-12-10 15:02:53,909 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14)]
2025-12-10 15:02:53,909 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57)]
2025-12-10 15:02:53,909 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514]
2025-12-10 15:02:56,360 [trainer.py] => All params: 125940251
2025-12-10 15:02:56,366 [trainer.py] => Trainable params: 185858
2025-12-10 15:02:56,366 [inflora.py] => Learning on 74-76
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.37.weight', 'image_encoder.blocks.7.attn.lora_B_v.37.weight', 'image_encoder.blocks.8.attn.lora_B_v.37.weight', 'classifier_pool.37.bias', 'image_encoder.blocks.1.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_k.37.weight', 'image_encoder.blocks.9.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_v.37.weight', 'image_encoder.blocks.6.attn.lora_B_k.37.weight', 'image_encoder.blocks.0.attn.lora_B_k.37.weight', 'image_encoder.blocks.4.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_k.37.weight', 'image_encoder.blocks.7.attn.lora_B_k.37.weight', 'image_encoder.blocks.1.attn.lora_B_v.37.weight', 'image_encoder.blocks.10.attn.lora_B_v.37.weight', 'image_encoder.blocks.11.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_k.37.weight', 'image_encoder.blocks.3.attn.lora_B_k.37.weight', 'image_encoder.blocks.5.attn.lora_B_v.37.weight', 'image_encoder.blocks.0.attn.lora_B_v.37.weight', 'image_encoder.blocks.9.attn.lora_B_v.37.weight', 'image_encoder.blocks.2.attn.lora_B_v.37.weight', 'classifier_pool.37.weight', 'image_encoder.blocks.8.attn.lora_B_k.37.weight', 'image_encoder.blocks.10.attn.lora_B_k.37.weight', 'image_encoder.blocks.11.attn.lora_B_v.37.weight'}
2025-12-10 15:05:04,467 [inflora.py] => Task 37, Epoch 20/20 => Loss 0.063, Train_accy 97.30
Threshold:  0.987
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 31/768 type remove
Layer 3 : 55/768 type remove
Layer 4 : 72/768 type remove
Layer 5 : 101/768 type remove
Layer 6 : 137/768 type remove
Layer 7 : 168/768 type remove
Layer 8 : 228/768 type remove
Layer 9 : 238/768 type remove
Layer 10 : 216/768 type remove
Layer 11 : 130/768 type remove
Layer 12 : 122/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:05:14,695 [trainer.py] => Time:138.32868432998657
7600 7600
7600 7600
2025-12-10 15:05:35,561 [trainer.py] => Time:20.866414785385132
2025-12-10 15:05:35,562 [inflora.py] => Exemplar size: 0
2025-12-10 15:05:35,562 [trainer.py] => CNN: {'total': np.float64(66.09), '00-01': np.float64(86.5), '02-03': np.float64(68.5), '04-05': np.float64(67.5), '06-07': np.float64(43.5), '08-09': np.float64(89.0), '10-11': np.float64(56.0), '12-13': np.float64(58.5), '14-15': np.float64(79.5), '16-17': np.float64(68.5), '18-19': np.float64(75.5), '20-21': np.float64(78.5), '22-23': np.float64(52.5), '24-25': np.float64(63.0), '26-27': np.float64(61.0), '28-29': np.float64(68.0), '30-31': np.float64(63.5), '32-33': np.float64(62.5), '34-35': np.float64(48.0), '36-37': np.float64(55.5), '38-39': np.float64(80.5), '40-41': np.float64(87.0), '42-43': np.float64(26.5), '44-45': np.float64(51.0), '46-47': np.float64(46.0), '48-49': np.float64(77.0), '50-51': np.float64(71.0), '52-53': np.float64(83.0), '54-55': np.float64(59.0), '56-57': np.float64(74.5), '58-59': np.float64(71.5), '60-61': np.float64(79.5), '62-63': np.float64(70.0), '64-65': np.float64(64.5), '66-67': np.float64(44.0), '68-69': np.float64(85.5), '70-71': np.float64(84.0), '72-73': np.float64(40.0), '74-75': np.float64(71.5), 'old': np.float64(65.95), 'new': np.float64(71.5)}
2025-12-10 15:05:35,562 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09)]
2025-12-10 15:05:35,562 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59)]
2025-12-10 15:05:35,562 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579]
2025-12-10 15:05:39,449 [trainer.py] => All params: 125940251
2025-12-10 15:05:39,457 [trainer.py] => Trainable params: 185858
2025-12-10 15:05:39,457 [inflora.py] => Learning on 76-78
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.38.weight', 'image_encoder.blocks.4.attn.lora_B_k.38.weight', 'image_encoder.blocks.7.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_k.38.weight', 'image_encoder.blocks.10.attn.lora_B_v.38.weight', 'classifier_pool.38.bias', 'image_encoder.blocks.1.attn.lora_B_v.38.weight', 'classifier_pool.38.weight', 'image_encoder.blocks.4.attn.lora_B_v.38.weight', 'image_encoder.blocks.5.attn.lora_B_k.38.weight', 'image_encoder.blocks.8.attn.lora_B_v.38.weight', 'image_encoder.blocks.10.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_v.38.weight', 'image_encoder.blocks.0.attn.lora_B_v.38.weight', 'image_encoder.blocks.6.attn.lora_B_v.38.weight', 'image_encoder.blocks.11.attn.lora_B_k.38.weight', 'image_encoder.blocks.2.attn.lora_B_v.38.weight', 'image_encoder.blocks.2.attn.lora_B_k.38.weight', 'image_encoder.blocks.9.attn.lora_B_k.38.weight', 'image_encoder.blocks.11.attn.lora_B_v.38.weight', 'image_encoder.blocks.3.attn.lora_B_k.38.weight', 'image_encoder.blocks.1.attn.lora_B_k.38.weight', 'image_encoder.blocks.5.attn.lora_B_v.38.weight', 'image_encoder.blocks.7.attn.lora_B_k.38.weight', 'image_encoder.blocks.6.attn.lora_B_k.38.weight'}
2025-12-10 15:07:48,032 [inflora.py] => Task 38, Epoch 20/20 => Loss 0.013, Train_accy 99.60
Threshold:  0.988
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 32/768 type remove
Layer 3 : 56/768 type remove
Layer 4 : 74/768 type remove
Layer 5 : 103/768 type remove
Layer 6 : 144/768 type remove
Layer 7 : 176/768 type remove
Layer 8 : 244/768 type remove
Layer 9 : 256/768 type remove
Layer 10 : 238/768 type remove
Layer 11 : 139/768 type remove
Layer 12 : 143/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:07:58,155 [trainer.py] => Time:138.69791841506958
7800 7800
7800 7800
2025-12-10 15:08:19,627 [trainer.py] => Time:21.472208499908447
2025-12-10 15:08:19,627 [inflora.py] => Exemplar size: 0
2025-12-10 15:08:19,628 [trainer.py] => CNN: {'total': np.float64(67.49), '00-01': np.float64(89.5), '02-03': np.float64(75.0), '04-05': np.float64(79.5), '06-07': np.float64(47.0), '08-09': np.float64(89.0), '10-11': np.float64(55.5), '12-13': np.float64(59.5), '14-15': np.float64(81.5), '16-17': np.float64(70.0), '18-19': np.float64(79.5), '20-21': np.float64(79.5), '22-23': np.float64(47.0), '24-25': np.float64(64.5), '26-27': np.float64(64.0), '28-29': np.float64(67.5), '30-31': np.float64(66.5), '32-33': np.float64(64.0), '34-35': np.float64(49.0), '36-37': np.float64(57.5), '38-39': np.float64(79.5), '40-41': np.float64(87.0), '42-43': np.float64(29.5), '44-45': np.float64(47.5), '46-47': np.float64(53.5), '48-49': np.float64(79.0), '50-51': np.float64(70.0), '52-53': np.float64(84.0), '54-55': np.float64(59.0), '56-57': np.float64(74.5), '58-59': np.float64(67.5), '60-61': np.float64(83.5), '62-63': np.float64(66.5), '64-65': np.float64(63.0), '66-67': np.float64(50.5), '68-69': np.float64(85.0), '70-71': np.float64(87.0), '72-73': np.float64(32.0), '74-75': np.float64(73.5), '76-77': np.float64(74.5), 'old': np.float64(67.3), 'new': np.float64(74.5)}
2025-12-10 15:08:19,628 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49)]
2025-12-10 15:08:19,628 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5)]
2025-12-10 15:08:19,628 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948]
2025-12-10 15:08:22,098 [trainer.py] => All params: 125940251
2025-12-10 15:08:22,105 [trainer.py] => Trainable params: 185858
2025-12-10 15:08:22,105 [inflora.py] => Learning on 78-80
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_k.39.weight', 'image_encoder.blocks.10.attn.lora_B_v.39.weight', 'classifier_pool.39.weight', 'image_encoder.blocks.3.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_v.39.weight', 'image_encoder.blocks.8.attn.lora_B_k.39.weight', 'classifier_pool.39.bias', 'image_encoder.blocks.6.attn.lora_B_k.39.weight', 'image_encoder.blocks.11.attn.lora_B_k.39.weight', 'image_encoder.blocks.9.attn.lora_B_k.39.weight', 'image_encoder.blocks.5.attn.lora_B_k.39.weight', 'image_encoder.blocks.0.attn.lora_B_v.39.weight', 'image_encoder.blocks.4.attn.lora_B_v.39.weight', 'image_encoder.blocks.6.attn.lora_B_v.39.weight', 'image_encoder.blocks.11.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_k.39.weight', 'image_encoder.blocks.1.attn.lora_B_k.39.weight', 'image_encoder.blocks.2.attn.lora_B_v.39.weight', 'image_encoder.blocks.5.attn.lora_B_v.39.weight', 'image_encoder.blocks.3.attn.lora_B_v.39.weight', 'image_encoder.blocks.7.attn.lora_B_v.39.weight', 'image_encoder.blocks.9.attn.lora_B_v.39.weight'}
2025-12-10 15:10:30,914 [inflora.py] => Task 39, Epoch 20/20 => Loss 0.040, Train_accy 98.50
Threshold:  0.989
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 57/768 type remove
Layer 4 : 77/768 type remove
Layer 5 : 106/768 type remove
Layer 6 : 151/768 type remove
Layer 7 : 185/768 type remove
Layer 8 : 263/768 type remove
Layer 9 : 274/768 type remove
Layer 10 : 255/768 type remove
Layer 11 : 147/768 type remove
Layer 12 : 146/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:10:41,501 [trainer.py] => Time:139.39623069763184
8000 8000
8000 8000
2025-12-10 15:11:03,407 [trainer.py] => Time:21.905401706695557
2025-12-10 15:11:03,407 [inflora.py] => Exemplar size: 0
2025-12-10 15:11:03,407 [trainer.py] => CNN: {'total': np.float64(67.36), '00-01': np.float64(88.5), '02-03': np.float64(76.5), '04-05': np.float64(78.5), '06-07': np.float64(49.5), '08-09': np.float64(88.0), '10-11': np.float64(50.0), '12-13': np.float64(53.5), '14-15': np.float64(77.5), '16-17': np.float64(71.5), '18-19': np.float64(80.0), '20-21': np.float64(79.0), '22-23': np.float64(47.5), '24-25': np.float64(60.0), '26-27': np.float64(65.0), '28-29': np.float64(67.5), '30-31': np.float64(67.0), '32-33': np.float64(64.0), '34-35': np.float64(50.0), '36-37': np.float64(59.0), '38-39': np.float64(77.0), '40-41': np.float64(88.5), '42-43': np.float64(34.0), '44-45': np.float64(50.5), '46-47': np.float64(60.0), '48-49': np.float64(82.5), '50-51': np.float64(68.5), '52-53': np.float64(73.0), '54-55': np.float64(59.0), '56-57': np.float64(74.0), '58-59': np.float64(71.0), '60-61': np.float64(80.5), '62-63': np.float64(61.0), '64-65': np.float64(67.0), '66-67': np.float64(50.0), '68-69': np.float64(88.0), '70-71': np.float64(85.0), '72-73': np.float64(31.0), '74-75': np.float64(74.5), '76-77': np.float64(72.0), '78-79': np.float64(75.0), 'old': np.float64(67.17), 'new': np.float64(75.0)}
2025-12-10 15:11:03,407 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36)]
2025-12-10 15:11:03,407 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54)]
2025-12-10 15:11:03,407 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625]
2025-12-10 15:11:08,750 [trainer.py] => All params: 125940251
2025-12-10 15:11:08,758 [trainer.py] => Trainable params: 185858
2025-12-10 15:11:08,758 [inflora.py] => Learning on 80-82
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.40.weight', 'image_encoder.blocks.2.attn.lora_B_k.40.weight', 'image_encoder.blocks.5.attn.lora_B_v.40.weight', 'image_encoder.blocks.1.attn.lora_B_v.40.weight', 'image_encoder.blocks.4.attn.lora_B_k.40.weight', 'image_encoder.blocks.1.attn.lora_B_k.40.weight', 'image_encoder.blocks.7.attn.lora_B_k.40.weight', 'image_encoder.blocks.6.attn.lora_B_v.40.weight', 'classifier_pool.40.weight', 'image_encoder.blocks.4.attn.lora_B_v.40.weight', 'image_encoder.blocks.7.attn.lora_B_v.40.weight', 'image_encoder.blocks.8.attn.lora_B_v.40.weight', 'image_encoder.blocks.6.attn.lora_B_k.40.weight', 'image_encoder.blocks.9.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_k.40.weight', 'image_encoder.blocks.8.attn.lora_B_k.40.weight', 'image_encoder.blocks.3.attn.lora_B_k.40.weight', 'image_encoder.blocks.11.attn.lora_B_v.40.weight', 'image_encoder.blocks.10.attn.lora_B_k.40.weight', 'classifier_pool.40.bias', 'image_encoder.blocks.3.attn.lora_B_v.40.weight', 'image_encoder.blocks.5.attn.lora_B_k.40.weight', 'image_encoder.blocks.10.attn.lora_B_v.40.weight', 'image_encoder.blocks.0.attn.lora_B_v.40.weight', 'image_encoder.blocks.9.attn.lora_B_k.40.weight'}
2025-12-10 15:13:17,610 [inflora.py] => Task 40, Epoch 20/20 => Loss 0.014, Train_accy 99.50
Threshold:  0.99
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 34/768 type remove
Layer 3 : 60/768 type remove
Layer 4 : 81/768 type remove
Layer 5 : 112/768 type remove
Layer 6 : 161/768 type remove
Layer 7 : 195/768 type remove
Layer 8 : 272/768 type remove
Layer 9 : 288/768 type remove
Layer 10 : 280/768 type remove
Layer 11 : 162/768 type remove
Layer 12 : 156/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:13:27,945 [trainer.py] => Time:139.18661451339722
8200 8200
8200 8200
2025-12-10 15:13:50,339 [trainer.py] => Time:22.393771409988403
2025-12-10 15:13:50,339 [inflora.py] => Exemplar size: 0
2025-12-10 15:13:50,339 [trainer.py] => CNN: {'total': np.float64(66.33), '00-01': np.float64(89.5), '02-03': np.float64(70.5), '04-05': np.float64(71.0), '06-07': np.float64(45.0), '08-09': np.float64(91.5), '10-11': np.float64(47.5), '12-13': np.float64(53.0), '14-15': np.float64(82.0), '16-17': np.float64(64.0), '18-19': np.float64(75.0), '20-21': np.float64(82.0), '22-23': np.float64(45.5), '24-25': np.float64(63.5), '26-27': np.float64(64.0), '28-29': np.float64(67.0), '30-31': np.float64(59.0), '32-33': np.float64(60.0), '34-35': np.float64(50.0), '36-37': np.float64(55.0), '38-39': np.float64(81.5), '40-41': np.float64(87.5), '42-43': np.float64(27.0), '44-45': np.float64(49.0), '46-47': np.float64(53.5), '48-49': np.float64(63.5), '50-51': np.float64(74.0), '52-53': np.float64(75.0), '54-55': np.float64(68.0), '56-57': np.float64(78.0), '58-59': np.float64(81.0), '60-61': np.float64(83.0), '62-63': np.float64(64.5), '64-65': np.float64(65.0), '66-67': np.float64(48.5), '68-69': np.float64(81.0), '70-71': np.float64(85.0), '72-73': np.float64(33.0), '74-75': np.float64(73.5), '76-77': np.float64(71.0), '78-79': np.float64(71.0), '80-81': np.float64(71.0), 'old': np.float64(66.21), 'new': np.float64(71.0)}
2025-12-10 15:13:50,339 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33)]
2025-12-10 15:13:50,340 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54)]
2025-12-10 15:13:50,340 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293]
2025-12-10 15:13:54,623 [trainer.py] => All params: 125940251
2025-12-10 15:13:54,630 [trainer.py] => Trainable params: 185858
2025-12-10 15:13:54,630 [inflora.py] => Learning on 82-84
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.41.weight', 'image_encoder.blocks.5.attn.lora_B_k.41.weight', 'image_encoder.blocks.7.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_v.41.weight', 'image_encoder.blocks.7.attn.lora_B_k.41.weight', 'image_encoder.blocks.5.attn.lora_B_v.41.weight', 'image_encoder.blocks.9.attn.lora_B_k.41.weight', 'image_encoder.blocks.10.attn.lora_B_v.41.weight', 'image_encoder.blocks.0.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_v.41.weight', 'image_encoder.blocks.10.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_v.41.weight', 'image_encoder.blocks.3.attn.lora_B_v.41.weight', 'image_encoder.blocks.11.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_k.41.weight', 'image_encoder.blocks.9.attn.lora_B_v.41.weight', 'image_encoder.blocks.8.attn.lora_B_k.41.weight', 'image_encoder.blocks.1.attn.lora_B_k.41.weight', 'image_encoder.blocks.4.attn.lora_B_v.41.weight', 'classifier_pool.41.weight', 'image_encoder.blocks.3.attn.lora_B_k.41.weight', 'image_encoder.blocks.6.attn.lora_B_k.41.weight', 'image_encoder.blocks.2.attn.lora_B_v.41.weight', 'classifier_pool.41.bias'}
2025-12-10 15:16:03,532 [inflora.py] => Task 41, Epoch 20/20 => Loss 0.011, Train_accy 99.80
Threshold:  0.991
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 35/768 type remove
Layer 3 : 63/768 type remove
Layer 4 : 86/768 type remove
Layer 5 : 121/768 type remove
Layer 6 : 173/768 type remove
Layer 7 : 211/768 type remove
Layer 8 : 297/768 type remove
Layer 9 : 312/768 type remove
Layer 10 : 296/768 type remove
Layer 11 : 180/768 type remove
Layer 12 : 164/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:16:13,976 [trainer.py] => Time:139.3463339805603
8400 8400
8400 8400
2025-12-10 15:16:36,936 [trainer.py] => Time:22.959768772125244
2025-12-10 15:16:36,936 [inflora.py] => Exemplar size: 0
2025-12-10 15:16:36,937 [trainer.py] => CNN: {'total': np.float64(66.43), '00-01': np.float64(86.5), '02-03': np.float64(73.5), '04-05': np.float64(74.0), '06-07': np.float64(41.5), '08-09': np.float64(90.0), '10-11': np.float64(52.0), '12-13': np.float64(50.5), '14-15': np.float64(80.5), '16-17': np.float64(66.5), '18-19': np.float64(76.0), '20-21': np.float64(78.0), '22-23': np.float64(46.5), '24-25': np.float64(66.5), '26-27': np.float64(62.5), '28-29': np.float64(66.0), '30-31': np.float64(61.5), '32-33': np.float64(62.5), '34-35': np.float64(49.5), '36-37': np.float64(55.0), '38-39': np.float64(82.0), '40-41': np.float64(84.5), '42-43': np.float64(28.0), '44-45': np.float64(51.5), '46-47': np.float64(51.0), '48-49': np.float64(63.5), '50-51': np.float64(72.5), '52-53': np.float64(78.0), '54-55': np.float64(66.0), '56-57': np.float64(81.5), '58-59': np.float64(71.0), '60-61': np.float64(82.0), '62-63': np.float64(61.0), '64-65': np.float64(65.5), '66-67': np.float64(51.5), '68-69': np.float64(82.5), '70-71': np.float64(87.0), '72-73': np.float64(30.0), '74-75': np.float64(76.0), '76-77': np.float64(72.5), '78-79': np.float64(66.5), '80-81': np.float64(70.5), '82-83': np.float64(76.5), 'old': np.float64(66.18), 'new': np.float64(76.5)}
2025-12-10 15:16:36,937 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43)]
2025-12-10 15:16:36,937 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57)]
2025-12-10 15:16:36,937 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143]
2025-12-10 15:16:42,375 [trainer.py] => All params: 125940251
2025-12-10 15:16:42,382 [trainer.py] => Trainable params: 185858
2025-12-10 15:16:42,382 [inflora.py] => Learning on 84-86
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.42.weight', 'image_encoder.blocks.1.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_k.42.weight', 'image_encoder.blocks.11.attn.lora_B_k.42.weight', 'image_encoder.blocks.4.attn.lora_B_v.42.weight', 'classifier_pool.42.bias', 'image_encoder.blocks.3.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_v.42.weight', 'image_encoder.blocks.5.attn.lora_B_v.42.weight', 'image_encoder.blocks.0.attn.lora_B_k.42.weight', 'image_encoder.blocks.8.attn.lora_B_v.42.weight', 'image_encoder.blocks.9.attn.lora_B_k.42.weight', 'image_encoder.blocks.0.attn.lora_B_v.42.weight', 'image_encoder.blocks.1.attn.lora_B_v.42.weight', 'classifier_pool.42.weight', 'image_encoder.blocks.5.attn.lora_B_k.42.weight', 'image_encoder.blocks.2.attn.lora_B_v.42.weight', 'image_encoder.blocks.10.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_v.42.weight', 'image_encoder.blocks.4.attn.lora_B_k.42.weight', 'image_encoder.blocks.10.attn.lora_B_v.42.weight', 'image_encoder.blocks.6.attn.lora_B_k.42.weight', 'image_encoder.blocks.7.attn.lora_B_k.42.weight', 'image_encoder.blocks.6.attn.lora_B_v.42.weight', 'image_encoder.blocks.11.attn.lora_B_v.42.weight'}
2025-12-10 15:18:51,152 [inflora.py] => Task 42, Epoch 20/20 => Loss 0.038, Train_accy 98.40
Threshold:  0.992
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 36/768 type remove
Layer 3 : 68/768 type remove
Layer 4 : 94/768 type remove
Layer 5 : 131/768 type remove
Layer 6 : 189/768 type remove
Layer 7 : 226/768 type remove
Layer 8 : 312/768 type remove
Layer 9 : 330/768 type remove
Layer 10 : 323/768 type remove
Layer 11 : 205/768 type remove
Layer 12 : 177/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:19:01,192 [trainer.py] => Time:138.81049799919128
8600 8600
8600 8600
2025-12-10 15:19:24,704 [trainer.py] => Time:23.51132035255432
2025-12-10 15:19:24,704 [inflora.py] => Exemplar size: 0
2025-12-10 15:19:24,704 [trainer.py] => CNN: {'total': np.float64(66.81), '00-01': np.float64(86.5), '02-03': np.float64(72.0), '04-05': np.float64(72.0), '06-07': np.float64(44.0), '08-09': np.float64(88.0), '10-11': np.float64(52.0), '12-13': np.float64(55.5), '14-15': np.float64(80.5), '16-17': np.float64(73.5), '18-19': np.float64(80.0), '20-21': np.float64(79.5), '22-23': np.float64(47.0), '24-25': np.float64(65.5), '26-27': np.float64(66.5), '28-29': np.float64(69.0), '30-31': np.float64(64.5), '32-33': np.float64(64.5), '34-35': np.float64(48.5), '36-37': np.float64(54.5), '38-39': np.float64(82.5), '40-41': np.float64(85.5), '42-43': np.float64(28.5), '44-45': np.float64(53.5), '46-47': np.float64(50.0), '48-49': np.float64(63.0), '50-51': np.float64(71.5), '52-53': np.float64(80.5), '54-55': np.float64(65.5), '56-57': np.float64(82.5), '58-59': np.float64(62.0), '60-61': np.float64(81.0), '62-63': np.float64(63.5), '64-65': np.float64(64.0), '66-67': np.float64(49.0), '68-69': np.float64(80.5), '70-71': np.float64(88.0), '72-73': np.float64(30.5), '74-75': np.float64(75.5), '76-77': np.float64(71.5), '78-79': np.float64(67.5), '80-81': np.float64(69.0), '82-83': np.float64(78.0), '84-85': np.float64(66.5), 'old': np.float64(66.82), 'new': np.float64(66.5)}
2025-12-10 15:19:24,705 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81)]
2025-12-10 15:19:24,705 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5)]
2025-12-10 15:19:24,705 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209]
2025-12-10 15:19:31,327 [trainer.py] => All params: 125940251
2025-12-10 15:19:31,333 [trainer.py] => Trainable params: 185858
2025-12-10 15:19:31,334 [inflora.py] => Learning on 86-88
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_v.43.weight', 'image_encoder.blocks.4.attn.lora_B_v.43.weight', 'image_encoder.blocks.1.attn.lora_B_k.43.weight', 'classifier_pool.43.weight', 'image_encoder.blocks.1.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_k.43.weight', 'image_encoder.blocks.8.attn.lora_B_k.43.weight', 'image_encoder.blocks.2.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_v.43.weight', 'image_encoder.blocks.5.attn.lora_B_v.43.weight', 'image_encoder.blocks.11.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_v.43.weight', 'image_encoder.blocks.3.attn.lora_B_v.43.weight', 'image_encoder.blocks.9.attn.lora_B_k.43.weight', 'image_encoder.blocks.10.attn.lora_B_v.43.weight', 'image_encoder.blocks.6.attn.lora_B_k.43.weight', 'image_encoder.blocks.5.attn.lora_B_k.43.weight', 'image_encoder.blocks.9.attn.lora_B_v.43.weight', 'image_encoder.blocks.10.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_k.43.weight', 'image_encoder.blocks.11.attn.lora_B_k.43.weight', 'image_encoder.blocks.4.attn.lora_B_k.43.weight', 'image_encoder.blocks.0.attn.lora_B_v.43.weight', 'image_encoder.blocks.7.attn.lora_B_k.43.weight', 'classifier_pool.43.bias'}
2025-12-10 15:21:40,146 [inflora.py] => Task 43, Epoch 20/20 => Loss 0.050, Train_accy 98.20
Threshold:  0.993
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 38/768 type remove
Layer 3 : 69/768 type remove
Layer 4 : 97/768 type remove
Layer 5 : 137/768 type remove
Layer 6 : 199/768 type remove
Layer 7 : 243/768 type remove
Layer 8 : 332/768 type remove
Layer 9 : 357/768 type remove
Layer 10 : 347/768 type remove
Layer 11 : 232/768 type remove
Layer 12 : 189/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:21:50,385 [trainer.py] => Time:139.05088710784912
8800 8800
8800 8800
2025-12-10 15:22:14,378 [trainer.py] => Time:23.99324679374695
2025-12-10 15:22:14,378 [inflora.py] => Exemplar size: 0
2025-12-10 15:22:14,378 [trainer.py] => CNN: {'total': np.float64(67.74), '00-01': np.float64(85.5), '02-03': np.float64(71.0), '04-05': np.float64(73.0), '06-07': np.float64(45.0), '08-09': np.float64(88.0), '10-11': np.float64(45.5), '12-13': np.float64(58.0), '14-15': np.float64(80.5), '16-17': np.float64(72.5), '18-19': np.float64(79.5), '20-21': np.float64(82.5), '22-23': np.float64(47.5), '24-25': np.float64(60.0), '26-27': np.float64(64.0), '28-29': np.float64(71.5), '30-31': np.float64(71.0), '32-33': np.float64(65.5), '34-35': np.float64(49.0), '36-37': np.float64(55.0), '38-39': np.float64(82.5), '40-41': np.float64(85.5), '42-43': np.float64(32.5), '44-45': np.float64(52.0), '46-47': np.float64(50.5), '48-49': np.float64(66.0), '50-51': np.float64(76.5), '52-53': np.float64(79.0), '54-55': np.float64(71.0), '56-57': np.float64(83.0), '58-59': np.float64(69.0), '60-61': np.float64(80.0), '62-63': np.float64(56.5), '64-65': np.float64(67.0), '66-67': np.float64(55.0), '68-69': np.float64(86.5), '70-71': np.float64(87.5), '72-73': np.float64(31.5), '74-75': np.float64(74.5), '76-77': np.float64(71.0), '78-79': np.float64(65.0), '80-81': np.float64(70.5), '82-83': np.float64(81.0), '84-85': np.float64(68.5), '86-87': np.float64(74.0), 'old': np.float64(67.59), 'new': np.float64(74.0)}
2025-12-10 15:22:14,378 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74)]
2025-12-10 15:22:14,379 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5)]
2025-12-10 15:22:14,379 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636]
2025-12-10 15:22:16,802 [trainer.py] => All params: 125940251
2025-12-10 15:22:16,809 [trainer.py] => Trainable params: 185858
2025-12-10 15:22:16,809 [inflora.py] => Learning on 88-90
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_k.44.weight', 'image_encoder.blocks.11.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_k.44.weight', 'classifier_pool.44.weight', 'image_encoder.blocks.1.attn.lora_B_k.44.weight', 'image_encoder.blocks.7.attn.lora_B_v.44.weight', 'image_encoder.blocks.0.attn.lora_B_v.44.weight', 'image_encoder.blocks.6.attn.lora_B_v.44.weight', 'image_encoder.blocks.7.attn.lora_B_k.44.weight', 'classifier_pool.44.bias', 'image_encoder.blocks.6.attn.lora_B_k.44.weight', 'image_encoder.blocks.8.attn.lora_B_k.44.weight', 'image_encoder.blocks.4.attn.lora_B_v.44.weight', 'image_encoder.blocks.2.attn.lora_B_v.44.weight', 'image_encoder.blocks.10.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_v.44.weight', 'image_encoder.blocks.3.attn.lora_B_v.44.weight', 'image_encoder.blocks.8.attn.lora_B_v.44.weight', 'image_encoder.blocks.11.attn.lora_B_k.44.weight', 'image_encoder.blocks.10.attn.lora_B_k.44.weight', 'image_encoder.blocks.2.attn.lora_B_k.44.weight', 'image_encoder.blocks.9.attn.lora_B_k.44.weight', 'image_encoder.blocks.1.attn.lora_B_v.44.weight', 'image_encoder.blocks.5.attn.lora_B_v.44.weight'}
2025-12-10 15:24:25,094 [inflora.py] => Task 44, Epoch 20/20 => Loss 0.042, Train_accy 98.30
Threshold:  0.994
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 40/768 type remove
Layer 3 : 77/768 type remove
Layer 4 : 108/768 type remove
Layer 5 : 148/768 type remove
Layer 6 : 211/768 type remove
Layer 7 : 253/768 type remove
Layer 8 : 342/768 type remove
Layer 9 : 364/768 type remove
Layer 10 : 357/768 type remove
Layer 11 : 247/768 type remove
Layer 12 : 199/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:24:35,489 [trainer.py] => Time:138.67967534065247
9000 9000
9000 9000
2025-12-10 15:25:00,135 [trainer.py] => Time:24.64582371711731
2025-12-10 15:25:00,135 [inflora.py] => Exemplar size: 0
2025-12-10 15:25:00,135 [trainer.py] => CNN: {'total': np.float64(66.21), '00-01': np.float64(81.5), '02-03': np.float64(72.0), '04-05': np.float64(71.5), '06-07': np.float64(41.5), '08-09': np.float64(88.0), '10-11': np.float64(45.5), '12-13': np.float64(59.5), '14-15': np.float64(80.0), '16-17': np.float64(67.5), '18-19': np.float64(77.5), '20-21': np.float64(83.0), '22-23': np.float64(54.0), '24-25': np.float64(58.5), '26-27': np.float64(65.0), '28-29': np.float64(70.5), '30-31': np.float64(67.5), '32-33': np.float64(63.0), '34-35': np.float64(48.5), '36-37': np.float64(46.5), '38-39': np.float64(81.0), '40-41': np.float64(86.5), '42-43': np.float64(23.5), '44-45': np.float64(53.0), '46-47': np.float64(49.0), '48-49': np.float64(53.0), '50-51': np.float64(74.0), '52-53': np.float64(78.5), '54-55': np.float64(68.5), '56-57': np.float64(77.0), '58-59': np.float64(69.5), '60-61': np.float64(80.5), '62-63': np.float64(63.0), '64-65': np.float64(64.0), '66-67': np.float64(45.0), '68-69': np.float64(84.0), '70-71': np.float64(89.0), '72-73': np.float64(32.0), '74-75': np.float64(72.5), '76-77': np.float64(64.5), '78-79': np.float64(66.5), '80-81': np.float64(66.0), '82-83': np.float64(81.0), '84-85': np.float64(67.0), '86-87': np.float64(73.5), '88-89': np.float64(76.5), 'old': np.float64(65.98), 'new': np.float64(76.5)}
2025-12-10 15:25:00,135 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21)]
2025-12-10 15:25:00,136 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52)]
2025-12-10 15:25:00,136 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111]
2025-12-10 15:25:09,796 [trainer.py] => All params: 125940251
2025-12-10 15:25:09,803 [trainer.py] => Trainable params: 185858
2025-12-10 15:25:09,803 [inflora.py] => Learning on 90-92
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.45.weight', 'image_encoder.blocks.9.attn.lora_B_k.45.weight', 'image_encoder.blocks.7.attn.lora_B_k.45.weight', 'classifier_pool.45.bias', 'image_encoder.blocks.1.attn.lora_B_v.45.weight', 'image_encoder.blocks.1.attn.lora_B_k.45.weight', 'image_encoder.blocks.6.attn.lora_B_k.45.weight', 'image_encoder.blocks.2.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_k.45.weight', 'image_encoder.blocks.11.attn.lora_B_v.45.weight', 'image_encoder.blocks.3.attn.lora_B_k.45.weight', 'image_encoder.blocks.3.attn.lora_B_v.45.weight', 'image_encoder.blocks.0.attn.lora_B_k.45.weight', 'image_encoder.blocks.10.attn.lora_B_v.45.weight', 'image_encoder.blocks.11.attn.lora_B_k.45.weight', 'image_encoder.blocks.4.attn.lora_B_v.45.weight', 'image_encoder.blocks.6.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_k.45.weight', 'image_encoder.blocks.5.attn.lora_B_v.45.weight', 'classifier_pool.45.weight', 'image_encoder.blocks.9.attn.lora_B_v.45.weight', 'image_encoder.blocks.10.attn.lora_B_k.45.weight', 'image_encoder.blocks.0.attn.lora_B_v.45.weight', 'image_encoder.blocks.2.attn.lora_B_v.45.weight', 'image_encoder.blocks.8.attn.lora_B_v.45.weight', 'image_encoder.blocks.7.attn.lora_B_v.45.weight'}
2025-12-10 15:27:18,940 [inflora.py] => Task 45, Epoch 20/20 => Loss 0.029, Train_accy 98.50
Threshold:  0.995
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 42/768 type remove
Layer 3 : 79/768 type remove
Layer 4 : 114/768 type remove
Layer 5 : 159/768 type remove
Layer 6 : 229/768 type remove
Layer 7 : 272/768 type remove
Layer 8 : 361/768 type remove
Layer 9 : 382/768 type remove
Layer 10 : 383/768 type remove
Layer 11 : 267/768 type remove
Layer 12 : 214/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:27:29,148 [trainer.py] => Time:139.34499764442444
9200 9200
9200 9200
2025-12-10 15:27:54,297 [trainer.py] => Time:25.14854335784912
2025-12-10 15:27:54,297 [inflora.py] => Exemplar size: 0
2025-12-10 15:27:54,298 [trainer.py] => CNN: {'total': np.float64(64.51), '00-01': np.float64(85.0), '02-03': np.float64(72.5), '04-05': np.float64(65.5), '06-07': np.float64(42.0), '08-09': np.float64(89.0), '10-11': np.float64(42.0), '12-13': np.float64(43.0), '14-15': np.float64(82.5), '16-17': np.float64(69.0), '18-19': np.float64(76.5), '20-21': np.float64(84.5), '22-23': np.float64(46.5), '24-25': np.float64(60.5), '26-27': np.float64(64.5), '28-29': np.float64(67.5), '30-31': np.float64(65.5), '32-33': np.float64(59.0), '34-35': np.float64(46.5), '36-37': np.float64(42.0), '38-39': np.float64(79.0), '40-41': np.float64(85.0), '42-43': np.float64(22.0), '44-45': np.float64(51.5), '46-47': np.float64(42.5), '48-49': np.float64(45.0), '50-51': np.float64(74.0), '52-53': np.float64(82.0), '54-55': np.float64(79.5), '56-57': np.float64(78.0), '58-59': np.float64(66.5), '60-61': np.float64(76.5), '62-63': np.float64(58.5), '64-65': np.float64(58.5), '66-67': np.float64(45.5), '68-69': np.float64(71.5), '70-71': np.float64(91.5), '72-73': np.float64(29.5), '74-75': np.float64(72.5), '76-77': np.float64(57.5), '78-79': np.float64(63.5), '80-81': np.float64(52.5), '82-83': np.float64(80.5), '84-85': np.float64(67.5), '86-87': np.float64(73.5), '88-89': np.float64(71.5), '90-91': np.float64(88.5), 'old': np.float64(63.98), 'new': np.float64(88.5)}
2025-12-10 15:27:54,298 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21), np.float64(64.51)]
2025-12-10 15:27:54,298 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52), np.float64(99.49)]
2025-12-10 15:27:54,298 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111, 0.6451086956521739]
2025-12-10 15:28:00,589 [trainer.py] => All params: 125940251
2025-12-10 15:28:00,596 [trainer.py] => Trainable params: 185858
2025-12-10 15:28:00,596 [inflora.py] => Learning on 92-94
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.46.weight', 'image_encoder.blocks.9.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_k.46.weight', 'image_encoder.blocks.10.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_v.46.weight', 'image_encoder.blocks.1.attn.lora_B_v.46.weight', 'image_encoder.blocks.4.attn.lora_B_k.46.weight', 'image_encoder.blocks.4.attn.lora_B_v.46.weight', 'classifier_pool.46.weight', 'image_encoder.blocks.5.attn.lora_B_k.46.weight', 'image_encoder.blocks.8.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_v.46.weight', 'image_encoder.blocks.1.attn.lora_B_k.46.weight', 'image_encoder.blocks.3.attn.lora_B_v.46.weight', 'image_encoder.blocks.8.attn.lora_B_k.46.weight', 'image_encoder.blocks.2.attn.lora_B_k.46.weight', 'image_encoder.blocks.6.attn.lora_B_v.46.weight', 'image_encoder.blocks.10.attn.lora_B_v.46.weight', 'classifier_pool.46.bias', 'image_encoder.blocks.5.attn.lora_B_v.46.weight', 'image_encoder.blocks.0.attn.lora_B_v.46.weight', 'image_encoder.blocks.11.attn.lora_B_k.46.weight', 'image_encoder.blocks.0.attn.lora_B_k.46.weight', 'image_encoder.blocks.9.attn.lora_B_v.46.weight', 'image_encoder.blocks.7.attn.lora_B_k.46.weight'}
2025-12-10 15:30:09,457 [inflora.py] => Task 46, Epoch 20/20 => Loss 0.013, Train_accy 99.40
Threshold:  0.996
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 45/768 type remove
Layer 3 : 83/768 type remove
Layer 4 : 122/768 type remove
Layer 5 : 174/768 type remove
Layer 6 : 251/768 type remove
Layer 7 : 301/768 type remove
Layer 8 : 370/768 type retain
Layer 9 : 342/768 type retain
Layer 10 : 331/768 type retain
Layer 11 : 321/768 type remove
Layer 12 : 259/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:30:20,130 [trainer.py] => Time:139.53444600105286
9400 9400
9400 9400
2025-12-10 15:30:45,813 [trainer.py] => Time:25.682775735855103
2025-12-10 15:30:45,814 [inflora.py] => Exemplar size: 0
2025-12-10 15:30:45,814 [trainer.py] => CNN: {'total': np.float64(64.4), '00-01': np.float64(81.0), '02-03': np.float64(65.5), '04-05': np.float64(68.5), '06-07': np.float64(48.5), '08-09': np.float64(88.0), '10-11': np.float64(48.0), '12-13': np.float64(47.5), '14-15': np.float64(82.0), '16-17': np.float64(61.0), '18-19': np.float64(77.0), '20-21': np.float64(83.5), '22-23': np.float64(54.0), '24-25': np.float64(55.0), '26-27': np.float64(64.0), '28-29': np.float64(73.0), '30-31': np.float64(62.0), '32-33': np.float64(56.5), '34-35': np.float64(46.5), '36-37': np.float64(42.0), '38-39': np.float64(79.5), '40-41': np.float64(86.5), '42-43': np.float64(22.0), '44-45': np.float64(50.0), '46-47': np.float64(45.0), '48-49': np.float64(42.0), '50-51': np.float64(72.5), '52-53': np.float64(84.0), '54-55': np.float64(72.5), '56-57': np.float64(75.5), '58-59': np.float64(65.5), '60-61': np.float64(76.5), '62-63': np.float64(61.0), '64-65': np.float64(65.0), '66-67': np.float64(46.0), '68-69': np.float64(83.0), '70-71': np.float64(90.0), '72-73': np.float64(30.0), '74-75': np.float64(73.0), '76-77': np.float64(53.5), '78-79': np.float64(65.0), '80-81': np.float64(56.0), '82-83': np.float64(77.5), '84-85': np.float64(67.5), '86-87': np.float64(77.5), '88-89': np.float64(71.0), '90-91': np.float64(82.0), '92-93': np.float64(54.0), 'old': np.float64(64.63), 'new': np.float64(54.0)}
2025-12-10 15:30:45,814 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21), np.float64(64.51), np.float64(64.4)]
2025-12-10 15:30:45,814 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52), np.float64(99.49), np.float64(99.47)]
2025-12-10 15:30:45,814 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111, 0.6451086956521739, 0.6440425531914894]
2025-12-10 15:30:54,103 [trainer.py] => All params: 125940251
2025-12-10 15:30:54,109 [trainer.py] => Trainable params: 185858
2025-12-10 15:30:54,109 [inflora.py] => Learning on 94-96
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.47.weight', 'image_encoder.blocks.4.attn.lora_B_v.47.weight', 'image_encoder.blocks.10.attn.lora_B_v.47.weight', 'image_encoder.blocks.10.attn.lora_B_k.47.weight', 'image_encoder.blocks.5.attn.lora_B_v.47.weight', 'image_encoder.blocks.6.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_k.47.weight', 'image_encoder.blocks.4.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_v.47.weight', 'image_encoder.blocks.2.attn.lora_B_k.47.weight', 'image_encoder.blocks.3.attn.lora_B_v.47.weight', 'image_encoder.blocks.7.attn.lora_B_k.47.weight', 'image_encoder.blocks.1.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_k.47.weight', 'image_encoder.blocks.9.attn.lora_B_v.47.weight', 'classifier_pool.47.weight', 'classifier_pool.47.bias', 'image_encoder.blocks.5.attn.lora_B_k.47.weight', 'image_encoder.blocks.0.attn.lora_B_v.47.weight', 'image_encoder.blocks.1.attn.lora_B_k.47.weight', 'image_encoder.blocks.8.attn.lora_B_k.47.weight', 'image_encoder.blocks.7.attn.lora_B_v.47.weight', 'image_encoder.blocks.3.attn.lora_B_k.47.weight', 'image_encoder.blocks.2.attn.lora_B_v.47.weight', 'image_encoder.blocks.11.attn.lora_B_v.47.weight'}
2025-12-10 15:33:02,562 [inflora.py] => Task 47, Epoch 20/20 => Loss 0.003, Train_accy 99.90
Threshold:  0.997
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 12/768 type remove
Layer 2 : 47/768 type remove
Layer 3 : 86/768 type remove
Layer 4 : 132/768 type remove
Layer 5 : 187/768 type remove
Layer 6 : 271/768 type remove
Layer 7 : 330/768 type remove
Layer 8 : 335/768 type retain
Layer 9 : 302/768 type retain
Layer 10 : 293/768 type retain
Layer 11 : 364/768 type remove
Layer 12 : 310/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:33:12,799 [trainer.py] => Time:138.69019889831543
9600 9600
9600 9600
2025-12-10 15:33:38,942 [trainer.py] => Time:26.14222478866577
2025-12-10 15:33:38,942 [inflora.py] => Exemplar size: 0
2025-12-10 15:33:38,942 [trainer.py] => CNN: {'total': np.float64(61.4), '00-01': np.float64(83.0), '02-03': np.float64(62.5), '04-05': np.float64(71.5), '06-07': np.float64(39.5), '08-09': np.float64(89.5), '10-11': np.float64(44.5), '12-13': np.float64(46.0), '14-15': np.float64(80.0), '16-17': np.float64(63.5), '18-19': np.float64(74.5), '20-21': np.float64(76.0), '22-23': np.float64(55.5), '24-25': np.float64(53.0), '26-27': np.float64(67.5), '28-29': np.float64(70.5), '30-31': np.float64(54.5), '32-33': np.float64(53.5), '34-35': np.float64(47.0), '36-37': np.float64(38.0), '38-39': np.float64(74.0), '40-41': np.float64(86.5), '42-43': np.float64(17.5), '44-45': np.float64(43.5), '46-47': np.float64(44.0), '48-49': np.float64(36.0), '50-51': np.float64(66.0), '52-53': np.float64(81.5), '54-55': np.float64(68.5), '56-57': np.float64(69.0), '58-59': np.float64(61.5), '60-61': np.float64(77.5), '62-63': np.float64(64.5), '64-65': np.float64(60.5), '66-67': np.float64(39.0), '68-69': np.float64(78.5), '70-71': np.float64(89.5), '72-73': np.float64(28.0), '74-75': np.float64(67.0), '76-77': np.float64(49.5), '78-79': np.float64(57.5), '80-81': np.float64(50.5), '82-83': np.float64(76.5), '84-85': np.float64(66.0), '86-87': np.float64(77.5), '88-89': np.float64(70.5), '90-91': np.float64(82.0), '92-93': np.float64(50.5), '94-95': np.float64(44.0), 'old': np.float64(61.77), 'new': np.float64(44.0)}
2025-12-10 15:33:38,942 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21), np.float64(64.51), np.float64(64.4), np.float64(61.4)]
2025-12-10 15:33:38,942 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52), np.float64(99.49), np.float64(99.47), np.float64(99.53)]
2025-12-10 15:33:38,942 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111, 0.6451086956521739, 0.6440425531914894, 0.6139583333333334]
2025-12-10 15:33:41,246 [trainer.py] => All params: 125940251
2025-12-10 15:33:41,253 [trainer.py] => Trainable params: 185858
2025-12-10 15:33:41,253 [inflora.py] => Learning on 96-98
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.48.weight', 'image_encoder.blocks.3.attn.lora_B_v.48.weight', 'image_encoder.blocks.6.attn.lora_B_k.48.weight', 'image_encoder.blocks.10.attn.lora_B_k.48.weight', 'image_encoder.blocks.0.attn.lora_B_k.48.weight', 'image_encoder.blocks.11.attn.lora_B_v.48.weight', 'image_encoder.blocks.0.attn.lora_B_v.48.weight', 'image_encoder.blocks.11.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_v.48.weight', 'image_encoder.blocks.7.attn.lora_B_v.48.weight', 'image_encoder.blocks.1.attn.lora_B_k.48.weight', 'classifier_pool.48.weight', 'image_encoder.blocks.1.attn.lora_B_v.48.weight', 'image_encoder.blocks.3.attn.lora_B_k.48.weight', 'image_encoder.blocks.6.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_v.48.weight', 'image_encoder.blocks.10.attn.lora_B_v.48.weight', 'image_encoder.blocks.8.attn.lora_B_k.48.weight', 'image_encoder.blocks.9.attn.lora_B_v.48.weight', 'image_encoder.blocks.2.attn.lora_B_k.48.weight', 'image_encoder.blocks.4.attn.lora_B_k.48.weight', 'classifier_pool.48.bias', 'image_encoder.blocks.8.attn.lora_B_v.48.weight', 'image_encoder.blocks.5.attn.lora_B_k.48.weight', 'image_encoder.blocks.9.attn.lora_B_k.48.weight'}
2025-12-10 15:35:50,418 [inflora.py] => Task 48, Epoch 20/20 => Loss 0.020, Train_accy 99.30
Threshold:  0.998
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 51/768 type remove
Layer 3 : 95/768 type remove
Layer 4 : 150/768 type remove
Layer 5 : 211/768 type remove
Layer 6 : 297/768 type remove
Layer 7 : 364/768 type remove
Layer 8 : 294/768 type retain
Layer 9 : 263/768 type retain
Layer 10 : 252/768 type retain
Layer 11 : 356/768 type retain
Layer 12 : 370/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:36:00,798 [trainer.py] => Time:139.54536843299866
9800 9800
9800 9800
2025-12-10 15:36:27,539 [trainer.py] => Time:26.740604877471924
2025-12-10 15:36:27,539 [inflora.py] => Exemplar size: 0
2025-12-10 15:36:27,539 [trainer.py] => CNN: {'total': np.float64(60.19), '00-01': np.float64(83.5), '02-03': np.float64(60.5), '04-05': np.float64(69.0), '06-07': np.float64(40.0), '08-09': np.float64(88.5), '10-11': np.float64(47.0), '12-13': np.float64(45.5), '14-15': np.float64(78.5), '16-17': np.float64(64.0), '18-19': np.float64(75.5), '20-21': np.float64(76.0), '22-23': np.float64(53.5), '24-25': np.float64(54.5), '26-27': np.float64(66.5), '28-29': np.float64(70.5), '30-31': np.float64(50.5), '32-33': np.float64(52.0), '34-35': np.float64(47.0), '36-37': np.float64(40.5), '38-39': np.float64(75.5), '40-41': np.float64(85.5), '42-43': np.float64(16.5), '44-45': np.float64(45.0), '46-47': np.float64(44.5), '48-49': np.float64(40.0), '50-51': np.float64(66.5), '52-53': np.float64(80.5), '54-55': np.float64(65.0), '56-57': np.float64(71.5), '58-59': np.float64(59.5), '60-61': np.float64(77.5), '62-63': np.float64(60.5), '64-65': np.float64(59.5), '66-67': np.float64(38.0), '68-69': np.float64(80.5), '70-71': np.float64(88.0), '72-73': np.float64(24.5), '74-75': np.float64(66.0), '76-77': np.float64(50.0), '78-79': np.float64(59.0), '80-81': np.float64(55.0), '82-83': np.float64(76.5), '84-85': np.float64(66.5), '86-87': np.float64(77.0), '88-89': np.float64(68.0), '90-91': np.float64(81.0), '92-93': np.float64(49.5), '94-95': np.float64(43.5), '96-97': np.float64(16.0), 'old': np.float64(61.11), 'new': np.float64(16.0)}
2025-12-10 15:36:27,540 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21), np.float64(64.51), np.float64(64.4), np.float64(61.4), np.float64(60.19)]
2025-12-10 15:36:27,540 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52), np.float64(99.49), np.float64(99.47), np.float64(99.53), np.float64(99.46)]
2025-12-10 15:36:27,540 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111, 0.6451086956521739, 0.6440425531914894, 0.6139583333333334, 0.601938775510204]
2025-12-10 15:36:35,974 [trainer.py] => All params: 125940251
2025-12-10 15:36:35,981 [trainer.py] => Trainable params: 185858
2025-12-10 15:36:35,981 [inflora.py] => Learning on 98-100
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.49.weight', 'image_encoder.blocks.6.attn.lora_B_v.49.weight', 'image_encoder.blocks.3.attn.lora_B_k.49.weight', 'image_encoder.blocks.4.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_k.49.weight', 'classifier_pool.49.weight', 'image_encoder.blocks.1.attn.lora_B_v.49.weight', 'image_encoder.blocks.7.attn.lora_B_k.49.weight', 'image_encoder.blocks.8.attn.lora_B_v.49.weight', 'image_encoder.blocks.3.attn.lora_B_v.49.weight', 'image_encoder.blocks.0.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_k.49.weight', 'image_encoder.blocks.5.attn.lora_B_k.49.weight', 'image_encoder.blocks.1.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_k.49.weight', 'image_encoder.blocks.10.attn.lora_B_k.49.weight', 'image_encoder.blocks.5.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_v.49.weight', 'image_encoder.blocks.11.attn.lora_B_v.49.weight', 'image_encoder.blocks.2.attn.lora_B_k.49.weight', 'image_encoder.blocks.9.attn.lora_B_v.49.weight', 'image_encoder.blocks.4.attn.lora_B_v.49.weight', 'image_encoder.blocks.6.attn.lora_B_k.49.weight', 'classifier_pool.49.bias'}
2025-12-10 15:38:44,727 [inflora.py] => Task 49, Epoch 20/20 => Loss 0.037, Train_accy 98.50
Threshold:  0.999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 57/768 type remove
Layer 3 : 107/768 type remove
Layer 4 : 176/768 type remove
Layer 5 : 251/768 type remove
Layer 6 : 352/768 type remove
Layer 7 : 333/768 type retain
Layer 8 : 218/768 type retain
Layer 9 : 178/768 type retain
Layer 10 : 159/768 type retain
Layer 11 : 238/768 type retain
Layer 12 : 256/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-10 15:38:55,470 [trainer.py] => Time:139.48881888389587
10000 10000
10000 10000
2025-12-10 15:39:22,768 [trainer.py] => Time:27.298134803771973
2025-12-10 15:39:22,768 [inflora.py] => Exemplar size: 0
2025-12-10 15:39:22,768 [trainer.py] => CNN: {'total': np.float64(59.55), '00-01': np.float64(80.0), '02-03': np.float64(60.5), '04-05': np.float64(67.0), '06-07': np.float64(38.0), '08-09': np.float64(86.0), '10-11': np.float64(40.5), '12-13': np.float64(47.5), '14-15': np.float64(76.5), '16-17': np.float64(61.0), '18-19': np.float64(70.0), '20-21': np.float64(71.5), '22-23': np.float64(52.0), '24-25': np.float64(53.0), '26-27': np.float64(63.0), '28-29': np.float64(69.0), '30-31': np.float64(50.0), '32-33': np.float64(51.0), '34-35': np.float64(44.5), '36-37': np.float64(39.5), '38-39': np.float64(79.0), '40-41': np.float64(86.5), '42-43': np.float64(18.0), '44-45': np.float64(44.5), '46-47': np.float64(36.5), '48-49': np.float64(36.0), '50-51': np.float64(65.0), '52-53': np.float64(81.0), '54-55': np.float64(66.0), '56-57': np.float64(74.5), '58-59': np.float64(62.5), '60-61': np.float64(79.0), '62-63': np.float64(62.0), '64-65': np.float64(61.0), '66-67': np.float64(39.0), '68-69': np.float64(81.0), '70-71': np.float64(88.5), '72-73': np.float64(26.5), '74-75': np.float64(64.5), '76-77': np.float64(51.0), '78-79': np.float64(59.5), '80-81': np.float64(52.5), '82-83': np.float64(79.0), '84-85': np.float64(61.5), '86-87': np.float64(77.0), '88-89': np.float64(68.5), '90-91': np.float64(73.0), '92-93': np.float64(51.0), '94-95': np.float64(44.5), '96-97': np.float64(15.5), '98-99': np.float64(73.0), 'old': np.float64(59.28), 'new': np.float64(73.0)}
2025-12-10 15:39:22,768 [trainer.py] => CNN top1 curve: [np.float64(100.0), np.float64(96.75), np.float64(91.5), np.float64(85.5), np.float64(91.0), np.float64(86.92), np.float64(88.71), np.float64(88.62), np.float64(87.83), np.float64(87.15), np.float64(85.41), np.float64(84.58), np.float64(83.08), np.float64(82.25), np.float64(82.33), np.float64(81.34), np.float64(79.68), np.float64(76.92), np.float64(75.68), np.float64(75.4), np.float64(75.9), np.float64(75.41), np.float64(73.41), np.float64(72.56), np.float64(72.58), np.float64(72.67), np.float64(72.09), np.float64(71.59), np.float64(71.84), np.float64(70.3), np.float64(68.77), np.float64(67.39), np.float64(67.68), np.float64(68.07), np.float64(68.53), np.float64(66.18), np.float64(67.14), np.float64(66.09), np.float64(67.49), np.float64(67.36), np.float64(66.33), np.float64(66.43), np.float64(66.81), np.float64(67.74), np.float64(66.21), np.float64(64.51), np.float64(64.4), np.float64(61.4), np.float64(60.19), np.float64(59.55)]
2025-12-10 15:39:22,769 [trainer.py] => CNN top1 with task curve: [np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(99.5), np.float64(99.6), np.float64(99.58), np.float64(99.57), np.float64(99.62), np.float64(99.78), np.float64(99.75), np.float64(99.73), np.float64(99.71), np.float64(99.77), np.float64(99.71), np.float64(99.73), np.float64(99.69), np.float64(99.56), np.float64(99.64), np.float64(99.66), np.float64(99.7), np.float64(99.69), np.float64(99.59), np.float64(99.57), np.float64(99.71), np.float64(99.6), np.float64(99.54), np.float64(99.56), np.float64(99.54), np.float64(99.59), np.float64(99.52), np.float64(99.5), np.float64(99.52), np.float64(99.52), np.float64(99.49), np.float64(99.53), np.float64(99.51), np.float64(99.57), np.float64(99.59), np.float64(99.5), np.float64(99.54), np.float64(99.54), np.float64(99.57), np.float64(99.5), np.float64(99.5), np.float64(99.52), np.float64(99.49), np.float64(99.47), np.float64(99.53), np.float64(99.46), np.float64(99.52)]
2025-12-10 15:39:22,769 [trainer.py] => CNN top1 task curve: [1.0, 0.9675, 0.915, 0.855, 0.91, 0.8691666666666666, 0.8878571428571429, 0.88625, 0.8783333333333333, 0.8715, 0.8540909090909091, 0.8458333333333333, 0.8307692307692308, 0.8225, 0.8233333333333334, 0.8134375, 0.7967647058823529, 0.7691666666666667, 0.7568421052631579, 0.754, 0.7590476190476191, 0.7540909090909091, 0.7341304347826086, 0.725625, 0.7258, 0.7267307692307692, 0.720925925925926, 0.7158928571428571, 0.7184482758620689, 0.703, 0.687741935483871, 0.67390625, 0.6768181818181818, 0.6807352941176471, 0.6852857142857143, 0.6618055555555555, 0.6713513513513514, 0.660921052631579, 0.6748717948717948, 0.673625, 0.6632926829268293, 0.6642857142857143, 0.6681395348837209, 0.6773863636363636, 0.6621111111111111, 0.6451086956521739, 0.6440425531914894, 0.6139583333333334, 0.601938775510204, 0.5955]
