logs/cifar100/10_10_sip/InfLoRA/adam/10/0.95_1.0-0.0005/42
2025-12-11 14:32:21,403 [trainer.py] => config: configs/cifar100_inflora_seed42.json
2025-12-11 14:32:21,403 [trainer.py] => device: [device(type='cuda', index=0)]
2025-12-11 14:32:21,403 [trainer.py] => prefix: reproduce
2025-12-11 14:32:21,404 [trainer.py] => dataset: cifar100
2025-12-11 14:32:21,404 [trainer.py] => data_path: data/
2025-12-11 14:32:21,404 [trainer.py] => memory_size: 0
2025-12-11 14:32:21,404 [trainer.py] => memory_per_class: 0
2025-12-11 14:32:21,404 [trainer.py] => fixed_memory: True
2025-12-11 14:32:21,404 [trainer.py] => shuffle: False
2025-12-11 14:32:21,404 [trainer.py] => init_cls: 10
2025-12-11 14:32:21,404 [trainer.py] => increment: 10
2025-12-11 14:32:21,404 [trainer.py] => model_name: InfLoRA
2025-12-11 14:32:21,404 [trainer.py] => net_type: sip
2025-12-11 14:32:21,404 [trainer.py] => embd_dim: 768
2025-12-11 14:32:21,404 [trainer.py] => num_heads: 12
2025-12-11 14:32:21,404 [trainer.py] => total_sessions: 10
2025-12-11 14:32:21,404 [trainer.py] => seed: 42
2025-12-11 14:32:21,404 [trainer.py] => EPSILON: 1e-08
2025-12-11 14:32:21,404 [trainer.py] => init_epoch: 20
2025-12-11 14:32:21,404 [trainer.py] => optim: adam
2025-12-11 14:32:21,404 [trainer.py] => init_lr: 0.0005
2025-12-11 14:32:21,404 [trainer.py] => init_lr_decay: 0.1
2025-12-11 14:32:21,404 [trainer.py] => init_weight_decay: 0.0
2025-12-11 14:32:21,404 [trainer.py] => epochs: 20
2025-12-11 14:32:21,404 [trainer.py] => lrate: 0.0005
2025-12-11 14:32:21,405 [trainer.py] => lrate_decay: 0.1
2025-12-11 14:32:21,405 [trainer.py] => batch_size: 128
2025-12-11 14:32:21,405 [trainer.py] => weight_decay: 0.0
2025-12-11 14:32:21,405 [trainer.py] => rank: 10
2025-12-11 14:32:21,405 [trainer.py] => lamb: 0.95
2025-12-11 14:32:21,405 [trainer.py] => lame: 1.0
2025-12-11 14:32:21,405 [trainer.py] => num_workers: 8
2025-12-11 14:32:21,405 [trainer.py] => use_wncm: True
2025-12-11 14:32:21,405 [trainer.py] => wncm_lambda: 0.07
2025-12-11 14:32:23,250 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
Loading ViT weights from local checkpoint: /leonardo/home/userexternal/lli00001/vit_b16_in21k.pth
Loaded 152 keys, missing 482, unexpected 0
2025-12-11 14:32:25,053 [whitened_ncm_head.py] => WhitenedNCM: Using CPU
2025-12-11 14:32:25,055 [trainer.py] => All params: 111194651
2025-12-11 14:32:25,057 [trainer.py] => Trainable params: 111194651
2025-12-11 14:32:25,057 [inflora.py] => Learning on 0-10
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight'}
2025-12-11 14:41:55,065 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.089, Train_accy 96.92
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 13/768 type remove
Layer 6 : 14/768 type remove
Layer 7 : 13/768 type remove
Layer 8 : 19/768 type remove
Layer 9 : 20/768 type remove
Layer 10 : 17/768 type remove
Layer 11 : 6/768 type remove
Layer 12 : 10/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 14:42:24,566 [trainer.py] => Time:599.5089151859283
1000 1000
1000 1000
2025-12-11 14:42:27,744 [trainer.py] => Time:3.177633285522461
2025-12-11 14:42:27,744 [inflora.py] => Exemplar size: 0
2025-12-11 14:42:27,744 [trainer.py] => CNN: {'total': np.float64(99.4), '00-09': np.float64(99.4), 'old': 0, 'new': np.float64(99.4)}
2025-12-11 14:42:27,744 [trainer.py] => CNN top1 curve: [np.float64(99.4)]
2025-12-11 14:42:27,744 [trainer.py] => CNN top1 with task curve: [np.float64(99.4)]
2025-12-11 14:42:27,744 [trainer.py] => CNN top1 task curve: [1.0]
2025-12-11 14:42:43,993 [trainer.py] => W-NCM: {'00-09': 99.4}
2025-12-11 14:42:43,994 [trainer.py] => Ave Acc (W-NCM): 99.40%
2025-12-11 14:42:43,994 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 99.40% (best 99.40%)
2025-12-11 14:42:43,994 [trainer.py] => Average forgetting (W-NCM): 0.00% | Max forgetting (W-NCM): 0.00%
2025-12-11 14:42:44,468 [trainer.py] => All params: 111194651
2025-12-11 14:42:44,470 [trainer.py] => Trainable params: 192010
2025-12-11 14:42:44,470 [inflora.py] => Learning on 10-20
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'classifier_pool.1.bias', 'classifier_pool.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight'}
2025-12-11 14:52:12,806 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.143, Train_accy 95.60
Threshold:  0.955
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 18/768 type remove
Layer 6 : 22/768 type remove
Layer 7 : 22/768 type remove
Layer 8 : 29/768 type remove
Layer 9 : 41/768 type remove
Layer 10 : 42/768 type remove
Layer 11 : 15/768 type remove
Layer 12 : 24/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 14:52:44,010 [trainer.py] => Time:599.5400230884552
2000 2000
2000 2000
2025-12-11 14:52:49,645 [trainer.py] => Time:5.634610891342163
2025-12-11 14:52:49,645 [inflora.py] => Exemplar size: 0
2025-12-11 14:52:49,646 [trainer.py] => CNN: {'total': np.float64(96.35), '00-09': np.float64(99.0), '10-19': np.float64(93.7), 'old': np.float64(99.0), 'new': np.float64(93.7)}
2025-12-11 14:52:49,646 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35)]
2025-12-11 14:52:49,646 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2)]
2025-12-11 14:52:49,646 [trainer.py] => CNN top1 task curve: [1.0, 0.968]
2025-12-11 14:53:08,797 [trainer.py] => W-NCM: {'00-09': 95.39999999999999, '10-19': 98.3}
2025-12-11 14:53:08,798 [trainer.py] => Ave Acc (W-NCM): 96.85%
2025-12-11 14:53:08,798 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 95.40% (best 99.40%); T2: W-NCM 98.30% (best 98.30%)
2025-12-11 14:53:08,798 [trainer.py] => Average forgetting (W-NCM): 4.00% | Max forgetting (W-NCM): 4.00%
2025-12-11 14:53:09,275 [trainer.py] => All params: 111194651
2025-12-11 14:53:09,276 [trainer.py] => Trainable params: 192010
2025-12-11 14:53:09,277 [inflora.py] => Learning on 20-30
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight'}
2025-12-11 15:02:37,806 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.133, Train_accy 95.80
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 15/768 type remove
Layer 5 : 22/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 28/768 type remove
Layer 8 : 39/768 type remove
Layer 9 : 56/768 type remove
Layer 10 : 56/768 type remove
Layer 11 : 21/768 type remove
Layer 12 : 34/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:03:08,988 [trainer.py] => Time:599.7113735675812
3000 3000
3000 3000
2025-12-11 15:03:17,266 [trainer.py] => Time:8.277477025985718
2025-12-11 15:03:17,266 [inflora.py] => Exemplar size: 0
2025-12-11 15:03:17,266 [trainer.py] => CNN: {'total': np.float64(94.43), '00-09': np.float64(97.6), '10-19': np.float64(93.0), '20-29': np.float64(92.7), 'old': np.float64(95.3), 'new': np.float64(92.7)}
2025-12-11 15:03:17,266 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43)]
2025-12-11 15:03:17,266 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77)]
2025-12-11 15:03:17,266 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952]
2025-12-11 15:03:38,631 [trainer.py] => W-NCM: {'00-09': 93.4, '10-19': 96.0, '20-29': 97.0}
2025-12-11 15:03:38,631 [trainer.py] => Ave Acc (W-NCM): 95.47%
2025-12-11 15:03:38,631 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 93.40% (best 99.40%); T2: W-NCM 96.00% (best 98.30%); T3: W-NCM 97.00% (best 97.00%)
2025-12-11 15:03:38,631 [trainer.py] => Average forgetting (W-NCM): 4.15% | Max forgetting (W-NCM): 6.00%
2025-12-11 15:03:39,094 [trainer.py] => All params: 111194651
2025-12-11 15:03:39,096 [trainer.py] => Trainable params: 192010
2025-12-11 15:03:39,096 [inflora.py] => Learning on 30-40
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight'}
2025-12-11 15:13:08,264 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.130, Train_accy 95.96
Threshold:  0.965
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 26/768 type remove
Layer 6 : 32/768 type remove
Layer 7 : 36/768 type remove
Layer 8 : 51/768 type remove
Layer 9 : 80/768 type remove
Layer 10 : 84/768 type remove
Layer 11 : 32/768 type remove
Layer 12 : 44/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:13:39,393 [trainer.py] => Time:600.2964973449707
4000 4000
4000 4000
2025-12-11 15:13:50,141 [trainer.py] => Time:10.747850179672241
2025-12-11 15:13:50,141 [inflora.py] => Exemplar size: 0
2025-12-11 15:13:50,141 [trainer.py] => CNN: {'total': np.float64(92.88), '00-09': np.float64(97.3), '10-19': np.float64(88.9), '20-29': np.float64(93.0), '30-39': np.float64(92.3), 'old': np.float64(93.07), 'new': np.float64(92.3)}
2025-12-11 15:13:50,141 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88)]
2025-12-11 15:13:50,141 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85)]
2025-12-11 15:13:50,141 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934]
2025-12-11 15:14:14,262 [trainer.py] => W-NCM: {'00-09': 92.30000000000001, '10-19': 91.60000000000001, '20-29': 96.0, '30-39': 97.8}
2025-12-11 15:14:14,262 [trainer.py] => Ave Acc (W-NCM): 94.43%
2025-12-11 15:14:14,262 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 92.30% (best 99.40%); T2: W-NCM 91.60% (best 98.30%); T3: W-NCM 96.00% (best 97.00%); T4: W-NCM 97.80% (best 97.80%)
2025-12-11 15:14:14,262 [trainer.py] => Average forgetting (W-NCM): 4.93% | Max forgetting (W-NCM): 7.10%
2025-12-11 15:14:14,728 [trainer.py] => All params: 111194651
2025-12-11 15:14:14,730 [trainer.py] => Trainable params: 192010
2025-12-11 15:14:14,730 [inflora.py] => Learning on 40-50
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight'}
2025-12-11 15:23:44,066 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.118, Train_accy 95.98
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 30/768 type remove
Layer 6 : 38/768 type remove
Layer 7 : 42/768 type remove
Layer 8 : 60/768 type remove
Layer 9 : 95/768 type remove
Layer 10 : 104/768 type remove
Layer 11 : 43/768 type remove
Layer 12 : 56/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:24:15,168 [trainer.py] => Time:600.4374876022339
5000 5000
5000 5000
2025-12-11 15:24:28,517 [trainer.py] => Time:13.348706483840942
2025-12-11 15:24:28,517 [inflora.py] => Exemplar size: 0
2025-12-11 15:24:28,517 [trainer.py] => CNN: {'total': np.float64(92.06), '00-09': np.float64(95.7), '10-19': np.float64(87.6), '20-29': np.float64(91.6), '30-39': np.float64(92.4), '40-49': np.float64(93.0), 'old': np.float64(91.82), 'new': np.float64(93.0)}
2025-12-11 15:24:28,517 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06)]
2025-12-11 15:24:28,517 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88)]
2025-12-11 15:24:28,517 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926]
2025-12-11 15:24:54,856 [trainer.py] => W-NCM: {'00-09': 92.4, '10-19': 88.9, '20-29': 93.60000000000001, '30-39': 95.1, '40-49': 96.8}
2025-12-11 15:24:54,856 [trainer.py] => Ave Acc (W-NCM): 93.36%
2025-12-11 15:24:54,856 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 92.40% (best 99.40%); T2: W-NCM 88.90% (best 98.30%); T3: W-NCM 93.60% (best 97.00%); T4: W-NCM 95.10% (best 97.80%); T5: W-NCM 96.80% (best 96.80%)
2025-12-11 15:24:54,856 [trainer.py] => Average forgetting (W-NCM): 5.62% | Max forgetting (W-NCM): 9.40%
2025-12-11 15:24:55,323 [trainer.py] => All params: 111194651
2025-12-11 15:24:55,325 [trainer.py] => Trainable params: 192010
2025-12-11 15:24:55,325 [inflora.py] => Learning on 50-60
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight'}
2025-12-11 15:34:24,870 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.121, Train_accy 95.48
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 34/768 type remove
Layer 6 : 46/768 type remove
Layer 7 : 51/768 type remove
Layer 8 : 81/768 type remove
Layer 9 : 122/768 type remove
Layer 10 : 125/768 type remove
Layer 11 : 55/768 type remove
Layer 12 : 68/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:34:56,257 [trainer.py] => Time:600.9320437908173
6000 6000
6000 6000
2025-12-11 15:35:12,110 [trainer.py] => Time:15.852090835571289
2025-12-11 15:35:12,110 [inflora.py] => Exemplar size: 0
2025-12-11 15:35:12,110 [trainer.py] => CNN: {'total': np.float64(88.92), '00-09': np.float64(95.1), '10-19': np.float64(85.6), '20-29': np.float64(90.1), '30-39': np.float64(90.6), '40-49': np.float64(92.5), '50-59': np.float64(79.6), 'old': np.float64(90.78), 'new': np.float64(79.6)}
2025-12-11 15:35:12,110 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06), np.float64(88.92)]
2025-12-11 15:35:12,110 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88), np.float64(98.75)]
2025-12-11 15:35:12,110 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926, 0.894]
2025-12-11 15:35:41,386 [trainer.py] => W-NCM: {'00-09': 90.10000000000001, '10-19': 88.9, '20-29': 93.5, '30-39': 93.8, '40-49': 91.10000000000001, '50-59': 95.7}
2025-12-11 15:35:41,387 [trainer.py] => Ave Acc (W-NCM): 92.18%
2025-12-11 15:35:41,387 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 90.10% (best 99.40%); T2: W-NCM 88.90% (best 98.30%); T3: W-NCM 93.50% (best 97.00%); T4: W-NCM 93.80% (best 97.80%); T5: W-NCM 91.10% (best 96.80%); T6: W-NCM 95.70% (best 95.70%)
2025-12-11 15:35:41,387 [trainer.py] => Average forgetting (W-NCM): 6.38% | Max forgetting (W-NCM): 9.40%
2025-12-11 15:35:41,864 [trainer.py] => All params: 111194651
2025-12-11 15:35:41,866 [trainer.py] => Trainable params: 192010
2025-12-11 15:35:41,866 [inflora.py] => Learning on 60-70
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight'}
2025-12-11 15:45:12,713 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.129, Train_accy 95.46
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 40/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 65/768 type remove
Layer 8 : 100/768 type remove
Layer 9 : 145/768 type remove
Layer 10 : 148/768 type remove
Layer 11 : 71/768 type remove
Layer 12 : 85/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:45:44,158 [trainer.py] => Time:602.2921166419983
7000 7000
7000 7000
2025-12-11 15:46:02,595 [trainer.py] => Time:18.436927556991577
2025-12-11 15:46:02,596 [inflora.py] => Exemplar size: 0
2025-12-11 15:46:02,596 [trainer.py] => CNN: {'total': np.float64(88.04), '00-09': np.float64(94.7), '10-19': np.float64(83.8), '20-29': np.float64(87.5), '30-39': np.float64(91.7), '40-49': np.float64(91.1), '50-59': np.float64(80.0), '60-69': np.float64(87.5), 'old': np.float64(88.13), 'new': np.float64(87.5)}
2025-12-11 15:46:02,596 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06), np.float64(88.92), np.float64(88.04)]
2025-12-11 15:46:02,596 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88), np.float64(98.75), np.float64(98.66)]
2025-12-11 15:46:02,596 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926, 0.894, 0.8845714285714286]
2025-12-11 15:46:34,000 [trainer.py] => W-NCM: {'00-09': 91.5, '10-19': 85.8, '20-29': 91.5, '30-39': 91.0, '40-49': 91.2, '50-59': 94.1, '60-69': 95.3}
2025-12-11 15:46:34,000 [trainer.py] => Ave Acc (W-NCM): 91.49%
2025-12-11 15:46:34,000 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 91.50% (best 99.40%); T2: W-NCM 85.80% (best 98.30%); T3: W-NCM 91.50% (best 97.00%); T4: W-NCM 91.00% (best 97.80%); T5: W-NCM 91.20% (best 96.80%); T6: W-NCM 94.10% (best 95.70%); T7: W-NCM 95.30% (best 95.30%)
2025-12-11 15:46:34,000 [trainer.py] => Average forgetting (W-NCM): 6.65% | Max forgetting (W-NCM): 12.50%
2025-12-11 15:46:34,474 [trainer.py] => All params: 111194651
2025-12-11 15:46:34,476 [trainer.py] => Trainable params: 192010
2025-12-11 15:46:34,476 [inflora.py] => Learning on 70-80
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight'}
2025-12-11 15:56:05,413 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.136, Train_accy 95.76
Threshold:  0.985
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 25/768 type remove
Layer 4 : 34/768 type remove
Layer 5 : 50/768 type remove
Layer 6 : 72/768 type remove
Layer 7 : 85/768 type remove
Layer 8 : 137/768 type remove
Layer 9 : 186/768 type remove
Layer 10 : 188/768 type remove
Layer 11 : 89/768 type remove
Layer 12 : 101/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 15:56:36,694 [trainer.py] => Time:602.2180802822113
8000 8000
8000 8000
2025-12-11 15:56:57,591 [trainer.py] => Time:20.89635920524597
2025-12-11 15:56:57,591 [inflora.py] => Exemplar size: 0
2025-12-11 15:56:57,591 [trainer.py] => CNN: {'total': np.float64(87.76), '00-09': np.float64(94.6), '10-19': np.float64(83.7), '20-29': np.float64(86.6), '30-39': np.float64(89.8), '40-49': np.float64(91.7), '50-59': np.float64(80.8), '60-69': np.float64(86.1), '70-79': np.float64(88.8), 'old': np.float64(87.61), 'new': np.float64(88.8)}
2025-12-11 15:56:57,591 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06), np.float64(88.92), np.float64(88.04), np.float64(87.76)]
2025-12-11 15:56:57,591 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88), np.float64(98.75), np.float64(98.66), np.float64(98.71)]
2025-12-11 15:56:57,591 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926, 0.894, 0.8845714285714286, 0.8815]
2025-12-11 15:57:31,982 [trainer.py] => W-NCM: {'00-09': 90.60000000000001, '10-19': 86.8, '20-29': 89.7, '30-39': 89.4, '40-49': 90.4, '50-59': 91.2, '60-69': 92.30000000000001, '70-79': 97.39999999999999}
2025-12-11 15:57:31,982 [trainer.py] => Ave Acc (W-NCM): 90.97%
2025-12-11 15:57:31,982 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 90.60% (best 99.40%); T2: W-NCM 86.80% (best 98.30%); T3: W-NCM 89.70% (best 97.00%); T4: W-NCM 89.40% (best 97.80%); T5: W-NCM 90.40% (best 96.80%); T6: W-NCM 91.20% (best 95.70%); T7: W-NCM 92.30% (best 95.30%); T8: W-NCM 97.40% (best 97.40%)
2025-12-11 15:57:31,982 [trainer.py] => Average forgetting (W-NCM): 7.13% | Max forgetting (W-NCM): 11.50%
2025-12-11 15:57:32,457 [trainer.py] => All params: 111194651
2025-12-11 15:57:32,459 [trainer.py] => Trainable params: 192010
2025-12-11 15:57:32,459 [inflora.py] => Learning on 80-90
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight'}
2025-12-11 16:07:03,705 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.104, Train_accy 96.52
Threshold:  0.99
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 34/768 type remove
Layer 4 : 45/768 type remove
Layer 5 : 66/768 type remove
Layer 6 : 99/768 type remove
Layer 7 : 118/768 type remove
Layer 8 : 180/768 type remove
Layer 9 : 252/768 type remove
Layer 10 : 282/768 type remove
Layer 11 : 171/768 type remove
Layer 12 : 171/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 16:07:35,363 [trainer.py] => Time:602.9044682979584
9000 9000
9000 9000
2025-12-11 16:07:58,959 [trainer.py] => Time:23.59550166130066
2025-12-11 16:07:58,959 [inflora.py] => Exemplar size: 0
2025-12-11 16:07:58,959 [trainer.py] => CNN: {'total': np.float64(87.84), '00-09': np.float64(93.9), '10-19': np.float64(85.2), '20-29': np.float64(85.7), '30-39': np.float64(90.0), '40-49': np.float64(91.6), '50-59': np.float64(81.7), '60-69': np.float64(85.0), '70-79': np.float64(87.5), '80-89': np.float64(90.0), 'old': np.float64(87.58), 'new': np.float64(90.0)}
2025-12-11 16:07:58,959 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06), np.float64(88.92), np.float64(88.04), np.float64(87.76), np.float64(87.84)]
2025-12-11 16:07:58,960 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88), np.float64(98.75), np.float64(98.66), np.float64(98.71), np.float64(98.61)]
2025-12-11 16:07:58,960 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926, 0.894, 0.8845714285714286, 0.8815, 0.8824444444444445]
2025-12-11 16:08:35,608 [trainer.py] => W-NCM: {'00-09': 90.10000000000001, '10-19': 86.9, '20-29': 89.60000000000001, '30-39': 89.8, '40-49': 89.1, '50-59': 91.5, '60-69': 90.9, '70-79': 96.0, '80-89': 97.7}
2025-12-11 16:08:35,608 [trainer.py] => Ave Acc (W-NCM): 91.29%
2025-12-11 16:08:35,608 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 90.10% (best 99.40%); T2: W-NCM 86.90% (best 98.30%); T3: W-NCM 89.60% (best 97.00%); T4: W-NCM 89.80% (best 97.80%); T5: W-NCM 89.10% (best 96.80%); T6: W-NCM 91.50% (best 95.70%); T7: W-NCM 90.90% (best 95.30%); T8: W-NCM 96.00% (best 97.40%); T9: W-NCM 97.70% (best 97.70%)
2025-12-11 16:08:35,608 [trainer.py] => Average forgetting (W-NCM): 6.72% | Max forgetting (W-NCM): 11.40%
2025-12-11 16:08:36,103 [trainer.py] => All params: 111194651
2025-12-11 16:08:36,105 [trainer.py] => Trainable params: 192010
2025-12-11 16:08:36,105 [inflora.py] => Learning on 90-100
Parameters to be updated: {'classifier_pool.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight'}
2025-12-11 16:18:07,469 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.086, Train_accy 97.16
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 46/768 type remove
Layer 4 : 67/768 type remove
Layer 5 : 92/768 type remove
Layer 6 : 143/768 type remove
Layer 7 : 180/768 type remove
Layer 8 : 266/768 type remove
Layer 9 : 364/768 type remove
Layer 10 : 356/768 type retain
Layer 11 : 307/768 type remove
Layer 12 : 314/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2025-12-11 16:18:39,639 [trainer.py] => Time:603.534078836441
10000 10000
10000 10000
2025-12-11 16:19:05,755 [trainer.py] => Time:26.11586594581604
2025-12-11 16:19:05,755 [inflora.py] => Exemplar size: 0
2025-12-11 16:19:05,755 [trainer.py] => CNN: {'total': np.float64(86.19), '00-09': np.float64(94.2), '10-19': np.float64(82.6), '20-29': np.float64(84.8), '30-39': np.float64(87.4), '40-49': np.float64(90.6), '50-59': np.float64(78.2), '60-69': np.float64(84.2), '70-79': np.float64(85.1), '80-89': np.float64(89.9), '90-99': np.float64(84.9), 'old': np.float64(86.33), 'new': np.float64(84.9)}
2025-12-11 16:19:05,755 [trainer.py] => CNN top1 curve: [np.float64(99.4), np.float64(96.35), np.float64(94.43), np.float64(92.88), np.float64(92.06), np.float64(88.92), np.float64(88.04), np.float64(87.76), np.float64(87.84), np.float64(86.19)]
2025-12-11 16:19:05,755 [trainer.py] => CNN top1 with task curve: [np.float64(99.4), np.float64(99.2), np.float64(98.77), np.float64(98.85), np.float64(98.88), np.float64(98.75), np.float64(98.66), np.float64(98.71), np.float64(98.61), np.float64(98.69)]
2025-12-11 16:19:05,756 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.952, 0.934, 0.926, 0.894, 0.8845714285714286, 0.8815, 0.8824444444444445, 0.8652]
2025-12-11 16:19:45,090 [trainer.py] => W-NCM: {'00-09': 89.9, '10-19': 86.1, '20-29': 90.10000000000001, '30-39': 83.89999999999999, '40-49': 85.9, '50-59': 87.6, '60-69': 90.5, '70-79': 93.30000000000001, '80-89': 95.1, '90-99': 96.3}
2025-12-11 16:19:45,090 [trainer.py] => Ave Acc (W-NCM): 89.87%
2025-12-11 16:19:45,090 [trainer.py] => Per-task accuracies (W-NCM): T1: W-NCM 89.90% (best 99.40%); T2: W-NCM 86.10% (best 98.30%); T3: W-NCM 90.10% (best 97.00%); T4: W-NCM 83.90% (best 97.80%); T5: W-NCM 85.90% (best 96.80%); T6: W-NCM 87.60% (best 95.70%); T7: W-NCM 90.50% (best 95.30%); T8: W-NCM 93.30% (best 97.40%); T9: W-NCM 95.10% (best 97.70%); T10: W-NCM 96.30% (best 96.30%)
2025-12-11 16:19:45,090 [trainer.py] => Average forgetting (W-NCM): 8.11% | Max forgetting (W-NCM): 13.90%
2025-12-11 16:19:45,567 [trainer.py] => 
===== Summary =====
2025-12-11 16:19:45,568 [trainer.py] => Final average accuracy: 86.19%
2025-12-11 16:19:45,568 [trainer.py] => Average accuracy over tasks: 91.39%
2025-12-11 16:19:45,568 [trainer.py] => Final average forgetting: 4.72%
2025-12-11 16:19:45,568 [trainer.py] => Final max forgetting: 11.10%
2025-12-11 16:19:45,568 [trainer.py] => W-NCM final average accuracy: 89.87%
2025-12-11 16:19:45,568 [trainer.py] => W-NCM average accuracy over tasks: 93.53%
2025-12-11 16:19:45,568 [trainer.py] => W-NCM final average forgetting: 8.11%
2025-12-11 16:19:45,568 [trainer.py] => W-NCM final max forgetting: 13.90%
